% !TeX root = P1-Polynomials.tex
\documentclass[Book-Poly]{subfiles}
\begin{document}


\setcounter{chapter}{0}%Just finished 0.

%---------------- Part ----------------%
\part{The category of polynomial functors}\label{part.poly}

% TODO: intro

\Opensolutionfile{solutions}[solution-file1]

%------------ Chapter ------------%
\chapter{Representable functors from the~category of sets} \label{ch.poly.rep-sets}

In this chapter, we lay the categorical groundwork needed to define our category of interest, the category of polynomial functors.
We begin by examining a special kind of polynomial functor that you may already be familiar with---representable functors from the category $\smset$ of sets and functions.
We highlight the role these representable functors play in what is arguably the fundamental theorem of category theory, the Yoneda lemma.
We will also discuss sums and products of sets and of set-valued functors, which we will need to construct our polynomial functors.

%-------- Section --------%
\section{Representable functors and the Yoneda lemma} \label{sec.poly.rep-sets.yon}

Representable functors---special functors to the category of sets---provide the foundation for the category $\poly$.
While much of the following theory applies to representable functors from any category, we need only focus on representable functors $\smset\to\smset$.

\begin{definition}[Representable functor] \label{def.representable}
    For a set $S$, we denote by $\yon^S\colon\smset\to\smset$ the functor that sends each set $X$ to the set $X^S=\smset(S,X)$ and each function $h\colon X\to Y$ to the function $h^S\colon X^S\to Y^S$ that sends $g\colon S\to X$ to $g\then h\colon S\to Y$.\footnote{Throughout this text, given morphisms $f \colon A \to B$ and $g \colon B \to C$ in a category, we will denote their composite morphism $A \to C$ interchangeably as $f \then g$ or $g \circ f$, whichever is more convenient.}

    We call a functor (isomorphic to one) of this form a \emph{representable functor}, or a \emph{representable}.
    In particular, we call $\yon^S$ the functor \emph{represented by} $S$, and we call $S$ the \emph{representing set of $\yon^S$}.
    As $\yon^S$ denotes raising a variable to the power of $S$, we will also refer to representables as \emph{pure powers}.
\end{definition}

The symbol $\yon$ stands for Yoneda, for reasons we will explain in \cref{lemma.yoneda} and \cref{exc.finish_proof_yoneda} \cref{exc.finish_proof_yoneda.embed}.

Throughout this book, when referring to finite sets, we will adopt the following convention: $\0\coloneqq\{\}=\varnothing,$ $\1\coloneqq\{1\},$ $\2\coloneqq\{1,2\},$ $\3\coloneqq\{1,2,3\}$, and so on, with $\ord{n}\coloneqq\{1,\ldots,n\}$, an $n$-element set, for each natural number $n$.
For example, in standard font, $5$ represents the usual natural number, while in sans serif font, $\5$ represents the $5$-element set $\{1,2,3,4,5\}$.
When the same variable name appears in both italicized and sans serif fonts, the italicized variable denotes a natural number and the sans serif variable denotes the corresponding set; for example, if we state that $m\in\nn$, then we also understand $\ord{m}$ to mean the set $\{1,\ldots,m\}$.

\begin{example}
    The functor that sends each set $X$ to $X\times X$ and each function $h\colon X\to Y$ to $(h\times h)\colon (X\times X)\to(Y\times Y)$ is representable.
    After all, $X\times X \iso X^\2$, so this functor is the pure power $\yon^\2$.
\end{example}

\begin{exercise}\label{exc.representable_fun}
    For each of the following functors $\smset\to\smset$, say if it is representable or not; if it is, give the set that represents it.
    \begin{enumerate}
        \item The identity functor $X\mapsto X$, which sends each function to itself.
        \item The constant functor $X\mapsto\2$, which sends every function to the identity on $\2$.
        \item The constant functor $X\mapsto\1$, which sends every function to the identity on $\1$.
        \item The constant functor $X\mapsto\0$, which sends every function to the identity on $\0$.
        \item A functor $X\mapsto X^\nn$.
        If it could be representable, where should it send each function?
        \item A functor $X\mapsto \2^X$.
        If it could be representable, where should it send each function? \qedhere
    \end{enumerate}

    \begin{solution}
        \begin{enumerate}
            \item The identity functor $X\mapsto X$ is represented by the set $\1$: a function $\1 \to X$ can be identified with an element of $X$, so $\smset(\1,X)\iso X$.
            Alternatively, note that $X^\1 \iso X$.
            \item \label{sol.representable_fun.2} The constant functor $X\mapsto\2$ is not representable: it sends $\1$ to $\2$, but $\1^S \iso \1 \not\iso \2$ for any set $S$.
            \item The constant functor $X\mapsto\1$ is represented by $S=\0$: there is exactly one function $\0 \to X$, so $\smset(\0,X) \iso \1$.
            Alternatively, note that $X^0 \iso \1$.
            \item The constant functor $X\mapsto\0$ is not representable for the same reason as in \cref{sol.representable_fun.2}.
            \item The functor $\yon^\nn$ that sends $X\mapsto X^\nn$ is represented by $\nn$, by definition.
            It should send each function $h \colon X \to Y$ to the function $h^\nn \colon X^\nn \to Y^\nn$ that sends each $g \colon \nn \to X$ to $g \then h \colon \nn \to Y$.
            \item No $\smset \to \smset$ functor $X\mapsto \2^X$ is representable, for the same reason as in \cref{sol.representable_fun.2}.
            (There \emph{is}, however, a functor $\smset\op \to \smset$ sending $X \mapsto 2^X$ that is understood to be representable in a more general sense.)
        \end{enumerate}
    \end{solution}
\end{exercise}

Now that we have introduced representable functors, we study the maps between them.
As representables are functors, the maps between them are natural transformations.

\begin{proposition}\label{prop.representable_nt}
    For any function $f\colon R\to S$, there is an induced natural transformation $\yon^f\colon\yon^S\to \yon^R$; on any set $X$ its $X$-component $X^f\colon X^S\to X^R$ is given by sending $g\colon S\to X$ to $f\then g\colon R\to X$.
\end{proposition}

\begin{proof}
    See \cref{exc.representable_nt}.
\end{proof}

\begin{exercise} \label{exc.representable_nt}
    To prove \cref{prop.representable_nt}, show that for any function $f\colon R\to S$, the given construction $\yon^f\colon\yon^S\to\yon^R$ really is a natural transformation.
    That is, for any function $h\colon X\to Y$, show that the following naturality square commutes:
    \begin{equation} \label{diag.yon_embed_nat}
        \begin{tikzcd}%[bottom base]
            X^S\ar[r, "h^S"]\ar[d, "X^f"']&
            Y^S\ar[d, "Y^f"]\\
            X^R\ar[r, "h^R"']&
            Y^R\ar[ul, phantom, "?"]
        \end{tikzcd}
    \end{equation}
    \qedhere

    \begin{solution}
        To show that \eqref{diag.yon_embed_nat} commutes, we note that by the construction of the components of $\yon^f$ in the statement of \cref{prop.representable_nt}, both vertical maps in the diagram compose functions from $S$ with $f \colon R \to S$ on the left; and by \cref{def.representable}, both horizontal maps compose functions to $X$ with $h \colon X \to Y$ on the right.
        So by the associativity of composition, the diagram commutes: $(f\then g)\then h=f\then(g\then h)$ for all $g\colon S\to X$.
    \end{solution}
\end{exercise}

\begin{exercise} \label{exc.representable_nt_components}
    Let $X$ be an arbitrary set. For each of the following sets $R,S$ and functions $f\colon R\to S$, describe the $X$-component $X^f\colon X^S\to X^R$ of the natural transformation $\yon^f\colon\yon^S\to\yon^R$.
    \begin{enumerate}
        \item \label{exc.representable_nt_components.id} $R=\5$, $S=\5$, $f=\id_\5$. (You should describe the function $X^{\id_\5}\colon X^\5\to X^\5$.)
        \item $R=\2$, $S=\1$, $f$ is the unique function.
        \item $R=\1$, $S=\2$, $f(1)=1$.
        \item $R=\1$, $S=\2$, $f(1)=2$.
        \item $R=\0$, $S=\5$, $f$ is the unique function.
        \item $R=\nn$, $S=\nn$, $f(n)=n+1$.
        \qedhere
    \end{enumerate}

    \begin{solution}
        In each case, given $f \colon R \to S$, we can find the $X$-component $X^f \colon X^S \to X^R$ of the natural transformation $\yon^f\colon\yon^S\to\yon^R$ by applying \cref{prop.representable_nt}, which says that $X^f$ sends each $g \colon S \to X$ to $f \then g \colon R \to X$.
        \begin{enumerate}
            \item Here $X^{\id_5}\colon X^\5\to X^\5$ is the identity function.
            \item If $f\colon\2\to\1$ is the unique function, then $X^f\colon X^\1\to X^\2$ sends each $g \in X$ (i.e.\ function $g \colon \1 \to X$) to the function that maps both elements of $\2$ to $g$.
            We can think of $X^f$ as the diagonal $X \to X \times X$.
            \item If $f\colon\1\to\2$ sends $1\mapsto1$, then $X^f\colon X^\2\to X^\1$ sends each $g \colon \2 \to X$ to $g(1)$, viewed as a function $\1 \to X$.
            We can think of $X^f$ as the left projection $X \times X \to X$.
            \item If $f\colon\1\to\2$ sends $1\mapsto2$, then $X^f\colon X^\2\to X^\1$ sends each $g \colon \2 \to X$ to $g(2)$, viewed as a function $\1 \to X$.
            We can think of $X^f$ as the right projection $X \times X \to X$.
            \item Here $X^f\colon X^\5\to X^\0\iso\1$ is the unique function.
            \item If $f\colon\nn\to\nn$ sends $n\mapsto n+1$, then $X^f\colon X^\nn\to X^\nn$ sends each $g \colon \nn \to X$ to the function $h \colon \nn \to X$ defined by $h(n)\coloneqq g(n+1)$ for all $n \in \nn$.
            We can think of $X^f$ as removing the first term of an infinite sequence of elements $(g(0),g(1),g(2),\ldots)$ of $X$ to obtain a new sequence $(g(1),g(2),g(3),\ldots)$.
        \end{enumerate}
    \end{solution}
\end{exercise}

These representable functors and natural transformations live in the larger category $\smset^\smset$, whose objects are functors $\smset\to\smset$ and whose morphisms are the natural transformations between them.

\begin{exercise} \label{exc.representable_nt_functorial}
    Show that the construction $f\mapsto\yon^f$ from \cref{prop.representable_nt} defines a functor
    \begin{equation} \label{eqn.yoneda_embedding}
        \yon^-\colon\smset\op\to\smset^\smset
    \end{equation}
    by verifying functoriality, as follows.
    \begin{enumerate}
        \item Show that for any set $S$, the natural transformation $\yon^{\id_S}\colon\yon^S\to\yon^S$ is the identity.
        \item Show that for functions $f\colon R\to S$ and $g\colon S\to T$, we have $\yon^g\then\yon^f=\yon^{f\then g}$. \qedhere
    \end{enumerate}

    \begin{solution}
        \begin{enumerate}
            \item The fact that $\yon^{\id_S}\colon\yon^S\to\yon^S$ is the identity is just a generalization of \cref{exc.representable_nt_components} \cref{exc.representable_nt_components.id}.
            For any set $X$, the $X$-component $X^{\id_S} \colon X^S \to X^S$ of $\yon^{\id_S}$ sends each $h \colon S \to X$ to $\id_S \then h = h$, so $X^{\id_S}$ is the identity natural transformation on $X^S$.
            Hence $\yon^{\id_S}$ is the identity on $\yon^S$.
            \item Fix $f \colon R \to S$ and $g \colon S \to T$; we wish to show that $\yon^g \then \yon^f = \yon^{f \then g}$.
            It suffices to show componentwise that $X^g \then X^f = X^{f \then g}$ for every set $X$.
            Indeed, $X^g$ sends each $h \colon T \to X$ to $g \then h$; then $X^f$ sends $g \then h$ to $f \then g \then h = X^{f \then g}(h)$.
        \end{enumerate}
    \end{solution}
\end{exercise}

We now have all the ingredients we need to state and prove the Yoneda lemma on the category of sets.

\begin{lemma}[Yoneda lemma]\label{lemma.yoneda}
    Given a functor $F\colon\smset\to\smset$ and a set $S$, there is an isomorphism
    \begin{equation}\label{eqn.yoneda}
        F(S)\iso\smset^\smset(\yon^S,F)
    \end{equation}
    where the right hand side is the set of natural transformations $\yon^S\to F$.
    Moreover, \eqref{eqn.yoneda} is natural in both $S$ and $F$.
\end{lemma}
\begin{proof}[Proof]
    Given a natural transformation $m\colon\yon^S\to F$, consider its $S$-component $m_S\colon S^S\to F(S)$.
    Applying this function to $\id_S\in S^S$ yields an element $m_S(\id_S)\in F(S)$.

    Conversely, given an element $a\in F(S)$, there is a natural transformation we denote by $m^a\colon\yon^S\to F$ whose $X$-component is the function $X^S\to F(X)$ that sends $g\colon S\to X$ to $F(g)(a)$.
    In \cref{exc.finish_proof_yoneda} we ask you to show that this is indeed natural in $X$; that these two constructions, $m\mapsto m_S(\id_S)$ and $a\mapsto m^a$, are mutually inverse; and that the resulting isomorphism is natural.
\end{proof}

\begin{exercise}\label{exc.finish_proof_yoneda}
    In this exercise, we fill in the details of the preceding proof.
    \begin{enumerate}
        \item Show that for any $a\in F(S)$, the maps $X^S\to F(X)$ defined in the proof of \cref{lemma.yoneda} are natural in $X$.
        \item Show that the two mappings given in the proof of \cref{lemma.yoneda} are mutually inverse, thus defining the isomorphism \eqref{eqn.yoneda}.
        \item Show that \eqref{eqn.yoneda} as defined is natural in $F$.
        \item Show that \eqref{eqn.yoneda} as defined is natural in $S$.
        \item \label{exc.finish_proof_yoneda.embed} As a corollary of \cref{lemma.yoneda}, show that the functor $\yon^-\colon\smset\op\to\smset^\smset$ from \eqref{eqn.yoneda_embedding} is fully faithful---in particular, there is an isomorphism $S^T\iso \smset^\smset(\yon^S,\yon^T)$ for sets $S,T$.
        For this reason, we call $\yon^-$ the \emph{Yoneda embedding}.
        \qedhere
    \end{enumerate}

    \begin{solution}
        \begin{enumerate}
            \item To check that $X^S \to F(X)$ is natural in $X$, we verify that the naturality square
            \[
            \begin{tikzcd}[ampersand replacement=\&]
                X^S\ar[r, "h^S"]\ar[d, "F(-)(a)"']\&
                Y^S\ar[d, "F(-)(a)"]\\
                F(X)\ar[r, "F(h)"']\&
                F(Y)
            \end{tikzcd}
            \]
            commutes for all $h \colon X \to Y$.
            The top map $h^S$ sends any $g \colon S \to X$ to $g \then h$ (\cref{def.representable}), which is then sent to $F(g \then h)(a)$ by the right map.
            Meanwhile, the left map sends $g$ to $F(g)(a)$, which is then sent to $F(h)(F(g)(a))$ by the bottom map.
            So by the functoriality of $F$, the square commutes.

            \item We show that the maps $m\mapsto m_S(\id_S)$ and $a\mapsto m^a$ defined in the proof of \cref{lemma.yoneda} are mutually inverse.
            First, we show that for any natural transformation $m \colon \yon^S \to F$, we have $m^{m_S(\id_S)} = m$.
            Given a set $X$, the $X$-component of $m^{m_S(\id_S)}$ sends each $g \colon S \to X$ to $F(g)(m_S(\id_S))$; it suffices to show that this is also where the $X$-component of $m$ sends $g$.
            Indeed, by the naturality of $m$, the square
            \[
            \begin{tikzcd}[ampersand replacement=\&]
                S^S\ar[r, "g^S"]\ar[d, "m_S"']\&
                X^S\ar[d, "m_X"]\\
                F(S)\ar[r, "F(g)"']\&
                F(X)
            \end{tikzcd}
            \]
            commutes, so in particular, following $\id_S\in S^S$ around this diagram, we have
            \begin{equation} \label{eqn.finish_proof_yoneda}
                F(g)(m_S(\id_S)) = m_X(g^S(\id_S)) = m_X(\id_S \then g) = m_X(g).
            \end{equation}
            In the other direction, we show that for any $a \in F(S)$, we have $m^a_S(\id_S) = a$: by construction, $m^a_S \colon S^S \to F(S)$ sends $\id_S$ to $F(\id_S)(a) = a$.

            \item It suffices to show that given functors $F, G\colon\smset\to\smset$ and a natural transformation $\alpha \colon F \to G$, the naturality square
            \[
            \begin{tikzcd}[ampersand replacement=\&]
                \smset^\smset(\yon^S,F)\ar[d, "- \then \alpha"']\ar[r, "\sim"]\&
                F(S)\ar[d, "\alpha_S"]\\
                \smset^\smset(\yon^S,G)\ar[r, "\sim"]\&
                G(S)
            \end{tikzcd}
            \]
            commutes.
            The top map sends any $m \colon \yon^S \to F$ to $m_S(\id_S)$, which in turn is sent by the right map to $\alpha_S(m_S(\id_S)) = (m \then \alpha)_S(\id_S)$.
            This is also where the bottom map sends $m \then \alpha$, so the square commutes.

            \item It suffices to show that given a function $g \colon S \to X$, the naturality square
            \[
            \begin{tikzcd}[ampersand replacement=\&]
                \smset^\smset(\yon^S,F)\ar[d, "\yon^g \then -"']\ar[r, "\sim"]\&
                F(S)\ar[d, "F(g)"]\\
                \smset^\smset(\yon^X,F)\ar[r, "\sim"]\&
                F(X)
            \end{tikzcd}
            \]
            commutes.
            The left map sends any $m \colon \yon^S \to F$ to $\yon^g \then m$, which is sent by the bottom map to $(\yon^g \then m)_X(\id_X) = m_X(X^g(\id_X)) = m_X(g \then \id_X) = m_X(g)$.
            Meanwhile, the top map sends $m$ to $m_S(\id_S)$, which is sent by the right map to $F(g)(m_S(\id_S))$.
            So the square commutes by \eqref{eqn.finish_proof_yoneda}.

            \item To show that $\smset^\smset(\yon^S, \yon^T) \iso S^T$, just take $F\coloneqq\yon^T$ in \cref{lemma.yoneda} so that $F(S)\iso S^T$.
        \end{enumerate}
    \end{solution}
\end{exercise}

How will we go from these representable functors to polynomial ones?
Recall that, in algebra, a polynomial is just a sum of pure powers.
So we will define a \emph{polynomial functor} $\smset\to\smset$ to be a sum of pure power functors---that is, the representable functors $\yon^A$ for each set $A$ we just introduced.%
\footnote{This analogy isn't perfect: in algebra, polynomials are generally finite sums of pure powers, whereas our polynomial functors may be infinite sums of representables.
However, we are not the first to use the term ``polynomial'' this way, and the name stuck.}

All of our polynomials will be in one variable, $\yon$.
Every other letter or number that shows up in our notation for a polynomial will denote a set.
For example, in the polynomial
\begin{equation} \label{eqn.biz_poly}
    \rr\yon^\zz+\3\yon^{\3}+\yon^A+\sum_{i\in I}\yon^{R_i+Q_i^\2},
\end{equation}
$\rr$ denotes the set of real numbers, $\zz$ denotes the set of integers, $\2$ and $\3$ respectively denote the sets $\{1,2\}$ and $\{1,2,3\}$, and $A$, $I$, $Q_i$, and $R_i$ denote arbitrary sets.

To make sense of these polynomials, we need to define functor addition, both in the binary case (i.e.\ what is $\yon^A+\yon^B$?) and more generally over arbitrary sets (i.e.\ what is $\sum_{i\in I}\yon^{A_i}$?).
This will allow us to interpret polynomials like \eqref{eqn.biz_poly}.
In particular, just as $3y^3=y^3+y^3+y^3$ in algebra, the summand $\3\yon^\3$ of \eqref{eqn.biz_poly} denotes the sum of representables $\yon^\3+\yon^\3+\yon^\3$, while the summand $\rr\yon^\zz$ denotes the sum over $\rr$ of copies of $\yon^\zz$.

While polynomial functors will be defined as sums, products of polynomials will turn out to be polynomials as well, again mimicking polynomials in algebra.
To make sense of these products, we will also define functor multiplication.
The construction of sums and products of functors $\smset\to\smset$ will rely on the construction of sets and products of sets themselves.

%-------- Section --------%
\section{Sums and products of sets} \label{sec.poly.rep-sets.sum-prod-set}

Let $I$ be a set, and let $X_i$ be a set for each $i\in I$.
Classically, we may denote this \emph{$I$-indexed family of sets} by $(X_i)_{i\in I}$.
Categorically, we may view this data as a specific kind of functor: if we identify the set $I$ with the \emph{discrete category} on $I$, whose objects are the elements of $I$ and whose morphisms are all identities, then $(X_i)_{i\in I}$ can be identified with a functor $X\colon I\to\smset$ with $X(i)\coloneqq X_i$.
To compromise, we will denote an indexed family of sets by $X\colon I\to\smset$ for a set $I$ viewed as a discrete category (although we will occasionally use the classical notation when convenient), but denote the set obtained by evaluating $X$ at each $i\in I$ by $X_i$ rather than $X(i)$.

To pick out an element of one of the sets in the indexed family $X\colon I\to\smset$, we need to specify both an index $i\in I$ and an element $x\in X_i$.
We call the set of such pairs $(i,x)$ the \emph{sum} of this indexed family, as below.

\begin{definition}[Sum of sets] \label{def.sum_sets}
    Let $I$ be a set and $X\colon I\to\smset$ be an $I$-indexed family of sets.
    The \emph{sum $\sum_{i\in I}X_i$ of the indexed family $X$} is the set
    \[
    \sum_{i\in I}X_i\coloneqq\{(i,x)\mid i\in I\text{ and }x\in X_i\}.
    \]
    When $I\coloneqq\{i_1,\ldots,i_n\}$ is finite, we may alternatively denote this sum as
    \[
    X_{i_1}+\cdots+X_{i_n}.
    \]
\end{definition}

Say instead we pick an element from \emph{every} set in the indexed family: that is, we construct an assignment $i\mapsto x_i$, where each $x_i\in X_i$.
If every $X_i$ were the same set $X$, then this would just be a function $I\to X$.
More generally, this assignment is what we call a \emph{dependent function}: its codomain $X_i$ \emph{depends} on its input $i$.
We write the signature of such a dependent function as
\[
f \colon (i \in I) \to X_i.
\]
Note that the indexed family of sets $X\colon I\to\smset$ completely determines this signature.
The set of all dependent functions whose signature is determined by a given indexed family of sets is the \emph{product} of that indexed family, as below.

\begin{definition}[Product of sets] \label{def.prod_sets}
    Let $I$ be a set and $X\colon I\to\smset$ be an $I$-indexed family of sets.
    The \emph{product $\prod_{i\in I}X_i$ of the indexed family $X$} is the set of dependent functions
    \[
    \prod_{i\in I}X_i\coloneqq\{f \colon (i \in I) \to X_i\}.
    \]
    When $I\coloneqq\{i_1,\ldots,i_n\}$ is finite, we may alternatively denote this product as
    \[
    X_{i_1}\times\cdots\times X_{i_n} \qqor X_{i_1}\cdots X_{i_n}.
    \]
\end{definition}

For a dependent function $f\colon(i\in I)\to X_i$, we may denote the element of $X_i$ that $f$ assigns to $i\in I$ by $f(i), fi,$ or $f_i$.
When $I\coloneqq\{i_1,\ldots,i_n\}$ is finite, we may identify $f$ with the $n$-tuple $(f(i_1),\ldots,f(i_n))$; similarly, when $I\coloneqq\nn$, we may identify $f$ with the infinite sequence $(f_0,f_1,f_2,\ldots)$.

\begin{example}\label{ex.two_sums_and_prods}
    If $I\coloneqq\2=\{1,2\}$, then an $I$-indexed family $X\colon I\to \smset$ consists of two sets---say $X_1\coloneqq\{a,b,c\}$ and $X_2\coloneqq\{c,d\}$.
    Their sum is then the disjoint union
    \[
    \sum_{i\in \2}X_i=X_1+X_2=\{(1,a),(1,b),(1,c),(2,c),(2,d)\}.
    \]
    The cardinality\footnote{The \emph{cardinality} of a set is the number of elements it contains, at least when the set is finite; with care the notion can be extended to infinite sets as well.} of $X_1+X_2$ will always be the sum of the cardinalities of $X_1$ and $X_2$, justifying the use of the word ``sum.''

    Meanwhile, their product is the usual cartesian product
    \[\prod_{i\in \2}X_i \cong X_1\times X_2=\{(a,c),(a,d),(b,c),(b,d),(c,c),(c,d)\}.\]
    The cardinality of $X_1\times X_2$ will always be the product of the cardinalities of $X_1$ and $X_2$, justifying the use of the word ``product.''
\end{example}

\begin{exercise}\label{exc.on_sums_prods_sets}
    Let $I$ be a set.
    \begin{enumerate}
        \item \label{exc.on_sums_prods_sets.sum} Show that there is an isomorphism of sets $I\iso\sum_{i\in I}\1$.
        \item \label{exc.on_sums_prods_sets.prod} Show that there is an isomorphism of sets $\1\iso\prod_{i\in I}\1$.
    \end{enumerate}
    As a special case, suppose that $I\coloneqq\0=\varnothing$ and that $X\colon \varnothing\to\smset$ is the unique empty indexed family of sets.
    \begin{enumerate}[resume]
        \item Is it true that $X_i=\1$ for each $i\in I$?
        \item Justify the statement ``the empty sum is $\0$'' by showing that there is an isomorphism of sets $\sum_{i\in\varnothing}X_i\iso\0$.
        \item Justify the statement ``the empty product is $\1$'' by showing that there is an isomorphism of sets $\prod_{i\in\varnothing}X_i\iso\1$.
        \qedhere
    \end{enumerate}

    \begin{solution}
        \begin{enumerate}
            \item \label{sol.on_sums_prods_sets.sum}
            To show that $I\iso\sum_{i \in I}\1$, observe that $x \in \1 = \{1\}$ if and only if $x = 1$, so $\sum_{i \in I} \1 = \{(i, 1) \mid i \in I\}$.
            Then the function $I \to \sum_{i \in I} \1$ that sends each $i \in I$ to $(i, 1)$ is clearly an isomorphism.

            \item \label{sol.on_sums_prods_sets.prod}
            To show that $\1 \iso \prod_{i \in I} \1$, it suffices to show that there is a unique dependent function $f \colon (i \in I) \to \1$.
            As $\1 = \{1\}$, such a function $f$ must always send $i \in I$ to $1$.
            This uniquely characterizes $f$, so there is indeed only one such dependent function.

            \item \label{sol.on_sums_prods_sets.vac} Yes: since $I$ is empty, there are no $i \in I$.
            So it is true that $X_i = 1$ holds whenever $i \in I$ holds, because $i \in I$ never holds.
            We say that this sort of statement is \emph{vacuously true}.

            \item As $I = \0 = \varnothing$, we have $\sum_{i\in\varnothing}X_i = \sum_{i\in I}\1 \iso I = \0$, where the equation on the left follows from \cref{sol.on_sums_prods_sets.vac} and the isomorphism in the middle follows from \cref{sol.on_sums_prods_sets.sum}.

            \item As $I = \varnothing$, we have $\prod_{i\in\varnothing}X_i = \prod_{i\in I}\1 \iso \1$, where the equation on the left follows from \cref{sol.on_sums_prods_sets.vac} and the isomorphism on the right follows from \cref{sol.on_sums_prods_sets.prod}.
        \end{enumerate}
    \end{solution}
\end{exercise}

The following standard fact describes the constructions from \cref{def.sum_sets,def.prod_sets} categorically and further justifies why we call them sums and products.

\begin{proposition} \label{prop.set_prod_coprod}
    Let $I$ be a set and $X\colon I\to\smset$ be an $I$-indexed family of sets. Then the sum $\sum_{i\in I}X_i$ is the categorical coproduct of these sets in $\smset$ (i.e.\ the colimit of the functor $X\colon I\to\smset$, viewed as a diagram), and the product $\prod_{i\in I}X_i$ is the categorical product of these sets in $\smset$ (i.e.\ the limit of the functor $X\colon I\to\smset$, viewed as a diagram).
\end{proposition}

\begin{proof}
    The sum $\sum_{i\in I}X_i$ comes equipped with an inclusion $\iota_j\colon X_j\to\sum_{i\in I}X_i$ for each $j\in I$ given by $x\mapsto(j,x)$.
    The product $\prod_{i\in I}X_i$ comes equipped with a projection $\pi_j\colon\prod_{i\in I}X_i\to X_j$ for each $j\in I$ sending each $f\colon(i\in I)\to X_i$ to $f(j)$.
    These satisfy the universal properties for categorical coproducts and products, respectively; see \cref{exc.set_prod_coprod}.
\end{proof}

\begin{exercise} \label{exc.set_prod_coprod}
    \begin{enumerate}
        \item Show that $\sum_{i\in I}X_i$ along with the inclusions $\iota_j\colon X_j\to\sum_{i\in I}X_i$ described in the proof of \cref{prop.set_prod_coprod} satisfy the universal property of a categorical coproduct: for any set $Y$ with functions $g_j\colon X_j\to Y$ for each $j\in I$, there exists a unique function $h\colon\sum_{i\in I}X_i\to Y$ for which $\iota_j\then h=g_j$ for all $j\in I$.
        \item Show that $\prod_{i\in I}X_i$ along with the projections $\pi_j\colon\prod_{i\in I}X_i\to X_j$ described in the proof of \cref{prop.set_prod_coprod} satisfy the universal property of a categorical product: for any set $Y$ with functions $g_j\colon Y\to X_j$ for each $j\in I$, there exists a unique function $h\colon Y\to\prod_{i\in I}X_i$ for which $h\then\pi_j=g_j$ for all $j\in I$. \qedhere
    \end{enumerate}
    \begin{solution}
        \begin{enumerate}
            \item Any function $h\colon\sum_{i\in I}X_i\to Y$ for which $\iota_j\then h=g_j$ for all $j\in I$ must satisfy $h(j,x)=h(\iota_j(x))=g_j(x)$ for all $j\in I$ and $x\in X_j$.
            This uniquely characterizes $h$, so if we define $h(j,x)\coloneqq g_j(x)$ we are done.

            \item Any function $h\colon Y\to\prod_{i\in I}X_i$ for which $h\then\pi_j=g_j$ for all $j\in I$ must satisfy $h(y)_j=\pi_j(h(y))=g_j(y)$ for all $y\in Y$ and $j\in I$.
            This uniquely characterizes $h(y)$ and thus $h$, so if we define $h(y)\colon(i\in I)\to X_i$ to be the dependent function given by $i\mapsto g_i(y)$ we are done.
        \end{enumerate}
    \end{solution}
\end{exercise}

Though we proved above explicitly that $\smset$ has all small products and coproducts, from here on out, we will assume the standard categorical fact that $\smset$ is complete (has all small limits) and cocomplete (has all small colimits). % TODO: add ref for this??

We have constructed categorical sums and products of sets, but we can also construct categorical sums and products of the maps between them: functions.

\begin{definition}[Categorical sum and product of functions]
    Let $I$ be a set and $X,Y\colon I\to\smset$ be $I$-indexed families of sets.
    Given a natural transformation $f\colon X\to Y$, i.e.\ an $I$-indexed family of functions $(f_i\colon X_i\to Y_i)_{i\in I}$, its \emph{categorical sum} (or \emph{coproduct}) is the function
    \[
    \sum_{i\in I}f_i\colon\sum_{i\in I}X_i\to\sum_{i\in I}Y_i
    \]
    that, given $i\in I$ and $x\in X_i$, sends $(i,x)\mapsto(i,f_i(x))$; while its \emph{categorical product} is the function
    \[
    \prod_{i\in I}f_i\colon\prod_{i\in I}X_i\to\prod_{i\in I}Y_i
    \]
    that sends each $g\colon(i\in I)\to X_i$ to the \emph{composite dependent function} $(i\in I)\to Y_i$, denoted $g\then f$ or $f\circ g$, which sends $i\in I$ to $f_i(g(i))$.

    When $I\coloneqq\{i_1,\ldots,i_n\}$ is finite, we may alternatively denote this categorical sum and product of functions respectively as\footnote{We will take care to highlight when this notation may clash with a sum (resp.\ product) of functions with common domain and codomain whose codomain has an additive (resp.\ multiplicative) structure.}
    \[
    f_{i_1}+\cdots+f_{i_n} \qqand f_{i_1}\times\cdots\times f_{i_n}.
    \]
\end{definition}

\begin{exercise}
    \begin{enumerate}
        \item Show that the categorical sum of functions is the one induced by the universal property of the categorical sum of sets.
        That is, given a set $I$, two $I$-indexed families of sets $X,Y\colon I\to\smset$, and a natural transformation $f\colon X\to Y$, the function $\sum_{i\in I}f_i\colon\sum_{i\in I}X_i\to\sum_{i\in I}Y_i$ that we called the categorical sum is induced by the following composite maps for $j\in J$:
        \[
        X_j\To{f_j}Y_j\To{\iota'_j}\sum_{i\in I}Y_i.
        \]
        It then follows by a standard categorical argument that the sum is functorial, i.e.\ that the sum of identities is an identity and that the sum of composites is the composite of sums.

        \item Similarly, show that the categorical product of functions is the one induced by the universal property of the categorical product of sets.
        That is, given the same setup as the previous part, the function $\prod_{i\in I}f_i\colon\prod_{i\in I}X_i\to\prod_{i\in I}Y_i$ that we called the categorical product is induced by the following composite maps for $j\in J$:
        \[
        \prod_{i\in I}X_i\To{\pi_j}X_j\To{f_j}Y_j.
        \]
        Again, this implies that the product is functorial. \qedhere
    \end{enumerate}
    \begin{solution}
        \begin{enumerate}
            \item It suffices to show that the following square commutes for all $j\in I$:
            \[
            \begin{tikzcd}[column sep=large]
                X_j \ar[d, "\iota_j"] \ar[r, "f_j"] & Y_j \ar[d, "\iota'_j"] \\
                \sum_{i\in I}X_i \ar[r, "\sum_{i\in I}f_i"] & \sum_{i\in I}Y_i
            \end{tikzcd}
            \]
            Given $x\in X_j$, the left inclusion map sends $x$ to $(j,x)$, which the bottom sum of maps sends to $(j,f_j(x))$.
            Meanwhile, the top map sends $x$ to $f_j(x)$, which the right inclusion map again sends to $(j,f_j(x))$.

            \item It suffices to show that the following square commutes for all $j\in I$:
            \[
            \begin{tikzcd}[column sep=large]
                \prod_{i\in I}X_i \ar[d, "\pi_j"] \ar[r, "\prod_{i\in I}f_i"] & \prod_{i\in I}Y_i \ar[d, "\pi'_j"] \\
                X_j \ar[r, "f_j"] & Y_j
            \end{tikzcd}
            \]
            Given $g\colon(i\in I)\to X_i$ in $\prod_{i\in I}X_i$, the top product of maps sends $g$ to $f\circ g$, which the right projection map sends to $f_j(g(j))$.
            Meanwhile, the left projection map sends $g$ to $g(j)$, which the bottom map again sends to $f_j(g(j))$.
        \end{enumerate}
    \end{solution}
\end{exercise}

We now highlight some tools and techniques to help us work with sum and product sets.

\begin{exercise}\label{exc.product_as_sections}
    Let $I$ be a set and $X \colon I \to \smset$ be an indexed family.
    There is a
    projection function
    $\pi_1 \colon \sum_{i \in I} X_i \to I$
    defined by $\pi_1(i, x) \coloneqq i$.
    \begin{enumerate}
        \item What is the signature of the second projection $\pi_2(i, x) \coloneqq x$?
        (Hint: it's a dependent function.)
        \item A \emph{section} of a function $r \colon A \to B$ is a function $s \colon B \to A$ such that $s \then r = \id_B$.
        Show that the product of the indexed family is isomorphic to the set of sections of $\pi_1$:
        \[\prod_{i \in I} X_i \cong \left\{s \colon I \to \sum_{i \in I} X_i \,\middle|\, s \then \pi_1 = \id_I\right\}.\]
        \qedhere
    \end{enumerate}
    \begin{solution}
        \begin{enumerate}
            \item The second projection $\pi_2(i, x) = x$ sends each pair $p \coloneqq (i, x) \in \sum_{i \in I} X_i$ to $x$, an element of $X_i$.
            Note that we can write $i$ in terms of $p$ as $\pi_1(p)$.
            This allows us to write the signature of $\pi_2$ as $\pi_2 \colon (p \in \sum_{i \in I} X_i) \to X_{\pi_1(p)}$.

            \item Let $S := \{s \colon I \to \sum_{i \in I} X_i \mid s \then \pi_1 = \id_I\}$ be the set of sections of $\pi_1$. To show that $\prod_{i \in I} X_i \cong S$, we will exhibit maps in either direction and show that they are mutually inverse.
            For each $f \colon (i \in I) \to X_i$ in $\prod_{i \in I} X_i$, we have $f(i) \in X_i$ for $i \in I$, so we can define a function $s_f \colon I \to \sum_{i \in I} X_i$ that sends $i\mapsto(i, f(i))$.
            Then $\pi_1(s_f(i)) = \pi_1(i, f(i)) = i$, so $s_f$ is a section of $\pi_1$.
            Hence $f \mapsto s_f$ is a map $\prod_{i \in I} X_i \to S$.

            In the other direction, for each section $s \colon I \to \sum_{i \in I} X_i$ we have $\pi_1(s(i)) = i$ for $i \in I$, so we can write $s(i)$ as an ordered pair $(i, \pi_2(s(i)))$ with $\pi_2(s(i)) \in X_i$.
            Hence we can define a dependent function $f_s \colon (i \in I) \to X_i$ sending $i\mapsto\pi_2(s(i))$.
            Then $s \mapsto f_s$ is a map $S \to \prod_{i \in I} X_i$.
            By construction $s_{f_s}(i) = (i, f_s(i)) = (\pi_1(s(i)), \pi_2(s(i))) = s(i)$ and $f_{s_f}(i) = \pi_2(s_f(i)) = \pi_2(i, f(i)) = f(i)$, so these maps are mutually inverse.
        \end{enumerate}
    \end{solution}
\end{exercise}

A helpful way to think about sum or product sets is to consider what choices must be made to specify an element of such a set.
In the following examples, say that we have a set $I$ and an $I$-indexed family $X \colon I \to \smset$.

Below, we give the instructions for choosing an element of $\sum_{i \in I} X_i$.

\begin{quote}
    To choose an element of $\sum_{i \in I} X_i$:
    \begin{enumerate}
        \item choose an element $i \in I$;
        \item choose an element of $X_i$.
    \end{enumerate}
\end{quote}

Then the projection $\pi_1$ from \cref{exc.product_as_sections} sends each element of $\sum_{i \in I} X_i$ to the element of $i \in I$ chosen in step 1, while the projection $\pi_2$ sends each element of $\sum_{i \in I} X_i$ to the element of $X_i$ chosen in step 2.

Next, we give the instructions for choosing an element of $\prod_{i \in I} X_i$.

\begin{quote}
    To choose an element of $\prod_{i \in I} X_i$:
    \begin{enumerate}
        \item for each element $i \in I$:
        \begin{enumerate}[label*=\arabic*.]
            \item choose an element of $X_i$.
        \end{enumerate}
    \end{enumerate}
\end{quote}

Armed with these interpretations, we can tackle more complicated expressions, including those with nested $\sum$'s and $\prod$'s such as
\begin{equation}\label{eqn.sum_prod_sum}
    A \coloneqq \sum_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k).
\end{equation}
The instructions for choosing an element of $A$ form a nested list, as follows.

\begin{quote}
    To choose an element of $A$:
    \begin{enumerate}
        \item choose an element $i \in I$;
        \item for each element $j \in J(i)$:
        \begin{enumerate}[label*=\arabic*.]
            \item choose an element $k \in K(i,j)$;
            \item choose an element of $X(i,j,k)$.
        \end{enumerate}
    \end{enumerate}
\end{quote}

Here the choice of $k\in K(i,j)$ may depend on $i$ and $j$: different values of $i$ and $j$ may lead to different sets $K(i,j)$.

By describing $A$ like this, we see that each $a \in A$ can be projected to an element $i\coloneqq\pi_1(a) \in I$, chosen in step 1, and a dependent function $\pi_2(a)$, chosen in step 2.
This dependent function in turn sends each $j \in J(i)$ to a pair that can be projected to an element $k\coloneqq\pi_1(\pi_2(a)(j)) \in K(i, j)$ chosen in step 2.1 and an element $\pi_2(\pi_2(a)(j)) \in X(i,j,k)$ chosen in step 2.2.

\begin{example}%[Notation for $\sum\prod$ stuff]
    \label{ex.notation_sum_prod}
    % Here we give notation for the elements of a set involving $\sum$'s and $\prod$'s such as that in \eqref{eqn.sum_prod_sum}.

    Let $I\coloneqq\{1,2\}$; let $J(1)\coloneqq\{j\}$ and $J(2)\coloneqq\{j,j'\}$; let $K(1,j)\coloneqq\{k_1,k_2\}$, $K(2,j)\coloneqq\{k_1\}$, and $K(2,j')\coloneqq\{k'\}$; and let $X(i,j,k)\coloneqq\{x,y\}$ for all $i,j,k$. Now the formula
    \[\sum_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k)\]
    from \eqref{eqn.sum_prod_sum} specifies a fixed set. Here is a list of all eight of its elements:
    \[
    \left\{
    \begin{gathered}
        \big(1, j\mapsto(k_1,x)\big),\qquad
        \big(1, j\mapsto(k_1,y)\big),\qquad
        \big(1, j\mapsto(k_2,x)\big),\qquad
        \big(1, j\mapsto(k_2,y)\big),\\
        \big(2, j\mapsto(k_1,x), j'\mapsto(k',x)\big),\qquad
        \big(2, j\mapsto(k_1,x), j'\mapsto(k',y)\big),\\
        \big(2, j\mapsto(k_1,y), j'\mapsto(k',x)\big),\qquad
        \big(2, j\mapsto(k_1,y), j'\mapsto(k',y)\big)
    \end{gathered}
    \right\}
    \]
    In each case, we first chose an element $i\in I$, either 1 or 2. Then for each $j\in J(i)$ we chose an element $k\in K(i,j)$ and an element of $X(i,j,k)$.
\end{example}

\begin{exercise}
    Consider the set
    \begin{equation}\label{eqn.prod_sum_prod}B \coloneqq \prod_{i\in I}\sum_{j\in J(i)}\prod_{k\in K(i,j)}X(i,j,k).\end{equation}
    \begin{enumerate}
        \item Give the instructions for choosing an element of $B$ as a nested list, like we did for $A$ just below \eqref{eqn.sum_prod_sum}.
        \item With $I$, $J$, $K$, and $X$ as in \cref{ex.notation_sum_prod}, how many elements are in $B$?
        \item Write out three of these elements in the style of \cref{ex.notation_sum_prod}.
        \qedhere
    \end{enumerate}
    \begin{solution}
        \begin{enumerate}
            \item Here are the instructions for choosing an element of $B$ as a nested list.
            \begin{quote}
                To choose an element of $B$:
                \begin{enumerate}[label=\arabic*.]
                    \item for each element $i \in I$:
                    \begin{enumerate}[label*=\arabic*.]
                        \item choose an element $j \in J(i)$;
                        \item for each element $k \in K(i, j)$:
                        \begin{enumerate}[label*=\arabic*.]
                            \item choose an element of $X(i,j,k)$.
                        \end{enumerate}
                    \end{enumerate}
                \end{enumerate}
            \end{quote}
            \item Given $I\coloneqq\{1,2\}$, $J(1)\coloneqq\{j\}$, $J(2)\coloneqq\{j,j'\}$, $K(1,j)\coloneqq\{k_1,k_2\}$, $K(2,j)\coloneqq\{k_1\}$, $K(2,j')\coloneqq\{k'\}$, and $X(i,j,k)\coloneqq\{x,y\}$ for all $i,j,k$, our goal is to count the number of elements in $B$.
            To compute the cardinality of $B$, we can use the fact that the cardinality of a sum (resp.\ product) is the sum (resp.\ product) of the cardinalities of the summands (resp.\ factors).
            So
            \begin{align*}
                |B| &= \prod_{i\in I}\sum_{j\in J(i)}\prod_{k\in K(i,j)}|X(i,j,k)| \\
                &= \prod_{i\in \{1,2\}}\sum_{j\in J(i)}\prod_{k\in K(i,j)}2 \\
                &= \left(\sum_{j\in J(1)} 2^{|K(1,j)|}\right)\left(\sum_{j\in J(2)} 2^{|K(2,j)|}\right) \\
                &= \left(2^2\right)\left(2^1 + 2^1\right) = 16.
            \end{align*}
            \item Here are three of the elements of $B$ (you may have written down others):
            \begin{itemize}
                \item $(1 \mapsto (j, k_1 \mapsto x, k_2 \mapsto y), 2 \mapsto (j', k' \mapsto x))$
                \item $(1 \mapsto (j, k_1 \mapsto y, k_2 \mapsto y), 2 \mapsto (j, k_1 \mapsto y))$
                \item $(1 \mapsto (j, k_1 \mapsto y, k_2 \mapsto x), 2 \mapsto (j', k' \mapsto y))$
            \end{itemize}
            \qedhere
        \end{enumerate}
    \end{solution}
\end{exercise}

%Henceforth, we will omit the full sequence of nested instructions corresponding to every sum or product of sets; we will assume you can read them for yourself.

%-------- Subsection --------%
\section{Expanding products of sums} \label{sec.poly.rep-sets.expand}

We will often encounter sums of sets nested within products, as in \eqref{eqn.sum_prod_sum} and \eqref{eqn.prod_sum_prod}.
The following proposition helps us work with these; it is sometimes called the \emph{type-theoretic axiom of choice}, but it is perhaps more familiar as a set-theoretic analogue of the \emph{distributive property} of multiplication over addition.
While the identity may look foreign, it captures for sets the same process that you would use to multiply multi-digit numbers from grade school arithmetic or polynomials from high school algebra.

\begin{proposition}[Pushing $\prod$ past $\sum$]\label{prop.push_prod_sum_set}
    For any sets $I,(J(i))_{i\in I},$ and $(X(i,j))_{i\in I, j\in J(i)}$, we have a natural isomorphism
    \begin{equation}\label{eqn.set_completely_distributive}
        \prod_{i\in I}\sum_{j\in J(i)}X(i,j)
        \iso
        \sum_{\bar{j}\in \prod_{i\in I}J(i)}\;\prod_{i\in I}X(i,\bar{j}(i)).\footnote{We draw a bar over $j$ in $\bar{j}$ to remind ourselves that $\bar{j}$ is no longer just an index but a (dependent) function.}
    \end{equation}
\end{proposition}
\begin{proof}
    First, we construct a map from the left hand set to the right. An element of the set on the left is a dependent function $f \colon (i \in I) \to \sum_{j \in J(i)} X(i, j)$, which we can compose with projections from its codomain to yield $\pi_1(f(i)) \in J(i)$ and $\pi_2(f(i)) \in X(i, \pi_1(f(i)))$ for every $i \in I$.
    We can then form the following pair:
    \[
    (i \mapsto \pi_1(f(i)), i \mapsto \pi_2(f(i))).
    \]
    This is an element of the right hand set, because $i \mapsto \pi_1(f(i))$ is a dependent function in $\prod_{i\in I}J(i)$ and $i \mapsto \pi_2(f(i))$ is a dependent function in $\prod_{i\in I}X(i,\pi_1(f(i)))$.

    Now we go from right to left.
    An element of the right hand set is a pair of dependent functions, $\bar{j} \colon (i \in I) \to J(i)$ and $g \colon
    (i \in I) \to X(i, \bar{j}(i))$.
    We map this pair to the following element of the left hand set, a dependent function $(i\in I)\to\sum_{j\in J(i)}X(i,j)$:
    \[
    i \mapsto (\bar{j}(i), g(i)).
    \]

    Finally, we verify that the maps are mutually inverse.
    An element $(\bar{j}, g)$ of the right hand set is sent by one map and then the other to the pair
    \[
    (i \mapsto \pi_1(\bar{j}(i), g(i)), i \mapsto \pi_2(\bar{j}(i), g(i)))=(i \mapsto \bar{j}(i), i \mapsto g(i))=(\bar{j},g)
    \]
    Meanwhile, an element $f$ of the left hand set is sent by one map and then the other to the dependent function
    \[
    i \mapsto (\pi_1(f(i)), \pi_2(f(i))).
    \]
    As $f(i)$ is a pair whose components are $\pi_1(f(i))$ and $\pi_2(f(i))$, the dependent function above is precisely $f$.
\end{proof}

When $J(i)=J$ does not depend on $i\in I$, we can simplify the formula in \eqref{eqn.set_completely_distributive}.

\begin{corollary} \label{cor.push_prod_sum_set_indep}
    For any sets $I, J,$ and $(X(i, j))_{i \in I, j \in J}$, we have a natural isomorphism
    \begin{equation} \label{eqn.push_prod_sum_set_indep}
        \prod_{i\in I}\sum_{j\in J}X(i,j)\cong\sum_{\bar{j}\colon I\to J}\prod_{i\in I}X(i,\bar{j}(i)),
    \end{equation}
    where $\bar{j}$ ranges over all (standard, non-dependent) functions $I\to J$.
\end{corollary}
\begin{proof}
    Take $J(i)\coloneqq J$ for all $i \in I$ in \eqref{eqn.set_completely_distributive}.
    Then the set $\prod_{i\in I}J(i)$ becomes $\prod_{i\in I}J$ (which we may denote in exponential form by $J^I$); its elements, dependent functions $\bar{j}\colon(i\in I)\to J(i)=J$, become standard functions $\bar{j}\colon I\to J$.
\end{proof}

It turns out that being able to push $\prod$ past $\sum$ as in \eqref{eqn.set_completely_distributive} is not a property that is unique to sets.
In general, we refer to a category having this property as follows.

\begin{definition}[Completely distributive category]
    A category $\Cat{C}$ with all small products and coproducts is \emph{completely distributive} if products distribute over coproducts as in \eqref{eqn.set_completely_distributive}; that is, for any set $I$, sets $(J(i))_{i\in I}$, and objects $(X(i,j))_{i\in I,j\in J(i)}$ from $\Cat{C}$, we have a natural isomorphism
    \begin{equation}\label{eqn.cat_completely_distributive}
        \prod_{i\in I}\sum_{j\in J(i)}X(i,j)
        \iso
        \sum_{\bar{j}\in \prod_{i\in I}J(i)}\;\prod_{i\in I}X(i,\bar{j}(i)).
    \end{equation}
\end{definition}

So \cref{prop.push_prod_sum_set} states that $\smset$ is completely distributive.
Once we define the category of polynomial functors, we will see that it, too, is completely distributive.

\cref{cor.push_prod_sum_set_indep} generalizes to all completely distributive categories as well; we state this formally below.

\begin{corollary} \label{cor.push_prod_sum_obj_indep}
    Let $\Cat{C}$ be a completely distributive category.
    For any sets $I$ and $J$ and objects $(X(i, j))_{i \in I, j \in J}$ in $\Cat{C}$, we have a natural isomorphism
    \begin{equation} \label{eqn.push_prod_sum_obj_indep}
        \prod_{i\in I}\sum_{j\in J}X(i,j)\iso\sum_{\bar{j}\colon I\to J}\prod_{i\in I}X(i,\bar{j}(i)).
    \end{equation}
\end{corollary}
\begin{proof}
    Again, take $J(i)\coloneqq J$ for all $i \in I$ in \eqref{eqn.cat_completely_distributive}.
\end{proof}

\begin{exercise}
    Let $\Cat{C}$ be a completely distributive category.
    How is the usual distributive law
    \[
    X\times(Y+Z)\iso X\times Y+X\times Z
    \]
    for $X,Y,Z\in\cat{C}$ a special case of \eqref{eqn.cat_completely_distributive}?
    \begin{solution}
        We wish to show that $X\times (Y+Z)\iso X\times Y+X\times Z$ using \eqref{eqn.cat_completely_distributive}.
        On the left hand side, we are taking a $2$-fold product: a single object times a $2$-fold sum.
        So we should let $I\coloneqq\2$ and let $J(1)\coloneqq\1$, with $X(1,1)\coloneqq X$; and $J(2)\coloneqq\2$, with $X(2,1)\coloneqq Y$ and $X(2,2)\coloneqq Z$.
        Then
        \[
        X\times(Y+Z)\iso \prod_{i\in\2}\sum_{j\in J(i)}X(i,j) \iso \sum_{\bar{j}\in\prod_{i\in\2}J(i)}\;\prod_{i\in \2}X(i,\bar{j}(i)) \iso \sum_{\bar{j}\in\prod_{i\in\2}J(i)}X(1,\bar{j}(1))\times X(2,\bar{j}(2)),
        \]
        where the middle isomorphism follows from \eqref{eqn.cat_completely_distributive}.
        The set $\prod_{i\in\2}J(i)$ contains two functions: $(1\mapsto1,2\mapsto1)$ and $(1\mapsto1,2\mapsto2)$.
        So we can rewrite the right hand side as
        \[
        X(1,1)\times X(2,1)+X(1,1)\times X(2,2)\iso X\times Y+X\times Z.
        \]
    \end{solution}
\end{exercise}

Throughout this book, such as in the exercise below, you will see expressions consisting of alternating products and sums.
Using \eqref{eqn.cat_completely_distributive}, you can always rewrite such an expression as a sum of products, in which every $\sum$ appears before every $\prod$.\footnote{When an expression is written so that every $\sum$ appears before every $\prod$, it is said to be in \emph{disjunctive normal form}.}
This is analogous to how products of sums in high school algebra can always be expanded into sums of products via the distributive property.

\begin{exercise} \label{exc.push_prod_sum_set}
    Let $I, (J(i))_{i\in I},$ and $(K(i,j))_{(i,j)\in IJ}$ be sets, and for each $(i,j,k)\in IJK$, let $X(i,j,k)$ be an object in a completely distributive category.
    \begin{enumerate}
        \item Rewrite
        \[
        \sum_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k)
        \]
        so that every $\sum$ appears before every $\prod$.
        \item Rewrite
        \[
        \prod_{i\in I}\sum_{j\in J(i)}\prod_{k\in K(i,j)}X(i,j,k)
        \]
        so that every $\sum$ appears before every $\prod$.
        \item Rewrite
        \[
        \prod_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k)
        \]
        so that every $\sum$ appears before every $\prod$.\qedhere
    \end{enumerate}
    \begin{solution}
        \begin{enumerate}
            \item By applying \eqref{eqn.cat_completely_distributive}, we can rewrite
            \[
            \sum_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k)
            \]
            as
            \[
            \sum_{i\in I}\sum_{\bar{k}\in \prod_{j\in J}K(i,j)}\prod_{j\in J(i)}X(i,j,\bar{k}(j)).
            \]
            \item By applying \eqref{eqn.cat_completely_distributive}, we can rewrite
            \[
            \prod_{i\in I}\sum_{j\in J(i)}\prod_{k\in K(i,j)}X(i,j,k)
            \]
            as
            \[
            \sum_{\bar{j}\in \prod_{i\in I}J(i)}\;\prod_{i\in I}X(i,\bar{j}(i))\prod_{k\in K(i,\bar{j}(i))}X(i,\bar{j}(i),k).
            \]
            \item By applying \eqref{eqn.cat_completely_distributive}, we can rewrite
            \[
            \prod_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k)
            \]
            once as
            \[
            \prod_{i\in I}\sum_{\bar{k}\in\prod_{j\in J}K(i,j)}\prod_{j\in J(i)}X(i,j,\bar{k}(j))
            \]
            and then again as
            \[
            \sum_{\bar{\bar{k}}\in\prod_{i\in I}\prod_{j\in J}K(i,j)}\prod_{i\in I}\prod_{j\in J(i)}X(i,j,\bar{\bar{k}}(i,j)).
            \]
        \end{enumerate}
    \end{solution}
\end{exercise}

Now that we understand sums and products of sets, we are ready to explore sums and products of set-valued functors.

%-------- Subsection --------%
\section{Sums and products of functors $\smset\to\smset$} \label{sec.poly.rep-sets.sum-prod-func}

Recall that our goal is to define polynomial functors such as $\yon^\2+\2\yon+\1$ and the maps between them.
We have defined representable functors such as $\yon^\2$, $\yon$, and $\1$; we just need to interpret sums of functors $\smset\to\smset$.
But we might as well introduce products of functors at the same time, because they will very much come in handy.
Both these concepts generalize to limits and colimits in $\smset^\smset$.

\begin{proposition} \label{prop.presheaf_lim_ptwise}
    The category $\smset^\smset$ has all small limits and colimits, and they are computed pointwise.
    In particular, on objects, given a small category $\cat{J}$ and a functor $F\colon \cat{J}\to\smset^\smset$, for all $X\in\smset$, the limit and colimit of $F$ satisfy isomorphisms
    \[
    \left(\lim_{j\in\cat{J}} F(j)\right)(X) \iso \lim_{j\in\cat{J}} \left(F(j)(X)\right) \qqand \left(\colim_{j\in\cat{J}} F(j)\right)(X) \iso \colim_{j\in\cat{J}} \left(F(j)(X)\right)
    \]
    natural in $X$.
\end{proposition}
\begin{proof}
    This is a special case of a more general fact when $\smset^\smset$ is replaced by an arbitrary functor category $\Cat{D}^\Cat{C}$, where $\Cat{D}$ is a category that (like $\smset$) has all small limits and colimits; see \cite[pages 22--23, displays (24) and (25)]{macLane1992sheaves}.
\end{proof}

Focusing on the case of coproducts and products, the following corollary is immediate.

\begin{corollary}[Sums and products of functors $\smset\to\smset$] \label{cor.sum_prod_set_endofuncs}
    Given functors $F,G\colon\smset\to\smset$, their categorical coproduct or sum in $\smset^\smset$, denoted $F+G$, is the functor $\smset\to\smset$ defined for $X,Y\in\smset$ and $f\colon X\to Y$ by
    \[
    (F+G)(X)\coloneqq F(X)+G(X) \qqand (F+G)(f)\coloneqq F(f)+G(f);
    \]
    while their categorical product in $\smset^\smset$, denoted $F\times G$ or $FG$, is the functor $\smset\to\smset$ defined for $X,Y\in\smset$ and $f\colon X\to Y$ by
    \[
    (F\times G)(X)\coloneqq F(X)\times G(X) \qqand (F\times G)(f)\coloneqq F(f)\times G(f).
    \]

    More generally, given functors $(F_i)_{i\in I}$ indexed over a set $I$, their categorical coproduct or sum and categorical product in $\smset^\smset$, respectively denoted
    \[
    \sum_{i\in I}F_i\colon\smset\to\smset
    \qqand
    \prod_{i\in I}F_i\colon\smset\to\smset,
    \]
    are respectively defined for $X\in\smset$ by
    \[
    \left(\sum_{i\in I}F_i\right)(X)\coloneqq\sum_{i\in I} F_i(X)
    \qqand
    \left(\prod_{i\in I}F_i\right)(X)\coloneqq\prod_{i\in I} F_i(X).
    \]
    and for functions $f\colon X\to Y$ by
    \[
    \left(\sum_{i\in I}F_i\right)(f)\coloneqq\sum_{i\in I} F_i(f)
    \qqand
    \left(\prod_{i\in I}F_i\right)(f)\coloneqq\prod_{i\in I} F_i(f).
    \]
\end{corollary}

% TODO: sum and products of nat trans & functoriality?

We also note the special case of initial and terminal objects.
Given a set $I\in\smset$, we will also use $I$ to denote the constant functor $\smset\to\smset$ that sends every set to $I$.

\begin{corollary}[Initial and terminal functors $\smset\to\smset$]
    The constant functor $\0\colon\smset\to\smset$ is initial in $\smset^\smset$, while the constant functor $\1\colon\smset\to\smset$ is terminal in $\smset^\smset$.
\end{corollary}
\begin{proof}
    As the set $\0$ is initial in $\smset$ (for every set $X$ there is a unique map $\0\to X$), \cref{prop.presheaf_lim_ptwise} implies that the constant functor $\0$ is initial in $\smset^\smset$.
    Similarly, as the set $\1$ is terminal in $\smset$ (for every set $X$ there is a unique map $X\to\1$), \cref{prop.presheaf_lim_ptwise} implies that the constant functor $\1$ is terminal in $\smset^\smset$.
\end{proof}

Finally, we note that $\smset^\smset$ inherits the distributivity of $\smset$.

\begin{proposition}\label{prop.set_endofunc_distrib}
    The category $\smset^\smset$ is completely distributive.
\end{proposition}
\begin{proof}
    This follows directly from the fact that $\smset$ itself is completely distributive (\cref{prop.push_prod_sum_set}) and the fact that sums and products in $\smset^\smset$ are computed pointwise (\cref{cor.sum_prod_set_endofuncs}).
\end{proof}

The following exercises justify some notational shortcuts we will use when denoting polynomial functors.
First, for any set $A$ and functor $F\colon\smset\to\smset$, we may write an $A$-indexed sum of copies of $F$ as $AF$, the product of $F$ and the constant functor $A$; for instance, $\yon+\yon\iso\2\yon$.

\begin{exercise} \label{exc.repeated_sum_is_product}
    Show that for a set $A\in\smset$ and a functor $F\colon\smset\to\smset$, an $A$-indexed sum of copies of $F$ is isomorphic to the product of the constant functor $A$ and $F$:
    \[
    \sum_{a \in A}F\iso AF.
    \]
    (This is analogous to the fact that adding up $n$ copies of number is equal to multiplying that same number by $n$.)
    \begin{solution}
        It suffices to show that for all $X\in\smset$, there is an isomorphism
        \[
        \sum_{a\in A}F(X)\iso(AF)(X)
        \]
        natural in $X$.
        The left hand side is the set $\{(a,s)\mid a\in A\text{ and }s\in F(X)\} \iso A \times F(X)$, while the right hand side is also naturally isomorphic to the set $A(X)\times F(X)\iso A\times F(X)$.
        Alternatively, since $\smset^\smset$ is completely distributive by \cref{prop.set_endofunc_distrib}, the result also follows from \eqref{eqn.cat_completely_distributive}, with $I\coloneqq\2, J(1)\coloneqq A, J(2)\coloneqq\1, X(1,a)\coloneqq\1$ (the constant functor) for $a\in A$, and $X(2,1)\coloneqq F$:
        \[
        AF \iso
        \left(\sum_{a\in A}\1\right)F \iso
        \prod_{i\in\2}\sum_{j\in J(i)}X(i,j)
        \iso
        \sum_{\ol{j}\in\prod_{i\in\2}J(i)}\prod_{i\in\2}X(i,\ol{j}(i))
        \iso
        \sum_{a\in A}\1\times F
        \iso
        \sum_{a\in A}F.
        \]
        Here we used the fact that $A\iso\sum_{a\in A}\1$ from \cref{exc.on_sums_prods_sets} \cref{exc.on_sums_prods_sets.sum} (there we proved the statement for sets, but the same statement for the corresponding constant set-valued functors follows immediately).
    \end{solution}
\end{exercise}

Similarly, we may wish to write an $A$-indexed product of copies of $F$ in exponential form as $F^A$.
But since we have already introduced exponential notation for representable functors, this yields two possible interpretations for the functor $\smset\to\smset$ denoted by $\yon^A$: as the functor represented by $A$, or as the $A$-indexed product of copies of the identity functor $\yon\colon\smset\to\smset$.
In fact, the following exercise shows that there is no ambiguity, as the two interpretations are isomorphic.

\begin{exercise}
    \begin{enumerate}
        \item Show that for a set $I\in\smset$, an $I$-indexed product of copies of the identity functor $\yon\colon\smset\to\smset$ is isomorphic to the functor $\yon^I\colon\smset\to\smset$ represented by $I$:
        \[
        \prod_{i\in I}\yon\iso\yon^I.
        \]
        (This is analogous to the fact that multiplying $n$ copies of a number together is equal to raising that same number to the power of $n$.)
        \item Show that the $I$-indexed product of copies of a representable functor $\yon^A\colon\smset\to\smset$ for some $A\in\smset$ is isomorphic to the functor $\yon^{IA}\colon\smset\to\smset$ represented by the product set $IA$:
        \[
        \prod_{i\in I}\yon^A\iso\yon^{IA}.
        \]
        (Hint: You may use the fact that following natural isomorphism holds between sets of functions:
        \[
        \{ f \colon I\times A\to X \} \iso \{ g \colon I\to X^A \}.
        \]
        The process of converting a function $f$ in the left hand set to the corresponding function $i\mapsto(a\mapsto f(i,a))$ in the right is known as \emph{currying}.) \qedhere
    \end{enumerate}
    \begin{solution}
        \begin{enumerate}
            \item It suffices to show that for all $X\in\smset$, there is an isomorphism
            \[
            \prod_{i\in I} \yon(X) \iso \yon^I(X).
            \]
            natural in $X$.
            We have that $\yon(X)\iso X$ and that $\yon^I(X)\iso X^I$.
            So both sides are naturally isomorphic to the set of functions $I\to X$.

            \item It suffices to show that for all $X\in\smset$, there is an isomorphism
            \[
            \prod_{i\in I} \yon^A(X) \iso \yon^{IA}(X).
            \]
            natural in $X$.
            We have that $\yon^A(X)\iso X^A$, so $\prod_{i\in I}\yon^A(X)\iso(X^A)^I$, and that $\yon^{IA}(X)\iso X^{IA}$.
            By currying, both sides are naturally isomorphic to the set of functions $I\times A\to X$.
        \end{enumerate}
    \end{solution}
\end{exercise}

Henceforth, given $A\in\smset$ and a functor $F\colon\smset\to\smset$, we define
\[
F^A\coloneqq\prod_{a\in A}F.
\]
The exercise above shows that this notation does not conflict with the way we write representable functors as powers of the identity functor $\yon$.
The exercise also shows how a power of a representable functor can be simplified to a single representable functor.

With these ingredients, we are finally ready to define what a polynomial functor is.
We will begin with this definition in the next chapter.

%-------- Section --------%
\section{Summary and further reading}

% TODO: fill in summary

%-------- Section --------%
\section{Exercise solutions}
\Closesolutionfile{solutions}
{\footnotesize
    \input{solution-file1}}

\Opensolutionfile{solutions}[solution-file2]

%------------ Chapter ------------%
\chapter{Polynomial functors} \label{ch.poly.obj}

In this chapter, we will formally introduce our objects of study: polynomial functors.
In addition to the set-theoretic perspective, we will present several more concrete ways to think about polynomials to aid intuition that we will use throughout the rest of this book.
We keep the mathematical content of this chapter fairly light, preferring to solidify our conceptual understanding of polynomials, before advancing to deeper categorical content.

%-------- Section --------%
\section{Introducing polynomial functors} \label{sec.poly.obj.intro}

\begin{definition}[Polynomial functor]
    A \emph{polynomial functor} (or simply \emph{polynomial}) is a functor $p\colon\smset\to\smset$ such that there exists a set $I$, an $I$-indexed family of sets $(p[i])_{i\in I}$, and an isomorphism
    \[
    p\iso\sum_{i\in I}\yon^{p[i]}
    \]
    to the corresponding $I$-indexed sum of representables.
\end{definition}

So, up to isomorphism, a polynomial functor is just a sum of representables.

\begin{remark}
    Given sets $I, A \in \smset$, it follows from \cref{exc.repeated_sum_is_product} that we have an isomorphism of polynomials
    \[
    \sum_{i \in I} \yon^A \iso I\yon^A.
    \]
    So when we write down a polynomial, we will often combine identical representable summands $\yon^A$ by writing them in the form $I\yon^A$.
    In particular, the constant functor $\1$ is a representable functor ($\1 \iso \yon^\0$), so every constant functor $I$ is a polynomial functor: $I \iso \sum_{i \in I} \1$.
\end{remark}

\begin{exercise}
    Consider the polynomial $q\coloneqq\yon^\8+\4\yon$.
    \begin{enumerate}
        \item Does the polynomial $q$ have a representable summand $\yon^\2$?
        \item Does the polynomial $q$ have a representable summand $\yon$?
        \item Does the polynomial $q$ have a representable summand $\4\yon$?
        \qedhere
    \end{enumerate}
    \begin{solution}
    \begin{enumerate}
        \item No, $q$ does not have $\yon^\2$ as a representable summand.
        \item Yes, $q$ does have $\yon$ as a representable summand.
        \item No, $q$ does not have $\4\yon$ as a representable summand, because $\4\yon$ is not a representable functor!
        But to make amends, we could say that $\4\yon$ is a \emph{summand}; this means that there is some $q'$ such that $q\iso q'+\4\yon$, namely $q'\coloneqq\yon^8$. So $\3\yon$ is also a summand, but $\yon^\2$ and $\5\yon$ are not.
    \end{enumerate}
    \end{solution}
\end{exercise}

\begin{example}\label{ex.verbose_poly_eval}
    Consider the polynomial $p\coloneqq\yon^\2+\2\yon+\1$.
    It denotes a functor $\smset\to\smset$; where does this functor send the set $X\coloneqq\{a,b\}$?
    To be precise, we will rather verbosely say that $I\coloneqq\4$ and
    \[
    p[1]\coloneqq\2,\quad
    p[2]\coloneqq\1,\quad
    p[3]\coloneqq\1,\qqand
    p[4]\coloneqq\0\
    \]
    so that $p\cong\sum_{i\in I}\yon^{p[i]}$. Now we have
    \begin{align*}
        p(X) &\iso
        \sum_{i\in\4}\{a,b\}^{p[i]} \\ &=
        \{a,b\}^\2 + \{a,b\}^\1 + \{a,b\}^\1 + \{a,b\}^\0 \\ &\iso
        \{(1,(a,a)),(1,(a,b)),(1,(b,a)),(1,(b,b)),(2,(a)),(2,(b)),(3,(a)),(3,(b)),(4,())\}.
    \end{align*}
    Above, we denote each function $f\colon p[i]\to\{a,b\}$ in the set $\{a,b\}^{p[i]}$ by the $n$-tuple $(f(1),\ldots,f(n))$ whenever $p[i]\coloneqq\ord{n}$.
    For ease of reading, we may drop the parentheses around these $n$-tuples to obtain the equivalent set
    \[
    p(X) \iso
    \{(1,a,a),(1,a,b),(1,b,a),(1,b,b),(2,a),(2,b),(3,a),(3,b),(4)\}.
    \]
    As we might expect, the set $p(X)$ contains $2^2+2+2+1=9$ elements, equal to the value obtained when we plug $|X|=2$ into the original polynomial $p$ when we interpret its coefficients and exponents as numbers instead of sets.
\end{example}

In general, a polynomial $p\coloneqq\sum_{i\in I}\yon^{p[i]}$ applied to a set $X$ expands to
\[
\sum_{i\in I}X^{p[i]}
\]
and can be thought of as the set of all pairs comprised of an element of $I$ and a function $p[i]\to X$ or, equivalently, a $p[i]$-tuple of elements of $X$.

\begin{exercise}
    In the verbose style of \cref{ex.verbose_poly_eval}, write out all the elements of $p(X)$ for $p$ and $X$ as follows (if there are infinitely many, denote the set $p(X)$ some other way):
    \begin{enumerate}
        \item $p\coloneqq\yon^\3$ and $X\coloneqq\{4,9\}.$
        \item $p\coloneqq\3\yon^\2+\1$ and $X\coloneqq\{a\}$.
        \item $p\coloneqq\0$ and $X\coloneqq\nn$.
        \item $p\coloneqq\4$ and $X\coloneqq\nn$.
        \item $p\coloneqq\yon$ and $X\coloneqq\nn$.
        \qedhere
    \end{enumerate}
    \begin{solution}
        \begin{enumerate}
            \item Let $I\coloneqq\1$ and $p[1]\coloneqq\3$ so that $p\coloneqq\yon^\3\iso\sum_{i\in I}\yon^{p[i]}$.
            Then
            \[
            p(X)\iso\{(1, 4, 4, 4), (1, 4, 4, 9), (1, 4, 9, 4), (1, 4, 9, 9), (1, 9, 4, 4), (1, 9, 4, 9), (1, 9, 9, 4), (1, 9, 9, 9)\}.
            \]

            \item Let $I\coloneqq\4$, $p[1]\coloneqq p[2]\coloneqq p[3]\coloneqq\2$, and $p[4]\coloneqq\1$, so that $p\coloneqq\3\yon^\2+\1\iso\sum_{i\in I}\yon^{p[i]}$.
            Then $p(X)\iso\{(1,a,a),(2,a,a),(3,a,a),(4)\}$.

            \item Let $I\coloneqq\0$ so that $p\coloneqq\0\iso\sum_{i\in I}\yon^{p[i]}$.
            Then $p(X)\iso\0$.
            Alternatively, note that $\0$ is the constant functor that sends every set to $\0$.

            \item Let $I\coloneqq\4$ and $p[i]\coloneqq\0$ for every $i\in I$ so that $p\coloneqq\4\iso\sum_{i\in I}\yon^{p[i]}$.
            Then $p(X)\iso\{(1), (2), (3), (4)\}\iso\4$.
            Alternatively, note that $\4$ is the constant functor that sends every set to $\4$.

            \item Let $I \coloneqq \1$ and $p[1] \coloneqq \1$ so that $p\coloneqq\yon\iso\sum_{i\in I}\yon^{p[i]}$.
            So $p(X)\iso\{(1,n)\mid n\in\nn\}\iso\nn$.
            Alternatively, note that $\yon$ is the identity functor, so it sends $\nn$ to itself.
        \end{enumerate}
    \end{solution}
\end{exercise}

The following proposition shows how the polynomial functor $p$ itself determines the set $I$ over which we sum up representables to obtain $p$.

\begin{proposition}\label{prop.apply1}
    Let $p\coloneqq\sum_{i\in I}\yon^{p[i]}$ be an arbitrary polynomial functor. Then $I\cong p(\1)$, so there is an isomorphism of functors
    \begin{equation}\label{eqn.sum_p1}
        p\iso\sum_{i\in p(\1)}\yon^{p[i]}.
    \end{equation}
\end{proposition}
\begin{proof}
    We need to show that $I\iso p(\1)$; the latter claim follows directly.
    In \cref{exc.on_sums_prods_sets} \cref{exc.on_sums_prods_sets.sum}, we showed that $I\iso\sum_{i\in I}\1$, so it suffices to show that $(\yon^{p[i]})(\1)\iso\1$ for all $i \in I$.
    Indeed, $\1^{p[i]}\iso \1$ because there is a unique function $p[i]\to \1$ for each $p[i]$.
\end{proof}
We can draw an analogy between \cref{prop.apply1} and evaluating $p(1)$ for a polynomial $p$ from high school algebra, which yields the sum of the coefficients of $p$.
The notation in \eqref{eqn.sum_p1} will be how we denote arbitrary polynomials from now on, and we will use the following terms to denote the sets $p(\1)$ and $p[i]$ for $i\in p(\1)$ on which a polynomial $p$ depends.

\begin{definition}[Position and direction]
    Given a polynomial functor
    \[
    p\iso\sum_{i\in p(\1)}\yon^{p[i]},
    \]
    we call an element $i\in p(\1)$ a \emph{position of $p$} or a \emph{$p$-position}, and we call an element $a\in p[i]$ a \emph{direction of $p$ at $i$} or a \emph{$p[i]$-direction}.
    We call $p(\1)$ the \emph{position-set of $p$} and $p[i]$ the \emph{direction-set of $p$ at $i$}.
\end{definition}

Note that the position-set $p(\1)$ along with the $p(\1)$-indexed family of direction-sets $p[-]\colon p(\1)\to\smset$ uniquely characterize a polynomial $p$ up to isomorphism.
Throughout this book, we will often specify a polynomial by giving its positions and its directions at each position.

\begin{exercise}\label{exc.apply0}
    We saw in \cref{prop.apply1} how to interpret the position-set $p(\1)$ of a polynomial $p$, e.g.\ $p\coloneqq\yon^\3+\3\yon^\2+\4$, as the sum of the coefficients of $p$: here $p(\1)\iso\1+\3+\4\iso\8$.
    How might you interpret $p(\0)$?
    \begin{solution}
        We consider $p(\0)$ for arbitrary polynomials $p$.
        A representable functor $\yon^S$ for $S\in\smset$ sends $\0\mapsto\0$ if $S\neq\0$ (as there are then no functions $S\to\0$), but sends $\0\mapsto\1$ if $S=\0$ (as there is a unique function $\0\to\0$).
        So
        \[
        p(\0)\iso\sum_{i\in p(\1)}\left(\yon^{p[i]}\right)(\0)\iso\sum_{\substack{i\in p(\1),\\ p[i]\neq\0}}\0+\sum_{\substack{i\in p(\1),\\ p[i]=\0}}\1\iso\{i\in p(\1)\mid p[i]=\0\}.
        \]
        That is, $p(\0)$ is the set of \emph{constant} positions of $p$.
        For example, if $p\coloneqq\yon^\3+\3\yon^\2+\4$, then $p(\0)=\4$.
        In the language of high school algebra, we might call $p(\0)$ the \emph{constant term} of $p$.
    \end{solution}
\end{exercise}

As a functor $\smset\to\smset$, a polynomial should act on functions as well as on sets.
Below, we explain how.

\begin{proposition} \label{prop.poly_on_functions}
    Let $p$ be an arbitrary polynomial functor, which our notation lets us write as $p\iso\sum_{i\in p(\1)} \yon^{p[i]}$, and let $f\colon X\to Y$ be an arbitrary function.
    Then $p(f)\colon p(X)\to p(Y)$ sends each $(i, g)\in p(X)$, with $i\in p(\1)$ and $g\colon p[i]\to X$, to $(i, g\then f)$ in $p(Y)$.
\end{proposition}
\begin{proof}
    For each $i\in p(\1)$, by \cref{def.representable}, the functor $\yon^{p[i]}$ sends $f$ to the function $X^{p[i]}\to Y^{p[i]}$ mapping each $g\colon p[i]\to X$ to $g\then f\colon p[i]\to Y$.
    So the sum of these functors over $i\in p(\1)$ sends each $(i, g)\in p(X)$ to $(i, g\then f)\in p(Y)$.
\end{proof}

\begin{example}
    Suppose $p\coloneqq\yon^\2+\2\yon+1$. Let $X\coloneqq\{a_1,a_2,b_1\}$ and $Y\coloneqq\{a,b,c\}$, and let $f\colon X\to Y$ be the function sending $a_1,a_2\mapsto a$ and $b_1\mapsto b$. The induced function $p(f)\colon p(X)\to p(Y)$, according to \cref{prop.poly_on_functions}, is shown below:
    \[
    \begin{tikzcd}[row sep=2pt, column sep=3pt, shorten <=-5pt, shorten >=-5pt, dashed]
        \LMO{(1,a_1,a_1)}\ar[rrrr, blue, bend left=25pt]&\LMO{(1,a_1,a_2)}\ar[rrr, blue, bend left=25pt]&\LMO{(1,a_1,b_1)}\ar[rrr, blue, bend left=25pt]&[30pt]&
        \LMO{(1,a,a)}&\LMO{(1,a,b)}&\LMO{(1,a,c)}&&
        \\
        \LMO{(1,a_2,a_1)}\ar[rrrru, blue, bend left=10pt]&\LMO{(1,a_2,a_2)}\ar[rrru, blue, bend left=10pt]&\LMO{(1,a_2,b_1)}\ar[rrru, blue, bend left=10pt]&&
        \LMO{(1,b,a)}&\LMO{(1,b,b)}&\LMO{(1,b,c)}&&
        \\
        \LMO{(1,b_1,a_1)}\ar[rrrru, blue, bend left=10pt]&\LMO{(1,b_1,a_2)}\ar[rrru, blue, bend left=10pt]&\LMO{(1,b_1,b_1)}\ar[rrru, blue, bend left=10pt]&&
        \LMO{(1,c,a)}&\LMO{(1,c,b)}&\LMO{(1,c,c)}&&
        \\
        \LMO{(2,a_1)}\ar[rrrr, blue, bend left=25pt]&\LMO{(2,a_2)}\ar[rrr, blue, bend left=25pt]&\LMO{(2,b_1)}\ar[rrr, blue, bend left=25pt]&&
        \LMO{(2,a)}&\LMO{(2,b)}&\LMO{(2,c)}&&
        \\
        \LMO{(3,a_1)}\ar[rrrr, blue, bend left=25pt]&\LMO{(3,a_2)}\ar[rrr, blue, bend left=25pt]&\LMO{(3,b_1)}\ar[rrr, blue, bend left=25pt]&&
        \LMO{(3,a)}&\LMO{(3,b)}&\LMO{(3,c)}&&
        \\&
        \LMO{(4)}\ar[rrrr, blue]&&&&\LMO{(4)}
    \end{tikzcd}
    \]
\end{example}

\begin{exercise}
    Let $p\coloneqq\yon^\2+\yon$. Choose a function $f\colon\1\to\2$ and write out the induced function $p(f)\colon p(\1)\to p(\2)$.
    \begin{solution}
        We have
        \[
        p(\1) \iso \{(1, 1, 1), (2, 1)\} \qqand p(\2) \iso \{(1, 1, 1), (1, 1, 2), (1, 2, 1), (1, 2, 2), (2, 1), (2, 2)\}.
        \]
        Say we choose the function $f\colon\1\to\2$ that sends $1 \mapsto 1$.
        Then $p(f)$ would send $(1, 1, 1) \mapsto (1, 1, 1)$ and $(2, 1) \mapsto (2, 1)$.
        If we had instead picked $1 \mapsto 2$ as our function $f$, then $p(f)$ would send $(1, 1, 1) \mapsto (1, 2, 2)$ and $(2, 1) \mapsto (2, 2)$.
    \end{solution}
\end{exercise}

%-------- Section --------%
\section{Special classes of polynomial functors} \label{sec.poly.obj.spec}

Here we describe several special classes of polynomials.
We have already defined two such special classes: \emph{representables} and \emph{constants}.
A \emph{representable polynomial} (or simply a \emph{representable}) is a representable functor, i.e.\ a polynomial functor isomorphic to $\yon^A$ for some set $A$.
Meanwhile, a \emph{constant polynomial} (or simply a \emph{constant}) is a constant functor, i.e.\ a polynomial functor isomorphic to $I$, interpreted as a functor, for some set $I$.

\begin{exercise}
\begin{enumerate}
  \item Characterize when a polynomial $p$ is \textit{representable} in terms of its positions and/or its directions.
  \item Characterize when a polynomial $p$ is \textit{constant} in terms of its positions and/or its directions. \qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
  \item A polynomial $p$ is representable when $p\iso\yon^A$ for some set $A$, and $\yon^A$ has exactly $1$ position.
  Conversely, if a polynomial $p$ has exactly $1$ position, then $p(\1)\iso\1$, so we may write $p$ (up to isomorphism) as $p\iso\sum_{1\in\1}\yon^{p[1]}\iso\yon^{p[1]}$, which is representable.
  So a polynomial $p$ is representable if and only if it has exactly $1$ position.

  \item A polynomial $p$ is constant when $p\iso I$ for some set $I$, and $I$ has no directions at any of its positions.
  Conversely, if every direction-set of a polynomial $p$ is empty, then
  \[
    p\iso\sum_{i\in p(\1)}\yon^{p[i]}\iso\sum_{i\in p(\1)}\yon^\0\iso\sum_{i\in p(\1)}\1\iso p(\1),
  \]
  i.e.\ the set $p(\1)$ viewed as a constant functor.
  So a polynomial $p$ is constant if and only if it has exactly $0$ directions at each position.
\end{enumerate}
\end{solution}
\end{exercise}

Like constants, the other two special classes of polynomials we define here will share their names with their algebraic analogues.
Throughout, let $p\iso\sum_{i\in p(\1)}\yon^{p[i]}$ be a polynomial functor.

\begin{definition}[Linear]
We say that $p$ is \emph{linear}\footnote{Unlike linear polynomials from high school algebra (which are really \emph{affine linear functions} rather than necessarily \emph{linear functions}), our linear polynomial functors have no (nonzero) constant terms: they always send $\0$ to $\0$.}
if $p\iso I\yon$ for some set $I$.
\end{definition}

\begin{definition}[Monomial]
We say that $p$ is a \emph{monomial} if $p\iso I\yon^A$ for sets $I$ and $A$.
\end{definition}

\begin{example}
  Every constant polynomial $I\iso I\yon^\0$ is a monomial, as is every linear polynomial $I\yon\iso I\yon^\1$ and every representable $\yon^A \iso \1\yon^A$.
  On the other hand, there are monomials that are neither constant, linear, nor representable, such as $\2\yon^\2$ or $\nn\yon^\rr$.
  Moreover, there are polynomials that are not monomials, such as $\yon^\4+\3$ or $\sum_{n\in\nn}\yon^{\ord{n}}$.

  There is only one polynomial that is both constant and linear, namely $\0\iso\0\yon$.
  Similarly, there is only one polynomial (up to isomorphism) that is both constant and representable, namely $\1\iso\yon^\0$.
  Finally, there is only one polynomial (up to isomorphism) that is both linear and representable, namely the identity functor $\1\yon\iso\yon\iso\yon^\1$.

  In general, every set $S$ has a corresponding constant polynomial $S$, linear polynomial $S\yon$, and representable polynomial $\yon^S$; and as long as $|S|\geq2$, these are all distinct.
\end{example}

\begin{exercise}
  \begin{enumerate}
    \item Characterize when a polynomial $p$ is \textit{linear} in terms of its positions and/or its directions.
    \item Characterize when a polynomial $p$ is a \textit{monomial} in terms of its positions and/or its directions. \qedhere
  \end{enumerate}
  \begin{solution}
    \begin{enumerate}
      \item A polynomial $p$ is linear when $p\iso I\yon$ for some set $I$, and $I\yon$ has exactly $1$ direction at each position. (Note that this is even true when $p\iso\0\yon\iso\0$, for then it is true vacuously.)
      Conversely, if a polynomial $p$ has exactly $1$ direction at each position, then $p[i]\iso\1$ for all $i\in p(\1)$, so
      \[
        p
          \iso
        \sum_{i\in p(\1)}\yon^{p[i]}
          \iso
        \sum_{i\in p(\1)}\yon^\1
          \iso
        \sum_{i\in p(\1)}\yon
          \iso
        p(\1)\yon,
      \]
      which is linear.
      So a polynomial $p$ is linear if and only if it has exactly $1$ direction at each position.

      \item A polynomial $p$ is a monomial when $p\iso I\yon^A$ for sets $I$ and $A$, implying that there is an isomorphism of direction-sets $p[i]\iso A\iso p[j]$ for all $p$-positions $i$ and $j$ (i.e.\ all the direction-sets of $p$ have the same cardinality).
      Conversely, if all the direction-sets of a polynomial $p$ are isomorphic to each other, then they are all isomorphic to some set $A$, so we have
      \[
      p
      \iso
      \sum_{i\in p(\1)}\yon^{p[i]}
      \iso
      \sum_{i\in p(\1)}\yon^A
      \iso
      p(\1)\yon^A,
      \]
      which is a monomial.
      So a polynomial $p$ is a monomial if and only if all of its direction-sets have the same cardinality.
    \end{enumerate}
  \end{solution}
\end{exercise}

Later on in \cref{sec.poly.bonus.adj}, we will see how all four of these special classes of polynomials arise from various adjunctions.

%-------- Section --------%
\section{Interpreting positions and directions}

Let us make an informal digression on how we will think about positions and directions of polynomials in this book.
While this section has little mathematical content, the intuition we build here will guide us as we delve into the deeper theory of polynomials and their applications to modeling interaction.

The main idea is that a \emph{position} is some status that may be held, while the \emph{directions} at each position are the options available when holding that status.
While these positions and directions may be imagined abstractly, here we give some concrete examples.

\begin{example}[Directions as menu options] \label{ex.reps-as-menus}
    Consider a representable and thus polynomial functor $\yon^A$ for a set $A$.
    It has $1$ position and the elements of $A$ as its directions.
    We may think of $A$ as a menu of options to choose from.

    The menu may consist of dinner options available at a wedding; then the corresponding representable functor could be
    \[
        \yon^{\{\text{`chicken', `beef', `vegetarian'}\}};
    \]
    or it may be the menu of a text editor, in which case the representable could be
    \[
        \yon^{\{\text{`cut', `copy', `paste'}\}}.
    \]
    In both these cases, there are exactly $3$ directions, so there is an isomorphism of representable functors
    \[
        \yon^{\{\text{`chicken', `beef', `vegetarian'}\}} \iso \yon^{\text{`cut', `copy', `paste'}\}}.
    \]

    Similarly, we may interpret the representable $\yon^\2$ as a $2$-option menu.
    Such menus are ubiquitous in life: yes or no, true or false, heads or tails, 0 or 1.
    A $1$-option menu, represented by $\yon^\1\iso\yon$, is also familiar as an unavoidable choice, the only option: ``sorry, ya just gotta go through it.''
    Having no options, represented by $\yon^\0\iso\1$, is when you actually don't get through it: an impossible decision, a ``dead end.''

    In contrast, we may interpret the representable $\yon^{[0,1]}$ as a menu with an infinite range of options: a slider with one end labeled $0$ and the other labeled $1$, able to take on any value in between.
\end{example}

For consistency, we will favor the term ``direction'' over ``option'' when referring to the elements of $A$ for a summand $\yon^A$ of a polynomial.
Nevertheless, when we think of a polynomial's directions, we will often think of them as options to choose from.

\cref{ex.reps-as-menus} shows how we may interpret the directions of a single representable summand as options in a menu.
By having multiple representable summands---one for each position---a polynomial may capture more general scenarios with a range of possible menus.

\begin{example}[A coin jar] \label{ex.coin-jar}
    Consider a coin jar with a slot that may be open or closed.
    When the slot is open, the jar may accept a penny, a nickel, a dime, or a quarter---there are $4$ options to choose from.
    When the slot is closed, the jar may not accept any coins at all---there are $0$ options.
    We may model this scenario with the polynomial
    \[
        \yon^{\{\text{`penny', `nickel', `dime', `quarter'}\}}+\yon^\0 \iso \yon^\4+\1.
    \]
    This polynomial has $2$ positions, corresponding to the two statuses the slot could take: open or closed.
    To delineate these positions, we could take advantage of the fact that every singleton set is isomorphic to $\1$ and that $\1\yon^A\iso\yon^A$ to rewrite the above polynomial as
    \[
        \{\text{`open'}\}\yon^{\{\text{`penny', `nickel', `dime', `quarter'}\}}+\{\text{`closed'}\}\yon^\0 \iso \yon^\4+\1.
    \]
\end{example}

\begin{exercise}
    Give another example of a real-world scenario that may be modeled by a polynomial with more than $1$ position.
\begin{solution}
    The stopwatch app on my phone has three positions: a `zero' position, from which I may tap a single `start' button; a `running' position, from which I may tap either a `lap' button or a `stop' button; and a `stopped' position, from which I may tap either a `start' button or a `reset' button. Thinking of the buttons available to press as the directions at each position, the corresponding polynomial is
    \[
        \{\text{`zero'}\}\yon^{\{\text{`start'}\}}+\{\text{`running'}\}\yon^{\{\text{`lap', `stop'}\}}+\{\text{`stopped'}\}\yon^{\{\text{`start', `reset'}\}}\iso\yon+\2\yon^\2.
    \]
\end{solution}
\end{exercise}

%-------- Section --------%
\section{Corolla forests}

We would like to have graphical depictions of our polynomials to make them easy to visualize.
These will take the form of special graphs known as \emph{corolla forests}.
We build up to defining them as follows.

Our first definition will be familiar to students of graph theory, although we will add some technical details suited to our purposes.
\begin{definition}[Rooted tree]
    A \emph{rooted tree} is a nonempty connected acyclic graph with a distinguished vertex called the \emph{root}.

    We allow infinitely and even uncountably many vertices and infinitely and even uncountably many edges incident to each vertex; on the other hand, each pair of vertices is connected by a (necessarily unique) path of finitely many adjacent edges.
\end{definition}
Since all our trees will be rooted, we may refer to them simply as \emph{trees}---roots are implied.
We will draw our trees with roots at the bottom and other vertices ``growing'' upward.

The following terminology will be handy when working with our trees; these terms should be familiar, or at the very least they should match your intuition.
\begin{definition}[Rooted path; height]
    A \emph{rooted path} is a path in a rooted tree from its root to any vertex.

    Given a vertex of a rooted tree, the length of (i.e.\ number of edges in) the rooted path to that vertex is the \emph{height} of that vertex.
\end{definition}
In any rooted tree, the root has height $0$; every neighbor of the root has height $1$; every neighbor of a vertex of height $1$ either is the root or has height $2$; and so forth.

Now we can define a special kind of tree that we will use to depict representable functors.
\begin{definition}[Corolla]
    A \emph{corolla} is a rooted tree in which every vertex aside from the root has height $1$. We call these vertices the \emph{leaves} of the corolla.

    The \emph{corolla associated to a representable functor $\yon^A$} for $A\in\smset$ is the corolla whose leaves are in bijection with $A$.
\end{definition}

\begin{example}
    Here are the corollas associated to various representables:
    \begin{equation*}
    \begin{tikzpicture}[trees, sibling distance=2mm]
        \node["$\yon^{\1}\iso\yon$" below] (1) {$\bullet$}
        child;
    \end{tikzpicture}
    \qquad
    \begin{tikzpicture}[trees, sibling distance=2mm]
        \node["$\yon^{\5}$" below] (1) {$\bullet$}
        child foreach \i in {1,...,5}
        ;
    \end{tikzpicture}
    \qquad
    \begin{tikzpicture}[trees, sibling distance=1mm]
        \node["$\yon^{\1\0}$" below] (1) {$\bullet$}
        child foreach \i in {1,...,10}
        ;
    \end{tikzpicture}
    \qquad
    \begin{tikzpicture}[trees, sibling distance=0.5mm]
        \node["$\yon^{\2\0}$" below] (1) {$\bullet$}
        child foreach \i in {1,...,20}
        ;
    \end{tikzpicture}
    \qquad
    \begin{tikzpicture}[trees, sibling distance=0.25mm]
        \node["$\yon^{\4\0}$" below] (1) {$\bullet$}
        child foreach \i in {1,...,40}
        ;
    \end{tikzpicture}
    \qquad
    \begin{tikzpicture}[trees, sibling distance=0.0625mm]
        \node["$\yon^{[0,1]}$" below] (1) {$\bullet$}
        child foreach \i in {1,...,160}
        ;
    \end{tikzpicture}
\end{equation*}
\end{example}

In the example above, the roots are indicated by dots ($\bullet$), and the leaves are indicated by arrows ($\uparrow$).
Because the direction-set of the representable is in bijection with the leaves of the associated corolla, we can think of each leaf as a direction, so it makes sense to draw the leaves as arrows pointing in different directions.
Thinking of directions as menu options like in the previous section, we may view these corollas as mini-decision trees, indicating all the possible options we could select.

\begin{example}
    The corolla associated to $\yon^\0\iso\1$ has \textit{no} leaves: it is the rooted tree consisting of one vertex---the root---and no edges.
    \begin{equation*}
        \begin{tikzpicture}[trees, sibling distance=2mm]
            \node["$\yon^\0\iso\1$" below] (1) {$\bullet$};
        \end{tikzpicture}
    \end{equation*}
    By definition, the root itself is \textit{not} a leaf, so the corolla above does in fact have $0$ leaves.
    With no arrows pointing out, it is the corolla associated to a representable with no directions.
\end{example}

As each representable functor has an associated corolla, each polynomial functor will have an associated disjoint union of corollas that we call a \emph{corolla forest}.

\begin{definition}[Corolla forest]
    A \emph{corolla forest} is a disjoint union of corollas, i.e.\ a graph whose connected components are all corollas (with distinguished roots).

    The \emph{corolla forest associated to a polynomial functor} $p\iso\sum_{i\in p(\1)}\yon^{p[i]}$ is the disjoint union of the corollas associated to each representable summand $\yon^{p[i]}$ of $p$.
    When we draw the corolla forest associated to $p$, we may say that we are \emph{drawing $p$ as a (corolla) forest}.
    We call the corollas in this forest corresponding to $p$-positions \emph{$p$-corollas} and the leaves corresponding to $p[i]$-directions \emph{$p[i]$-leaves}.
\end{definition}

\begin{example} \label{ex.corolla-forest}
    We may draw $p\coloneqq\yon^\2+\2\yon+\1$ as a forest like so:
    \begin{equation} \label{pic.forest-example}
    \begin{tikzpicture}[trees]
        \node (1) {$\bullet$}
        child {}
        child {};
        \node[right=.5 of 1] (2) {$\bullet$}
        child {};
        \node[right=.5 of 2] (3) {$\bullet$}
        child {};
        \node[right=.5 of 3] (4) {$\bullet$};
    \end{tikzpicture}
    \end{equation}
    Each of the $4$ corollas in \eqref{pic.forest-example} corresponds to one of the $4$ representable summands of $p$.
    The $4$ roots in \eqref{pic.forest-example} correspond to the $4$ positions of $p$, and the leaves connected to each root correspond to the directions at each position.
    Note that $p$ has $1$ position with $2$ directions, $2$ positions with $1$ direction each, and $1$ position with $0$ directions.
    Hence \eqref{pic.forest-example} is the disjoint union of $1$ corolla with $2$ leaves, $2$ corollas with $1$ leaf each, and $1$ corolla with $0$ leaves.

    Since $p(\1)\iso\4$, we could label the positions of $p$ with the elements of $\4=\{1,2,3,4\}$ so that
    \[
        p[1] = \2, \qquad p[2] = \1, \qquad p[3] = \1, \qquad p[4] = \0.
    \]
    Then we could give these same labels to the roots in \eqref{pic.forest-example}:
    \[
    \begin{tikzpicture}[trees]
        \node["$1$" below] (1) {$\bullet$}
        child {}
        child {};
        \node["$2$" below, right=.5 of 1] (2) {$\bullet$}
        child {};
        \node["$3$" below, right=.5 of 2] (3) {$\bullet$}
        child {};
        \node["$4$" below, right=.5 of 3] (4) {$\bullet$};
    \end{tikzpicture}
    \]
    Similarly, we could label the directions and their corresponding leaves, but we will reserve leaf labels for another purpose.
\end{example}

\begin{exercise}
    Consider the polynomial $p\coloneqq\2\yon^\3+\2\yon+\1$.
    \begin{enumerate}
        \item Draw $p$ as a corolla forest.
        \item How many roots does this forest have?
        \item How many positions of $p$ do these roots represent?
        \item For each $p$-corolla, say how many leaves it has.
        \item For each $p$-position, say how many directions it has. \qedhere
    \end{enumerate}
    \begin{solution}
        \begin{enumerate}
            \item Here is the corolla forest associated to $p\coloneqq\2\yon^\3+\2\yon+\1$ (note that the order in which the corollas are drawn does not matter):
            \[
            \begin{tikzpicture}[trees, sibling distance=3mm]
                \node (1) {$\bullet$}
                child {}
                child {}
                child {};
                \node[right=.7 of 1] (2) {$\bullet$}
                child {}
                child {}
                child {};
                \node[right=.5 of 2] (3) {$\bullet$}
                child {};
                \node[right=.3 of 3] (4) {$\bullet$}
                child {};
                \node[right=.3 of 4] (5) {$\bullet$};
            \end{tikzpicture}
            \]
            \item The forest has $5$ roots.
            \item The roots represent the $5$ positions, one position per root.
            \item \label{sol.forest.leaves} The first and second corollas have $3$ leaves each, the third and fourth corollas have $1$ leaf each, and the fifth corolla has $0$ leaves.
            \item The directions at each position correspond to the leaves in each corolla, so just copy the answer from \cref{sol.forest.leaves}, replacing ``corolla'' with ``position'' and ``leaf'' with ``direction'': the first and second positions have $3$ directions each, the third and fourth positions have $1$ direction each, and the fifth position has $0$ directions.
        \end{enumerate}
    \end{solution}
\end{exercise}

The position-set or any of the direction-sets of a polynomial may be infinite.
This makes their associated corolla forests impossible to draw precisely, but they may be approximated.
We sketch the polynomial $\yon^\3+\nn\yon^{[0,1]}$ as a forest below.
\[%\label{eqn.represented_interval}
\begin{tikzpicture}[trees, sibling distance=0.0625mm]
\node (1) {$\bullet$}
child[sibling distance=3mm] foreach \i in {1,2,3}
;
\node[right=1 of 1] (2) {$\bullet$}
child foreach \i in {1,...,160}
;
\node[right=1 of 2] (3) {$\bullet$}
child foreach \i in {1,...,160}
;
\node[right=1 of 3] (4) {$\bullet$}
child foreach \i in {1,...,160}
;
\node[right=.7 of 4] (5) {$\cdots$};
\end{tikzpicture}
\]

\begin{exercise}
If you were a suitor choosing the corolla forest you love, aesthetically speaking, which would strike your interest? Answer by selecting the associated polynomial:
\begin{enumerate}
    \item $\yon^\2+\yon+\1$
    \item $\yon^\3+\3\yon^\2+\3\yon+\1$
    \item $\yon^\2$
    \item $\yon+\1$
    \item $(\nn\yon)^\nn$
    \item $S\yon^S$ for some set $S$
    \item $\yon^{\1\0\0}+\yon^\2+\3\yon$
    \item $\yon + \2\yon^\4 + \3\yon^\9 + \4\yon^{\1\6} + \cdots$
    \item Your polynomial's name $p$ here.
\end{enumerate}
Any reason for your choice? Draw a sketch of your forest.
\begin{solution}
    Aesthetically speaking, here is a polynomial that may be drawn as a beautiful corolla forest:
    \[
        p\coloneqq\yon^\0+\yon^\1+\yon^\2+\yon^\3+\cdots
    \]
    It is reminiscent (and formally related) to the notion of lists: if $A$ is any set, then $p(A)\iso A^\0+A^\1+A^\2+\cdots$ is the set $\lst(A)$ of lists (i.e.\ finite ordered sequences) with entries in $A$.
    Here is a picture of the lovely forest associated to $p$:
    \[
    \begin{tikzpicture}[trees, sibling distance=3mm]
        \node (1) {$\bullet$};
        \node[right=.3 of 1] (2) {$\bullet$}
        child {};
        \node[right=.4 of 2] (3) {$\bullet$}
        child {}
        child {};
        \node[right=.6 of 3] (4) {$\bullet$}
        child {}
        child {}
        child {};
        \node[right=.6 of 4] {$\cdots$};
    \end{tikzpicture}
    \]
\end{solution}
\end{exercise}

Corolla forests help us visualize the positions and directions of polynomials, and they will especially come in handy in the next chapter, when we describe the morphisms between our polynomials and how they interact with positions and directions.
They may also depict the elements of a polynomial functor applied to a given set, as follows.
We have seen that for a polynomial $p$ and a set $X$,
\[
    p(X) \iso \sum_{i\in p(\1)}X^{p[i]} \iso \{(i,f)\mid i\in p(\1), f\colon p[i]\to X\}.
\]
So an element of $p(X)$ is a $p$-position $i$ along with a function $f$ that maps each direction at $i$ to an element of $X$.
Equivalently, it is a $p$-corolla along with a function that maps each of its leaves to an element of $X$.
Then to draw an element $(i,f)\in p(X)$, we simply need to draw the $p$-corolla corresponding to $i$ and label its leaves with elements of $X$ according to $f$.

\begin{example}
    In \cref{ex.corolla-forest}, we drew $p\coloneqq\yon^\2+\2\yon+\1$ as a corolla forest like so:
    \[
    \begin{tikzpicture}[trees]
        \node["$1$" below] (1) {$\bullet$}
        child {}
        child {};
        \node["$2$" below, right=.5 of 1] (2) {$\bullet$}
        child {};
        \node["$3$" below, right=.5 of 2] (3) {$\bullet$}
        child {};
        \node["$4$" below, right=.5 of 3] (4) {$\bullet$};
    \end{tikzpicture}
    \]
    Previously, in \cref{ex.verbose_poly_eval}, we wrote out all $9$ elements of $p$ applied to the set $X\coloneqq\{a,b\}$ as tuples.
    We could draw them out instead---an element of $p(X)$ may be depicted as one of the four corollas above with each of its leaves labeled with an element of $X$:
    \[
    \begin{tikzpicture}[trees, sibling distance=5mm]
        \node["$1$" below] (1) {$\bullet$}
            child {node {$a$}}
            child {node {$a$}};
        \node["$1$" below, right=1.2 of 1] (2) {$\bullet$}
            child {node {$a$}}
            child {node {$b$}};
        \node["$1$" below, right=1.2 of 2] (3) {$\bullet$}
            child {node {$b$}}
            child {node {$a$}};
        \node["$1$" below, right=1.2 of 3] (4) {$\bullet$}
            child {node {$b$}}
            child {node {$b$}};
        \node["$2$" below, right=1.1 of 4] (5) {$\bullet$}
            child {node {$a$}};
        \node["$2$" below, right=of 5] (6) {$\bullet$}
            child {node {$b$}};
        \node["$3$" below, right=of 6] (7) {$\bullet$}
            child {node {$a$}};
        \node["$3$" below, right=of 7] (8) {$\bullet$}
            child {node {$b$}};
        \node["$4$" below, right=0.9 of 8] (9) {$\bullet$};
    \end{tikzpicture}
    \]
\end{example}

Throughout this book, we will generally use corolla forests to depict polynomials with relatively small numbers of positions or directions, where drawing out entire corolla forests is manageable.
Later, we will study how building larger rooted trees out of these corollas corresponds to conducting various categorical operations on our polynomials.

%% TODO: how much later?

%-------- Section --------%
\section{Polyboxes}

Before we conclude this chapter, we introduce one more tool for visualizing polynomials whose full power will not be evident until later.

Throughout this book, we may depict a polynomial $p$ as a pair of boxes stacked on top of each other, like so:
\[
  \begin{tikzpicture}[polybox, tos]
    \node[poly, "$p$" below] (p) {};
    \node[left=0pt of p_pos] {$p(\1)$};
    \node[left=0pt of p_dir] {$p[-]$};
  \end{tikzpicture}
\]
We call this picture the \emph{polyboxes for $p$}.
Think of these boxes as cells in a spreadsheet.
The bottom cell, or the \emph{position box}, is restricted to values in the set $p(\1)$ (as indicated by the label to its left)---it must be filled with a $p$-position, say $i\in p(\1)$:
\[
  \begin{tikzpicture}[polybox, tos]
    \node[poly, "$p$" below] (p) {\at$i$};
    \node[left=0pt of p_pos] {$p(\1)$};
    \node[left=0pt of p_dir] {$p[-]$};
  \end{tikzpicture}
\]
The top cell, or the \emph{direction box}, cannot be filled until the position box below it is.
Once the position box contains a $p$-position $i$, the direction box must be filled with a $p[i]$-direction, say $a\in p[i]$:
\[
  \begin{tikzpicture}[polybox, tos]
    \node[poly, "$p$" below] (p) {$a$\at$i$};
    \node[left=0pt of p_pos] {$p(\1)$};
    \node[left=0pt of p_dir] {$p[-]$};
  \end{tikzpicture}
\]
The $p[-]$ label to the left of the direction box reminds us that the $a$ within it is an element of $p[i]$, where $i$ is the entry in the position box.
Once we are accustomed to polyboxes, we will often drop these reminder labels, so that
\[
  \begin{tikzpicture}[polybox, tos]
    \node[poly, "$p$" below] (p) {$a$\at$i$};
  \end{tikzpicture}
\]
serves as a graphical shorthand for the statement ``consider a polynomial functor $p$ with position $i\in p(\1)$ and direction $a\in p[i]$.''

Viewing polynomials as these restricted two-cell spreadsheets reinforces the idea that directions are like menu options: imagine a dropdown menu for the direction box above a filled position box that lists all the directions to choose from at the given position.
Polyboxes also help us conceptualize the possible pairs of positions and directions of a polynomial whose corolla forest is impractical to draw, as suggested by the following example.

\begin{example}
  Consider the polynomial
  \[
    p\coloneqq\sum_{r\in\rr}\yon^{[-|r|,|r|]},
  \]
  whose positions are the real numbers and whose directions at position $r$ are the real numbers with magnitude at most $|r|$.
  There is no clear way to draw $p$ as a corolla forest, but we could draw its polyboxes
  \[
  \begin{tikzpicture}[polybox, tos]
    \node[poly, "$p$" below] (p) {$s$\at$r$};
    \node[left=0pt of p_pos] {$p(\1)$};
    \node[left=0pt of p_dir] {$p[-]$};
  \end{tikzpicture}
  \]
  with the condition that $r$ and $s$ are real numbers satisfying $|s|\leq|r|$.
\end{example}

We may also use polyboxes to highlight our special classes of polynomials.
When a position box may only be filled with one possible entry, we shade it in like so:
\[
\begin{tikzpicture}[polybox, tos]
  \node[poly, "$p\iso\yon^A$" below, pure] (p) {};
  \node[left=0pt of p_pos] {$p(\1)\iso\1$};
  \node[left=0pt of p_dir] {$A$};
\end{tikzpicture}
\]
The idea is that if there is only one entry that could fill a given box, then it should come pre-filled---no further choice needs to be made to fill it.
Here $p(\1)\iso\1$, so $p$ is \textit{representable}; indeed, $p\iso\yon^A$, where $A$ is the set of possible entries for the unfilled direction box.

Similarly, the polyboxes for a \textit{linear} polynomial $I\yon$, whose direction-set at each position is a singleton, can be drawn like so:
\[
\begin{tikzpicture}[polybox, tos]
  \node[poly, "$I\yon$" below, linear] (p) {};
  \node[left=0pt of p_pos] {$I$};
\end{tikzpicture}
\]
No matter what fills the position box, there is exactly one entry that could fill the direction box, so it comes pre-filled.
The identity polynomial functor $\yon$, which is both representable and linear, therefore has the following polyboxes:
\[
\begin{tikzpicture}[polybox, tos]
  \node[poly, "$\yon$" below, identity] (p) {};
\end{tikzpicture}
\]
It has exactly one position and exactly one direction, so both its boxes come pre-filled.

Finally, a \textit{constant} polynomial $I$ for some set $I$ has empty direction-sets.
We indicate this by coloring its direction box red:
\[
\begin{tikzpicture}[polybox, tos]
  \node[poly, "$I$" below, constant] (p) {};
  \node[left=0pt of p_pos] {$I$};
\end{tikzpicture}
\]
Because every direction-set is empty, there is nothing that may be written in the direction box.
The red suggests a kind of error---the direction box cannot be filled.
The polynomial functor $\1$, which is both representable and constant, therefore has the following polyboxes:
\[
\begin{tikzpicture}[polybox, tos]
  \node[poly, "$\1$" below, terminal] (p) {};
\end{tikzpicture}
\]


In the next chapter, we will introduce the morphisms between polynomial functors and see how their behavior may be depicted using polyboxes.

%-------- Section --------%
\section{Summary and further reading}

In this chapter we explained the mathematics behind our main objects of study in this book, polynomial functors. A polynomial $p=\sum_{i\in I}\yon^{p[i]}$ can be considered as
\begin{enumerate}
    \item combinatorial data: an indexed family of sets $(p[i])_{i\in I}$;
    \item a picture: for each $i\in I$, a corolla with $p[i]$-many leaves;
    \item a functor $\smset\to\smset$: for each $X:\smset$, a new set $\sum_{i\in I}X^{p[i]}$.
\end{enumerate}

There are many fine sources on polynomial functors. Some of the computer science literature is more relaxed about what a polynomial is. For example, the ``coalgebra community'' often defines a polynomial to include finite power sets (see e.g.\ \cite{jacobs2017introduction}). Other computer science communities use the same definition of polynomial, but refer to it as a \emph{container} and use different words for its positions (they call them ``shapes'') and directions (they call them, rather unfortunately, ``positions''). See e.g.\ \cite{abbot2003categoriesthesis,abbott2005containers}.

But the notion of polynomial functors seems to have originated from Andr\'{e} Joyal. A good introduction to polynomial functors can be found in \cite{kock2012polynomial}; in particular the related work section on page~3 provides a nice survey of the field. A reader may also be interested in the Workshops on Polynomial Functors organized by the Topos Institute: \url{https://topos.site/p-func-workshop/}.

%-------- Section --------%
\section{Exercise solutions}
\Closesolutionfile{solutions}
{\footnotesize
    \input{solution-file2}}

\Opensolutionfile{solutions}[solution-file3]

%------------ Chapter ------------%
\chapter{The category of polynomial functors} \label{ch.poly.cat}

% TODO: Should we name it "Morphisms between polynomial functors" or maybe "Dependent lenses" instead?? If so, split off symmetric monoidal structures except little bit about coproducts

In this chapter, we will define $\poly$, our main category of interest, so that we have a firm foundation from which to speak about interactive systems.
The objects of $\poly$ are the polynomial functors that we defined in the previous chapter.
Here we will examine the morphisms of $\poly$: natural transformations between polynomial functors.
Along the way, we will present some of $\poly$'s most versatile categorical properties.

%-------- Section --------%
\section{Dependent lenses between polynomial functors}
\label{sec.poly.cat.morph}

Before we define the category $\poly$ of polynomial functors, we note that polynomial functors live inside a category already: the category $\smset^\smset$ of functors $\smset\to\smset$, whose morphisms are natural transformations.
This leads to a very natural (if you will) definition of morphisms between polynomial functors, from which we can derive a category of polynomial functors for free.
We call such a morphism a \emph{dependent lens}, or a \emph{lens} for short.
If you are familiar with lenses from functional programming, we'll see in \cref{subsec.poly.cat.morph.bimorphic-lens} how our notion of a dependent lens is related.

\begin{definition}[Dependent lens, $\poly$] \label{def.poly_cat}
Given polynomial functors $p$ and $q$, a \emph{dependent lens} (or simply \emph{lens}) \emph{from $p$ to $q$} is a natural transformation $p\to q$.
Then $\poly$ is the category whose objects are polynomial functors and whose morphisms are dependent lenses.
\end{definition}

In other words, $\poly$ is the full subcategory of $\smset^\smset$ spanned by the polynomial functors: we take the category $\smset^\smset$, throw out all the objects that are not (isomorphic to) polynomials, but keep all the same morphisms between the objects that remain.

Unraveling the familiar definition of a natural transformation, a dependent lens between polynomial functors $p \to q$ thus consists of a function $p(X) \to q(X)$ for every set $X$ such that naturality squares commute.
That is a lot of data to keep track of!
Fortunately, there is a much simpler way to think about these lenses, which we will discover using the Yoneda lemma.

\begin{exercise} \label{exc.poly_morph_yoneda}
Given a set $S$ and a polynomial $q$, show that a lens $\yon^S \to q$ can be naturally identified with an element of the set $q(S)$.
That is, show that there is an isomorphism
\[
    \poly(\yon^S, q) \iso q(S).
\]
natural in both $S$ and $q$.
Hint: Use the Yoneda lemma (\cref{lemma.yoneda}).
\begin{solution}
We know that $\poly$ is the full subcategory of $\smset^\smset$ spanned by polynomial functors, including $\yon^S$ and $q$.
So $\poly(\yon^S, q)=\smset^\smset(\yon^S, q)$.
Hence the natural isomorphism $\poly(\yon^S, q)\iso q(S)$ follows directly from the Yoneda lemma (\cref{lemma.yoneda}) with $F\coloneqq q$.
\end{solution}
\end{exercise}

The above exercise gives us an alternative characterization for lenses out of representable functors.
But before we can characterize lenses out of polynomial functors in general, we need to describe how coproducts work in $\poly$.
Fortunately, since polynomial functors are defined as coproducts of representables, coproducts in $\poly$ are easy to understand.

\begin{proposition} \label{prop.poly_coprods}
  The category $\poly$ has all small coproducts, coinciding with coproducts in $\smset^\smset$ given by the operation $\sum_{i \in I}$ for each set $I$.
\end{proposition}
\begin{proof}
  By \cref{cor.sum_prod_set_endofuncs}, the category $\smset^\smset$ has all small coproducts given by $\sum_{i\in I}$.
  The full subcategory inclusion $\poly\to \smset^\smset$ reflects these coproducts, and by definition $\poly$ is closed under the operation $\sum_{i \in I}$.
\end{proof}

Explicitly, given an $I$-indexed family of polynomials $(p_i)_{i \in I}$, its coproduct is
\begin{equation} \label{eqn.poly_coprod}
  \sum_{i \in I} p_i \iso \sum_{i \in I} \sum_{j \in p_i(\1)} \yon^{p_i[j]} \iso \sum_{(i,j) \in \sum_{i \in I} p_i(\1)} \yon^{p_i[j]}
\end{equation}
by \cref{cor.sum_prod_set_endofuncs}.
This coincides with our notion of polynomial addition from high school algebra: just add all the terms together, combining like terms to simplify.
Binary coproducts are given by binary sums of functors, appropriately denoted by $+$, while the initial object of $\poly$ is the constant polynomial $\0$.

In particular, \eqref{eqn.poly_coprod} implies that for any polynomials $p$ and $q$, their coproduct $p+q$ is given as follows.
The position-set of $p+q$ is the coproduct of sets $p(\1) + q(\1)$.
At position $(1,i) \in p(\1) + q(\1)$ with $i \in p(\1)$, the directions of $p+q$ are just the $p[i]$-directions; at position $(2,j) \in p(\1) + q(\1)$ with $j \in q(\1)$, the directions of $p+q$ are just the $q[j]$-directions.

% TODO: Revisit this? maybe shift this paragraph down

Crucially, we have the following corollary.

\begin{corollary} \label{cor.poly-coprod-repr}
  In the category $\poly$, every polynomial $p$ is the coproduct of its representable summands $(\yon^{p[i]})_{i\in p(\1)}$.
\end{corollary}

In other words, writing $p$ as the sum $\sum_{i\in p(\1)}\yon^{p[i]}$ is not just a coproduct in $\smset^\smset$; it is also a coproduct in $\poly$ itself.

We are now ready to give our alternative characterization of dependent lenses.
Recall that a polynomial $p\iso\sum_{i\in p(\1)}\yon^{p[i]}$ can be uniquely identified with an indexed family $p[-]\colon p(\1)\to\smset$, a functor from the set $p(\1)$ viewed as a discrete category.

\begin{proposition}\label{prop.lens-prod-sum}
Given polynomials $p$ and $q$, there is an isomorphism
\begin{equation}\label{eqn.main_formula}
\poly(p,q)\cong\prod_{i\in p(\1)}\sum_{j\in q(\1)}{p[i]}^{q[j]}
\end{equation}
natural in $p$ and $q$.
In particular, a lens $f\colon p\to q$ can be identified with a pair $(f_\1,f^\sharp)$
\begin{equation}\label{eqn.colax_poly_map}
\begin{tikzcd}[column sep=small]
	p(\1)\ar[dr, "p{[-]}"']\ar[rr, "f_\1"]&~&
	q(\1)\ar[dl, "q{[-]}"]\\&
	\smset\ar[u, phantom, near end, "\overset{f^\sharp}{\Leftarrow}"]
\end{tikzcd}
\end{equation}
where $f_\1\colon p(\1)\to q(\1)$ is a function (equivalently, a functor between discrete categories) and $f^\sharp \colon q[f_\1(-)] \to p[-]$ is a natural transformation: a function $f^\sharp_i\colon q[f_\1(i)]\to p[i]$ for each $i\in p(\1)$.
\end{proposition}
\begin{proof}
We have $p\iso\sum_{i\in p(\1)}\yon^{p[i]}$.
Then by \cref{cor.poly-coprod-repr} and the universal property of the coproduct, we have a natural isomorphism
\[
    \poly\left(\sum_{i\in p(\1)}\yon^{p[i]}, q\right) \iso \prod_{i\in p(\1)}\poly(\yon^{p[i]},q).
\]
Applying \cref{exc.poly_morph_yoneda} (i.e.\ the Yoneda lemma) and the fact that $q\iso\sum_{j\in q(\1)}\yon^{q[j]}$ yields the natural isomorphism
\[
  \prod_{i\in p(\1)}\poly(\yon^{p[i]},q) \iso \prod_{i\in p(\1)}q(p[i]) \iso \prod_{i\in p(\1)}\sum_{j\in q(\1)}p[i]^{q[j]},
\]
so \eqref{eqn.main_formula} follows.

The right hand side of \eqref{eqn.main_formula} is the set of dependent functions $f\colon(i\in p(\1))\to\sum_{j\in q(\1)}p[i]^{q[j]}$.
Each such dependent function is uniquely determined by its two projections $(\pi_1\circ f)\colon(i\in p(\1))\to q(\1)$ and $(\pi_2\circ f)\colon(i\in p(\1))\to p[i]^{q[(\pi_1\circ f)(i)]}$.
These can be identified respectively with a (non-dependent) function $f_\1\coloneqq\pi_1\circ f$ with signature $p(\1)\to q(\1)$ and a natural transformation $f^\sharp\colon q[f_\1(-)]\to p[-]$ whose $i$-component for $i\in p(\1)$ is $f^\sharp_i\coloneqq\pi_2(f(i))\in p[i]^{q[f_\1(i)]}$.
\end{proof}

We have now greatly simplified our characterization of a dependent lens $f\colon p\to q$: rather than infinitely many functions satisfying infinitely many naturality conditions, $f$ may simply be specified by a function $f_\1\colon p(\1)\to q(\1)$ and, for each $i\in p(\1)$, a function $f^\sharp_i\colon q[f_\1(i)]\to p[i]$, without any additional restrictions.
This characterization can be expressed entirely in the language of positions and directions: $f_\1$ is a function from $p$-positions to $q$-positions, while $f^\sharp_i$ for a $p$-position $i$ is a function from $q[f_\1(i)]$-directions to $p[i]$-directions.
This leads to the following definition.

\begin{definition}[On-positions function, on-directions map and function]
  Given a lens $f\colon p\to q$, let $(f_\1, f^\sharp)$ denote the pair identified with $f$ via \cref{prop.lens-prod-sum}.
  Then we call the function $f_\1\colon p(\1)\to q(\1)$ the \emph{(forward) on-positions function of $f$}, while we call the natural transformation $f^\sharp\colon q[f_\1(-)]\to p[-]$ the \emph{(backward) on-directions map of $f$}.
  For $i\in p(\1)$, we call the $i$-component $f^\sharp_i\colon q[f_\1(i)]\to p[i]$ of $f^\sharp$ the \emph{(backward) on-directions function of $f$ at $i$}.
\end{definition}

The above definition highlights the bidirectional nature of a lens $f\colon p\to q$: it consists of a function going \textit{forward} on positions, following the direction of $f$ from $p$ to $q$, as well as functions going \textit{backward} on directions, opposing the direction of $f$ from $q$ to $p$.
This forward-backward interaction is what drives the applications of $\poly$ we will study.

We prefer to call a morphism between polynomial functors a ``lens'' rather than a ``natural transformation'' because we wish to emphasize this concrete on-positions and on-directions perspective.
Whenever we do need to view a morphism in $\poly$ as a natural transformation, we will refer to them as such.

In the next several sections, we will give some examples of lenses and intuition for thinking about them in terms of interaction protocols, corolla forests, and polyboxes.

%-------- Section --------%
\section{Dependent lenses as interaction protocols}

Here is our first example of a dependent lens and a real-world interaction it might model.

\begin{example}[Interacting with the coin jar]
  Recall our coin jar polynomial from \cref{ex.coin-jar}:
  \[
    q\coloneqq\{\text{`open'}\}\yon^{\{\text{`penny', `nickel', `dime', `quarter'}\}}+\{\text{`closed'}\}\yon^\0.
  \]
  It has $2$ positions: its `open' position has $4$ directions representing the $4$ denominations of coins it may take, while its `closed' position has $0$ directions to indicate that it cannot take anything.

  Now imagine that we model the owner of this coin jar with the following polynomial:
  \begin{align*}
    p\coloneqq\:
    &\{\text{`needy'}\}\yon^{\{\text{`save', `spend'}\}}
      \\
    +\:
    &\{\text{`greedy'}\}\yon^{\{\text{`accept', `reject', `ask for more'}\}}
      \\
    +\:
    &\{\text{`content'}\}\yon^{\{\text{`count', `rest'}\}}.
  \end{align*}
  Each of its $3$ positions represents a possible mood of the owner, and the directions at each position represent the options available to an owner in the corresponding mood.
  We will construct a lens $f\colon p\to q$ to model the interaction between the owner and their coin jar.

  Say that a needy or greedy owner will keep their coin jar open, while a content owner will keep their coin jar closed.
  We can express this with an on-positions function $f_\1$ from the set of $p$-positions (on the left) to the set of $q$-positions (on the right), as follows (the dashed arrows indicate the function assigments):
  \[
  \scalebox{.7}{
  \begin{tikzpicture}
    % q-positions (right)
      % from top to bottom: open, closed
    \node (open) {`open'};
    \node[below=.5 of open] (closed) {`closed'};

    \node[draw, ellipse, inner sep=0pt, fit=(open)(closed),
      label={[anchor=south,below]270:$q(\1)$}] (qpos) {};

    % p-positions (left)
      % from top to bottom: needy, greedy, content
      % greedy aligned with middle of q-positions
    \node[left=3 of qpos] (greedy) {`greedy'};
    \node[above=.5 of greedy] (needy) {`needy'};
    \node[below=.5 of greedy] (content) {`content'};

    \node[draw, ellipse, inner sep=0pt, fit=(greedy)(needy)(content),
      label={[anchor=south,below]270:$p(\1)$}] (ppos) {};

    \draw[mapsto] (needy) -- (open);
    \draw[mapsto] (greedy) -- (open);
    \draw[mapsto] (content) -- (closed);
  \end{tikzpicture}
  }
  \]

  From there, say that a needy owner whose coin jar receives a nickel or higher will choose to save it, but one whose coin jar receives a penny will choose to spend it.
  Meanwhile, a greedy owner whose coin jar receives a penny or nickel will ask for more, but one whose coin jar receives a dime or quarter will accept it.
  We can express this behavior with an on-directions map $f^\sharp\colon q[f_\1(-)]\to p[-]$.
  Its `needy' component is the on-directions function $f^\sharp_{\text{`needy'}}\colon q[f_\1(\text{`needy'})]\to p[\text{`needy'}]$ drawn as follows:
  \[
  \scalebox{.7}{
  \begin{tikzpicture}
    % p-directions (left)
      % from top to bottom: save, spend
    \node (save) {`save'};
    \node[below=.5 of save] (spend) {`spend'};

    \node[draw, ellipse, inner sep=0pt, fit=(save)(spend),
      label={[anchor=south,below]270:$p[\text{`needy'}]$}] (pdir) {};

    % q-directions (right)
      % from top to bottom: penny, nickel, dime, quarter
      % nickel aligned with save
    \node[right=3 of save] (nickel) {`nickel'};
    \node[above=.5 of nickel] (penny) {`penny'};
    \node[below=.5 of nickel] (dime) {`dime'};
    \node[below=.5 of dime] (quarter) {`quarter'};

    \node[draw, ellipse, inner sep=0pt,
      fit=(penny)(nickel)(dime)(quarter),
      label={[anchor=south,below]270:$q[\text{`open'}]$}] (qdir) {};

    \draw[mapsto] (penny) -- (spend);
    \draw[mapsto] (nickel) -- (save);
    \draw[mapsto] (dime) -- (save);
    \draw[mapsto] (quarter) -- (save);
  \end{tikzpicture}
  }
  \]
  Notice that, by keeping the positions and directions of $p$ on the left and those of $q$ on the right, the on-positions function is drawn from left to right, while the on-directions functions must be drawn right to left.
  The `greedy' component of $f^\sharp$ is the on-directions function $f^\sharp_{\text{`greedy'}}\colon q[f_\1(\text{`greedy'})]\to p[\text{`greedy'}]$ drawn like so:
  \[
  \scalebox{.7}{
    \begin{tikzpicture}
      % q-directions (right)
        % from top to bottom: penny, nickel, dime, quarter
      \node (nickel) {`nickel'};
      \node[above=.5 of nickel] (penny) {`penny'};
      \node[below=.5 of nickel] (dime) {`dime'};
      \node[below=.5 of dime] (quarter) {`quarter'};

      \node[draw, ellipse, inner sep=0pt,
        fit=(penny)(nickel)(dime)(quarter),
        label={[anchor=south,below]270:$q[\text{`open'}]$}] (qdir) {};

      % p-directions (left)
        % from top to bottom: accept, reject, ask for more
        % reject aligned with middle of q-directions
      \node[left=3 of qdir] (reject) {`reject'};
      \node[above=.5 of reject] (accept) {`accept'};
      \node[below=.5 of reject] (ask) {`ask for more'};

      \node[draw, ellipse, inner sep=0pt, fit=(accept)(reject)(ask),
        label={[anchor=south,below]270:$p[\text{`needy'}]$}] (pdir) {};

      \draw[mapsto] (penny) -- (ask);
      \draw[mapsto] (nickel) -- (ask);
      \draw[mapsto] (dime) -- (accept);
      \draw[mapsto] (quarter) -- (accept);
    \end{tikzpicture}
  }
  \]
  Finally, since $q[f_\1(\text{`content'})]=q[\text{`closed'}]=\0$, the `content' component of $f^\sharp$ is the vacuously-defined on-directions function $f^\sharp_{\text{`content'}}\colon \0\to p[\text{`content'}]$.
  Together, the on-positions function $f_\1$ and the on-directions map $f^\sharp$ defined above completely characterize a lens $f\colon p\to q$ depicting the interaction between the coin jar and its owner.
\end{example}

More generally, a lens depicts what we call an \emph{interaction protocol}, a kind of dialogue between two agents regarding their positions and directions.
Say that one agent is represented by a polynomial $p$ and another by a polynomial $q$.
Then a lens $f\colon p\to q$ is an interaction protocol that prescribes how the positions of $p$ influence the positions of $q$ and how the directions of $q$ influence the directions of $p$.
Each $p$-position $i\in p(\1)$ is passed forward via the on-positions function of $f$ to a $q$-position $f_\1(i)\in q(\1)$.
Then each $q[f_\1(i)]$-direction $b$ is passed backward via the on-directions function of $f$ at $i$ to a $p[i]$-direction $f^\sharp_i(b)$.

To visualize these lenses, we may use either our corolla forests or our polyboxes.

%-------- Section --------%

\section{Corolla forest pictures of dependent lenses}

The corolla forest associated to a polynomial concretely depicts its positions and directions, making it easy to extend our corolla forest pictures to depict the dependencies between the positions and directions of two polynomials that a lens between them prescribes.

\begin{example}\label{ex.practice_with_poly_morphisms}
Let $p\coloneqq \yon^\3+\2\yon$ and $q\coloneqq\yon^\4+\yon^\2+\2$.
We draw them as corolla forests with their positions labeled:
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, blue!50!black, "$p$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$}
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$}
      child {};
    \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$}
      child {};
  \end{tikzpicture}
  };
%
	\node (p2) [draw, red!75!black, right=2 of p1, "$q$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$}
      child {}
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$}
      child {}
      child {};
    \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$}
    ;
    \node[right=.5 of 3,"\tiny 4" below] (4) {$\bullet$}
    ;
  \end{tikzpicture}
  };
\end{tikzpicture}
\]
To give a lens $p\to q$, we must send each $p$-position $i\in p(\1)$ to a $q$-position $j\in q(\1)$, then send each direction in $q[j]$ back to one in $p[i]$.
We can draw such a lens as follows.
\[
\begin{tikzpicture}
	\node (p1) {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "{\color{blue!50!black}\tiny 1}" below] (1) {$\bullet$}
      child[blue!50!black] {coordinate (11)}
      child[blue!50!black] {coordinate (12)}
      child[blue!50!black] {coordinate (13)};
    \node[right=1.5 of 1, red!75!black, "{\color{red!75!black}\tiny 1}" below] (2) {$\bullet$}
      child[red!75!black] {coordinate (21)}
      child[red!75!black] {coordinate (22)}
      child[red!75!black] {coordinate (23)}
      child[red!75!black] {coordinate (24)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (13);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (13);
      \draw[postaction={decorate}] (24) to (13);
    \end{scope}
  \end{tikzpicture}
	};
%
	\node (p2) [right=1 of p1] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "{\color{blue!50!black}\tiny 2}" below] (1) {$\bullet$}
      child[blue!50!black] {coordinate (11)};
    \node[right=of 1, red!75!black, "{\color{red!75!black}\tiny 1}" below] (2) {$\bullet$}
      child[red!75!black] {coordinate (21)}
      child[red!75!black] {coordinate (22)}
      child[red!75!black] {coordinate (23)}
      child[red!75!black] {coordinate (24)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (11);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (11);
      \draw[postaction={decorate}] (24) to (11);
    \end{scope}
  \end{tikzpicture}
	};
%
	\node (p3) [below right=-1.05cm and 1 of p2] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "{\color{blue!50!black}\tiny 3}" below] (1) {$\bullet$}
      child[blue!50!black] {};
    \node[right=of 1, red!75!black, "{\color{red!75!black}\tiny 4}" below] (2) {$\bullet$}
		;
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
  \end{tikzpicture}
	};
\end{tikzpicture}
\]
This represents one possible lens $f\colon p\to q$.
The horizontal solid arrows pointing rightward in the picture above tell us that the on-positions function $f_\1\colon p(\1)\to q(\1)$ is given by
\[
  f_\1(1) \coloneqq 1, \qquad f_\1(2) \coloneqq 1, \qqand f_\1(3) \coloneqq 4.
\]
Then the curved dashed arrows pointing leftward in the picture above describe the on-directions map $f^\sharp\colon q[f_\1(-)]\to p[-]$.
On the left, the arrows depict one possible on-directions function $f^\sharp_1\colon q[1]\to p[1]$ from the $4$ directions in $q[1]$ to the $3$ directions in $p[1]$.
In the middle, the arrows depict the only possible on-directions function $f^\sharp_2\colon q[1]\to p[2]$ because $|p[2]|=1$.
Finally, on the right, there are no curved arrows, depicting the only possible on-directions function $f^\sharp_3\colon q[4]\to p[3]$ because $|q[4]|=0$.
\end{example}

\begin{exercise}\label{exc.practice_poly_maps}
\begin{enumerate}
	\item Draw the corolla forests associated to $p\coloneqq\yon^\3+\yon+\1$, $q\coloneqq \yon^\2+\yon^\2+\2$, and $r\coloneqq\yon^\3$.
	\item Pick an example of a dependent lens $p\to q$ and draw it as we did in \cref{ex.practice_with_poly_morphisms}.
	\item Explain the behavior of your lens as an interaction protocol in terms of positions and directions.
	\item Explain in those terms why there can't be any lenses $p\to r$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item Here are the corolla forests associated to $p\coloneqq\yon^\3+\yon+\1$, $q\coloneqq \yon^\2+\yon^\2+\2$, and $r\coloneqq\yon^\3$ (with each root labeled for convenience).
    \[
    \begin{tikzpicture}[rounded corners]
    	\node (p) [draw, blue!50!black, "$p$" above] {
    	\begin{tikzpicture}[trees, sibling distance=2.5mm]
            \node["\tiny 1" below] (1) {$\bullet$}
              child {}
              child {}
              child {};
            \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$}
              child {};
            \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$};
        \end{tikzpicture}
        };
    %
    	\node (q) [draw, red!75!black, right=2 of p, "$q$" above] {
    	\begin{tikzpicture}[trees, sibling distance=2.5mm]
            \node["\tiny 1" below] (1) {$\bullet$}
              child {}
              child {};
            \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$}
              child {}
              child {};
            \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$};
            \node[right=.5 of 3,"\tiny 4" below] (4) {$\bullet$};
        \end{tikzpicture}
        };
    %
    	\node (r) [draw, green!50!black, right=2 of q, "$r$" above] {
    	\begin{tikzpicture}[trees, sibling distance=2.5mm]
        \node["\tiny 1" below] (1) {$\bullet$}
          child {}
          child {}
          child {};
        \end{tikzpicture}
      };
    \end{tikzpicture}
    \]
	\item Here is one possible lens $p\to q$ (you may have drawn others).
  % TODO: why does the pic look like this?? fix baseline somehow?
	\[
    \begin{tikzpicture}
    	\node (p1) {
        	\begin{tikzpicture}[trees, sibling distance=2.5mm]
                \node[blue!50!black, "\tiny 1" below] (1) {$\bullet$}
                  child[blue!50!black] {coordinate (11)}
                  child[blue!50!black] {coordinate (12)}
                  child[blue!50!black] {coordinate (13)};
                \node[right=1.5 of 1, red!75!black, "\tiny 2" below] (2) {$\bullet$}
                  child[red!75!black] {coordinate (21)}
                  child[red!75!black] {coordinate (22)};
                \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
                \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
                  \draw[postaction={decorate}] (21) to (13);
                  \draw[postaction={decorate}] (22) to (11);
                \end{scope}
            \end{tikzpicture}
    	};
        %
    	\node (p2) [right=1 of p1, yshift=-2mm] {
        	\begin{tikzpicture}[trees, sibling distance=2.5mm]
                \node[blue!50!black, "\tiny 2" below] (1) {$\bullet$}
                  child[blue!50!black] {coordinate (11)};
                \node[right=of 1, red!75!black, "\tiny 4" below] (2) {$\bullet$};
                \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
            \end{tikzpicture}
    	};
        %
    	\node (p3) [right=1 of p2, yshift=-2mm] {
        	\begin{tikzpicture}[trees, sibling distance=2.5mm]
                \node[blue!50!black, "\tiny 3" below] (1) {$\bullet$};
                \node[right=of 1, red!75!black, "\tiny 3" below] (2) {$\bullet$};
                \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
            \end{tikzpicture}
    	};
    \end{tikzpicture}
    \]
    \item As depicted, our lens assigns to the first position of $p$ the second position of $q$, whose first and second directions are passed back to the third and first directions, respectively, of the first position of $p$.
    Then the second position of $p$ is assigned the fourth position of $q$, which has no directions; effectively, the choice of direction of the second position of $p$ has been canceled.
    Finally, the third position of $p$ is assigned the third position of $q$; here neither position has any directions.
    \item There cannot be a lens $p \to r$ for the following reason: if we send the third position of $p$, which has no directions, to the sole position of $r$, which has $3$ directions, then there is no way to pass a choice of one of those $3$ directions back to any of the options on the third menu of $p$, as there are no such options.
\end{enumerate}
\end{solution}
\end{exercise}

% TODO: finish & integrate above

%-------- Section --------%
\section{Polybox pictures of dependent lenses}

% TODO: fill in

%-------- Section --------%
\section{Computations with dependent lenses}


\begin{example}
  How many ways are there to do this? Before answering this, let's just pick one.

  So how many different lenses are there from $p$ to $q$? The first $p$-position can be sent to any $q$-position: 1, 2, 3, or 4. Sending it to $1$ requires choosing how each of the four options $(q[1]=\4)$ are to be assigned one of $p[1]=\3$ options; there are $3^4$ ways to do this. Similarly, we can calculate the remaining ways to handle the first $p$-position, then add them up: there are $3^4+3^2+3^0+3^0=92$ ways total.

  The second $p$-position can also be sent to 1, 2, 3, or 4, before sending back directions; there are $1^4+1^2+1^0+1^0=4$ ways to do this.
  Similarly there are four ways to send the third $p$-position to a $q$-position and send back directions.

  In total, there are $92 \cdot 4 \cdot 4=1472$ lenses $p\to q$.

  Unsurprisingly, this is exactly what is given by \eqref{eqn.main_formula}:
  \begin{align*}
    |\poly(p, q)| &= \prod_{i \in p(\1)} |q(p[i])| \\
    &= \prod_{i \in p(\1)} |p[i]|^4 + |p[i]|^2 + 2 \\
    &= (3^4 + 3^2 + 2)(1^4 + 1^2 + 2)^2 \\
    &= 92 \cdot 4^2 = 1472.
  \end{align*}
\end{example}


% TODO: integrate below

\begin{exercise}
For any polynomial $p$ and set $A$, e.g.\ $A=\2$, the Yoneda lemma gives an isomorphism $p(A)\cong \poly(\yon^A,p)$.
\begin{enumerate}
	\item Choose a polynomial $p$ and draw both $\yon^\2$ and $p$ as corolla forests.
	\item Count all the lenses $\yon^\2\to p$. How many are there?
	\item Is the previous answer equal to (the cardinality of) $p(\2)$?
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We let $p \coloneqq \yon^\3 + 1$ (you could have selected others) and draw both $p$ and $\yon^\2$ as corolla forests, labeling each root for convenience.
    \[
    \begin{tikzpicture}[rounded corners]
    	\node (y2) [draw, blue!50!black, "$\yon^\2$" above] {
    	\begin{tikzpicture}[trees, sibling distance=2.5mm]
            \node["\tiny 1" below] (1) {$\bullet$}
              child {}
              child {};
        \end{tikzpicture}
        };
    %
    	\node (p) [draw, red!75!black, right=2 of y2, "$p$" above] {
    	\begin{tikzpicture}[trees, sibling distance=2.5mm]
            \node["\tiny 1" below] (1) {$\bullet$}
              child {}
              child {}
              child {};
            \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$};
        \end{tikzpicture}
        };
    \end{tikzpicture}
    \]
    \item We count all the lenses from $\yon^\2$ to $p$.
    The unique position of $\yon^\2$ can be sent to either $p$-position.
    If it is sent to the first $p$-position, then there are $2$ directions of $\yon^\2$ for each of the $3$ directions in $p[1]$ to be sent to, for a total of $2^3 = 8$ lenses.
    Otherwise, the unique position of $\yon^\2$ is sent to the second $p$-position---at which there are no directions, so there is exactly $1$ way to do this.
    So we have $8 + 1 = 9$ lenses from $\yon^\2$ to $p$.
    \item Yes, the previous answer is equal to the cardinality of $p(\2)\iso\2^\3+\1\iso\9$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
For each of the following polynomials $p,q$, compute the number of morphisms $p\to q$.
\begin{enumerate}
	\item $p=\yon^\3$,\quad $q=\yon^\4$.
	\item $p=\yon^\3+\1$,\quad $q=\yon^\4$.
	\item $p=\yon^\3+\1$,\quad $q=\yon^\4+\1$.
	\item $p=\4\yon^\3+\3\yon^\2+\yon$,\quad $q=\yon$.
	\item $p=\4\yon^\3$,\quad $q=\3\yon$.
\qedhere
\end{enumerate}
\begin{solution}
Our goal is to compute the number of natural transformations $p\to q$ for each of the following polynomials $p,q$.
By \eqref{eqn.main_formula}, we always have
\[
    |\poly(p, q)| = \prod_{i \in p(\1)} |q(p[i])|.
\]
\begin{enumerate}
	\item If $p=\yon^\3$ and $q=\yon^\4$, then
	\[
	    |\poly(p, q)| = \prod_{i \in \1} |p[i]|^4 = 3^4 = 81.
	\]
	\item If $p=\yon^\3+\1$ and $q=\yon^\4$, then
	\[
	    |\poly(p, q)| = \prod_{i \in \2} |p[i]|^4 = 3^4 \cdot 0^4 = 0.
	\]
	\item If $p=\yon^\3+\1$ and $q=\yon^\4+\1$, then
	\[
	    |\poly(p, q)| = \prod_{i \in \2} |p[i]|^4 + 1 = (3^4 + 1)(0^4 + 1) = 82.
	\]
	\item If $p=\4\yon^\3+\3\yon^\2+\yon$ and $q=\yon$, then
	\[
	    |\poly(p, q)| = \prod_{i \in \8} |p[i]| = 3^4 \cdot 2^3 \cdot 1 = 648.
	\]
	\item If $p=\4\yon^\3$ and $q=\3\yon$, then
	\[
	    |\poly(p, q)| = \prod_{i \in \4} 3|p[i]| = (3 \cdot 3)^4 = 6561.
	\]
\qedhere
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.practice_sum_prod}
\begin{enumerate}
\item Show that the following are isomorphic:
\begin{equation}\label{eqn.poly_p_q}
  \poly(p,q)
  \cong^?
  \prod_{i\in p(\1)}\sum_{j\in q(\1)}\prod_{b\in q[j]}\sum_{a\in p[i]}\1
\end{equation}
\item \label{exc.practice_sum_prod.useful} Show that the following are isomorphic:
	\begin{equation}\label{eqn.useful_misc472}
	\poly(p,q)\cong^?\sum_{f_\1\colon p(\1)\to q(\1)}\prod_{j\in q(\1)}\smset\Bigg(q[j],\prod_{\substack{i \in p(\1), \\ f_\1(i) = j}}p[i]\Bigg)
	\end{equation}
\item Describe in the language of decision-making how any element of the right-hand side of \eqref{eqn.useful_misc472} gives a way of delegating decisions from $p$ to $q$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{longenum}
\item We will show that the following isomorphism holds:
\[
  \poly(p,q)
  \cong
  \prod_{i\in p(\1)}\sum_{j\in q(\1)}\prod_{b\in q[j]}\sum_{a\in p[i]}\1.
\]
By \eqref{eqn.main_formula}, it suffices to show that for all $i \in p(\1)$ and $j \in q(\1)$, we have
\[
    p[i]^{q[j]} \iso \prod_{b \in q[j]} \sum_{a \in p[i]} \1.
\]
Indeed, by \eqref{eqn.push_prod_sum_set_indep} and \cref{exc.on_sums_prods_sets}, we have
\begin{align*}
    \prod_{b \in q[j]} \sum_{a \in p[i]} \1 &\iso \sum_{\bar{a} \colon q[j] \to p[i]} \, \prod_{b \in q[j]} \1 \tag*{\eqref{eqn.push_prod_sum_set_indep}} \\
    &\iso \sum_{\bar{a} \colon q[j] \to p[i]} \1 \tag{\cref{exc.on_sums_prods_sets} \cref{exc.on_sums_prods_sets.prod}} \\
    &\iso \smset(q[j], p[i]) \tag{\cref{exc.on_sums_prods_sets} \cref{exc.on_sums_prods_sets.sum}} \\
    &\iso p[i]^{q[j]}.
\end{align*}

\item We will show that the following isomorphism holds:
\[
	\poly(p,q) \iso \sum_{f_\1\colon p(\1)\to q(\1)} \; \prod_{j\in q(\1)}\smset\Bigg(q[j],\prod_{\substack{i \in p(\1), \\ f_\1(i) = j}}p[i]\Bigg).
\]
By \eqref{eqn.main_formula} and \eqref{eqn.push_prod_sum_set_indep}, we have
\begin{align*}
    \poly(p, q) &\iso \prod_{i \in p(\1)} \sum_{j \in q(\1)} p[i]^{q[j]} \tag*{\eqref{eqn.main_formula}} \\
    &\iso \sum_{f_\1 \colon p(\1) \to q(\1)} \; \prod_{i \in p(\1)} p[i]^{q[f_\1(i)]} \tag*{\eqref{eqn.push_prod_sum_set_indep}} \\
    &\iso \sum_{f_\1 \colon p(\1) \to q(\1)} \; \prod_{j \in q(\1)} \; \prod_{\substack{i \in p(\1), \\ f_\1(i) = j}} p[i]^{q[j]} \tag{$\ast$} \\
    &\iso \sum_{f_\1\colon p(\1)\to q(\1)} \; \prod_{j\in q(\1)}\smset\Bigg(q[j],\prod_{\substack{i \in p(\1), \\ f_\1(i) = j}}p[i]\Bigg) \tag{Universal property of products}
\end{align*}
where ($\ast$) follows from the fact that for any function $f_\1 \colon p(\1) \to q(\1)$, the set $p(\1)$ can be written as the disjoint union of sets of the form $f_\1\inv(j) = \{i \in p(1) \mid f_\1(i) = j\}$ for each $j \in q(\1)$.

\item To explain how the set
\[
	D_{p,q} \coloneqq \sum_{f_\1\colon p(\1)\to q(\1)}\prod_{j\in q(\1)}\smset\Bigg(q[j],\prod_{\substack{i \in p(\1), \\ f_\1(i) = j}}p[i]\Bigg)
\]
specifies a way of delegating decisions from $p$ to $q$, we first give the instructions for choosing an element of $D_{p,q}$ as a nested list:
\begin{quote}
To choose an element of $D_{p,q}$:
\begin{longenum}
    \item choose a function $f_\1 \colon p(\1) \to q(\1)$;
    \item for each element $j \in q(\1)$:
    \begin{longenum}
        \item for each element of $q[j]$:
        \begin{longenum}
            \item for each element $i \in p(\1)$ satisfying $f_\1(i) = j$:
            \begin{longenum}
                \item choose an element of $p[i]$.
            \end{longenum}
        \end{longenum}
    \end{longenum}
\end{longenum}
\end{quote}
So $f_\1$ delegates each of $p$'s menus to one of $q$'s menus.
Then for every option on every menu $j$ of $q$, we choose an option on each of $p$'s menus that has been delegated to $j$ by $f_\1$.
\end{longenum}
\end{solution}
\end{exercise}

\begin{exercise}%\label{exc.poly_coprod}
Use \eqref{eqn.poly_coprod} and \eqref{eqn.main_formula} to verify that
\[
    \poly\left(\sum_{i \in I} p_i, q\right) \iso \prod_{i \in I} \poly(p_i, q)
\]
for all polynomials $(p_i)_{i \in I}$ and $q$, as expected from the universal property of coproducts.
\begin{solution}
Given $q \in \poly$ and $p_i \in \poly$ for each $i \in I$ for some set $I$, we use \eqref{eqn.poly_coprod} and \eqref{eqn.main_formula} to verify that
\begin{align*}
    \poly\left(\sum_{i \in I} p_i, q\right)
    &\iso \poly\left(\sum_{(i,j) \in \sum_{i \in I} p_i(\1)} \yon^{p_i[j]}, q\right)
    \tag*{\eqref{eqn.poly_coprod}} \\
    &\iso \prod_{(i,j) \in \sum_{i \in I} p_i(\1)} q(p_i[j])
    \tag*{\eqref{eqn.main_formula}} \\
    &\iso \prod_{i \in I} \prod_{j \in p_i(\1)} q(p_i[j]) \\
    &\iso \prod_{i \in I} \poly(p_i, q).
    \tag*{\eqref{eqn.main_formula}}
\end{align*}
\end{solution}
\end{exercise}

\begin{example}[Derivatives]\label{ex.derivatives}
The \emph{derivative} of a polynomial $p$, denoted $\dot{p}$, is defined as follows:
\[
\dot{p}\coloneqq\sum_{i\in p(\1)}\sum_{a\in p[i]}\yon^{p[i]-\{a\}}.
\]
For example, if $p\coloneq\yon^{\{U,V,W\}}+\{A,B\}\yon^{\{X\}}$ then
\[\dot{p}=\{U\}\yon^{\{V,W\}}+\{V\}\yon^{\{U,W\}}+\{W\}\yon^{\{U,V\}}+\{(A,X),(B,X)\}\yon^\0.\]
Up to isomorphism $p\cong\yon^\3+\2\yon$ and $\dot{p}\cong\3\yon^\2+\2$.
Unsurprisingly, this coincides with the familiar notion of derivatives of polynomials from calculus.

Thus we get a canonical lens $\dot{p}\yon\to p$, because we have an isomorphism
\[
\dot{p}\yon\cong\sum_{i\in p(\1)}\sum_{a\in p[i]}\yon^{p[i]}.
\]
This natural transformation comes up in computer science in the context of ``plugging in to one-hole contexts''; we will not explore that here, but see \cite{mcbride2001derivative} and \cite{abbott2003derivatives} for more info.%The Derivative of a Regular Type is its Type of One-Hole Contexts. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.22.8611&rep=rep1&type=pdf.
%https://www.cs.nott.ac.uk/~psztxa/publ/tlca03.pdf

A morphism $f\colon p\to \dot{q}$ can be interpreted as something like a lens from $p$ to $q$, except that each $p$-position explicitly selects a direction of $q$ to remain unassigned. More precisely, for each $i\in p(\1)$ we have $f_\1(i)=(j,a)\in \sum_{j\in q(\1)}q[j]$, i.e.\ a choice of position $j$ of $q$, as usual, together with a chosen direction $a\in q[j]$. Then every $q[j]$-direction \emph{other than $a$} is sent back to a $p[i]$-direction.
\end{example}

\begin{exercise} \label{exc.deriv_directions}
Show that $\dot{p}(\1)$ is isomorphic to the set of all directions of $p$ (i.e.\ the union of all direction-sets of $p$), so there is a canonical function $\pi_p \colon \dot{p}(\1) \to p(\1)$ that sends each direction $a$ of $p$ to the position $i$ of $p$ for which $a \in p[i]$.
\begin{solution}
We can evaluate $\dot{p}(\1)$ directly from the definition of $\dot{p}$ to obtain
\[
    \dot{p}(\1) = \sum_{i \in p(\1)} \sum_{a \in p[i]} \1^{p[i]-\{a\}} \iso \sum_{i \in p(\1)} p[i],
\]
which is isomorphic to the set of all directions of $p$.
Then $\pi_p \colon \dot{p}(\1) \to p(\1)$ is the canonical projection, sending each direction $a$ of $p$ to the position $i$ of $p$ for which $a \in p[i]$.
\end{solution}
\end{exercise}

\begin{exercise}
The derivative is not very well-behaved categorically.
However, it is intriguing.
Below $p, q \in \poly$.
\begin{enumerate}
	\item Explain the canonical lens $\dot{p}\yon\to p$ from \cref{ex.derivatives} in more detail.
	\item Is there always a canonical lens $p\to \dot{p}$?
	\item Is there always a canonical lens $\dot{p}\to p$?
	\item If given a lens $p\to q$, does one get a lens $\dot{p}\to\dot{q}$?
	\item We will define the binary operations $\otimes$ and $\ihom{-,-}$ on $\poly$ later on in \eqref{eqn.parallel_def} and \eqref{eqn.par_hom}, and in \cref{exc.dir_hom_p_yon_dir_p}, you will be able to use \cref{exc.par_hom_sum} to deduce that
	\begin{equation} \label{eqn.dir_hom_p_yon_dir_p}
	    \ihom{p, \yon} \otimes p \iso \sum_{f \in \prod_{i \in p(\1)} p[i]} \; \sum_{i \in p(\1)} \yon^{p(\1) \times p[i]},
	\end{equation}
% 	Using this, find a formula for a lens $p\otimes\ihom{p,\yon}\to \dot{p}$ that works for any $p\in\poly$.
	Is there always a canonical lens $\ihom{p,\yon}\otimes p\to \dot{p}$?
	\item When talking to someone who explains lenses $p\to\dot{q}$ in terms of ``unassigned directions,'' how might you describe what is modeled by a lens $p\yon\to q$?
	\qedhere
\end{enumerate}
\begin{solution}
Here $p, q \in \poly$.
\begin{enumerate}
	\item Our goal is to characterize the canonical lens $\dot{p}\yon\to p$.
	If we unravel the definitions, this is a lens
	\[
	    \sum_{i \in p(\1)} \sum_{a \in p[i]} \yon^{p[i]} \to \sum_{i \in p(\1)} \yon^{p[i]}.
	\]
	We observe that there is always such a lens sending every position $(i, a) \in \sum_{i \in p(\1)} p[i]$ of $\dot{p}\yon$ to its first projection $i \in p(\1)$ and is the identity on directions.
	This is the canonical lens.

	\item There cannot always be a canonical lens $p\to \dot{p}$, for if $p \coloneqq \1$, then $\dot{p} \coloneqq \0$, and there is no lens $\1 \to \0$.

	\item We show that there cannot always be a canonical lens $\dot{p}\to p$.
	Take $p \coloneqq \yon$, so $\dot{p} \coloneqq \1$.
	A lens $\1 \to \yon$ must have an on-directions function $\1 \to \0$, but such a function does not exist.

	\item We show that even when there is a lens $p \to q$, there is not necessarily a lens $\dot{p}\to\dot{q}$.
	Take $p \coloneqq \yon$ and $q \coloneqq \1$.
	Then there is a lens $p \to q$ that sends the unique position of $\yon$ to the unique position of $\1$ and is the empty function on directions.
	But $\dot{p} = \1$ and $\dot{q} = \0$, and there is no lens $\1 \to \0$.

	\item We show that there is a canonical lens $g \colon \ihom{p,\yon} \otimes p \to \dot{p}$, where $\ihom{p,\yon} \otimes p$ is given by \eqref{eqn.dir_hom_p_yon_dir_p}.
	The on-positions function $g_\1$ takes $f \in \prod_{i \in p(\1)} p[i]$ and $i \in p(\1)$ and sends the pair of them to the $\dot{p}$-position corresponding to $i \in p(\1)$ and $f(i) \in p[i]$.
	We then have $\dot{p}[(i, f(i))] \iso p[i]$ and $(\ihom{p,\yon} \otimes p)[(f, i)] \iso p(\1) \times p[i]$, so the on-directions function $g^\sharp_{(f,i)}$ can send each $a \in p[i]$ to $(i, a) \in p(\1) \times p[i]$.

	\item We wish to describe a lens $p\yon \to q$ in terms of ``unassigned to directions.''
	Observe that $p\yon$ has the same positions as $p$ but has one more direction than $p$ does at each position.
	We denote this extra direction at each position $i \in p(\1)$ of $p\yon$ by $\ast_i$.
	So a lens $f \colon p\yon\to q$ sends each position $i$ of $p$ to a position $j$ of $q$, but every $q[j]$-direction could either be sent back to a $p[i]$-direction or the extra direction $\ast_i$.
	We can say that a lens $f \colon p\yon \to q$ is like a lens $p \to q$, except that any of the directions of $q$ may remain unassigned---i.e.\ we may have partial on-directions functions.
\end{enumerate}
\end{solution}
\end{exercise}

%---- Subsection ----%
\subsection{Relation to lenses from functional programming}\label{subsec.poly.cat.morph.bimorphic-lens}

Monomials are special polynomials: those of the form $B\yon^A$ for sets $A,B$. % TODO: make this its own def?
Here's a picture of $\5\yon^{\1\2}$:
\[
\begin{tikzpicture}
\node[draw, rounded corners, "$\5\yon^{\1\2}$"] {
	\begin{tikzpicture}[trees, sibling distance=1mm]
	\foreach \i in {1,...,5}
	{
    \node["\tiny \i" below] at (1.8*\i,0) {$\bullet$}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
    ;
	};
	\end{tikzpicture}
};
\end{tikzpicture}
\]

The formula for morphisms between these is particularly simple:
\begin{align*}
  \poly\left(B_1\yon^{A_1},\,B_2\yon^{A_2}\right) &\iso \prod_{b \in B_1} \sum_{b' \in B_2} A_1^{A_2} \tag*{\eqref{eqn.main_formula}} \\
  &\iso \smset(B_1, B_2 \times A_1^{A_2}) \\
  &\iso \smset(B_1,B_2)\times\smset(B_1\times A_2,A_1).
\end{align*}
It says that to give a morphism from one monomial to another, you just need to give two (non-dependent!) functions. Let's rewrite it to make those two functions explicit---they are the familiar on-positions and on-directions functions:
\[
  \poly\left(B_1\yon^{A_1},\,B_2\yon^{A_2}\right)
  \iso
  \left\{
    (f_\1,f^\sharp)
  \;\middle|\;
  	\parbox{1.2in}{
    $
    \begin{aligned}
  	  f_\1&\colon B_1\to B_2\\
  	  f^\sharp&\colon B_1\times A_2\to A_1
    \end{aligned}
    $
  }.
  \right\}
\]
Ordinarily, $f^\sharp$ is more involved: its type depends on the directions at each position of the domain and its image position via $f_\1$.
But for monomials, every position has the same set of directions, so $f^\sharp$ is just a standard function.

The monomials in $\poly$ span a full subcategory of $\poly$ equivalent to \emph{the category of bimorphic lenses} \cite{hedges2018limits}.
This category arises in functional programming.
The functions $f_\1, f^\sharp$ corresponding to a morphism $f \colon B_1\yon^{A_1}\to B_2\yon^{A_2}$ are given special names:
\begin{equation}\label{eqn.bimorphic_lens}
\begin{aligned}
	\lensget\coloneqq f_\1 &\colon B_1\to B_2\\
	\lensput\coloneqq f^\sharp &\colon B_1\times A_2\to A_1
\end{aligned}
\end{equation}
Each position $b \in B_1$ of $B_1\yon^{A_1}$ ``gets'' a position $f_\1(b) \in B_2$ of $B_2\yon^{A_2}$, and given $b \in B_1$, every direction at $f_\1(b)$ in $A_2$ ``puts'' a direction back at $b$ in $A_1$.

So a morphism between two monomials in $\poly$ is just a bimorphic lens.
Then a morphism between any two polynomials in $\poly$ is a more generalized lens: a \emph{dependent} lens, where the direction-sets depend on the positions.
This is why we refer to these morphisms as \emph{lenses}.

\begin{example} \label{ex.lens_get_put}
Consider the monomial $S\yon^S$.
Its position-set is $S$, and its direction-set at each position $s\in S$ is again just $S$.
In the language of decision-making, each $s \in S$ is a menu whose options are themselves just the menus in $S$ again.
Notice that there is a natural way to string together a series of such decisions into a cycle: at each step, you start at some element of $S$, and the option you select is the element of $S$ that you will move to next.
We will start to formalize this idea in \cref{ex.do_nothing} and continue this work throughout the following chapters.

A lens $(\lensget,\lensput)\colon S\yon^S\to T\yon^T$ is as usual a way to delegate decisions of $S\yon^S$ to decisions of $T\yon^T$.
When you need to select an option from the menu $s \in S$, you ask your friend to check the menu $\lensget(s) \in T$ for help.
If your friend selects option $t \in T$, then you know to select option $\lensput(s, t) \in S$.
Then the options you have each selected are your new positions: your friend moves to $t$, you move to $\lensput(s, t)$, and you may each check the corresponding menus at your new positions.

But what happens when we string together these decisions into cycles?
Now you are moving between elements of $S$, looking to your friend for help at each step as they move between elements of $T$.
In this setting, there are a few conditions that a lens $S\yon^S \to T\yon^T$ should satisfy to ensure that the associated delgation behaves nicely with respect to the movements of both you and your friend:
\begin{enumerate}
    \item If your friend chooses to stay put, then you should stay put, too.
    This is reflected by the equation
    \[
        \lensput(s,\lensget(s))=s.
    \]

    \item After your friend moves, and you move accordingly, you should delegate the decision at your new location to the decision at your friend's new location.
    This is reflected by the equation
    \[
        \lensget(\lensput(s,t))=t.
    \]

    \item If your friend moves to $t$, then to $t'$, the place where you end up should be where you would have ended up if your friend had moved directly to $t'$ in the first place.
    This is reflected by the equation
    \[
        \lensput(\lensput(s,t),t')=\lensput(s,t')
    \]
\end{enumerate}
Such a lens is known to functional programmers as a \emph{very well-behaved lens}; the three conditions above are its \emph{lens laws}.
But we will see these conditions emerge from more general theory in \cref{ex.very_well_behaved_lenses}.
\end{example}

%---- Subsection ----%
\subsection{Translating between natural transformations and lenses} \label{subsec.poly.cat.morph.translate}
We now know that we can specify a morphism $p\to q$ from $\poly$ in two ways:
\begin{itemize}
    \item in the language of functors, by specifying a natural transformation $p \to q$, i.e.\ for each $X\in\smset$, a function $p(X)\to q(X)$ such that naturality squares commute; or
    \item in the language of positions and directions, by specifying a lens $p\to q$, i.e.\ a function $f_\1 \colon p(\1) \to q(\1)$ and, for each $i \in p(\1)$, a function $f^\sharp_i \colon q[f_\1(i)] \to p[i]$.
\end{itemize}
But how are these two formulations related?
Given the data of a lens and that of a natural transformation between polynomials, how could we tell if they correspond to the same morphism?
We want to be able to translate between these two languages.

Our Rosetta Stone turns out to be the proof of the Yoneda lemma.
The lemma itself is the crux of the proof of \cref{prop.lens-prod-sum}, that these two formulations of morphisms between polynomials are equivalent; so unraveling this proof reveals the translation we seek.

\begin{proposition} \label{prop.morph_arena_to_func}
Let $p$ and $q$ be polynomial functors, let $f_\1\colon p(\1)\to q(\1)$ be a function between their position-sets (i.e.\ an on-positions function), and let $f^\sharp\colon q[f_\1(-)]\to p[-]$ be a natural transformation whose every component is a function between their direction-sets (i.e.\ an on-directions function).
Then the isomorphism in \eqref{eqn.main_formula} sends $(f_\1, f^\sharp)$ to the natural transformation $f\colon p \to q$ whose $X$-component $f_X\colon p(X)\to q(X)$ for $X\in\smset$ sends each
\[
    (i,g)\in\sum_{i'\in p(\1)} X^{p[i']}\iso p(X)
\]
with $i\in p(\1)$ and $g\colon p[i]\to X$ to
\[
    (f_\1(i),f^\sharp_i\then g)\in\sum_{j\in q(\1)}X^{q[j]}\iso q(X).
\]
\end{proposition}
\begin{proof}
As an element of the product over $I$ on the right hand side of \eqref{eqn.main_formula}, the pair $(f_\1,f^\sharp)$ can equivalently be thought of as an $I$-indexed family of pairs $((f_\1(i),f^\sharp_i))_{i\in I}$, where each pair $(f_\1(i),f^\sharp_i)$ is an element of
\[
    \sum_{j\in q(\1)}p[i]^{q[j]}\iso q(p[i]).
\]
By the Yoneda lemma (\cref{lemma.yoneda}), we have an isomorphism $q(p[i]) \iso \poly(\yon^{p[i]}, q)$, and by the proof of the Yoneda lemma, this isomorphism sends $(f_\1(i), f^\sharp_i)$ to the natural transformation $f^i \colon \yon^{p[i]} \to q$ whose $X$-component is the function $f^i_X \colon X^{p[i]} \to q(X)$ given by sending $g \colon p[i] \to X$ to
\[
    q(g)(f_\1(i), f^\sharp_i) = \left(\sum_{j \in q(\1)} g^{q[j]}\right)(f_\1(i), f^\sharp_i) = \left(f_\1(i), g^{q[f_\1(i)]}(f^\sharp_i)\right) = (f_\1(i), f^\sharp_i \then g).
\]
Then the $p(\1)$-indexed family of natural transformations $(f^i)_{i\in p(\1)}$ is an element of
\[
  \prod_{i\in p(\1)}\poly(\yon^{p[i]}, q)\iso\poly\left(\sum_{i\in p(\1)}\yon^{p[i]},q\right),
\]
where the isomorphism follows from the universal property of coproducts, as in the proof of \cref{prop.lens-prod-sum}.
Unwinding this isomorphism, we find that $(f^i)_{i\in I}$ corresponds to the natural transformation $f$ from $\sum_{i\in p(\1)}\yon^{p[i]}\iso p$ to $q$ that we desire.
\end{proof}

\begin{example} \label{ex.morph-corolla-with-labels}
Let us return to the polynomials $p \coloneqq \yon^\3 + \2\yon$ and $q \coloneqq \yon^\4 + \yon^\2 + \2$ from \cref{ex.practice_with_poly_morphisms} and the lens $f \colon p \to q$ depicted below:
\[
\begin{tikzpicture}
	\node (p1) {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 1" below] (1) {$\bullet$}
      child[blue!50!black] {coordinate (11)}
      child[blue!50!black] {coordinate (12)}
      child[blue!50!black] {coordinate (13)};
    \node[right=1.5 of 1, red!75!black, "\tiny 1" below] (2) {$\bullet$}
      child[red!75!black] {coordinate (21)}
      child[red!75!black] {coordinate (22)}
      child[red!75!black] {coordinate (23)}
      child[red!75!black] {coordinate (24)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (13);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (13);
      \draw[postaction={decorate}] (24) to (13);
    \end{scope}
  \end{tikzpicture}
	};
%
	\node (p2) [right=1 of p1] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 2" below] (1) {$\bullet$}
      child[blue!50!black] {coordinate (11)};
    \node[right=of 1, red!75!black, "\tiny 1" below] (2) {$\bullet$}
      child[red!75!black] {coordinate (21)}
      child[red!75!black] {coordinate (22)}
      child[red!75!black] {coordinate (23)}
      child[red!75!black] {coordinate (24)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (11);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (11);
      \draw[postaction={decorate}] (24) to (11);
    \end{scope}
  \end{tikzpicture}
	};
%
	\node (p3) [below right=-1.05cm and 1 of p2] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 3" below] (1) {$\bullet$}
      child[blue!50!black] {};
    \node[right=of 1, red!75!black, "\tiny 4" below] (2) {$\bullet$}
		;
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
  \end{tikzpicture}
	};
\end{tikzpicture}
\]
Fix a set $X \coloneqq \{a,b,c,d,e\}$.
When viewed as a natural transformation, $f$ has as its $X$-component a function $f_X \colon p(X) \to q(X)$.
In other words, for any element of $p(X)$, the lens $f$ should be able to give us an element of $q(X)$.

What does an element of $p(X)$ look like?
To specify such an element, we would need to choose a position $i$ of $p$ and a function $p[i] \to X$.
We can depict this by selecting a $p$-corolla and labeling each leaf of that corolla with an element of $X$.
For example, here we depict an element $(1, g)$ of $p(X)$, where $g \colon p[1] \to X$ is given by $1 \mapsto c, 2 \mapsto e,$ and $3 \mapsto a$:
\[
\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 1" below] (1) {$\bullet$}
      child[blue!50!black] {node {$c$}}
      child[blue!50!black] {node {$e$}}
      child[blue!50!black] {node {$a$}};
\end{tikzpicture}
\]
Similarly, an element of $q(X)$ can be drawn as a $q$-corolla, with each leaf labeled by an element of $X$.
So what element of $q(X)$ is $f_X(1, g)$?

\cref{prop.morph_arena_to_func} tells us that $f_X(1, g)$ can be read off of the forest depiction of $f$ at position $1$:
\[
\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 1" below] (1) {$\bullet$}
      child[blue!50!black] {coordinate (11)}
      child[blue!50!black] {coordinate (12)}
      child[blue!50!black] {coordinate (13)};
    \node[right=1.5 of 1, red!75!black, "\tiny 1" below] (2) {$\bullet$}
      child[red!75!black] {coordinate (21)}
      child[red!75!black] {coordinate (22)}
      child[red!75!black] {coordinate (23)}
      child[red!75!black] {coordinate (24)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (13);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (13);
      \draw[postaction={decorate}] (24) to (13);
    \end{scope}
\end{tikzpicture}
\]
To draw $f_X(1, g)$, we first draw the $q$-corolla corresponding to $f_\1(1)$: the corolla on the right hand side above.
Then we label each leaf of that corolla by following the arrow from that leaf (as given by $f^\sharp_i$) to a $p[1]$-leaf, and use the label there that is given by $(1, g)$.
So $f_X(1, g)$ looks like
\[
\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[red!75!black, "\tiny 1" below] (1) {$\bullet$}
      child[red!75!black] {node {$a$}}
      child[red!75!black] {node {$c$}}
      child[red!75!black] {node {$a$}}
      child[red!75!black] {node {$a$}};
\end{tikzpicture}
\]
\end{example}

\cref{prop.morph_arena_to_func} lets us translate from lenses to natural transformations.
The following corollary tells us how to go in the other direction.
In particular, it justifies the notation $f_\1$ for the on-positions function of $f$: it is the $\1$-component of $f$ as a natural transformation.

\begin{corollary} \label{cor.morph_func_to_arena}
Let $p$ and $q$ be polynomial functors, and let $f \colon p \to q$ be a natural transformation between them.
Then the isomorphism in \eqref{eqn.main_formula} sends $f$ to the lens with on-positions function $f_\1\colon p(\1)\to q(\1)$ and on-directions map $f^\sharp\colon q[f_\1(-)]\to p[-]$, such that $f_\1$ is the $\1$-component of $f$ and, for each $i\in p(\1)$,
\[
    (f_\1(i), f^\sharp_i) = f_{p[i]}(i, \id_{p[i]}).
\]
\end{corollary}
\begin{proof}
By \cref{prop.morph_arena_to_func}, the $\1$-component of $f$ is a function $p(\1)\to q(\1)$ sending every $i \in p(\1)$ to $f_\1(i) \in q(\1)$, so the on-positions function $f_\1$ is indeed equal to the $\1$-component of $f$.
Moreover, for each $i\in p(\1)$, the $p[i]$-component $f_{p[i]} \colon p(p[i]) \to q(p[i])$ of $f$ sends $(i,\id_{p[i]})\in p(p[i])$ to $(f_\1(i), f^\sharp_i \then \id_{p[i]}) = (f_\1(i), f^\sharp_i)$.
\end{proof}

%---- Subsection ----%
\subsection{Identity and composition of lenses} \label{subsec.poly.cat.morph.id-comp}

Thus far, we have seen how the category $\poly$ of polynomial functors and natural transformations can just as easily be thought of as the category of indexed families of sets and lenses.
But in order to actually discuss the latter category, we need to be able to give identity lenses and describe how these lenses compose.
To do so, we can leverage our ability to translate back and forth between lenses and natural transformations.

For instance, given a polynomial $p$, its identity lens should correspond to the identity natural transformation of $p$ as a functor.

\begin{exercise}[Identity lenses] \label{exc.arena_morph_id}
For $p\in\poly$, let $\id_p \colon p \to p$ be its identity natural transformation, whose $X$-component $(\id_p)_X\colon p(X)\to p(X)$ for $X\in\smset$ is the identity function on $p(X)$; that is, $(\id_p)_X=\id_{p(X)}$.

Use \cref{cor.morph_func_to_arena} to show that the on-positions function $(\id_p)_\1\colon p(\1)\to p(\1)$ and the on-directions functions $(\id_p)^\sharp_i\colon p[(\id_p)_\1(i)]\to p[i]$ for $i\in p(\1)$ of $\id_p$ are all identity functions.
\begin{solution}
Fix $i\in p(\1)$.
Since the $p[i]$-component $(\id_p)_{p[i]}$ of $\id_p$ is the identity function on $p(p[i])$, by \cref{cor.morph_func_to_arena},
\[
    ((\id_p)_\1(i), (\id_p)^\sharp_i) = (\id_p)_{p[i]}(i,\id_{p[i]}) = (i,\id_{p[i]}).
\]
Hence the on-positions function $(\id_p)_\1\colon p(\1)\to p(\1)$ maps every $i\in p(\1)$ to itself, so it is an identity function; and each on-directions function $(\id_p)^\sharp_i\colon p[i]\to p[i]$ is equal to $\id_{p[i]}$.
\end{solution}
\end{exercise}

Similarly, we should be able to deduce how two lenses compose by translating them to natural transformations, composing those, then translating back to lenses.

\begin{exercise}[Composing lenses] \label{exc.arena_morph_comp}
For $p,q,r\in\poly$, let $f\colon p\to q$ and $g\colon q\to r$ be natural transformations, and let $h\coloneqq f\then g$ be their composite, whose $X$-component $h_X\colon p(X)\to r(X)$ for $X\in\smset$ is the composite of the $X$-components of $f$ and $g$; that is, $h_X=f_X\then g_X$.

Use \cref{cor.morph_func_to_arena} to show that the on-positions function $h_\1\colon p(\1)\to r(\1)$ of $h$ is given by $h_\1=f_\1\then g_\1$, while the on-directions function $h^\sharp_i$ of $h$ for $i\in p(\1)$ is given by $h^\sharp_i=g^\sharp_{f_\1(i)}\then f^\sharp_i$.
\begin{solution}
Fix $i\in p(\1)$.
By \cref{cor.morph_func_to_arena} and \cref{prop.morph_arena_to_func},
\begin{align*}
    (h_\1(i), h^\sharp_i) &= h_{p[i]}(i, \id_{p[i]}) \tag{\cref{cor.morph_func_to_arena}} \\
    &= g_{p[i]}(f_{p[i]}(i, \id_{p[i]})) \tag{$h = f \then g$} \\
    &= g_{p[i]}(f_\1(i), f^\sharp_i) \tag{\cref{cor.morph_func_to_arena}} \\
    &= (g_\1(f_\1(i)), g^\sharp_{f_\1(i)} \then f^\sharp_i). \tag{\cref{prop.morph_arena_to_func}}
\end{align*}
\end{solution}
\end{exercise}

% TODO: turn following to prop?
% The following corollary about interpreting commutative diagrams in $\poly$ is immediate from the preceding exercise.

The following proposition, a restatement of the previous exercise, allows us to interpret commutative diagrams in $\poly$ in terms of commutative diagrams in the more familiar setting of $\smset$.

\begin{proposition} \label{prop.comm_poly}
Given $p,q,r\in\poly$ and lenses $f\colon p\to q, g\colon q\to r,$ and $h\colon p\to r$, the diagram
\[
\begin{tikzcd}
    p \ar[r, "f"] \ar[dr, "h"'] & q \ar[d, "g"] \\
    & r
\end{tikzcd}
\]
commutes in $\poly$ if and only if the forward on-positions diagram
\[
\begin{tikzcd}
    p(\1) \ar[r, "f_\1"] \ar[dr, "h_\1"'] & q(\1) \ar[d, "g_\1"] \\
    & r(\1)
\end{tikzcd}
\]
commutes in $\smset$ and, for each $i \in p(\1)$, the backward on-directions diagram
\[
\begin{tikzcd}
    p[i] & q[f_\1(i)] \ar[l, "f^\sharp_i"'] \\
    & r[h_\1(i)] \ar[u, "g^\sharp_{f_\1(i)}"'] \ar[ul, "h^\sharp_i"]
\end{tikzcd}
\]
commutes in $\smset$.
\end{proposition}

We can use this fact to determine whether a given diagram in $\poly$ commutes, as in the following exercise.

\begin{exercise}
Using \cref{prop.comm_poly}, verify explicitly that, for $p, q \in \poly$, the polynomial $p+q$ given by the binary sum of $p$ and $q$ satisfies the universal property of the coproduct of $p$ and $q$.
That is, provide lenses $\iota \colon p \to p + q$ and $\kappa \colon q \to p + q$, then show that for any other polynomial $r$ equipped with lenses $f \colon p \to r$ and $g \colon q \to r$, there exists a unique lens $h\colon p+q\to r$---shown dashed---making the following diagram commute:
\begin{equation} \label{eqn.coprod_univ_prop}
\begin{tikzcd}
	p \ar[r, "\iota"] \ar[dr, "f"'] &
	p + q \ar[d, "h", dashed] &
	q \ar[l, "\kappa"'] \ar[dl, "g"] \\
	& r
\end{tikzcd}
\end{equation}
\begin{solution}
We provide lenses $\iota\colon p\to p+q$ and $\kappa\colon q\to p+q$ as follows.
On positions, they are the canonical inclusions $\iota_\1\colon p(\1)\to p(\1)+q(\1)$ and $\kappa_\1\colon q(\1)\to p(\1)+q(\1)$; on directions, they are identities.
To show that $p+q$ equipped with $\iota$ and $\kappa$ satisfies the universal property of the coproduct, we apply \cref{prop.comm_poly}.
In order for \eqref{eqn.coprod_univ_prop} to commute, it must commute on positions---that is, the following diagram of sets must commute:
\begin{equation} \label{eqn.coprod_univ_prop_pos}
\begin{tikzcd}
	p(\1) \ar[r, "\iota_\1"] \ar[dr, "f_\1"'] &
	p(\1) + q(\1) \ar[d, "h_\1", dashed] &
	q(\1) \ar[l, "\kappa_\1"'] \ar[dl, "g_\1"] \\
	& r(\1)
\end{tikzcd}
\end{equation}
But since $p(1)+q(\1)$ along with the inclusions $\iota_\1$ and $\kappa_\1$ form the coproduct of $p(\1)$ and $q(\1)$ in $\smset$, there exists a unique $h_\1$ for which \eqref{eqn.coprod_univ_prop_pos} commutes.
Hence $h$ is uniquely characterized on positions.
In particular, it must send each $(1,i) \in p(\1)+q(\1)$ with $i \in p(\1)$ to $f_\1(i)$ and each $(2,j) \in p(\1)+q(\1)$ with $j \in q(\1)$ to $g_\1(j)$.

Moreover, if \eqref{eqn.coprod_univ_prop} is to commute on directions, then for every $i\in p(\1)$ and $j \in q(\1)$, the following diagrams of sets must commute:
\begin{equation} \label{eqn.coprod_univ_prop_dir}
\begin{tikzcd}[sep=large]
	p[i] & (p+q)[(1,i)] \ar[l, "\iota^\sharp_i"'] & (p+q)[(2,j)] \ar[r, "\kappa^\sharp_j"] & q[j] \\
	& r[f_\1(i)] \ar[ul, "f^\sharp_i"] \ar[u, "h^\sharp_{(1,i)}"', dashed] & r[g_\1(j)] \ar[u, "h^\sharp_{(2,j)}", dashed] \ar[ur, "g^\sharp_j"']
\end{tikzcd}
\end{equation}
But $(p+q)[(1,i)] \iso p[i]$ and $\iota^\sharp_i$ is the identity, so we must have $h^\sharp_{(1,i)} = f^\sharp_i$.
Similarly, $(p+q)[(2,j)] \iso q[j]$ and $\kappa^\sharp_j$ is the identity, so we must have $h^\sharp_{(2,j)} = g^\sharp_j$.
Hence $h$ is also uniquely characterized on directions, so it is unique overall.
Moreover, we have shown that we can define $h$ on positions so that \eqref{eqn.coprod_univ_prop_pos} commutes, and that we can define $h$ on directions such that the diagrams in \eqref{eqn.coprod_univ_prop_dir} commute.
As the commutativity of the diagrams in \eqref{eqn.coprod_univ_prop_pos} and \eqref{eqn.coprod_univ_prop_dir} together imply the commutativity of \eqref{eqn.coprod_univ_prop}, it follows that there exists $h$ for which \eqref{eqn.coprod_univ_prop} commutes.
\end{solution}
\end{exercise}

\begin{exercise}[A functor $\Cat{Top}\to\poly$] \label{exc.top_poly_func}
This exercise is for those who know what topological spaces and continuous maps are. It will not be used again in this book.
\begin{enumerate}
	\item Given a topological space $X$, define a polynomial $p_X$ whose positions are the points in $X$ and whose directions at $x\in X$ are the open neighborhoods of $x$.
  That is,
  \[
    p_X\coloneqq\sum_{x \in X}\yon^{\{ U\ss X \mid x\in U, \, U\text{ open} \}}.
  \]
  Give a formula by which any continuous map $X\to Y$ induces a lens $p_X\to p_Y$.
	\item Show that your formula defines a functor.
	\item Is it full? Faithful?
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
	\item \label{exc.top_poly_func.morphs} For every continuous map $f \colon X \to Y$, we give a lens $p_f \colon p_X \to p_Y$.
	The on-positions function is just $f$; then for each $p_X$-position $x\in X$, the on-directions function $(p_f)^\sharp_x\colon p_Y[f(x)]\to p_X[x]$ sends each open neighborhood $U$ of $f(x)$ to $f\inv(U)$, which we know is an open neighborhood of $x$ because $f$ is continuous.

	\item To show that $p_X$ is functorial in $X$, it suffices to show that sending continuous maps $f\colon X\to Y$ to their induced lenses $p_f\colon p_X\to p_Y$ preserves identities and composition.
	First, we show that for any topological space $X$, the lens $p_{\id_X}$, where $\id_X$ is the identity map on $X$, is the identity lens.
	By \cref{exc.top_poly_func.morphs}, the on-positions function of $p_{\id_X}$ is $\id_X$, and for each $x\in X$ the on-directions function $(p_f)^\sharp_x\colon p_X[x]\to p_X[x]$ sends $U\in p_X[x]$ to $(\id_X)\inv(U) = U$.
	Hence $p_{\id_X}$ is the identity on both positions and directions; it follows from \cref{exc.arena_morph_id} that $p_{\id_X}$ is the identity lens.

	We now show that for topological spaces $X,Y,$ and $Z$ and continuous maps $f\colon X\to Y$ and $g\colon Y\to Z$, we have $p_f\then p_g = p_{f\then g}$.
	By \cref{exc.top_poly_func.morphs} and \cref{exc.arena_morph_comp}, the on-positions function of either side is equal to $f\then g$, so it suffices to show for each $x\in X$ that
	\[
	    (p_{f \then g})^\sharp_x = (p_g)^\sharp_{f(x)} \then (p_f)^\sharp_x.
	\]
	Again by \cref{exc.top_poly_func.morphs}, the left hand side sends each $U\in p_Z[g(f(x))]$ to $(f \then g)\inv(U)$, while the right hand side sends $U$ to $f\inv(g\inv(U))$; by elementary set theory, these sets are equal.

	\item The functor is not full.
	Consider the spaces $X\coloneqq\2$ with the indiscrete topology (i.e.\ the only open sets are the empty set and $X$) and $Y\coloneqq\2$ with the discrete topology (i.e.\ all subsets are open).
	Then $p_X\iso\2\yon$ (each point in $X$ has one open neighborhood: the entire space $X$) and $p_Y\iso\2\yon^\2$ (each point in $Y$ has two open neighborhoods: a singleton set and $Y$ itself), so our functor induces a map from the set of continuous functions $X\to Y$ to the set of lenses $\2\yon\to\2\yon^\2$.
	We claim this map is not surjective: in particular, consider the lens $h\colon\2\yon\to\2\yon^\2$ that is the identity on positions (and uniquely defined on directions).
	Then a continuous function $f\colon X\to Y$ that our functor sends to $h$ must also be the identity on the underlying sets of $X$ and $Y$.
	But such a function cannot be continuous: a singleton subset of $Y$ is open, but its preimage under $f$ is a singleton subset of $X$ and therefore not open.
	So our functor sends no continuous function $X\to Y$ to $h$ and therefore is not full.

	The functor is, however, faithful: given spaces $X$ and $Y$ and continuous function $f\colon X\to Y$, we can uniquely recover $f$ from $p_f$ by taking its on-positions function $(p_f)_\1$.
\end{enumerate}
\end{solution}
\end{exercise}

%-------- Section --------%
\section{Symmetric monoidal products of polynomial functors} \label{sec.poly.cat.monoidal}

One of the reasons $\poly$ is so versatile is that there is an abundance of monoidal structures on it.
Monoidal structures are the key ingredient to many applications of categories to real-world settings, and $\poly$ is no different in that regard.
But there is an added bonus about the monoidal products on $\poly$ that we'll introduce that isn't shared by some fancy monoidal products on other categories like ``the tensor product of modules over a commutative ring'': if you know how to add, multiply, and exponentiate expressions from high school algebra, then you already know how to compute these monoidal products!

We have already seen one monoidal structure on $\poly$: the cocartesian monoidal structure, which gives $\poly$ its finite coproducts.
In fact, we know from \cref{prop.poly_coprods} that $\poly$ has all coproducts---and they are given by an operation that looks just like addition.
We'll quickly see in \cref{subsec.poly.cat.monoidal.prod} that $\poly$ has all products as well, giving it a cartesian monoidal structure.
Can you guess what that operation looks like?

Much of \cref{part.comon} will focus on the astonishing features of another monoidal structure, an asymmetric one, which we will hold off on defining---we'll save its surprises for when we can better savor them.
But here in \cref{subsec.poly.cat.monoidal.par}, we will introduce a third symmetric monoidal structure, given by an operation you probably weren't allowed to do to polynomials back in high school.
This monoidal structure will really come in handy when we apply $\poly$ to dynamics in the next chapter.

%---- Subsection ----%
\subsection{The categorical product} \label{subsec.poly.cat.monoidal.prod}
The category $\poly$ has limits and colimits, is cartesian closed, has epi-mono factorizations, etc., etc. However, in order to tell a good story of dynamics, we only need products right now. These will be useful for letting many different interfaces control the same internal dynamics.

\begin{proposition}\label{prop.poly_prods}
The category $\poly$ has arbitrary products, coinciding with products in $\smset^\smset$ given by the operation $\prod_{i \in I}$.
\end{proposition}
\begin{proof}
Unsurprisingly, the proof is very similar to that of \cref{prop.poly_coprods}.

By \cref{cor.sum_prod_set_endofuncs}, the category $\smset^\smset$ has arbitrary products given by $\prod_{i \in I}$.
The full subcategory inclusion $\poly \to \smset^\smset$ reflects these products.
It remains to show that $\poly$ is closed under the operation $\prod_{i \in I}$.

By \cref{prop.set_endofunc_distrib}, $\smset^\smset$ is completely distributive.
Hence, given polynomials $(p_i)_{i \in I}$, we can use \eqref{eqn.cat_completely_distributive} to write their product in $\smset^\smset$ as
\begin{equation} \label{eqn.poly_prod}
    \prod_{i \in I} p_i \iso \prod_{i \in I} \sum_{j \in p_i(\1)} \yon^{p_i[j]} \iso \sum_{\bar{j} \in \prod_{i \in I} p_i(\1)} \prod_{i \in I} \yon^{p_i[\bar{j}(i)]} \iso \sum_{\bar{j} \in \prod_{i \in I} p_i(\1)} \yon^{\sum_{i \in I} p_i[\bar{j}(i)]},
\end{equation}
which, as a coproduct of representables, is in $\poly$.
% We will see that $\1$ is a terminal object and that the product of $p$ and $q$ in $\poly$ is the usual product of $p$ and $q$ as polynomials. That is, if $p\coloneqq\sum_{i\in p(\1)}\yon^{p[i]}$ and $q\coloneqq\sum_{j\in q(\1)}\yon^{q[j]}$ are in standard notation, then
% \begin{equation}\label{eqn.poly_times}
% p\times q\cong\sum_{i\in p(\1)}\sum_{j\in q(\1)}\yon^{p[i]+q[j]}.
% \end{equation}
% We leave the proof as an exercise; see \cref{exc.poly_times}.
\end{proof}

\begin{corollary} \label{prop.poly_completely_distributive}
The category $\poly$ is completely distributive.
\end{corollary}
\begin{proof}
This is a direct consequence of the fact that $\poly$ has arbitrary (co)products coinciding with (co)products in $\smset^\smset$ (\cref{prop.poly_coprods,prop.poly_prods}) and the fact that $\smset^\smset$ itself is completely distributive (\cref{prop.set_endofunc_distrib}).
\end{proof}

The result above will allow us to apply \eqref{eqn.cat_completely_distributive}, or sometimes specifically \eqref{eqn.push_prod_sum_obj_indep}, to push $\prod$'s past $\sum$'s of polynomials whenever we so desire.

\begin{exercise}%\label{exc.poly_prod}
% \begin{enumerate}
% 	\item Use \eqref{eqn.main_formula} to verify that $\1$ is terminal in $\poly$.

% 	\item Use \eqref{eqn.main_formula} and  \eqref{eqn.poly_times} to verify that
% 	\[
% 	    \poly(r, p \times q) \iso \poly(r, p) \times \poly(r, q)
% 	\]
% 	for all polynomials $p,q,r$.

% 	\item
	Use \eqref{eqn.main_formula}
% 	and \eqref{eqn.poly_prod}
	to verify that
	\[
	    \poly\left(q, \prod_{i \in I} p_i\right) \iso \prod_{i \in I} \poly(q, p_i)
	\]
	for all polynomials $(p_i)_{i \in I}$ and $q$, as one would expect from the universal property of products.
\qedhere
% \end{enumerate}
\begin{solution}
% \begin{enumerate}
    % \item To verify that $\1$ is terminal in $\poly$, we use \eqref{eqn.main_formula} to show that $\poly(p, \1) \iso \1$ for all $p \in \poly$:
    % \[
    %     \poly(p, \1) \iso \prod_{i \in p(\1)} \sum_{j \in \1} p[i]^\0 \iso \prod_{i \in p(\1)} \1 \iso \1.
    % \]

    % \item Given $p,q,r \in \poly$, we use \eqref{eqn.main_formula} and  \eqref{eqn.poly_times} to verify that
    % \begin{align*}
    %     \poly(r, p \times q) &\iso \poly\left(r, \sum_{i \in p(\1)} \sum_{j \in q(\1)} \yon^{p[i] + q[j]} \right)
    %     \tag*{\eqref{eqn.poly_times}} \\
    %     &\iso \prod_{k \in r(\1)} \sum_{i \in p(\1)} \sum_{j \in q(\1)} r[k]^{p[i] + q[j]} \tag*{\eqref{eqn.main_formula}} \\
    %     &\iso \prod_{k \in r(\1)} \left(\sum_{i \in p(\1)} r[k]^{p[i]}\right) \times \left(\sum_{j \in q(\1)} r[k]^{q[j]}\right)  \\
    %     &\iso \left(\prod_{k \in r(\1)} \sum_{i \in p(\1)} r[k]^{p[i]}\right) \times \left(\prod_{k \in r(\1)} \sum_{j \in q(\1)} r[k]^{q[j]}\right) \\
    %     &\iso \poly(r, p) \times \poly(r, q).
    %     \tag*{\eqref{eqn.main_formula}}
    % \end{align*}

    % \item
    Given $q \in \poly$ and $p_i \in \poly$ for each $i \in I$ for some set $I$, we use \eqref{eqn.main_formula} to verify that
    \begin{align*}
        \poly\left(q, \prod_{i \in I} p_i\right) &\iso \prod_{k \in q(\1)} \left(\prod_{i \in I} p_i\right)(q[k])
        \tag*{\eqref{eqn.main_formula}} \\
        &\iso \prod_{k \in q(\1)} \prod_{i \in I} p_i(q[k]) \\
        &\iso \prod_{i \in I} \prod_{k \in q(\1)} p_i(q[k]) \\
        &\iso \prod_{i \in I} \poly(q, p_i).
        \tag*{\eqref{eqn.main_formula}}
    \end{align*}
% \end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Let $p_1\coloneqq\yon+\1, p_2\coloneqq\yon+\2,$ and $p_3\coloneqq\yon^\2$.
What is $\prod_{i\in\3}p_i$ according to \eqref{eqn.poly_prod}? Is the answer what you would expect?
\begin{solution}
Given $p_1\coloneqq\yon+\1,p_2\coloneqq\yon+\2,$ and $p_3\coloneqq\yon^\2$, we compute $\prod_{i\in\3}p_i$ via \eqref{eqn.poly_prod} as follows:
\begin{align*}
    \prod_{i\in\3} p_i
    &\iso
    \sum_{\bar{j} \in \prod_{i\in\3} p_i(\1)} \yon^{\sum_{i\in\3} p_i[\bar{j}(i)]}
    \tag*{\eqref{eqn.poly_prod}} \\
    &\iso
    \sum_{\bar{j} \colon (i\in\3) \to p_i(\1)} \yon^{p_1[\bar{j}(1)] + p_2[\bar{j}(2)] + p_3[\bar{j}(3)]} \\
    &\iso
    \yon^{p_1[1] + p_2[1] + p_3[1]}
    + \yon^{p_1[1] + p_2[2] + p_3[1]}
    + \yon^{p_1[1] + p_2[3] + p_3[1]} \\
    &+ \yon^{p_1[2] + p_2[1] + p_3[1]}
    + \yon^{p_1[2] + p_2[2] + p_3[1]}
    + \yon^{p_1[2] + p_2[3] + p_3[1]} \\
    &\iso
    \yon^{\1 + \1 + \2}
    + \yon^{\1 + \0 + \2}
    + \yon^{\1 + \0 + \2} \\
    &+ \yon^{\0 + \1 + \2}
    + \yon^{\0 + \0 + \2}
    + \yon^{\0 + \0 + \2} \\
    % &\iso
    % \yon^\4 + \yon^\3 + \yon^\3 + \yon^\3 + \yon^\2 + \yon^\2 \\
    &\iso
    \yon^\4 + \3\yon^\3 + \2\yon^\2,
\end{align*}
as we might expect from standard polynomial multiplication.
\end{solution}
\end{exercise}

It follows from \eqref{eqn.poly_prod} that the terminal object of $\poly$ is $\1$, and that binary products are given by
\begin{equation}\label{eqn.poly_times}
    p \times q \iso \sum_{i \in p(\1)} \sum_{j \in q(\1)} \yon^{p[i] + q[j]}.
\end{equation}

We will sometimes write $pq$ rather than $p\times q$:
\begin{equation} \tag{Notation}
pq\coloneqq p\times q
\end{equation}

\begin{example}
We can draw the product of two polynomials in terms of their associated forests. Let $p\coloneqq\yon^\3+\yon$ and $q\coloneqq\yon^\4+\yon^\2+\1$.
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, blue!50!black, "$p$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$}
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$}
      child {};
  \end{tikzpicture}
  };
%
	\node (p2) [draw, red!75!black, right=2 of p1, "$q$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$}
      child {}
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$}
      child {}
      child {};
    \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$}
    ;
  \end{tikzpicture}
  };
\end{tikzpicture}
\]
Then $pq\cong\yon^\7+\2\yon^\5+\2\yon^\3+\yon$.
We take all pairs of positions, and for each pair we take the disjoint union of the directions.
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, "$pq$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny {(1,1)}" below] (11) {$\bullet$}
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 11, "\tiny {(1,2)}" below] (12) {$\bullet$}
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 12, "\tiny {(1,3)}" below] (13) {$\bullet$}
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[blue!50!black] {};
    \node[right=1.5 of 13, "\tiny {(2,1)}" below] (21) {$\bullet$}
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 21, "\tiny {(2,2)}" below] (22) {$\bullet$}
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 22, "\tiny {(2,3)}" below] (23) {$\bullet$}
      child[blue!50!black] {};
	\end{tikzpicture}
	};
\end{tikzpicture}
\]
\end{example}

In practice, we can multiply polynomial functors the same way we would multiply two polynomials in high school algebra.

\begin{exercise} \label{exc.general_poly_times}
\begin{enumerate}
    \item \label{exc.general_poly_times.monomial} Show that for sets $A_1, B_1, A_2, B_2$, we have
    \[
        B_1\yon^{A_1} \times B_2\yon^{A_2} \iso B_1 B_2\yon^{A_1 + A_2}.
    \]
    \item \label{exc.general_poly_times.polynomial} Show that for sets $(A_i)_{i \in I},(A_j)_{j \in J},(B_i)_{i \in I},$ and $(B_j)_{j \in J}$, we have
    \[
        \left(\sum_{i \in I} B_i\yon^{A_i}\right) \times \left(\sum_{j \in J} B_j\yon^{A_j}\right) \iso \sum_{i \in I} \sum_{j \in J} B_i B_j \yon^{A_i + A_j}.
    \]
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We compute the product $B_1\yon^{A_1} \times B_2\yon^{A_2}$ using \eqref{eqn.poly_times}:
    \begin{align*}
        B_1\yon^{A_1} \times B_2\yon^{A_2} &\iso \left(\sum_{i \in B_1} \yon^{A_1}\right) \times \left(\sum_{j \in B_2} \yon^{A_2}\right) \\
        &\iso \sum_{i \in B_1} \sum_{j \in B_2} \yon^{A_1 + A_2} \\
        &\iso B_1 B_2\yon^{A_1 + A_2}.
    \end{align*}

    \item We expand the product $\left(\sum_{i \in I} B_i\yon^{A_i}\right) \times \left(\sum_{j \in J} B_j\yon^{A_j}\right)$ by applying \eqref{eqn.set_completely_distributive}, with $I_1 \coloneqq I$ and $I_2 \coloneqq J$:
    \begin{align*}
        \left(\sum_{i \in I} B_i\yon^{A_i}\right) \times \left(\sum_{j \in J} B_j\yon^{A_j}\right) &\iso \prod_{k \in \2} \sum_{i \in I_k} B_i\yon^{A_i} \\
        &\iso \sum_{\bar{i} \in \prod_{k \in \2} I_k} \prod_{k \in \2} B_{\bar{i}(k)}\yon^{A_{\bar{i}(k)}} \\
        &\iso \sum_{i \in I} \sum_{j \in J} B_i\yon^{A_i} \times B_j\yon^{A_j} \\
        &\iso \sum_{i \in I} \sum_{j \in J} B_i B_j \yon^{A_i + A_j}
    \end{align*}
    where the last isomorphism follows from \cref{exc.general_poly_times.monomial}.
\end{enumerate}
\end{solution}
\end{exercise}

As lenses, the canonical projections $\pi \colon pq \to p$ and $\varphi \colon pq \to q$ behave as you might expect: on positions, they are the projections from $(pq)(\1) \iso p(\1) \times q(\1)$ to $p(\1)$ and $q(\1)$, respectively; on directions, they are the inclusions $p[i] \to p[i] + q[j]$ and $q[j] \to p[i] + q[j]$ for each position $(i, j)$ of $pq$.

\begin{exercise} \label{exc.poly_prod}
Verify that, for $p, q \in \poly$, the polynomial $pq$ given by \eqref{eqn.poly_times} along with the lenses $\pi \colon pq \to p$ and $\varphi \colon pq \to q$ described above satisfy the universal property of the product of $p$ and $q$.
\begin{solution}
We wish to show that, for $p, q \in \poly$, the polynomial $pq$ along with the lenses $\pi \colon pq \to p$ and $\varphi \colon pq \to q$ as described in the text satisfy the universal property of the product.
That is, we must show that for any $r \in \poly$ and lenses $f \colon r \to p$ and $g \colon r \to q$, there exists a unique lens $h \colon r \to pq$ for which the following diagram commutes:
\begin{equation} \label{eqn.prod_univ_prop}
\begin{tikzcd}
	r \ar[d, "f"'] \ar[r, "g"] \ar[dr, "h", dashed] & q \\
	p & pq. \ar[l, "\pi"] \ar[u, "\varphi"']
\end{tikzcd}
\end{equation}
We apply \cref{prop.comm_poly}.
In order for \eqref{eqn.prod_univ_prop} to commute, it must commute on positions---that is, the following diagram of sets must commute:
\begin{equation} \label{eqn.prod_univ_prop_pos}
\begin{tikzcd}
	r(\1) \ar[d, "f_\1"'] \ar[r, "g_\1"] \ar[dr, "h_\1", dashed] & q(\1) \\
	p(\1) & (pq)(\1). \ar[l, "\pi_\1"] \ar[u, "\varphi_\1"']
\end{tikzcd}
\end{equation}
But since $(pq)(\1) \iso p(1) \times q(\1)$ along with the projections $\pi_\1$ and $\varphi_\1$ form the product of $p(\1)$ and $q(\1)$ in $\smset$, there exists a unique $h_\1$ for which \eqref{eqn.prod_univ_prop_pos} commutes.
Hence $h$ is uniquely characterized on positions.
In particular, it must send each $k \in r(\1)$ to the pair $(f_\1(k), g_\1(k)) \in (pq)(\1)$.

Moreover, if \eqref{eqn.coprod_univ_prop} is to commute on directions, then for every $k \in r(\1)$, the following diagram of sets must commute:
\begin{equation} \label{eqn.prod_univ_prop_dir}
\begin{tikzcd}[sep=large]
	r[k] & q[g_\1(k)] \ar[l, "g^\sharp_k"'] \ar[d, "\varphi^\sharp_{(f_\1(k), g_\1(k))}"] \\
	p[f_\1(k)] \ar[u, "f^\sharp_k"] \ar[r, "\pi^\sharp_{(f_\1(k), g_\1(k))}"'] & (pq)[(f_\1(k), g_\1(k))]. \ar[ul, "h^\sharp_k"', dashed]
\end{tikzcd}
\end{equation}
As $(pq)[(f_\1(k), g_\1(k))] \iso p[f_\1(k)] + q[g_\1(k)]$ along with the inclusions $\pi^\sharp_{(f_\1(k), g_\1(k))}$ and $\varphi^\sharp_{(f_\1(k), g_\1(k))}$ form the coproduct of $p[f_\1(k)]$ and $q[g_\1(k)]$ in $\smset$, there exists a unique $h^\sharp_k$ for which \eqref{eqn.prod_univ_prop_dir} commutes.
Hence $h$ is also uniquely characterized on directions, so it is unique overall.
Moreover, we have shown that we can define $h$ on positions so that \eqref{eqn.prod_univ_prop_pos} commutes, and that we can define $h$ on directions such that \eqref{eqn.prod_univ_prop_dir} commutes.
As the commutativity of \eqref{eqn.prod_univ_prop_pos} and \eqref{eqn.prod_univ_prop_dir} together imply the commutativity of \eqref{eqn.prod_univ_prop}, it follows that there exists $h$ for which \eqref{eqn.prod_univ_prop} commutes.
\end{solution}
\end{exercise}

%---- Subsection ----%
\subsection{The parallel product} \label{subsec.poly.cat.monoidal.par}
There is a closely related monoidal structure on $\poly$ that will be useful for putting dynamical systems in parallel.

\begin{definition}[Parallel product of polynomials] \label{def.parallel}
Let $p$ and $q$ be polynomials. Their \emph{parallel product}, denoted $p\otimes q$, is given by the formula:
\begin{equation}\label{eqn.parallel_def}
p\otimes q\iso\sum_{i\in p(\1)}\sum_{j\in q(\1)}\yon^{p[i]\times q[j]}.
\end{equation}
% On arenas, this is defined by
% \begin{equation}\label{eqn.parallel_product_dependent}
% \sum_{i \in p(\1)}\yon^{p[i]} \otimes \sum_{j \in q(\1)}\yon^{q[j]} := \sum_{(i,j) \in p(\1) \times q(\1)}\yon^{p[i]\times q[j]}.
% \end{equation}
\end{definition}

One should compare this with the formula for the product of polynomials shown in \eqref{eqn.poly_times}. The difference is that the parallel product multiplies exponents where the categorical product adds them.

\begin{example}
We can draw the parallel product of two polynomials in terms of their associated forests. Let $p\coloneqq\yon^\3+\yon$ and $q\coloneqq\yon^\4+\yon^\2+\1$.
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, blue!50!black, "$p$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$}
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$}
      child {};
  \end{tikzpicture}
  };
%
	\node (p2) [draw, red!75!black, right=2 of p1, "$q$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$}
      child {}
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$}
      child {}
      child {};
    \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$}
    ;
  \end{tikzpicture}
  };
\end{tikzpicture}
\]
Then $p\otimes q\cong\yon^{\1\2}+\yon^\6+\yon^\4+\yon^\2+\2$.
We take all pairs of positions, and for each pair we take the product of the directions.
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, "$p \otimes q$" above] {
	\begin{tikzpicture}[trees, sibling distance=2mm]
    \node["\tiny {(1,1)}" below] (11) {$\bullet$}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
    ;
    \node[right=2 of 11, "\tiny {(1,2)}" below] (12) {$\bullet$}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
    ;
    \node[right=1.5 of 12, "\tiny {(1,3)}" below] (13) {$\bullet$}
    ;
   \node[right=1.5 of 13, "\tiny {(2,1)}" below] (21) {$\bullet$}
      child {}
      child {}
      child {}
      child {}
 		;
		\node[right=1.5 of 21, "\tiny {(2,2)}" below] (22) {$\bullet$}
      child {}
      child {}
 		;
    \node[right=1.5 of 22, "\tiny {(2,3)}" below] (23) {$\bullet$}
 		;
	\end{tikzpicture}
	};
\end{tikzpicture}
\]
\end{example}

% \begin{exercise}
% \begin{enumerate}
%     \item Compute the parallel product of monomials $B_1\yon^{A_1}\otimes B_2\yon^{A_2}$.
%     \item
% \end{enumerate}
% \begin{solution}
% The parallel product of monomials $B_1\yon^{A_1}\otimes B_2\yon^{A_2}$ is $B_1 B_2\yon^{A_1 A_2}$.
% \end{solution}
% \end{exercise}


\begin{exercise} \label{exc.general_poly_parallel_times}
\begin{enumerate}
    \item \label{exc.general_poly_parallel_times.monomial} Show that for sets $A_1, B_1, A_2, B_2$, we have
    \[
        B_1\yon^{A_1} \otimes B_2\yon^{A_2} \iso B_1 B_2\yon^{A_1 A_2}.
    \]
    \item \label{exc.general_poly_parallel_times.polynomial} Show that for sets $(A_i)_{i \in I},(A_j)_{j \in J},(B_i)_{i \in I},$ and $(B_j)_{j \in J}$, we have
    \[
        \left(\sum_{i \in I} B_i\yon^{A_i}\right) \otimes \left(\sum_{j \in J} B_j\yon^{A_j}\right) \iso \sum_{i \in I} \sum_{j \in J} B_i B_j \yon^{A_i A_j}.
    \]
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We compute the parallel product $B_1\yon^{A_1} \otimes B_2\yon^{A_2}$ using \eqref{eqn.parallel_def}:
    \begin{align*}
        B_1\yon^{A_1} \otimes B_2\yon^{A_2} &\iso \left(\sum_{i \in B_1} \yon^{A_1}\right) \otimes \left(\sum_{j \in B_2} \yon^{A_2}\right) \\
        &\iso \sum_{i \in B_1} \sum_{j \in B_2} \yon^{A_1 \times A_2} \\
        &\iso B_1 B_2\yon^{A_1 A_2}.
    \end{align*}

    \item We expand the parallel product $\left(\sum_{i \in I} B_i\yon^{A_i}\right) \otimes \left(\sum_{j \in J} B_j\yon^{A_j}\right)$ as follows:
    \begin{align*}
        \left(\sum_{i \in I} B_i\yon^{A_i}\right) \otimes \left(\sum_{j \in J} B_j\yon^{A_j}\right) &\iso \left(\sum_{i \in I} \sum_{i' \in B_i} \yon^{A_i}\right) \otimes \left(\sum_{j \in J} \sum_{j' \in B_j} \yon^{A_j}\right) \\
        &\iso \sum_{i \in I} \sum_{i' \in B_i} \sum_{j \in J} \sum_{j' \in B_j} \yon^{A_i \times A_j} \\
        &\iso \sum_{i \in I} \sum_{j \in J} \sum_{i' \in B_i} \sum_{j' \in B_j} \yon^{A_i A_j} \\
        &\iso \sum_{i \in I} \sum_{j \in J} B_i B_j \yon^{A_i A_j}.
    \end{align*}
\end{enumerate}
\end{solution}
\end{exercise}




\begin{exercise}
Let $p\coloneqq\yon^\2+\yon$ and $q\coloneqq\2\yon^\4$.
\begin{enumerate}
	\item Draw $p$ and $q$ as corolla forests.
	\item Draw $pq=p\times q$ as a corolla forest.
	\item Draw $p\otimes q$ as a corolla forest.
\qedhere
\end{enumerate}
\begin{solution}
Here $p\coloneqq\yon^\2+\yon$ and $q\coloneqq\2\yon^\4$.
\begin{enumerate}
\item Here are $p$ and $q$ drawn as corolla forests:
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, blue!50!black, "$p$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$}
      child {};
  \end{tikzpicture}
  };
%
	\node (p2) [draw, red!75!black, right=2 of p1, "$q$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$}
      child {}
      child {}
      child {}
      child {};
    \node[right=1 of 1,"\tiny 2" below] (2) {$\bullet$}
      child {}
      child {}
      child {}
      child {};
  \end{tikzpicture}
  };
\end{tikzpicture}
\]

\item Here is $pq$ drawn as a corolla forest:
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, "$pq$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny {(1,1)}" below] (11) {$\bullet$}
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 11, "\tiny {(1,2)}" below] (12) {$\bullet$}
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 12, "\tiny {(2,1)}" below] (21) {$\bullet$}
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 21, "\tiny {(2,2)}" below] (22) {$\bullet$}
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
	\end{tikzpicture}
	};
\end{tikzpicture}
\]
\item Here is $p \otimes q$ drawn as a corolla forest:
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, "$pq$" above] {
	\begin{tikzpicture}[trees, sibling distance=2mm]
    \node["\tiny {(1,1)}" below] (11) {$\bullet$}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
    ;
    \node[right=2 of 11, "\tiny {(1,2)}" below] (12) {$\bullet$}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
    ;
   \node[right=1.5 of 12, "\tiny {(2,1)}" below] (21) {$\bullet$}
      child {}
      child {}
      child {}
      child {}
 	;
	\node[right=1.5 of 21, "\tiny {(2,2)}" below] (22) {$\bullet$}
      child {}
      child {}
      child {}
      child {}
 	;
	\end{tikzpicture}
	};
\end{tikzpicture}
\]
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Consider the polynomials $p\coloneqq \2\yon^\2+\3\yon$ and $q\coloneqq\yon^\4+\3\yon^\3$.
\begin{enumerate}
	\item What is $p\times q$?
	\item What is $p\otimes q$?
	\item What is the product of the following purely formal expression we'll see \emph{only this once!}:
	\[
	(2\mdot2^\yon+3\mdot 1^\yon) \cdot
	(1\mdot4^\yon+3\mdot 3^\yon)
	\]
    The factors of the above product are called Dirichlet series.
	\item Describe the connection between the last two parts. (An alternative name we give for the parallel product $\otimes$ is the \emph{Dirichlet product}.) \qedhere
\end{enumerate}
\begin{solution}
Here $p\coloneqq \2\yon^\2+\3\yon$ and $q\coloneqq\yon^\4+\3\yon^\3$.
\begin{enumerate}
    \item We compute $p \times q$ using \cref{exc.general_poly_times} \cref{exc.general_poly_times.polynomial}:
    \begin{align*}
        p \times q &\iso \2\yon^{\2 + \4} + (\2 \times \3)\yon^{\2 + \3} + \3\yon^{\1 + \4} + (\3 \times \3)\yon^{\1 + \3} \\
        &\iso \2\yon^\6 + \6\yon^\5 + \3\yon^\5 + \9\yon^\4 \\
        &\iso \2\yon^\6 + \9\yon^\5 + \9\yon^\4.
    \end{align*}

    \item We compute $p \otimes q$ using \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.polynomial}:
    \begin{align*}
        p \otimes q &\iso \2\yon^{\2 \times \4} + (\2 \times \3)\yon^{\2 \times \3} + \3\yon^\4 + (\3 \times \3)\yon^\3 \\
        &\iso \2\yon^\8 + \6\yon^\6 + \3\yon^\4 + \9\yon^\3.
    \end{align*}

    \item We evaluate $(2\mdot2^\yon+3\mdot 1^\yon+1) \cdot
	(1\mdot4^\yon+3\mdot 3^\yon+2)$ using standard high school algebra:
    \begin{align*}
	    (2\mdot2^\yon+3\mdot 1^\yon) \cdot (1\mdot4^\yon+3\mdot 3^\yon) &= 2\mdot1\mdot2^\yon\mdot4^\yon + 2\mdot3\mdot2^\yon\mdot3^\yon + 3\mdot1\mdot1^\yon\mdot4^\yon + 3\mdot3\mdot1^\yon\mdot3^\yon \\
	    &= 2\mdot8^\yon + 6\mdot6^\yon + 3\mdot4^\yon + 9\mdot3^\yon.
	\end{align*}

	\item We describe the connection between the last two parts as follows.
	Given a polynomial $p$, we let $d(p)$ denote the Dirichlet series $\sum_{i \in p(\1)} |p[i]|^\yon$.
	Then by \eqref{eqn.parallel_def},
	\begin{align*}
	    d(p \otimes q) &= \sum_{i \in p(\1)} \sum_{j \in q(\1)} |p[i] \times q[j]|^\yon \\
	    &= \sum_{i \in p(\1)} |p[i]|^\yon \sum_{j \in q(\1)} |q[j]|^\yon \\
	    &= d(p) \cdot d(q).
	\end{align*}
	The last two parts are simply an example of this identity for a specific choice of $p$ and $q$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
What is $(\3\yon^\5+\6\yon^\2)\otimes\4$? Hint: $\4=\4\yon^\0$.
\begin{solution}
We compute $(\3\yon^\5+\6\yon^\2)\otimes\4$ using \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.polynomial} and the fact that $\4=\4\yon^\0$:
\begin{align*}
    (\3\yon^\5+\6\yon^\2)\otimes\4\yon^\0 &\iso (\3\times\4)\yon^{\5\times\0} + (\6\times\4)\yon^{\2\times\0} \\
    &\iso \1\2\yon^\0 + \2\4\yon^\0 \\
    &\iso \3\6.
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.prepare_poly_smc}
Let $p,q,r\in\poly$ be any polynomials.
\begin{enumerate}
  \item Show that there is an isomorphism $p\otimes\yon\cong p$.
  \item Show that there is an isomorphism $(p\otimes q)\otimes r\cong p\otimes (q\otimes r)$.
  \item Show that there is an isomorphism $p\otimes q \cong q\otimes p$.
 \qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
  \item We show that $p\otimes\yon\cong p$:
  \begin{align*}
      p \otimes y &\iso \sum_{i \in p(\1)} \sum_{j \in \1} \yon^{p[i] \times \1} \tag*{\eqref{eqn.parallel_def}} \\
      &\iso \sum_{i \in p(\1)} \yon^{p[i]} \iso p.
  \end{align*}

  \item We show that $(p\otimes q)\otimes r\cong p\otimes (q\otimes r)$:
  \begin{align*}
      (p \otimes q) \otimes r &\iso \left(\sum_{i \in p(\1)} \sum_{j \in q(\1)} \yon^{p[i] \times q[j]}\right) \otimes r \tag*{\eqref{eqn.parallel_def}} \\
      &\iso \sum_{i \in p(\1)} \sum_{j \in q(\1)} \left(\sum_{k \in r(\1)} \yon^{(p[i] \times q[j]) \times r[k]}\right) \tag*{\eqref{eqn.parallel_def}} \\
      &\iso \sum_{i \in p(\1)} \left(\sum_{j \in q(\1)} \sum_{k \in r(\1)} \yon^{p[i] \times (q[j] \times r[k])}\right) \tag{Associativity of $\sum$ and $\times$} \\
      &\iso p \otimes \left(\sum_{j \in q(\1)} \sum_{k \in r(\1)} \yon^{q[j] \times r[k]}\right) \tag*{\eqref{eqn.parallel_def}} \\
      &\iso p \otimes (q \otimes r). \tag*{\eqref{eqn.parallel_def}} \\
  \end{align*}

  \item We show that $(p\otimes q)\cong(q\otimes p)$:
  \begin{align*}
      p \otimes q &\iso \sum_{i \in p(\1)} \sum_{j \in q(\1)} \yon^{p[i] \times q[j]} \tag*{\eqref{eqn.parallel_def}} \\
      &\iso \sum_{j \in q(\1)} \sum_{i \in p(\1)} \yon^{q[j] \times p[i]} \tag{Commutativity of $\sum$ and $\times$} \\
      &\iso q \otimes p. \tag*{\eqref{eqn.parallel_def}} \\
  \end{align*}
\end{enumerate}
\end{solution}
\end{exercise}

In \cref{exc.prepare_poly_smc}, we have gone most of the way to proving that $(\poly,\yon,\otimes)$ is a symmetric monoidal category.

\begin{proposition}\label{prop.parallel_monoidal}
The category $\poly$ has a symmetric monoidal structure $(\yon,\otimes)$ where $\otimes$ is the parallel product from \cref{def.parallel}.
\end{proposition}
\begin{proof}[Sketch of proof]
Given lenses $f\colon p\to p'$ and $g\colon q\to q'$, we need to give a lens $(f\otimes g)\colon (p\otimes q)\to (p'\otimes q')$. On positions, define
\[
(f\otimes g)_\1(i,j)\coloneqq \big(f_\1(i),g_\1(j)\big)
\]
On directions at $(i,j)\in p(\1)\times q(\1)$, define
\[
  (f\otimes g)^\sharp_{(i,j)}(a,b)\coloneqq
  \big(f^\sharp_i(a),g^\sharp_j(b)\big).
\]
Then \cref{exc.prepare_poly_smc} gives us the unitors, associator, and braiding.
We have not proven the functoriality of $\otimes$, the naturality of the isomorphisms from \cref{exc.prepare_poly_smc}, or all the coherences between these isomorphisms, but we ask the reader to take them on trust or to check them for themselves.
Alternatively, we may invoke the Day convolution to obtain the monoidal structure $(\yon, \otimes)$ directly (see \cref{prop.day}).
\end{proof}

\begin{exercise} \label{exc.some_parallel_prods}
\begin{enumerate}
	\item \label{exc.some_parallel_prods.const} If $p=A$ and $q=B$ are constant polynomials, what is $p\otimes q$?
	\item If $p=A$ is constant and $q$ is arbitrary, what can you say about $p\otimes q$?
	\item \label{exc.some_parallel_prods.lin} If $p=A\yon$ and $q=B\yon$ are linear polynomials, what is $p\otimes q$?
	\item \label{exc.some_parallel_prods.pos_prod} For arbitrary $p,q\in\poly$, what is the relationship between the sets $(p\otimes q)(\1)$ and $p(\1)\times q(\1)$?
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item By \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial}, we have $A \otimes B \iso AB$.
    \item We use \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.polynomial} to compute $A \otimes q$:
    \[
        A \otimes q \iso \sum_{j \in q(\1)} A\yon^{\0 \times q[i]} \iso \sum_{j \in q(\1)} A \iso A \times q(\1).
    \]
    \item By \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial}, we have $A\yon \otimes B\yon \iso AB\yon$.
    \item We show that $(p\otimes q)(\1)$ and $p(\1)\times q(\1)$ are isomorphic. By \eqref{eqn.parallel_def},
    \[
        (p \otimes q)(\1) \iso \sum_{i \in p(\1)} \sum_{j \in q(\1)} \1^{p[i] \times q[j]} \iso p(\1) \times q(\1).
    \]
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.dir_closed_classes}
Which of the following classes of polynomials are closed under $\otimes$? Note also whether they contain $\yon$.
\begin{enumerate}
	\item The set $\{A\yon^\0\mid A\in\smset\}$ of constant polynomials.
	\item The set $\{A\yon\mid A\in\smset\}$ of linear polynomials.
	\item The set $\{A\yon+B\mid A,B\in\smset\}$ of affine polynomials.
	\item The set $\{A\yon^\2+B\yon+C\mid A,B,C\in\smset\}$ of quadratic polynomials.
	\item The set $\{A\yon^B\mid A,B\in\smset\}$ of monomials.
	\item The set $\{S\yon^S\mid S\in\smset\}$.
	\item The set $\{p\in\poly\mid p(\1)\text{ is finite}\}$. \qedhere
\end{enumerate}
\begin{solution}
For each of the following classes of polynomials, we determine whether they are closed under $\otimes$ and whether they contain $\yon$.
\begin{enumerate}
	\item The set $\{A\yon^\0\mid A\in\smset\}$ of constant polynomials is closed under $\otimes$ by the solution to \cref{exc.some_parallel_prods} \cref{exc.some_parallel_prods.const}.
	But the set does not contain $\yon$, as $\yon$ is not a constant polynomial.
	\item The set $\{A\yon\mid A\in\smset\}$ of linear polynomials is closed under $\otimes$ by the solution to \cref{exc.some_parallel_prods} \cref{exc.some_parallel_prods.lin} and does contain $\yon$, as $\yon \iso \1\yon$.
	\item The set $\{A\yon+B\mid A,B\in\smset\}$ of affine polynomials is closed under $\otimes$, for \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.polynomial} yields
	\[
	    (A\yon + B) \otimes (A'\yon + B') \iso AA'\yon + AB' + BA' + BB'.
	\]
	The set contains $\yon$, as $\yon \iso \1\yon + \0 $.
	\item The set $\{A\yon^\2+B\yon+C\mid A,B,C\in\smset\}$ of quadratic polynomials is not closed under $\otimes$, for even though $\yon^\2 \iso \1\yon^\2 + \0\yon + \0$ is a quadratic polynomial, \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial} implies that
	\[
	    \yon^\2 \otimes \yon^\2 \iso \yon^\4,
	\]
	which is not quadratic.
	The set contains $\yon$, as $\yon \iso \0\yon^\2 + \1\yon + \0$.
	\item The set $\{A\yon^B\mid A,B\in\smset\}$ of monomials is closed under $\otimes$ by \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial} and does contain $\yon$, as $\yon \iso \1\yon^\1$.
	\item The set $\{S\yon^S\mid S\in\smset\}$ is closed under $\otimes$, for \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial} returns
	\[
	    S\yon^S \otimes T\yon^T \iso ST\yon^{ST}.
	\]
	The set contains $\yon$, as $\yon \iso \1\yon^\1$.
	\item The set $\{p\in\poly\mid p(\1)\text{ is finite}\}$ is closed under $\otimes$ by the solution to \cref{exc.some_parallel_prods} \cref{exc.some_parallel_prods.pos_prod}.
	The set contains $\yon$, as $\yon(\1) \iso \1$ is finite.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
What is the smallest class of polynomials that's closed under $\otimes$ and contains $\yon$?
\begin{solution}
The smallest class of polynomials that's closed under $\otimes$ and contains $\yon$ is just $\{\yon\}$.
This is because by \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial}, we have $\yon \otimes \yon \iso \yon$.
\end{solution}
\end{exercise}

\begin{exercise}
Show that for any $p_1,p_2,q\in\poly$ there is an isomorphism
\[
(p_1+p_2)\otimes q\cong (p_1\otimes q)+(p_2\otimes q).
\]
\begin{solution}
We show that $(p_1 + p_2) \otimes q \iso (p_1 \otimes q) + (p_2 \otimes q)$ using \eqref{eqn.parallel_def}:
\begin{align*}
    (p_1 + p_2) \otimes q &\iso \sum_{k \in \2} \sum_{i \in p_k(\1)} \sum_{j \in q(\1)} \yon^{p_k[i] \times q[j]} \\
    &\iso \sum_{i \in p_1(\1)} \sum_{j \in q(\1)} \yon^{p_1[i] \times q[j]} + \sum_{i \in p_2(\1)} \sum_{j \in q(\1)} \yon^{p_2[i] \times q[j]} \\
    &\iso (p_1 \otimes q) + (p_2 \otimes q).
\end{align*}
\end{solution}
\end{exercise}

\begin{proposition} \label{prop.day}
For any monoidal structure $(I,\star)$ on $\smset$, there is a corresponding monoidal structure $(\yon^I, \odot)$ on $\poly$, where $\odot$ is the Day convolution.
Moreover, $\odot$ distributes over coproducts.

In the case of $(\0,+)$ and $(\1,\times)$, this procedure returns the $(\1,\times)$ and $(\yon,\otimes)$ monoidal structures respectively.
\end{proposition}
\begin{proof}
Any monoidal structure $(I,\star)$ on $\smset$ induces a monoidal structure on $\smset^\smset$ with the Day convolution $\odot$ as the tensor product and $\yon^I$ as the unit.
To prove that this monoidal structure restricts to $\poly$, it suffices to show that $\poly$ is closed under the Day convolution.

Given polynomials $p\coloneqq\sum_{i \in p(\1)} \yon^{p[i]}$ and $q\coloneqq\sum_{j \in q(\1)} \yon^{q[j]}$, their Day convolution is given by the coend
\begin{equation} \label{eqn.day_conv.coend}
    p \odot q \iso \int^{(A,B)\in\smset^\2} \yon^{A \star B} \times p(A) \times q(B).
\end{equation}
We can rewrite the product $p(A) \times q(B)$ as
\[
    p(A) \times q(B) \iso \left(\sum_{i \in p(\1)} A^{p[i]}\right) \times \left(\sum_{j \in q(\1)} B^{q[i]}\right) \iso \sum_{(i,j) \in p(\1) \times q(\1)} A^{p[i]} \times B^{q[i]} %\\
    % &\iso \sum_{(i,j) \in p(\1) \times q(\1)} \smset(p[i], A) \times \smset(q[j], B) \\
    % &\iso \sum_{(i,j) \in p(\1) \times q(\1)} \smset^\2((p[i], q[j]), (A, B))
\]
So because products distribute over coproducts in $\smset$ and coends commute with coproducts, we can rewrite \eqref{eqn.day_conv.coend} as
\[
    p \odot q \iso \sum_{(i,j) \in p(\1) \times q(\1)} \int^{(A,B)\in\smset^\2} \yon^{A \star B} \times A^{p[i]} \times B^{q[i]},
\]
which, by the co-Yoneda lemma, can be rewritten as
\begin{equation} \label{eqn.day_conv.poly}
    p \odot q \iso \sum_{(i,j) \in p(\1) \times q(\1)} \yon^{p[i] \star q[j]}
\end{equation}
in $\poly$.
That the Day convolution distributes over coproducts also follows from the fact that products distribute over coproducts in $\smset$ and coends commute with coproducts; or, alternatively, directly from \eqref{eqn.day_conv.poly}.

We observe that \eqref{eqn.day_conv.poly} gives $(\yon^I, \odot) = (\1, \times)$ when $(I, \star) = (\0, +)$ and $(\yon^I, \odot) = (\yon, \otimes)$ when $(I, \star) = (\1, \times)$.
\end{proof}

\begin{exercise}
\begin{enumerate}
	\item Show that the operation $(A, B)\mapsto A+AB+B$ on $\smset$ is associative.
	\item Show that $\0$ is unital for the above operation.
	\item Let $(\1,\odot)$ denote the corresponding monoidal structure on $\poly$. Compute the monoidal product $(\yon^\3+\yon)\odot(\2\yon^\2+\2)$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item To show that the operation $(A, B)\mapsto A+AB+B$ on $\smset$ is associative, we observe that
    \begin{align*}
        (A + AB + B) + (A + AB + B)C + C &\iso A + AB + B + AC + ABC + BC + C \\
        &\iso A + AB + ABC + AC + B + BC + C \\
        &\iso A + A(B + BC + C) + (B + BC + C).
    \end{align*}
    \item To show that $\0$ is unital for this operation, we observe that
    \[
        (A, \0) \mapsto A + A\0 + \0 \iso A
    \]
    and
    \[
        (\0, B) \mapsto \0 + \0B + B \iso B.
    \]
    \item We let $(\1,\odot)$ denote the corresponding monoidal product on $\poly$ and evaluate $(\yon^\3+\yon)\odot(\2\yon^\2+\2)$.
    By \eqref{eqn.day_conv.poly}, with $A \star B \iso A + AB + B$, we have
    \begin{align*}
        (\yon^\3+\yon)\odot(\2\yon^\2+\2) &\iso (\yon^\3+\yon^\1)\odot(\yon^\2+\yon^\2+\yon^\0+\yon^\0) \\
        &\iso \yon^{\3\star\2} + \yon^{\3\star\2} + \yon^{\3\star\0} + \yon^{\3\star\0} + \yon^{\1\star\2} + \yon^{\1\star\2} + \yon^{\1\star\0} + \yon^{\1\star\0} \\
        &\iso \2\yon^{\1\1} + \2\yon^\3 + \2\yon^\5 + \2\yon^\1.
    \end{align*}
\end{enumerate}
\end{solution}
\end{exercise}

\begin{remark}
Monoids in $\poly$ with respect to the parallel product $\otimes$ are particularly interesting---they have a kind of collective semantics, letting agents aggregate their contributions and distribute returns on those contributions in a coherent way.
We leave discussion of them to future work, so as not to distract us from our main story.
\end{remark}

%-------- Section --------%
\section{Summary and further reading}

In this chapter we explained the mathematics behind our main object of study in this book, the category $\poly$ of polynomial functors. A polynomial $p=\sum_{i\in I}\yon^{p[i]}$ can be considered as
\begin{enumerate}
	\item combinatorial data: an indexed family of sets $(p[i])_{i\in I}$;
	\item a picture: for each $i\in I$, a corolla with $p[i]$-many leaves;
	\item a functor $\smset\to\smset$: for each $X:\smset$, a new set $\sum_{i\in I}X^{p[i]}$.
\end{enumerate}
The last of these ties anchors us to the rest of category theory, so that rather than needing to ask for morphisms between combinatorial objects or between pictures, we can ask for morphisms between functors $\smset\to\smset$, and that's well established territory. Once we said that morphisms $p\to q$ between polynomial functors should just be natural transformations, we translated that back to the combinatorial and pictorial settings and saw the ``on-positions, on-directions'' description. We want to emphasize that we will be referring to each morphism (natural transformation) $p\to q$ between polynomials as a \emph{lens}.

Once $\poly$ was defined, we considered various properties it has, e.g.\ that it has all products and coproducts, and that these distribute: $\prod\sum\to\sum\prod$.
\begin{align*}
	\sum_{a\in A}p_a&\coloneqq\sum_{(a,i)\in\sum_{a\in A}p_a(1)}\yon^{p_a[i]}&
	\prod_{a\in A}p_a&\coloneqq\sum_{i\in\prod_{a\in A}p_a(1)}\yon^{\sum_{a\in A}p_a[i a]}
\\
	p_1+p_2&\coloneqq\sum_{(a,i)\in\{(1,i_1)\mid i_1\in p_1(\1)\}+\{(2,i_2)\mid i_2\in p_2(\1)\}}\yon^{p_a[i]}&
	p_1\times p_2&\coloneqq\sum_{(i_1,i_2)\in p_1(1)\times p_2(1)}\yon^{p_1[i_1]+p_2[i_2]}
\end{align*}
We also discussed how one can take any monoidal product $\cdot$ from $\smset$ and lift it to a monoidal product $\cdot$ on $\poly$:
\[
	p_1\odot p_2\coloneqq\sum_{i_1,i_2)\in p_1(1)\times p_2(1)}\yon^{p_1[i_1]\cdot p_2[i_2]}
\]
A special case of this is the product structure $\times$ on $\poly$, which emerges from the coproduct structure $+$ on $\smset$. The other case of interest is the parallel (or Dirichlet) product structure $\otimes$ on $\poly$, which emerges from the product structure $\times$ on $\smset$:
\[
	p_1\otimes p_2\coloneqq\sum_{i_1,i_2)\in p_1(1)\times p_2(1)}\yon^{p_1[i_1]\times p_2[i_2]}
\]

There are many fine sources on polynomial functors. Some of the computer science literature is more relaxed about what a polynomial is. For example, the ``coalgebra community'' often defines a polynomial to include finite power sets (see e.g.\ \cite{jacobs2017introduction}). Other computer science communities use the same definition of polynomial, but refer to it as a \emph{container} and use different words for its positions (they call them ``shapes'') and directions (they call them, rather unfortunately, ``positions''). See e.g.\ \cite{abbot2003categoriesthesis,abbott2005containers}.

But the notion of polynomial functors seems to have originated from Andr\'{e} Joyal. A good introduction to polynomial functors can be found in \cite{kock2012polynomial}; in particular the related work section on page~3 provides a nice survey of the field. A reader may also be interested in the Workshops on Polynomial Functors organized by the Topos Institute: \url{https://topos.site/p-func-workshop/}.

%-------- Section --------%
\section{Exercise solutions}
\Closesolutionfile{solutions}
{\footnotesize
\input{solution-file3}}

\Opensolutionfile{solutions}[solution-file4]

%------------ Chapter ------------%
\chapter{Dynamical systems as dependent lenses} \label{ch.poly.dyn_sys}

Let's start putting all this $\poly$ stuff to use.

%-------- Section --------%
\section{Moore machines}\label{sec.poly.dyn_sys.moore}

We begin with our simplest example of a dynamical system: a deterministic machine with internal states that can return output and be updated according to input.

\begin{definition}[Moore machine]\label{def.moore_machine}
If $A$, $B$, and $S$ are sets, an $(A,B)$-\emph{Moore machine} with \emph{states} $S$ consists of two functions
\begin{align*}
	\text{return}&\colon S\to B\\
	\text{update}&\colon S\times A\to S
\end{align*}
\end{definition}

We can visualize a Moore machine as follows.
It has a set $S$ of possible states.
At any point in time, the machine is in one of those states: say $s \in S$.
While there, whenever we ask the machine to produce output, it will give us $\text{return}(s) \in B$.
But if we feed the machine input $a \in A$, the machine will switch to a new state, $\text{update}(s, a)$.
Note that this new state depends not only on the input the machine receives, but also on the state the machine is in when it receives that input.

\begin{example}\label{ex.Moore_three}
Here's a labeled transition diagram of a Moore machine with $S\coloneqq\3$ states:
\begin{equation} \label{eqn.trans_diag}
\begin{tikzpicture}
	\node[draw] {
  \begin{tikzcd}[column sep=small]
  	\LMO{b_1}\ar[rr, dgreen, thick, bend left]\ar[loop left, thick, orange]&&
  	\LMO{b_2}\ar[ll, thick, orange, bend left]\ar[dl, bend left, thick, dgreen]\\&
  	\LMO{b_2} \ar[ul, thick, orange, bend left] \ar[loop left, thick, dgreen]
  \end{tikzcd}
  };
\end{tikzpicture}
\end{equation}
Each state is labeled by the output value it returns: an element of $B\coloneqq\{b_1,b_2\}$. Each state has two outgoing arrows, one orange and one green, representing the two possible inputs, so $A\coloneqq\{{\color{orange}\text{orange}},{\color{dgreen}\text{green}}\}$.
The targets of the arrows indicate the updated state.

For example, let's say the machine starts at the bottom state.
We imagine barking a sequence of inputs---say ``{\color{orange}orange! orange!} {\color{dgreen}green!} {\color{orange}orange!} \ldots''---at this machine to make it run through its states and return the output at each state:
\begin{enumerate}
    \item Starting at the bottom state, the machine returns the output $b_2$.
    \item Receiving the input ``{\color{orange}orange}'' at the bottom state, the machine follows the {\color{orange}orange} arrow from the bottom state to update its state to the left state.
    \item At the left state, the machine returns the output $b_1$.
    \item Receiving the input ``{\color{orange}orange}'' at the left state, the machine follows the {\color{orange}orange} arrow from the left state to update its state to---once again---the left state.
    \item At the left state, the machine returns the output $b_1$.
    \item Receiving the input ``{\color{dgreen}green}'' at the left state, the machine follows the {\color{dgreen}green} arrow from the left state to update its state to the right state.
    \item At the right state, the machine returns the output $b_2$.
    \item Receiving the input ``{\color{orange}orange}'' at the left state, the machine follows the {\color{orange}orange} arrow from the right state to update its state to the left state.
    \item At the left state, the machine returns the output $b_1$.

    \ldots

    So given a fixed initial state, namely the bottom state, this Moore machine sends the sequence $({\color{orange}\text{orange}}, {\color{orange}\text{orange}}, {\color{dgreen}\text{green}}, {\color{orange}\text{orange}},\ldots)$ of elements in $A$ to the sequence $(b_2,b_1,b_1,b_2,b_1,\ldots)$ of elements in $B$.
\end{enumerate}
\end{example}

In general, given an initial state $s_0\in S$, an $(A,B)$-Moore machine with states $S$ sends every sequence $(a_1,a_2,a_3,\ldots)$ of elements in $A$ to a sequence $(b_0,b_1,b_2,b_3,\ldots)$ of elements in $B$, defined as follows via an intermediary sequence $(s_0,s_1,s_2,s_3,\ldots)$:
\[
    b_k\coloneqq \text{return}(s_k) \qqand s_{k+1}\coloneqq \text{update}(s_k, a_{k+1})
\]
for all $k\in\nn$.
We'll see that $\poly$ gives us a more concise way to express this in \cref{ex.input_output}.

\subsection{Moore machines as lenses}

Does \cref{def.moore_machine} look familiar?
It's easy to see that an $(A,B)$-Moore machine with states $S$ is just a lens between monomials
\[
(\lensget, \lensput)\colon S\yon^S\to B\yon^A,
\]
with $\lensget\coloneqq\text{return}$ and $\lensput\coloneqq\text{update}$.\footnote{Recall from \eqref{eqn.bimorphic_lens} that ``get'' and ``put'' are our special names for the on-positions and on-directions functions of a lens between monomials.}
Given such a Moore machine, we will call the monomial $B\yon^A$ the \emph{interface}, because it encodes how an outsider can interact with the machine, namely the possible inputs and outputs; and we will call the monomial $S\yon^S$ the \emph{state system}.

\begin{exercise}
Write the Moore machine from \cref{ex.Moore_three} as a lens between monomials.
\begin{enumerate}
    \item What is the state system?
    \item What is the interface?
\end{enumerate}
Call the left state $1$, the right state $2$, and the bottom state $3$.
\begin{enumerate}[resume]
    \item What is the on-positions function ``get''?
    \item What is the on-directions function ``put''? \qedhere
\end{enumerate}
\begin{solution}
We can write the Moore machine from \cref{ex.Moore_three} as a lens $S\yon^S\to B\yon^A$ between monomials as follows.
\begin{enumerate}
    \item As $S=\3$, the state system is $S\yon^S=\3\yon^\3$.
    \item As $B=\{b_1,b_2\}$ and $A=\{{\color{orange}\text{orange}},{\color{dgreen}\text{green}}\}$, the interface is $B\yon^A=\{b_1,b_2\}\yon^{\{{\color{orange}\text{orange}},\,{\color{dgreen}\text{green}}\}}$.
    \item The on-positions function ``get'' sends $1\mapsto b_1, 2\mapsto b_2,$ and $3\mapsto b_2$.
    \item The on-directions function ``put'' sends
    \begin{align*}
        (1, {\color{orange}\text{orange}})\mapsto1&,\quad(1, {\color{dgreen}\text{green}})\mapsto2, \\
        (2, {\color{orange}\text{orange}})\mapsto1&,\quad(2, {\color{dgreen}\text{green}})\mapsto3, \\
        (3, {\color{orange}\text{orange}})\mapsto1&,\quad(3, {\color{dgreen}\text{green}})\mapsto3.
    \end{align*}
\end{enumerate}
\end{solution}
\end{exercise}

\subsection{More Moore machines}

\begin{example}\label{ex.counting_trajectory}
There is a dynamical system that takes unchanging input and produces as output the sequence of natural numbers $0,1,2,3,\ldots.$ It is a Moore machine with states $\nn$ and interface $\nn\yon$. The associated lens $\nn\yon^\nn\to\nn\yon$ is given by the identity $\nn \to \nn$ on positions and the function $\nn \iso \nn \times \1 \to \nn$ sending $n\mapsto n+1$ on directions.
% \[
% \begin{tikzpicture}[polybox, mapstos]
% 	\node[poly, dom] (s) {$n+1$\nodepart{two}$n$};
% 	\node[poly, cod, linear, right=of s] (p) {$\vphantom{1}$\nodepart{two}$n$};
% 	\draw (s_pos) to[first] (p_pos);
% 	\draw (p_dir) to[last] (s_dir);
% \end{tikzpicture}
% \]
\end{example}

\begin{example}\label{ex.R2_moore}
Here's a(n infinite) Moore machine with states $\rr^\2$:
\[
\rr^\2\yon^{\rr^\2}\to\rr^\2\yon^{[0,1]\times[0,2\pi)}%]
\]
Its output type is $\rr^\2$, which we might think of as a location in the 2-dimensional plane, and its input type is $[0,1]\times[0,2\pi)$, which we can think of as a command to move a certain distance in a certain direction. The lens itself is given by
\begin{align*}
  \rr^\2&\To{\text{return}}\rr^\2&
  \rr^\2\times[0,1]\times[0,2\pi)&\To{\text{update}}\rr^\2
  	%\nonumber\\\label{eqn.r2moore}
  	\\
  (x,y)&\mapsto(x,y)&
  (x,y,r,\theta)&\mapsto(x+r\cos\theta, y+r\sin\theta)
\end{align*}
\end{example}

\begin{exercise}
Explain in words what the Moore machine in \cref{ex.R2_moore} does.
\begin{solution}
At any point in time, the Moore machine in \cref{ex.R2_moore} is located somewhere on the $2$-dimensional plane, say at the coordinates $(x, y) \in \rr^\2$.
This location is its current state.
Whenever we ask the machine to produce output, it will tell us those coordinates, since $\text{return}(x, y) = (x, y)$.
But if we give the machine input of the form $(r, \theta)$ for some distance $r \in [0,1]$ and direction $\theta \in [0, 2\pi)$, the machine will move by that distance, in that direction, going from $(x, y)$ to
\[
    \text{update}(x,y,r,\theta) = (x,y) + r(\cos\theta, \sin\theta)
\]
(here we treat $\rr^\2$ as a vector space, so that $r(\cos\theta, \sin\theta)$ is a vector of length $r$ in the direction of $\theta$).
\end{solution}
\end{exercise}

\begin{example}[From functions to memoryless Moore machines]\label{ex.funs_to_moore}
For any function $f\colon A\to B$, there is a corresponding $(A,B)$-Moore machine with states $B$ that takes in an element of $A$ and returns the element of $B$ obtained by applying $f$.

It is given by the lens $(\id_B, \pi_A\then f)\colon B\yon^B\to B\yon^A$. That is, it is the identity on positions, returning the state directly as output, and on directions it is the function $B\times A\To{\pi_A}A\To{f} B$, which ignores the current state and applies $f$ to the input to compute the new state.

If the machine begins in state $b_0$ and is given a sequence $(a_1,a_2,\ldots)$ of elements in $A$, the machine's outputs will be $(b_0,f(a_1),f(a_2),\ldots)$. We could say this machine is \emph{memoryless}, because at no point does the state of the machine depend on any previous states.
\end{example}

\begin{exercise}\label{exc.funs_to_moore}
Suppose we have a function $f\colon A\times B\to B$.
\begin{enumerate}
	\item Find a corresponding $(A,B)$-Moore machine $B\yon^B\to B\yon^A$.
	\item Would you say the machine is memoryless?
	\item Now for any function $g\colon A\to B$, give a corresponding $(A,B)$-Moore machine $B\yon^B\to B\yon^A$ by first turning $g\colon A\to B$ into a function $A\times B\to B$ that discards its second input.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We seek an $(A,B)$-Moore machine $B\yon^B\to B\yon^A$ corresponding to the function $f\colon A\times B\to B$.
    We know that an $(A,B)$-Moore machine $B\yon^B \to B\yon^A$ consists of a return function $B \to B$ and an update function $B \times A \to B$.
    So we can simply let the return function be the identity on $B$ and the update function be $B \times A \iso A \times B \To{f} B$, i.e.\ the function $f$ with its inputs swapped.

    \item Generally, such a machine is not memoryless.
    Unlike in \cref{ex.funs_to_moore}, the update function $B \times A \iso A \times B \To{f} B$ does appear to depend on its first input, namely the previous state, which $f$ takes as its second input.

    However, if $f$ factors through the projection $\pi_A \colon A \times B \to A$, i.e.\ if $f$ can be written as a composite $A \times B \To{\pi_A} A \To{f'} B$ for some $f' \colon A \to B$, then the resulting machine \emph{is} memoryless: it is the memoryless Moore machine from \cref{ex.funs_to_moore} corresponding to $f'$.

    \item Given a function $g\colon A\to B$, we can compose it with the projection $\pi_A\colon A\times B\to A$ to obtain a function $A\times B\To{\pi_A}A\To{g}B$, which sends its first input through $g$ and discards its second input.
    Then the corresponding $(A,B)$-Moore machine $B\yon^B \to B\yon^A$ has the identity function on $B$ as its return function and the composite $B\times A\To{\pi_A}A\To{g}B$, which discards its first input and sends its second input through $g$, as its update function.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Find $A,B\in\smset$ such that the following can be identified with a lens $S\yon^S\to B\yon^A$, and explain in words what the corresponding Moore machine does (there may be multiple possible solutions):
\begin{enumerate}
	\item a \emph{discrete dynamical system}, i.e.\ a set of states $S$ and a transition function $S\to S$ that tells us how to move from state to state.
	\item a \emph{magma}, i.e.\ a set $S$ and a function $S\times S\to S$.
	\item a set $S$ and a subset $S'\ss S$.\qedhere
\end{enumerate}
\begin{solution}
For each of the following constructs, we find $A, B \in \smset$ such that the construct can be identified with a lens $S\yon^S \to B\yon^A$, i.e.\ a function $\text{return} \colon S \to B$ and a function $\text{update} \colon S \times A \to S$.
\begin{enumerate}
    \item Given a discrete dynamical system with states $S$ and transition funtion $n \colon S \to S$, we can set $A \coloneqq B \coloneqq \1$.
    Then $\text{return} \colon S \to \1$ is unique, while $\text{update} \colon S \times \1 \to S$ is given by $S \times 1 \iso S \To{n} S$.
    The corresponding Moore machine has unchanging input (you could think of it as a button that says ``advance to the next state'') and unchanging output (which effectively tells us nothing).
    So it is just a set of states, and a deterministic way to move from state to state.

    We could have also set $A \coloneqq \0$ and $B \coloneqq S$, so that $\text{return} \coloneqq n$ and $\text{update} \colon S \times \0 \to S$ is unique, but this formulation is somewhat less satisfying: this is a Moore machine that never moves between its states, effectively functioning as a lookup table between whatever state the machine happens to be in and its output, which happens to refer to some state.

    \item Given a magma consisting of a set $S$ and a function $m \colon S \times S \to S$, we can set $A \coloneqq S$ and $B \coloneqq 1$.
    Then $\text{return} \colon S \to \1$ is unique, while $\text{update} \colon S \times S \to S$ is equal to $m$.
    The corresponding Moore machine produces unchanging output.
    It uses the binary operation $m$ to combine the current state with the input---which also refers to a state---to obtain the new state.

    Alternatively, we could have set the update function to be $m$ with its inputs swapped.
    The difference here is that the new state is given by applying $m$ with the input on the left and the current state on the right, rather than the other way around.
    If $m$ is noncommutative, this would yield a different Moore machine.

    We could have also set $A \coloneqq \0$ and $B \coloneqq S^S$, so that $\text{update} \colon S \times \0 \to S$ is unique, while currying $m$ gives $\text{return}$, so that $\text{return}(s)$ is the function $S \to S$ given by $s' \mapsto m(s, s')$.
    Alternatively, $\text{return}(s)$ could be the function $s' \mapsto m(s', s)$.
    Either way, this is a Moore machine that never moves between its states, functioning as a lookup table between the machine's current state and the function $m$ partially applied to that state on one side or the other.

    \item Given a set $S$ and a subset $S' \ss S$, we can set $A \coloneqq \0$ and $B \coloneqq \2$.
    Then $\text{update} \colon S \times \0 \to S$ is unique, while we define $\text{return} \colon S \to \2$ by
    \[
        \text{return}(s) =
        \begin{cases}
            1 & \text{if } s \in S' \\
            2 & \text{if } s \notin S',
        \end{cases}
    \]
    so that $S'$ can be recovered from the return function as its fiber over $1$.
    The corresponing Moore machine never moves between its states, but gives one of two outputs indicating whether or not the current state is in the subset $S'$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Consider the Moore machine in \cref{ex.R2_moore}, and think of it as a robot. Using the terminology from that example, modify the robot as follows.

Add to its state a ``health meter,'' which has a real value between 0 and 10. Make the robot lose half its health whenever it moves to a location whose $x$-coordinate is negative. Do not output its health; instead, use its health $h$ as a multiplier, allowing it to move a distance of $hr$ given an input of $r$.
\begin{solution}
We modify the Moore machine from \cref{ex.R2_moore} as follows.
The original Moore machine had states $\rr^\2$, so to add a health meter with values in $[0,10]$, we take the cartesian product to obtain the new set of states $\rr^\2 \times [0,10]$.
The inputs and outputs are unchanged, so the Moore machine is a lens
\[
    \rr^\2 \times [0,10] \yon^{\rr^\2 \times [0,10]} \to \rr^\2 \yon^{[0,1] \times [0,2\pi)}.
\]
Its return function $\rr^\2 \times [0,10] \to \rr^\2$ is the canonical projection, as the machine only outputs its location in $\rr^\2$ and not its health; while its update function
\[
    \rr^\2 \times [0,10] \times [0,1] \times [0,2\pi) \to \rr^\2 \times [0,10]
\]
sends $(x, y, h, r, \theta)$ to
\[
    (x + hr\cos\theta, y + hr\sin\theta, h'),
\]
where $h' = h/2$ if the machine's new $x$-coordinate $x + hr\cos\theta < 0$ and $h' = h$ otherwise.
\end{solution}
\end{exercise}

\begin{exercise}[Tape of a Turing machine]
A Turing machine has a tape. The tape has a cell for each integer, and each cell holds a value $v\in V=\{0,1,-\}$ of 0,1, or blank. At any given time the tape not only holds this function $f\colon\zz\to V$ from cell numbers to values, but also a distinguished choice $c\in\zz$ of the ``current'' cell. Thus the set of states of the tape is $V^\zz\times\zz$.

The Turing machine interacts with the tape by asking for the value at the current cell, an element of $V$, and by telling it to change the value there as well as whether to move left (i.e.\ decrease the current cell number by $1$) or right (i.e.\ increase by $1$). Thus the set of outputs of the tape is $V$ and the set of inputs is $V\times\{L,R\}$.

\begin{enumerate}
	\item Give the form of the tape as a Moore machine, i.e.\ lens $t\colon S\yon^S\to B\yon^A$ for appropriate sets $S, A, B$.
	\item Write down the specific $t$ that makes it act like a tape as specified above.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item The tape of a Turing machine has states $V^\zz \times \zz$, outputs $V$, and inputs $V \times \{L, R\}$, so as a Moore machine, it is a lens
    \[
        t \colon (V^\zz \times \zz)\yon^{V^\zz \times \zz} \to V\yon^{V \times \{L,R\}}.
    \]
    \item The return function of $t$ should output the value at the current cell of the tape. So $\text{return} \colon V^\zz \times \zz \to V$ is the evaluation map: it sends $(f,c)$ with $f \colon \zz \to V$ and $c \in \zz$ to $f(c)$.
    Then the update function of $t$ should write the input value of $V$ in the current cell of the tape, then shift the cell number down or up one according to whether the second input value is $L$ (left) or $R$ (right).
    So
    \[
        \text{update} \colon (V^\zz \times \zz) \times (V \times \{L,R\}) \to V^\zz \times \zz
    \]
    sends old tape $f \colon \zz \to V$, old cell number $c \in \zz$, new value $v \in V$, and direction $D \in \{L,R\}$ to the new tape $f' \colon \zz \to V$ defined by
    \[
        f'(n)\coloneqq
        \begin{cases}
            v & \text{if } n = c \\
            f(n) & \text{if } n \neq c
        \end{cases}
    \]
    and the new cell number $c-1$ if $D=L$ and $c+1$ if $D = R$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.file_reader}
Let's say a file of length $n$ is a function $f\colon\ord{n}\to\Set{ascii}$, where $\Set{ascii}\coloneqq\2\5\6$.
We refer to elements of $\ord{n}=\{1,\ldots,n\}$ as entries in the file and, for each entry $i \in \ord{n}$, the value $f(i)$ as the character at entry $i$.

Given a file $f$, make a file-reading Moore machine whose output type is $\Set{ascii} + \{\text{``done''}\}$
and whose input type is
\[
\{(s,t)\mid 1\leq s\leq t\leq n\}+\{\text{``continue''}\}.
\]
For any input, if it is of the form $(s,t)$, then the file-reader should go to entry $s$ in the file and read the character at that entry.
If the input is ``continue,'' the file-reader should move to the next entry (i.e.\ from $s$ to $s+1$) and read that character—unless the new entry would be greater than $t$, in which case the file-reader should continually output ``done'' until it receives another $(s,t)$ pair.
\begin{solution}
Given a file $f \colon \ord{n} \to \Set{ascii}$, we construct our file-reader as a Moore machine as follows.
There are many options for what states the machine should record, but we will use pairs of values $(i, t) \in \ord{n}^\2$, where $i$ is the entry where the file-reader is currently located, while $t$ is the entry where the file-reader should stop.
But we will also include a ``stopped'' state to record when the file-reader has already stopped.
So the Moore machine is a lens
\[
    (\ord{n}^\2+\{\text{``stopped''}\})\yon^{\ord{n}^\2+\{\text{``stopped''}\}} \to (\Set{ascii} + \{\text{``done''}\})\yon^{\{(s,t)\mid 1\leq s\leq t\leq n\}+\{\text{``continue''}\}}.
\]
If the file-reader's current state is ``stopped,'' then the file-reader should output ``done.'' Otherwise, the file-reader should output the character at the entry where the file-reader is currently located.
So its return function $\ord{n}^\2+\{\text{``done''}\} \to \Set{ascii} + \{\text{``done''}\}$ sends $(i, t)$ to $f(i)$ and ``done'' to ``done.''
Meanwhile, the update function
\[
    (\ord{n}^\2+\{\text{``done''}\}) \times (\{(s,t)\mid 1\leq s\leq t\leq n\}+\{\text{``continue''}\}) \to \ord{n}^\2+\{\text{``done''}\}
\]
sends any state with input $(s,t)$ to the state $(s,t)$.
On the other hand, if the input is ``continue,'' an old state $(i,t)$ is sent to the new state $(i+1,t)$ if $i + 1 \leq t$ and ``done'' otherwise.
Finally, if the old state is ``done'' and the input is ``continue,'' the new state is still ``done.''
\end{solution}
\end{exercise}

While \cref{exc.file_reader} gives us a functioning file-reader, it is a little strange that we are still able to give the input ``continue'' even when the output is ``done,'' or input a new range of entries before the file-reader has finished reading from the previous range.
In \cref{sec.poly.dyn_sys.depend_sys}, we'll introduce a generalization of Moore machines to handle cases like these, where the array of inputs the machine can receive changes depending on the output that it just returned.
In particular, we will be able to let the file-reader ``close its port,'' so that it can't receive signals while it's busy reading, but open its port again once it's ``done''; see \cref{ex.generalized_file_reader}.

\subsubsection{Deterministic state automata}

The diagram in \cref{ex.Moore_three} may look familiar to those who have studied automata theory; in fact, a deterministic state automaton can be expressed as a Moore machine (with a preset initial state).

\begin{definition}[Deterministic state automaton, language]\label{def.dfa}
A \emph{deterministic state automaton} consists of
\begin{enumerate}
	\item a set $S$, elements of which are called \emph{states};
	\item a set $A$, elements of which are called \emph{input symbols};
	\item a function $u\colon S\times A\to S$, called the \emph{update function};
	\item an element $s_0\in S$, called the \emph{initial state}; and
	\item a subset $F\ss S$, called the \emph{accept states}.
\end{enumerate}
Given a sequence (or ``word'') $(a_1,\ldots,a_n)$ of elements from $A$ (i.e.\ an element of $\lst(A)$, the free monoid on $A$), we say that the automaton \emph{accepts} this word if starting at the initial state and following the elements in the sequence leads us to an accept state---or, more formally, if the sequence $(s_0,s_1,\ldots,s_n)$ defined
\[
    s_{k+1}\coloneqq u(s_k,a_{k+1})
\]
for all $k\in\nn$ is such that $s_n$ is an accept state: $s_n\in F$.

We call any set of words in $\lst(A)$ a \emph{language}, and we say that the set of all words that the automaton accepts is the language \emph{recognized} by the automaton.
\end{definition}

\begin{remark}
When we study a deterministic state automaton, we are usually interested in which words the automaton accepts and, more generally, what language the automaton recognizes.
While intuitive, the condition we provided for when an automaton accepts a word is rather cumbersome to work with.
In \cref{ex.dsa_lang_recog}, we'll see a simpler way of saying whether an automaton accepts a word, as well as specifying what language the automaton recognizes.
Better yet, we'll find that this alternative formulation arises naturally from the theory of $\poly$.
\end{remark}

\begin{proposition} \label{prop.dsa}
A deterministic state automaton with a set of states $S$ and a set of input symbols $A$ can be identified with a pair of lenses
\[
\yon\to S\yon^S\to \2\yon^A.
\]
\end{proposition}
\begin{proof}
A lens $\yon\to S\yon^S$ can be identified with an initial state $s_0\in S$.
A lens $S\yon^S\to\2\yon^A$ consists of a function $f\colon S\to\2$, which can be identified with a subset of accept states $F \ss S$, together with an update function $u \colon S\times A\to S$.
\end{proof}

But what if we wanted to make a version of this automaton where, whenever the machine hits an accept state, it stops---no longer taking in any inputs? Again, to do this requires a machine whose set of possible inputs is dependent on the output the current state returns.
In this case, instead of an update function $u\colon S\times A\to S$, we want an update function that takes in an input $a\in A$ if the state $s\in S$ is \emph{not} an accept state (say, if $f(s)=1$) but takes in an input in $\0$ (i.e.\ no input) if the state $s$ \emph{is} an accept state (if $f(s)=2$).
So there is one update function $u_s\colon A\to S$ if $f(s)=1$, and a different update function $u_s\colon\0\to S$ if $f(s)=2$.
But these are exactly the on-directions functions of a lens $S\yon^S\to\yon^A+\1$.
Indeed, replacing our interface monomial with a general polynomial is exactly how we will obtain our generalized dependent Moore machines.

%-------- Section --------%
\section{Dependent dynamical systems}\label{sec.poly.dyn_sys.depend_sys}

As lenses, each of our Moore machines above has an interface of the form $B\yon^A$, i.e.\ a monomial.
Every representable summand of the interface has the same exponent $A$, corresponding to the fact that the set of available inputs into the machine is always $A$.
But by replacing $B\yon^A$ with an arbitrary polynomial $p$---a sum of monomials, in which different representable summands may have different exponents---we will allow our input set to change.

\begin{definition}[Dependent dynamical system]\label{def.gen_moore}
A \emph{dependent dynamical system} (or a \emph{dependent Moore machine}, or simply a \emph{dynamical system}) is a lens
\[\varphi\colon S\yon^S\to p\]
for some $S\in\smset$ and $p\in\poly$. The set $S$ is called the set of \emph{states}---with $S\yon^S$ called the \emph{state system}---and the polynomial $p$ is called the \emph{interface}.
Positions of the interface are called \emph{outputs}, and directions of the interface are called \emph{inputs}.

The lens's on-positions function $\varphi_\1\colon S\to p(\1)$ is called the \emph{return function}, and for each $s\in S$, the lens's on-directions function $\varphi^\sharp_s\colon p[\varphi_\1(s)]\to S$ is called the \emph{update function} at $s$.
\end{definition}

\subsection{Thinking about dependency}

The current output $i$ of a dependent dynamical system with interface $p$ is a $p$-position: an element of the set $p(\1)$.
Then any input provided to the update function at that time must come from the direction-set $p[i]$.
So the set of available inputs varies depending on the current output.
What kind of system has that kind of a relationship between its inputs and outputs?

Think back to our promises for more generalized dynamical systems from \cref{sec.poly.intro.dyn_sys}.
We can begin to think of outputs not only as outward expressions, but as positions that one takes---terminology we've been using all along.
If the polynomial is your body, then your outputs would be the positions that your body could take.
This includes where you go, as well as the direction you're looking with your head and eyes, whether your lips are pursed or not, etc.
In fact, every form of output you provide, from talking to gesturing to moving another object, is performed by changing your position. And your position determines what inputs you'll notice: if your eyes are closed, your input type is different than if your eyes are open.

\slogan{The position you're in is itself a sensory organ: a hand outstretched, an eye open or closed.}

If we squint, we could even see an output more as a sensing apparatus than anything else. This is pretty philosophical, but imagine your outputs---what you say and do---are more there as a question to the world, a way of sensing what the world is like.

\begin{remark}
It may seem limiting that the set of possible inputs of a dependent dynamical system should depend on the current \emph{output}, rather than the current \emph{state}.
Isn't it possible to have a system with different states that give the same output but take in inputs from different sets?

Philosophically, the answer to this question is ``no'' as long as we think about the interface of a dynamical system as capturing everything about how the system interacts with the outside world.
In particular, the output of a system should capture everything an external observer could possibly perceive about the system, while the input set should capture every way an external force can independently decide to interact with the system.

But if the range of ways in which an external agent can choose to interact with the system \emph{changes}, the external agent should be able to detect this fact!
If the internal state changes, but the external output remains the same, the agent wouldn't see any difference---they wouldn't know to interact with the system any differently, so their range of possible inputs would have to stay the same, too.
That's why it makes sense for the set of possible inputs to depend not on the hidden internal state, but on the output currently exposed.
\end{remark}

But however you think of dependent dynamical systems, we need to get a feel for how they work mathematically.
We'll do this by going through several examples.

\subsection{Examples of dependent dynamical systems}
To start, let's finish up the example from the end of the last section.

\begin{example}\label{ex.regular_lang_stop}
Recall deterministic state automata from \cref{def.dfa}.
Say we want such an automaton to halt after reaching an accept state and no longer take input.
For that, rather than use a lens $S\yon^S\to \2\yon^A$, we could use a lens
\[
\varphi\colon S\yon^S\to \yon^A+\1.
\]
To give such a lens, we provide a return function $\varphi_\1\colon S\to\2$ (here we are thinking of $\2$ as the position-set of $\yon^A+\1$).
A function $\varphi_\1$ sends some elements of $S$ to $1$ and others to $2$; those that are sent to $2$ are said to be accept states.

If we reach an accept state, we want the machine to halt.
So at the position $2$, corresponding to the summand $\1$, there are no directions---and thus no inputs available.
In other words, the update function $\varphi^\sharp_s$ is trivial when $\varphi_\1(s)=2$.

On the other hand, $\varphi^\sharp_s$ is not trivial on those elements $s\in S$ for which $\varphi_\1(s)=1$.
Instead, these update functions $\varphi^\sharp_s\colon A\to S$ specify how the machine updates its state on each input from $A$, if the current state is $s$.
This is in line with the way the automaton's update function behaves.

When equipped with an initial state $s_0\in S$ specified by a lens $\yon\to S\yon^S$, we call these dependent dynamical systems \emph{halting deterministic state automata}.
Given a sequence (or ``word'') $(a_1,\ldots,a_n)$ in $\lst(A)$, we say that the automaton \emph{accepts} this word if starting at the initial state and following the elements in the sequence leads us to an accept state, \emph{without hitting an accept state and stopping too early}---or, more formally, if there exists a sequence $(s_0,s_1,\ldots,s_n)$ such that $\varphi_\1(s_n)=2$ and, for all $k\in[0,n-1]$, we have $\varphi_\1(s_k)=1$ and
\[
    s_{k+1}=\varphi^\sharp_{s_k}(a_{k+1}).
\]
We call the set of all words accepted by the automaton the language \emph{recognized} by the automaton.
\end{example}

\begin{remark}
Again, the conditions for when one of these automata accepts a word are rather awkward to formally state.
We will see in \cref{ex.halt_dsa_accept} an alternative way of saying whether a word is accepted by a halting deterministic state automaton.
\end{remark}

\begin{exercise}\label{exc.halt_dsa}
Consider the halting deterministic state automaton shown below:
\begin{equation} \label{eqn.halt_dsa}
\begin{tikzcd}[column sep=small]
	\bul[blue]\ar[rr, bend left, orange]\ar[loop left, dgreen]&&
	\bul[dyellow]\ar[dl, bend left, orange]\ar[ll, dgreen, bend left]\\&
	\bul[red]
\end{tikzcd}
\end{equation}
% \begin{equation} \label{eqn.halt_dsa}
% \begin{tikzpicture}
% \node[draw] {
% \begin{tikzcd}[column sep=small]
% 	\LMO{}\ar[rr, bend left, orange]\ar[loop left, dgreen]&&
% 	\LMO{}\ar[dl, bend left, orange]\ar[ll, dgreen, bend left]\\&
% 	\LMO{}
% \end{tikzcd}
% };
% \end{tikzpicture}
% \end{equation}
Let the left state $\bul[blue]$ be $1$, the right state $\bul[dyellow]$ be $2$, and the bottom state $\bul[red]$ be $3$.
We designate $\bul[blue]$, state $1$, as the initial state.
We can also call the orange arrows ``{\color{orange}orange}'' and the green arrows ``{\color{dgreen}green}.''
Answer the following questions, in keeping with the notation from \cref{ex.regular_lang_stop}.

\begin{enumerate}
	\item What is $S$?
	\item What is $A$?
	\item Based on the labeled transition diagram, which states are accept states, and which are not?
	\item Specify the corresponding lens $S\yon^S\to\yon^A+\1$.
	\item Name a word that is accepted by this automaton.
	\item Name a word that is not accepted by this automaton.
	Why not?
	Can you find another word that is not accepted by this automaton for a different reason?
\qedhere
\end{enumerate}
\begin{solution}
Here we examine the halting deterministic state automaton depicted in \eqref{eqn.halt_dsa}, with left state (and initial state) 1, right state 2, bottom state 3, and arrows {\color{orange}orange} and {\color{dgreen}green}.

\begin{enumerate}
    \item The set of states is $S\coloneqq\{1,2,3\}=\3$ (or equivalently $S\coloneqq\{\bul[blue],\bul[dyellow],\bul[red]\}\iso\3$).
    \item The set of input symbols is $A\coloneqq\{{\color{orange}\text{orange}},{\color{dgreen}\text{green}}\}$.
    \item The automaton should halt at the accept states, so the accept states are exactly the states that have no arrows coming out of them---in this case, only state 3.
    States 1 and 2 are not accept states.
    \item Let the corresponding lens be $\varphi\colon S\yon^S\to\yon^A+\1$, or $\varphi\colon \3\yon^\3\to\yon^{\{{\color{orange}\text{orange}},{\color{dgreen}\text{green}}\}}+\1$.
    According to the previous part, $\varphi$ has a return function $\varphi_\1\colon S\to\2$ sending states 1 and 2, as non-accept states, to 1, and sending state 3, as an accept state, to 2.
    Then the two nontrivial update functions behave as follows, according to the targets of the arrows in \eqref{eqn.halt_dsa}:
    \begin{align*}
        \varphi^\sharp_1&\colon\quad{\color{orange}\text{orange}}\mapsto2,{\color{dgreen}\text{green}}\mapsto1\\
        \varphi^\sharp_2&\colon\quad{\color{orange}\text{orange}}\mapsto3,{\color{dgreen}\text{green}}\mapsto1,
    \end{align*}
    while $\varphi^\sharp_3\colon\0\to S$ is trivial.
    \item Some examples of words accepted by this automaton include the word $({\color{orange}\text{orange}},{\color{orange}\text{orange}}),$ the word $({\color{orange}\text{orange}},{\color{dgreen}\text{green}},{\color{orange}\text{orange}},{\color{orange}\text{orange}}),$ and the word $({\color{dgreen}\text{green}},{\color{orange}\text{orange}},{\color{dgreen}\text{green}},{\color{dgreen}\text{green}},{\color{dgreen}\text{green}},{\color{orange}\text{orange}},{\color{orange}\text{orange}})$.
    \item Some words are not accepted by the automaton because they lead you to a non-accept state (1 or 2); others are not accepted by the automaton because they lead you to an accept state (3) too early.
    Some examples of the former possibility include the words $({\color{dgreen}\text{green}},{\color{dgreen}\text{green}})$ and $({\color{orange}\text{orange}},{\color{dgreen}\text{green}},{\color{orange}\text{orange}},{\color{dgreen}\text{green}})$, while some examples of the latter possibility include the words $({\color{dgreen}\text{green}},{\color{orange}\text{orange}},{\color{orange}\text{orange}},{\color{dgreen}\text{green}})$ and $({\color{orange}\text{orange}},{\color{orange}\text{orange}},{\color{orange}\text{orange}},{\color{orange}\text{orange}},{\color{orange}\text{orange}},{\color{orange}\text{orange}},{\color{orange}\text{orange}})$.
\end{enumerate}
\end{solution}
\end{exercise}

Every graph gives rise to a dynamical system---but to ensure that we are talking about the same thing, let us fix the definition of a graph.

\begin{definition}[Graph] \label{def.graph}
A \emph{graph} $G = (E \tto V)$ consists of an edge set $E$, a vertex set $V$, a source function $s\colon E\to V$, and a target function $t\colon E\to V$.
\end{definition}

So when we say ``graph,'' we mean a \emph{directed} graph, and we allow multiple edges between the same pair of vertices as well as self-loops.

\begin{example}[Graphs as dynamical systems] \label{ex.graph_dyn}
Given a graph $G = (E \tto V)$ with source map $s \colon E \to V$ and target map $t \colon E \to V$, there is an associated polynomial
\[
    g := \sum_{v \in V} \yon^{s\inv(v)}.
\]
Its positions are the vertices of the graph, and at each position $v\in V$, its directions are the edges coming out of $v$.
We call this the \emph{emanation polynomial} of $G$.

The graph itself can be seen as a dynamical system $\varphi\colon V\yon^V \to g$, where $\varphi_\1 = \id_V$ and $\varphi^\sharp_v(e) = t(e)$.
So its states are the vertices of the graph, each vertex returns itself as output, and an input at a vertex $v\in V$ is an edge $e\in E$ coming out of that vertex that takes us from the vertex $v=s(e)$ along the edge $e$ to its target vertex $\varphi^\sharp_v(e)=t(e)$.
\end{example}

\begin{exercise}
Pick your favorite graph $G$, and consider the associated dynamical system as in \cref{ex.graph_dyn}.
Draw its labeled transition diagram as in \eqref{eqn.trans_diag} or \eqref{eqn.halt_dsa}.
\begin{solution}
No matter what graph you chose, \cref{ex.graph_dyn} tells us that if you were to draw out the labeled transition diagram of its associated dynamical system, you would just end up with a picture of your graph!
The vertices of your graph are the states, and the edges of your graph are the possible transitions between them.
\end{solution}
\end{exercise}

% \begin{example}[Inputting an initial state]
% Suppose you have a closed system $f^\sharp\colon S\yon^S\to\yon$. The modeler can choose an initial state $\yon\to S\yon^S$, but what if we want some other system to choose the initial state? We haven't gotten to wiring diagrams yet, but the idea is to create a system that starts as not-closed---accepting as input a state $s\in S$---and then dives into its closed loop with that initial state.

% Let $S'\coloneqq S+\1$, so that the initial state $\yon\to S'\yon^{S'}$ now is canonical: it's the new $\1$. We also have a canonical inclusion $S\To{i}S'$. We will give a lens
% \[
% S'\yon^{S'}\to\yon+\yon^S
% \]
% that starts out with its outer box in the mode $\yon^S$ of accepting an $S$-input, and then moves to the mode $\yon$ so that it is a closed system forever after.

% To give a lens $S'\yon^{S'}\to\yon+\yon^S$, it is sufficient to give two morphisms: $S\yon^{S'}\to\yon$ and $\yon^{S'}\to\yon^S$. The first is equivalent to a function $S\to S'$ and we take the map $S\To{f^\sharp}S\To{i}S'$; this means that whenever we want to update the state from a state in $S$ we'll just do whatever our original closed system did. The second is also equivalent to a function $S\to S'$ and we use $i$; this means that whatever state is input at the beginning will be what we take as our first noncanonical state.
% \end{example}

\begin{example}\label{ex.generalized_file_reader}
In \cref{exc.file_reader} one is tasked with building a file-reader as a Moore machine, where a file is a function $f\colon\ord{n}\to\Set{ascii}$.
Now we turn that file-reader into a dependent dynamical system $\varphi\colon S\yon^S\to p$ that cannot take in input while it is reading.

We let $S \coloneqq \{(s,t)\mid 1\leq s\leq t\leq n\}$, so that each state consists of a current entry $s$ and a terminal entry $t$.
Meanwhile, our interface $p$ will have two labeled copies of $\Set{ascii}$ as positions:
\[
    p(\1)\coloneqq\{\text{`ready'}, \text{`busy'}\}\times\Set{ascii}.
\]
So each $p$-position is a pair $(m,c)$, where $c\in\Set{ascii}$ and $m$ is one of two modes: `ready' or `busy.'
These form the possible outputs of the file-reader.
As for the inputs, we define the direction-sets of $p$ as follows, for all $c\in\Set{ascii}$:
\[
    p[(\text{`ready'}, c)]\coloneqq S \qqand p[(\text{`busy'}, c)]\coloneqq\1.
\]
That way, our file-reader can receive any pair of entries in $S$ as input when it is `ready,' but can only be told to advance when it is `busy.'

We want our file-reader to be `ready' if its current entry is the terminal entry; otherwise, it will be `busy.'
In either case, it will return the ascii character at the current position.
So we define the return function $\varphi_\1$ such that, for all $(s,t)\in S$,
\begin{align*}
  \varphi_\1(s, t) =
  \begin{cases}
    (\text{`ready'}, f(s)) &\mbox{if $s = t$}\\
    (\text{`busy'}, f(s)) &\mbox{otherwise}
  \end{cases}
\end{align*}

While the file-reader is `ready,' we want it to set its new current and terminal entries to be the input.
So for each $(s,t)\in S$ for which $s=t$, we define the update function $\varphi^\sharp_{(s,t)}\colon S\to S$ to be the identity on $S$.

On the other hand, while the file-reader is `busy,' we want it to step forward through the file each time it receives an input.
So for each $(s,t)\in S$ for which $s\neq t$, we let the update function $\varphi^\sharp_{(s,t)}\colon \1\to S$ specify the element $(s+1, t)\in S$, thus shifting its current entry up by $1$.
\end{example}

\begin{exercise} \label{exc.file_searcher}
Say instead of a file-reader, we wanted a file-searcher, which acts just like the file-reader from \cref{ex.generalized_file_reader} except that it only emits output $c\in\Set{ascii}$ when $c$ is a specific character---say, $c=100$.
Give the lens for this file-searcher by explicitly defining its return (on-positions) and update (on-directions) functions.
Hint: You should be able to use the same state system.
\begin{solution}
We give a file-searcher $\psi\colon S\yon^S\to q$ that acts just like the file-reader $\varphi\colon S\yon^S\to p$ from \cref{ex.generalized_file_reader}, except that it only emits output $o\in\Set{ascii}$ when $c=100$.
In place of any other ascii character, we'll have it output $\_$ instead.
So its possible outputs should form the set
\[
    q(\1)\coloneqq\{\text{`ready'}, \text{`busy'}\}\times\{100,\_\}.
\]
The direction-sets of $q$ can be defined in the same way we defined the direction-sets of $p$: for each $c\in\{100,\_\}$, we have
\[
    q[(\text{`ready'}, c)]\coloneqq S \qqand q[(\text{`busy'}, c)]\coloneqq\1.
\]
Then we define the return function $\psi_\1$ like $\varphi_\1$, but first checking to see if the character at the current entry is $100$: so for all $(s,t)\in S$,
\begin{align*}
  \psi_\1(s, t) =
  \begin{cases}
    (\text{`ready'}, 100) &\mbox{if $s=t$ and $f(s)=100$}\\
    (\text{`ready'}, \_) &\mbox{if $s=t$ and $f(s)\neq100$}\\
    (\text{`busy'}, 100) &\mbox{if $s\neq t$ and $f(s)=100$}\\
    (\text{`busy'}, \_) &\mbox{otherwise}\\
  \end{cases}
\end{align*}
Then the update functions of $\psi$ behave just like those of $\varphi$.
For each $(s,t)\in S$ for which $s=t$, we define the update function $\psi^\sharp_{(s,t)}\colon S\to S$ to be the identity on $S$.
On the other hand, for each $(s,t)\in S$ for which $s\neq t$, we let the update function $\psi^\sharp_{(s,t)}\colon \1\to S$ specify the element $(s+1, t)\in S$, thus shifting its current entry up by $1$.
\end{solution}
\end{exercise}

In the previous exercise, we manually constructed a file-searcher that acted very much like a file-reader.
In \cref{exc.file_searcher_wrap}, we'll see a simpler way to construct a file-searcher by leveraging the file-reader we have already defined.
Moreover, this construction highlights precisely how our file-searcher is related to our file-reader.
This will be possible using \emph{wrapper interfaces}, which we'll introduce in \cref{subsec.poly.dyn_sys.new.wrap}.

\begin{example}\label{ex.grid_robot}
Choose $n\in\nn$, a \emph{grid size}, and for each $i\in\ord{n}$, let $D_i$ be the set
\[
	D_i\coloneqq
	\begin{cases}
		\{0,+1\}&\tn{ if }i=1\\
		\{-1,0,+1\}&\tn{ if } 1<i<n\\
		\{-1,0\}&\tn{ if }i=n
	\end{cases}
\]
We can think of $D_i$ as the set of all directions a robot could move if at position $i$.
In particular, a robot already at $i=1$ cannot move in the $-1$ direction; likewise, a robot already at $i=n$ cannot move in the $+1$ direction.

Then we can model a robot that can be instructed to move within an $\ord{n}\times\ord{n}$ grid as a dependent dynamical system $\varphi\colon S\yon^S\to p$, with $S\coloneqq\ord{n}\times\ord{n}$ and
\[
    p\coloneqq\sum_{(i,j)\in\ord{n}\times\ord{n}}\yon^{D_i\times D_j}.
\]
The robot's state is a position in the grid.
We let $\varphi_\1\coloneqq\id_{\ord{n}\times\ord{n}}$ so that each state returns itself as output---in particular, the robot returns a position as output by moving to that position in the grid.

Then for each $(i,j)\in\ord{n}\times\ord{n}$, we let $\varphi^\sharp_{(i,j)}$ send each pair of directions $(d,e)\in D_i\times D_j$ to the grid position given by $(i+d,j+e)$.
Concretely, this says that if a robot at position $(i,j)$ in the grid receives the pair of directions $(d,e)$ as its input, its new position in the grid will be $(i+d,j+e)$.
Our definition of $D_i$ for each $i\in\ord{n}$ guarantees that this position is still in the grid.
With this setup, the robot has more movement options when it is in the center of the grid than when it is on the sides or corners:
\[
\begin{tikzpicture}[scale=.5]
  \draw[step=1cm,gray,very thin] (-3,-3) grid (4,4);
	\draw[->, red ] (-2.5,3.5) -- (-1.5, 3.5);
	\draw[->, red ] (-2.5,3.5) -- (-1.5, 2.5);
	\draw[->, red ] (-2.5,3.5) -- (-2.5, 2.5);
	\draw[->, blue] (1.5, 0.5) -- (0.5, 0.5);
	\draw[->, blue] (1.5, 0.5) -- (2.5, 0.5);
	\draw[->, blue] (1.5, 0.5) -- (1.5, 1.5);
	\draw[->, blue] (1.5, 0.5) -- (1.5,-0.5);
	\draw[->, blue] (1.5, 0.5) -- (0.5, 1.5);
	\draw[->, blue] (1.5, 0.5) -- (0.5,-0.5);
	\draw[->, blue] (1.5, 0.5) -- (2.5, 1.5);
	\draw[->, blue] (1.5, 0.5) -- (2.5,-0.5);
\end{tikzpicture}
\]

Note that, in this example, the positions of $p$ are literally the positions in the grid where the robot could be, and the directions of $p$ at each position are literally the directions in which the robot can move!
\end{example}

\begin{exercise} \label{exc.grid_reward}
Modify the dynamical system from \cref{ex.grid_robot} as follows.
\begin{enumerate}
	\item Replace $p$ with another interface $p'$ so that at each grid value, the robot can receive not only the direction it should move in but also a ``reward value'' $r\in\rr$.
	\item Replace $S$ with another set of states $S'$ so that an element $s\in S'$ may include both the robot's position and a list of all reward values so far.
	\item With your new $p'$ and $S'$, define a new lens $\varphi'\colon S'\yon^{S'}\to p'$ that preserves the behavior of $\varphi\colon S\yon^S\to p$ from \cref{ex.grid_robot}, but also properly updates the robots list of rewards.
\qedhere
\end{enumerate}
\begin{solution}
We modify the dynamical system from \cref{ex.grid_robot}.
\begin{enumerate}
    \item Previously, the set of inputs at each output $(i,j)\in\ord{n}\times\ord{n}$ of our interface $p$ was $D_i\times D_j$.
    But now we also want to be able to give the robot a ``reward value'' $r\in\rr$.
    So our new input set should be $D_i\times D_j\times\rr$, making
    \[
        p'\coloneqq\sum_{(i,j)\in\ord{n}\times\ord{n}} \yon^{D_i\times D_j\times\rr}.
    \]
    \item Previously, a state was just a position in $\ord{n}\times\ord{n}$.
    But now we want to be able to record a list of reward values as well.
    Since each reward value is a real number, it suffices to define the state set to be $S'\coloneqq\ord{n}\times\ord{n}\times\lst(\rr)$.
    \item The former return function $\varphi_\1\colon S\to p(\1)$ was the identity on $\ord{n}\times\ord{n}$.
    The new return function $\varphi'_\1$ should still just yield the robot's current grid position, but since it is now a function from $S'=\ord{n}\times\ord{n}\times\lst(\rr)$, it should instead be the canonical projection $\varphi'_\1\colon \ord{n}\times\ord{n}\times\lst(\rr)\to\ord{n}\times\ord{n}$.

    For each former state $(i,j)\in\ord{n}\times\ord{n}$, the former update function $\varphi^\sharp_{(i,j)}\colon D_i\times D_j\to\ord{n}\times\ord{n}$ sent $(d,e)\mapsto(i+d,j+e)$.
    With an extra component $(r_1,\ldots,r_k)\in\lst(\rr)$ of the state, the new update function $(\varphi')^\sharp_{(i,j,(r_1,\ldots,r_k))}\colon D_i\times D_j\times\rr\to\ord{n}\times\ord{n}\times\lst(\rr)$ sends $(d,e,r)\mapsto(i+d,j+e,(r_1,\ldots,r_k,r))$, updating the list of rewards.
\end{enumerate}
\end{solution}
\end{exercise}

In the previous exercise, we added a reward system to the robot on the grid by manually redefining the associated lens.
But there is a much simpler way to think about the new system as the juxtaposition of two systems, a robot system and a reward system, in parallel.
We'll see how to express this in terms of lenses in \cref{exc.grid_reward_par}, once we explain how to juxtapose systems like this in general in \cref{subsec.poly.dyn_sys.new.par}.
In fact, we'll see in \cref{exc.grid_robot_par} that the robot-on-a-grid system itself can be viewed as the juxtaposition of two systems, and this perspective will provide a structured way to generalize \cref{ex.grid_robot} to more than two dimensions.


%-------- Section --------%
\section{Constructing new dynamical systems from old}\label{sec.poly.dyn_sys.new}

We have now seen how dependent dynamical systems can be modeled as lenses in $\poly$ of the form $S\yon^S\to p$.
But we have yet to take full advantage of the categorical structure that $\poly$ provides.
In particular, purely based on what we know of $\poly$ so far from \cref{ch.poly.func_nat}, we have three rather different ways of obtaining new dynamical systems from old ones:
\begin{enumerate}
    \item Given dynamical systems $S\yon^S\to p$ and $S\yon^S\to q$, we can use the universal property of the \emph{categorical product} to obtain a dynamical system $S\yon^S\to p\times q$; see \cref{subsec.poly.dyn_sys.new.prod}.
    \item Given dynamical systems $\varphi\colon S\yon^S\to p$ and $\psi\colon T\yon^T\to q$, we can take their parallel product to obtain a dynamical system $\varphi\otimes\psi\colon ST\yon^{ST}\to p\otimes q$; see \cref{subsec.poly.dyn_sys.new.par}.
    \item Given a dynamical system $\varphi\colon S\yon^S\to p$ and a lens $f\colon p\to q$, we can compose them to obtain a dynamical system $\varphi\then f\colon S\yon^S\to q$; see \cref{subsec.poly.dyn_sys.new.wrap}.
\end{enumerate}
Each of these operations has a concrete interpretation in terms of dynamical systems.
In this section, we'll review each of them in turn.

%---- Subsection ----%
\subsection{Categorical products: multiple interfaces operating on the same states}\label{subsec.poly.dyn_sys.new.prod}

For $n\in\nn$, say that we have $n$ dynamical systems, $\varphi_i\colon S\yon^S\to p_i$ for each $i\in\ord{n}$, that all share the same state system.
Then by the universal property of products in $\poly$, there is an induced lens \[\varphi\colon S\yon^S\to\prod_{i\in\ord{n}}p_i,\] which is itself a dynamical system with state system $S\yon^S$.

We can characterize the dynamics of $\varphi$ in terms of each original dynamical system $\varphi_i$ as follows.
By \cref{exc.poly_prod}, the return function \[\varphi_\1\colon S\to\prod_{i\in\ord{n}}p_i(\1)\] sends each state $s\in S$ to the corresponding $n$-tuple of outputs $((\varphi_i)_\1(s))_{i\in\ord{n}}$ returned by each of the original dynamical systems at that state.
Then at each state $s\in S$, the update function \[\varphi^\sharp_s\colon\sum_{i\in\ord{n}}p_i[(\varphi_i)_\1(s)]\to S\] sends each pair $(i,d)$ in its domain, with $i\in\ord{n}$ and $d\in p_i[(\varphi_i)_\1(s)]$, to where the update function of $\varphi_i$ at $s$ sends $d$: namely $(\varphi_i)^\sharp_s(d)$.

In other words, if there are multiple interfaces that can drive the same set of states, we may view them as a single product interface that can drives those states.
This single dynamical system returns output in all of the original systems at once; then it can receive input from any one of the original systems' input sets and update its state accordingly.
It's as though each of the dynamical systems can see where the combined system is at any time, but only one of them can actually operate it at any given time.
So products give us a universal way to combine multiple polynomial interfaces into one.

\begin{exercise}
Given $n\in\nn$, suppose we have an $(A_i,B_i)$-Moore machine with state set $S$ for each $i \in \ord{n}$.
Show that there is an induced $\left(\sum_{i \in \ord{n}} A_i, \prod_{i \in \ord{n}} B_i\right)$-Moore machine, again with state set $S$.
\begin{solution}
We are given $n\in\nn$ and an $(A_i,B_i)$-Moore machine with state set $S$, i.e.\ a lens $S\yon^S\to B_i\yon^{A_i}$, for each $i\in\ord{n}$.
The universal property of products in $\poly$ gives us a lens
\[
    S\yon^S\to\prod_{i\in\ord{n}}B_i\yon^{A_i}\iso\left(\prod_{i\in\ord{n}}B_i\right)\yon^{\sum_{i\in\ord{n}}A_i},
\]
which is a $\left(\sum_{i \in \ord{n}} A_i, \prod_{i \in \ord{n}} B_i\right)$-Moore machine with state set $S$ (after all, the product of monomials is still a monomial).
\end{solution}
\end{exercise}

\begin{example} \label{ex.prod_diagrams}
Consider two four-state dependent dynamical systems $\varphi\colon\4\yon^\4\to\rr\yon^{\{r,b\}}$ and $\psi\colon\4\yon^\4\to \zz_{\geq0}\yon^{\{g,p\}}+\zz_{<0}\yon^{\{g\}}$, drawn below as labeled transition diagrams (we think of $r,b,g,$ and $p$ as red, blue, green, and purple, respectively):
\[
\begin{tikzpicture}
	\node[draw] (1) {
  \begin{tikzcd}[row sep=15pt]
  	\LMO{\pi}\ar[r, bend left=15pt, red]\ar[loop left=15pt, blue]&
  	\LMO{0}\ar[l, bend left=15pt, red]\ar[d, bend left=15pt, blue]\\
  	\LMO[under]{-1.41}\ar[u,bend left=15pt, red]\ar[r, bend right=15pt, blue]&
  	\LMO[under]{2.72}\ar[l, bend right=15pt, red]\ar[loop right=15pt, blue]
  \end{tikzcd}
	};
	\node[draw, right=of 1] {
  \begin{tikzcd}[row sep=15pt]
  	\LMO{-2}\ar[d, green!50!black]&
  	\LMO{4}\ar[l, green!50!black]\ar[d, blue!50!purple]\\
  	\LMO[under]{-8}\ar[loop left, green!50!black]&
  	\LMO[under]{16}\ar[ul, green!50!black]\ar[l, blue!50!purple]
  \end{tikzcd}
  };
 \end{tikzpicture}
\]

The universal property of products provides a unique way to put these systems together to obtain a dynamical system $\4\yon^\4\to\rr\zz_{\geq0}\yon^{\{r,b,g,p\}}+\rr\zz_{<0}\yon^{\{r,b,g\}}$ that looks like this:
\[
\begin{tikzpicture}
	\node[draw] (1) {
  \begin{tikzcd}
  	\LMO{(\pi,-2)}\ar[r, bend left=15pt, red]\ar[loop left, blue]\ar[d, bend left=15pt, green!50!black]&
  	\LMO{(0,4)}\ar[l, bend left=15pt, red]\ar[d, bend left=15pt, blue]\ar[d, bend right=15pt, blue!50!purple]\ar[l, green!50!black]\\
  	\LMO[under]{(-1.41,-8)}\ar[u,bend left=15pt, red]\ar[r, bend right=15pt, blue]\ar[loop left, green!50!black]&
  	\LMO[under]{(2.72,16)}\ar[l, bend right=15pt, red]\ar[l, blue!50!purple]\ar[loop right=15pt, blue]\ar[ul, green!50!black]
  \end{tikzcd}
  };
\end{tikzpicture}
\]
Each state now returns two outputs: one according to the return function of $\varphi$, and another according to the return function of $\psi$.
As for the possible inputs, we now have the option of giving either an input from a direction-set of $\varphi$ (either $r$ or $b$), in which case the dynamical system will update its state according to the corresponding update function of $\varphi$, or an input from a direction-set of $\psi$ (either $g$ or sometimes $p$), in which case the dynamical system will update its state according to the corresponding update function of $\psi$.
\end{example}

\begin{exercise}[Toward event-based systems]
Let $\varphi\colon S\yon^S\to p$ be a dynamical system. It is constantly needing input at each time step. An event-based system is one that doesn't always get input, and only reacts when it does.

So suppose we want to allow our dynamical system not to do anything. That is, rather than needing to press a button corresponding to a direction of $p$ at each time step, we want to be able to \emph{not} press any button, in which case the system just stays where it is. We want a new system $\varphi'\colon S\yon^S\to p'$ that has this behavior; what are $p'$ and $\varphi'$?
\begin{solution}
Given a dynamical system $\varphi\colon S\yon^S\to p$, we seek a new dynamical system $\varphi'\colon S\yon^S\to p'$ that has the added option to provide no input at a step so that the state does not change.
We can think of this as having two different interfaces acting on the same system: the original interface $p$ of $\varphi$, and a new interface with only one possible input---namely the option to provide no input at all---that does not change the state.
This latter interface also does not need to distinguish between its outputs; it should have just one possible output that says nothing.
So the second interface we want acting on $S\yon^S$ is $\yon$.

If $\yon$ were the only interface acting on the system, we would have a Moore machine $\epsilon\colon S\yon^S\to\yon$ whose return function is the unique function $S \to \1$ and whose update function is the identity function on $S$, since the input never changes the system.
Then $p'$ is the product of the two interfaces $p$ and $\yon$, while $\varphi'\colon S\yon^S \to p'$ is the unique lens induced by $\varphi\colon S\yon^S\to p$ and $\epsilon\colon S\yon^S\to\yon$.
In particular, $p'\iso p\yon\iso\sum_{i\in p(\1)}\yon^{p[i]+\1}$, while $\varphi'$ consists of a return function $\varphi'_\1\colon S \to p(\1)$ that is the same as the return function of $\varphi$ and, for each state $s\in S$, an update function $\varphi'^\sharp_s\colon p[i]+\1\to S$ that behaves like the update function of $\varphi$ at $s$ when the input is from $p[i]$ (sending $a\in p[i]$ to $\varphi^\sharp_s(a)$) but does not change the state when the input is from $\1$ (sending the unique element of $\1$ to $s$).

This construction is actually universal; it is known as \emph{copointing}.
\end{solution}
\end{exercise}

%---- Subsection ----%
\subsection{Parallel products: juxtaposing dynamical systems}\label{subsec.poly.dyn_sys.new.par}

Another way to combine two polynomials---and indeed two lenses---is by taking their parallel product, as in \cref{subsec.poly.cat.monoidal.par}.
In particular, the parallel product of two state systems is still a state system.
So parallel products give us another way to create new dynamical systems from old ones.
In fact, it's about as easy as you could hope: you just multiply the set of states, multiply the set of outputs, and multiply the set of inputs at each of those outputs.

For $n\in\nn$, say that we have $n$ dynamical systems, $\varphi_i\colon S_i\yon^{S_i}\to p_i$ for every $i\in\ord{n}$.
Then we can take the parallel product of all of them to get a lens \[\varphi\colon \left(\prod_{i\in\ord{n}}S_i\right)\yon^{\prod_{i\in\ord{n}}S_i}\iso\bigotimes_{i\in\ord{n}}S_i\yon^{S_i}\to\bigotimes_{i\in\ord{n}}p_i,\] which is itself a dynamical system.

We can characterize the dynamics of $\varphi$ in terms of each constituent dynamical system $\varphi_i$ as follows.
By the proof of \cref{prop.parallel_monoidal}, the return function \[\varphi_\1\colon \prod_{i\in\ord{n}}S_i\to\prod_{i\in\ord{n}}p_i(\1)\] sends each $n$-tuple of states $(s_i)_{i\in\ord{n}}$ in its domain, with each $s_i\in S_i$, to the $n$-tuple of outputs $((\varphi_i)_\1(s_i))_{i\in\ord{n}}$ returned by each of the constituent dynamical systems at each state.
Then at the $n$-tuple of states $(s_i)_{i\in\ord{n}}\in\prod_{i\in\ord{n}}S_i$, the update function \[\varphi^\sharp_{(s_i)_{i\in\ord{n}}}\colon\prod_{i\in\ord{n}}p_i[(\varphi_i)_\1(s_i)]\to\prod_{i\in\ord{n}}S_i\] sends each $n$-tuple of directions $(d_i)_{i\in\ord{n}}$ in its domain, with each $d_i\in p_i[(\varphi_i)_\1(s_i)]$, to the $n$-tuple $((\varphi_i)^\sharp_{s_i}(d_i))_{i\in\ord{n}}$ consisting of states where the update function of each $\varphi_i$ at $s_i$ sends $d_i$.

In other words, multiple dynamical systems running in parallel can be thought of as a single dynamical system.
This system stores the states of all the constituent systems at once and returns output from all of them together; then it can receive input from all of the constituent systems' input sets at once and update each constituent state accordingly.
So parallel products give us a way to juxtapose multiple dynamical systems in parallel to form a single system.

\begin{exercise}
Given $n\in\nn$, suppose we have an $(A_i,B_i)$-Moore machine with state set $S_i$ for every $i \in \ord{n}$.
Show that there is an induced $\left(\prod_{i \in \ord{n}} A_i, \prod_{i \in \ord{n}} B_i\right)$-Moore machine with state set $\prod_{i \in \ord{n}} S_i$.
\begin{solution}
We are given $n\in\nn$ and an $(A_i,B_i)$-Moore machine with state set $S_i$, i.e.\ a lens $S_i\yon^{S_i}\to B_i\yon^{A_i}$, for each $i\in\ord{n}$.
Taking their parallel product in $\poly$ gives us a lens
\[
    \left(\prod_{i\in\ord{n}}S_i\right)\yon^{\prod_{i\in\ord{n}}S_i}\iso\bigotimes_{i\in\ord{n}}S_i\yon^{S_i}\to\bigotimes_{i\in\ord{n}}B_i\yon^{A_i}\iso\left(\prod_{i\in\ord{n}}B_i\right)\yon^{\prod_{i\in\ord{n}}A_i},
\]
which is a $\left(\sum_{i \in \ord{n}} A_i, \prod_{i \in \ord{n}} B_i\right)$-Moore machine with state set $S$ (after all, the product of monomials is still a monomial).
\end{solution}
\end{exercise}

\begin{example} \label{ex.par_diagrams}
Consider two dependent dynamical systems $\varphi\colon\2\yon^\2\to \rr_{<0}\yon^{\{b,r\}}+\rr_{\geq0}\yon^{\{b\}}$ and $\psi\colon\3\yon^\3\to\zz_{<0}\yon^{\{r\}}+\{0\}\yon^{\{r,y\}}+\zz_{>0}\yon^{\{y\}}$, drawn below as labeled transition diagrams (we think of $b,r,$ and $y$ as blue, red, and yellow, respectively):
\[
\begin{tikzpicture}
	\node[draw] (1) {
  \begin{tikzcd}[row sep=15pt]
  	\LMO{\sqrt{7}}\ar[d, bend left=15pt, blue]\\
  	\LMO[under]{-e}\ar[u, bend left=15pt, red]\ar[loop right=15pt, blue]
  \end{tikzcd}
	};
	\node[draw, right=of 1] {
  \begin{tikzcd}[row sep=15pt]
  	\LMO{-5}\ar[r, bend left=15pt, red]&
  	\LMO{0}\ar[l, bend left=15pt, dyellow]\ar[r, red]&
  	\LMO{8}\ar[loop right=15pt, dyellow]
  \end{tikzcd}
  };
 \end{tikzpicture}
\]
Taking their parallel product, we obtain a dynamical system with state system $\6\yon^\6$ and interface
\begin{align*}
    &\rr_{<0}\zz_{<0}\yon^{\{(b,r),(r,r)\}}+\rr_{<0}\{0\}\yon^{\{(b,r),(b,y),(r,r),(r,y)\}}+\rr_{<0}\zz_{>0}\yon^{\{(b,y),(r,y)\}}\\
    +\:&\rr_{\geq0}\zz_{<0}\yon^{\{(b,r)\}}+\rr_{\geq0}\{0\}\yon^{\{(b,r),(b,y)\}}+\rr_{\geq0}\zz_{>0}\yon^{\{(b,y)\}}
\end{align*}
that looks like this (we use purple to indicate $(b,r)$, red to indicate $(r,r)$, green to indicate $(b,y)$, and orange to indicate $(r,y)$):
\[
\begin{tikzpicture}
	\node[draw] (1) {
  \begin{tikzcd}
    \LMO{(\sqrt{7},-5)}\ar[dr, bend left=15pt, blue!50!purple] &
    \LMO{(\sqrt{7},0)}\ar[dl, green!50!black]\ar[dr, blue!50!purple] &
    \LMO{(\sqrt{7},8)}\ar[d, bend left=15pt, green!50!black] \\
    \LMO[under]{(-e,-5)}\ar[ur, bend left=15pt, red]\ar[r, blue!50!purple] &
    \LMO[under]{(-e,0)}\ar[ul, orange!75!black]\ar[ur, red]\ar[l, bend left=15pt, green!50!black]\ar[r, blue!50!purple] &
    \LMO[under]{(-e,8)}\ar[u, bend left=15pt, orange!75!black]\ar[loop right=15pt, green!50!black]
  \end{tikzcd}
  };
\end{tikzpicture}
\]
Each state---really a pair of states from the constituent state sets---returns two outputs, one according to the return function of $\varphi$ and another according to the return function of $\psi$.
Then every input must be a pair of inputs from the constituent state sets, with the update function updating each state in the pair given each input in the pair according to the constituent update functions $\varphi$ and $\psi$.
\end{example}

\begin{exercise} \label{exc.grid_reward_par}
Explain how the dynamical system $\varphi'\colon S'\yon^{S'}\to p'$ you built in \cref{exc.grid_reward} can be expressed as the parallel product of the robot-on-a-grid dynamical system $\varphi\colon S\yon^S\to p$ from \cref{ex.grid_robot} with another dynamical system, $\psi\colon T\yon^T\to q$.
Be sure to specify $T, q,$ and $\psi$.
\begin{solution}
We will show that taking the parallel product of the robot-on-a-grid dynamical system $\varphi\colon S\yon^S\to p$ from \cref{ex.grid_robot} and a reward-tracking dynamical system $\psi\colon T\yon^T\to q$ that we will define yields the dynamical system $\varphi'\colon S'\yon^{S'}\to p'$ from \cref{exc.grid_reward}.

The reward-tracking dynamical system should have states in $\lst(\rr)$ to record a list of reward values, unchanging output, and inputs in $\rr$ to give new reward values.
So it is the lens $\lst(\rr)\yon^{\lst(\rr)}\to\yon^\rr$ that has a uniquely defined return function, while its update function sends each state $(r_1,\ldots,r_k)\in\lst(\rr)$ and each input $r\in\rr$ to the new state $(r_1,\ldots,r_k,r)$.

Then the dynamical system from \cref{exc.grid_reward} is the parallel product of the robot-on-a-grid dynamical system from \cref{ex.grid_robot} with the reward-tracking dynamical system $\lst(\rr)\yon^{\lst(\rr)}\to\yon^\rr$, as can be seen in the solution to \cref{exc.grid_reward}.
\end{solution}
\end{exercise}

\begin{exercise} \label{exc.grid_robot_par}
\begin{enumerate}
    \item Explain how the robot-on-a-grid dynamical system $\varphi\colon S\yon^S\to p$ from \cref{ex.grid_robot} can be written as the parallel product of some dynamical system with itself.
    \item Use $k$-fold parallel products to generalize \cref{ex.grid_robot} to robots on $k$-dimensional grids.\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item The robot-on-a-grid dynamical system from \cref{ex.grid_robot} can be written as the parallel product of two robot-on-a-line dynamical systems of the form $\lambda\colon\ord{n}\yon^{\ord{n}}\to\sum_{i\in\ord{n}}\yon^{D_i}$, where $\lambda_\1\coloneqq\id_{\ord{n}}$ and $\lambda^\sharp_i$ for each $i\in\ord{n}$ sends each direction $d\in D_i$ to the position on the line given by $i+d$.
    This yields a robot that can move along a single axis, and the parallel product of this robot with itself yields a robot that can move along two different axes at once, which is precisely our robot-on-a-grid dynamical system.
    \item To create a dynamical system consisting of a robot moving in a $k$-dimensional grid of size $n$ along every dimension, we just take the $k$-fold parallel product of the dynamical system $\lambda\colon\ord{n}\yon^{\ord{n}}\to\sum_{i\in\ord{n}}\yon^{D_i}$ we just defined to obtain a dynamical system \[\lambda^{\otimes k}\colon\ord{n}^\ord{k}\yon^{\ord{n}^\ord{k}}\to\sum_{(i_1,\ldots,i_k)\in\ord{n}^\ord{k}}\yon^{\prod_{j\in\ord{k}}D_{i_j}}.\]
    In fact, we could have used a different $n_j$ for each $j\in\ord{k}$ instead of $n$ to obtain a robot moving in an arbitrary $k$-dimensional grid of size $n_1\times\cdots\times n_k$ as a $k$-fold parallel product.
\end{enumerate}
\end{solution}
\end{exercise}

The parallel product takes two dynamical systems and essentially puts them in the same room together so that they can be run at the same time.
But it doesn't allow for any interaction \emph{between} the two systems.
For that, we will need to use what we call a wrapper interface.
We'll introduce wrapper interfaces in the next section, before describing how they can be used in conjunction with parallel products to model general interaction in \cref{sec.poly.dyn_sys.interact}.

%---- Subsection ----%
\subsection{Composing lenses: wrapper interfaces}\label{subsec.poly.dyn_sys.new.wrap}

The idea behind a wrapper interface is simple.
Given a dynamical system $\varphi\colon S\yon^S\to p$, say that we wanted to interact with it using a new interface $q$ rather than $p$.
We can do this as long as we have a lens $f\colon p\to q$, which we can then compose with our original dynamical system to obtain a new system $S\yon^S\To{\varphi}p\To{f}q$.
We call the lens $f$ the \emph{wrapper} and its codomain $q$ the \emph{wrapper interface}, which we \emph{wrap} around $\varphi$ (or sometimes just $p$, if a dynamical system $\varphi$ has yet to be specified) using $f$.

To see how this new composite system $\varphi\then f$ relates to the original dynamical system $\varphi$, it is best to view $f$ as a delegation of decisions like we did in \cref{???}.
The lens $f$ converts an output $i$ from $p$ to an output $f_\1(i)$ from $q$ on positions, but at the same time is allowing the decision of which input in $p[i]$ to choose next to depend on a choice of input from $q[f_\1(i)]$ instead, via its on-directions function at $i$.
So the wrapper $f$ converts output from the original interface $p$ to output from the wrapper interface $q$, and it converts input into the wrapper interface $q$ to input into the original interface $p$.
Precomposed with a dynamical system, we obtain a new dynamical system that allows you to interact with the original system using only this new interface wrapped around it.

\begin{example} \label{ex.wrap_diagrams}
Consider a dependent dynamical system $\varphi\colon\6\yon^\6\to p$ with
\[
    p\coloneqq\{1\}\yon^{\{b,y,r\}}+\{2\}\yon^{\{b,r\}}+\{3\}\yon^{\{b\}}+\{4\}\yon^{\{r\}},
\]
drawn below as a labeled transition diagram (we think of $b,y,$ and $r$ as blue, yellow, and red, respectively):
\[
\begin{tikzpicture}
	\node[draw] (1) {
  \begin{tikzcd}
    \LMO{1}\ar[r, blue]\ar[dr, dyellow]\ar[d, red] &
    \LMO{2}\ar[loop above=5pt, blue]\ar[d, bend right=15pt, red] &
    \LMO{3}\ar[l, blue] \\
    \LMO[under]{4}\ar[loop left=15pt, red] &
    \LMO[under]{1}\ar[l, blue]\ar[u, bend right=15pt, dyellow]\ar[r, red] &
    \LMO[under]{4}\ar[u, red]
  \end{tikzcd}
  };
\end{tikzpicture}
\]
We will wrap the interface \[q\coloneqq\{a\}\yon^{\{g,p,o\}}+\{b\}\yon^{\{g,p\}}+\{c\}\] around $\varphi$ using the following lens $f\colon p\to q$ (we think of $g,p,$ and $o$ as green, purple, and orange, respectively):
\[
\begin{tikzpicture}
	\node (p1) {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$}
      child[blue] {coordinate (11)}
      child[dyellow] {coordinate (12)}
      child[red] {coordinate (13)};
    \node[right=1.5 of 1, "\tiny $b$" below] (2) {$\bullet$}
      child[green!50!black] {coordinate (21)}
      child[blue!50!purple] {coordinate (22)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (13);
      \draw[postaction={decorate}] (22) to (12);
    \end{scope}
  \end{tikzpicture}
	};
%
	\node (p2) [below right=-1.05cm and 1 of p1] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 2" below] (1) {$\bullet$}
      child[blue] {coordinate (11)}
      child[red] {coordinate (12)};
    \node[right=of 1, "\tiny $c$" below] (2) {$\bullet$};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
  \end{tikzpicture}
	};
%
	\node (p3) [right=3.5 of p1] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 3" below] (1) {$\bullet$}
      child[blue] {coordinate (11)};
    \node[right=1.5 of 1, "\tiny $b$" below] (2) {$\bullet$}
      child[green!50!black] {coordinate (21)}
      child[blue!50!purple] {coordinate (22)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (11);
      \draw[postaction={decorate}] (22) to (11);
    \end{scope}
  \end{tikzpicture}
	};
%
	\node (p4) [right=1 of p3] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 4" below] (1) {$\bullet$}
      child[red] {coordinate (11)};
    \node[right=1.5 of 1, "\tiny $a$" below] (2) {$\bullet$}
      child[green!50!black] {coordinate (21)}
      child[blue!50!purple] {coordinate (22)}
      child[orange!75!black] {coordinate (23)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (11);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (11);
    \end{scope}
  \end{tikzpicture}
	};
\end{tikzpicture}
\]
Composing $\varphi$ with $f$, we obtain a dynamical system $\6\yon^\6\To{\varphi}p\To{f}q$ that looks like this:
\[
\begin{tikzpicture}
	\node[draw] (1) {
  \begin{tikzcd}
    \LMO{b}\ar[d, green!50!black]\ar[dr, blue!50!purple] &
    \LMO{c} &
    \LMO{b}\ar[l, bend right=15pt, green!50!black]\ar[l, bend left=15pt, blue!50!purple] \\
    \LMO[under]{a}\ar[loop left=15pt, green!50!black]\ar[loop below=5pt, blue!50!purple]\ar[loop right=15pt, orange!75!black] &
    \LMO[under]{b}\ar[r, green!50!black]\ar[u, blue!50!purple] &
    \LMO[under]{a}\ar[u, bend left=20pt, green!50!black]\ar[u, blue!50!purple]\ar[u, bend right=20pt, orange!75!black]
  \end{tikzcd}
  };
\end{tikzpicture}
\]
Each state returns an output in $q$ according to where the on-positions function of $f$ sends the output the state returns in $p$.
Then each input in $q$ is passed to an input in $p$ via the corresponding on-directions function of $f$, whereupon the update function of $\varphi$ computes the new state.
So $f$ allows us to operate $\varphi$ with the wrapper interface $q$ instead of the original interface $p$.
\end{example}

\begin{exercise} \label{exc.file_searcher_wrap}
In \cref{exc.file_searcher}, you constructed a file-searcher $\psi\colon S\yon^S\to q$ by taking the file-reader $\varphi\colon S\yon^S\to p$ from \cref{ex.generalized_file_reader} and replacing its interface $p$ with a new interface $q$ while keeping its state system $S\yon^S$ the same.
Express this construction as wrapping $q$ around $\varphi$ by giving a lens $f\colon p\to q$ for which composing $\varphi$ with $f$ yields $\psi$.
\begin{solution}
We give a lens $f\colon p\to q$ for which composing the file-reader $\varphi\colon S\yon^S\to p$ from \cref{ex.generalized_file_reader} with $f$ yields the file-searcher $\psi\colon S\yon^S\to q$ from \cref{exc.file_searcher}.
The file-searcher returns the same output as the file-reader when the second coordinate is $100$, but replaces the second coordinate with a blank $\_$ otherwise.
So the on-positions function of $f$ should send each $(m,c)\in p(\1)$ to
\[
    f_\1(m,c) =
        \begin{cases}
            (m,c) & \text{if } c = 100 \\
            (m,\_) & \text{otherwise}.
        \end{cases}
\]
Then the file-searcher acts just like the file-reader does on inputs, so the on-directions functions of $f$ should be $\id_S$ at each position whose first coordinate is `ready' and $\id_\1$ at each position whose first coordinate is `busy.'
\end{solution}
\end{exercise}

In the next section, we describe a special kind of wrapper.

%---- Subsection ----%
\subsection{Situations as enclosures}\label{subsec.poly.dyn_sys.new.sit_encl}
Say we wanted to model a dynamical system $\varphi\colon S\yon^S\to p$ within a closed system, for which an external agent can perceive no change in output and effect no change in input.
We can think of this as wrapping $\yon$, the interface with one output and one input, around $\varphi$.
To do so, we must specify a wrapper $\gamma\colon p\to\yon$.
We call such a wrapper an \emph{enclosure} for an interface $p$, since it is a way of closing off $p$ to the outside world.

Let us zoom out from the dynamical systems interpretation of polynomials for the time being to examine the categorical properties of enclosures.
Given a polynomial $p$, we denote the set of lenses $p\to\yon$ by
\begin{equation} \label{eqn.gamma_def}
\Gamma(p)\coloneqq\poly(p,\yon)
\end{equation}
as we did in \cref{prop.adjoint_quadruple}.
By \eqref{eqn.main_formula}, we have that
\begin{equation} \label{eqn.gamma_prod}
    \Gamma(p) \iso \prod_{i \in p(\1)} p[i],
\end{equation}
so a map $\gamma\in\Gamma(p)$ can be thought of as a dependent function that sends each $p$-position $i$ to a direction $\gamma(i)$ of $p$ at $i$.
So we call an element of $\Gamma(p)$ a \emph{situation} for $p$.
The idea is that the situation you're in gives you a direction to go along at any position you may take.

Returning to the language of dynamical systems, a situation for $p$ corresponds to an enclosure for the interface $p$, in that it chooses a fixed input at every output of $p$.
An enclosure for your interface dictates what you'll see (the input you receive) given anything you might do (the output you provide); there is no need for any further outside interference.

\begin{remark}
Although they both refer to lenses into $\yon$, we will favor the term \emph{enclosure} in the context of dynamical systems and wrappers and the term \emph{situation} more generally.
\end{remark}

\begin{exercise} \label{exc.enclosures_as_functions}
Let $\varphi\colon S\yon^S\to B\yon^A$ be an $(A,B)$-Moore machine.
\begin{enumerate}
	\item Is it true that an enclosure $\gamma\colon B\yon^A\to\yon$ can be identified with a function $A\to B$?
	\item Describe how to interpret an enclosure $\gamma\colon B\yon^A\to\yon$ as a wrapper around an interface $B\yon^A$.
	\item Given an enclosure $\gamma$, describe the dynamics of the composite Moore machine $S\yon^S\To{\varphi}B\yon^A\To{\gamma}\yon$ obtained by wrapping $\yon$ around $\varphi$ using $\gamma$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
	\item No, it represents a function $B\to A$!
	An enclosure sends each output $b\in B$ to an input $a\in A$.
	\item As a wrapper around an interface $B\yon^A$, an enclosure $\gamma\colon B\yon^A\to\yon$ corresponds to a function $g\colon B\to A$ that feeds the input $g(b)\in A$ into the system whenever it returns the output $b\in B$.
	\item Composing our original Moore machine $S\yon^S\to B\yon^A$ with an enclosure $\gamma$ yields a Moore machine $S\yon^S\To{\varphi}B\yon^A\To{\gamma}\yon$ that returns unchanging output and receives unchanging input.
	If we identify the Moore machine with its return function $S\to B$ and its update function $S\times A\to S$, and if we identify the enclosure $\gamma$ with a function $g\colon B\to A$, then their composite Moore machine $S\yon^S\to\yon$ can be identified with a function $S\to S$, equal to the composite
	\[
	    S\To{\Delta}S\times S\To{\id_S\times\text{return}}S\times B\To{\id_S\times g}S\times A\To{\text{update}}S,
	\]
	where $\Delta$ is the diagonal map $s\mapsto(s,s)$.
	This composite map $S\to S$ sends every state to the next according to the output the original state returns, the input that the enclosure gives in response to that output, and the update function that sends the original state and the input to the new state.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{example}[The do-nothing enclosure] \label{ex.do_nothing}
There is something rather off-putting about the way we model dynamical systems as lenses $\varphi\colon S\yon^S\to p$.
We know that $\varphi$ tells us how state-positions return output-positions and, given a current state-position, how input-directions update state-directions.
But we rely only on the labels of elements in $S$ to tell us which positions and directions refer to the same states!

Nothing inherent in the language of $\poly$ makes these associations between state-positions and state-directions for us; we have to bank on the position-set and direction-sets of the state system being the same set for the machine to work properly.
Put another way, the monomials $\{4,6\}\yon^{\{4,6\}},\{4,6\}\yon^{\{4,8\}},$ and $\{3,5\}\yon^{\{6,7\}}$ are all isomorphic in $\poly$, but the first can be a state system while the other two cannot!

To address this issue, we need a way to connect the positions of a polynomial to its own directions in the language of $\poly$.
Here's where situations save the day: a lens $S\yon^S\to\yon$ is just a way of assigning to each position in $S$ a direction in $S$.
So we can define $\epsilon\colon S\yon^S\to\yon$ to be the situation that sends each position $s\in S$ to the direction $s$ at $s$ corresponding to the same state.
Note that $\epsilon$ can be identified with the identity function on $S$ (see \cref{exc.enclosures_as_functions}).
Now $\poly$ knows which direction is associated with the same state as the position it is at.

In this way, we can generalize our notion of state systems to monomials $S\yon^{S'}$ equipped with a bijection $S\to S'$, which we can then translate to a situation $\epsilon\colon S\yon^{S'}\to\yon$.
But for convenience of notation, we will continue to identify the position-set of a state system with each of its direction-sets.

More concretely, the enclosure $\epsilon\colon S\yon^S\to\yon$ acts as a very special (if rather unexciting) dynamical system: it is the \emph{do-nothing enclosure}, with only one possible output and one possible input that always keeps the current state the same.
While the system does, well, nothing, we do know one key fact about it: given any state system, regardless of the state set, we can always define a do-nothing enclosure on it.\footnote{It may have bothered you that we call $S\yon^S$, which is a single polynomial, a state \emph{system}, when we also use the word ``system'' to refer to dependent dynamical systems, which are lenses $S\yon^S\to p$.
The existence of the do-nothing enclosure explains why our terminology does not clash: every polynomial $S\yon^S$ comes equipped with a dependent dynamical system $\epsilon\colon S\yon^S\to\yon$, so it really is a state \emph{system}.
Later on we'll see other systems that come with $S\yon^S$ for free.}

Yet this isn't the whole story.
The do-nothing enclosure knows, at each position, the direction that keeps the system at the same state; but it doesn't know which of the other directions send the system to which of the other positions' states.
We're still relying on the labels of direction-sets being the same for that: for instance, the polynomials $\{1',2',3'\}\yon^{\{1,2,3\}}$ and $\{1'\}\yon^{\{0,1,4\}}+\{2'\}\yon^{\{2,5,6\}}+\{3'\}\yon^{\{-8,-1,3\}}$ are isomorphic, but even with a do-nothing enclosure matching $1'\mapsto1,2'\mapsto2,3'\mapsto3$ to make the first one into a state system, we don't have a way to tell $\poly$ how to make the second one a state system yet.

From another perspective, $\epsilon\colon S\yon^S\to\yon$ does nothing, while $\varphi\colon S\yon^S\to p$ does ``one thing'': it steps through the system once, generating the current state's output with the return function and taking in input with the update function.
It's all set to take another step, but how does $\poly$ know which state to visit next?
Is there a lens that does ``two things,'' ``$n$ things,'' or ``arbitrarily many things''?
Can we actually $\emph{run}$ a dynamical system in $\poly$?
We'll develop the machinery to answer these questions over the course of the three chapters in \cref{part.comon}, starting in \cref{subsec.comon.comp.def.dyn_sys}.
\end{example}

\begin{exercise}
The notation $\Gamma(p)$ for the set of situations for a polynomial $p$ comes from the common mathematical concept of a ``global section.''
Show that situations $\gamma\colon p\to\yon$ are precisely \emph{sections} (as defined in \cref{exc.product_as_sections}) of the canonical function $\pi_p\colon\dot{p}(\1)\to p(\1)$ from \cref{exc.deriv_directions}.
\begin{solution}
Given a polynomial $p$, we wish to show that situations for $p$ are precisely sections of the function $\pi_p\colon\dot{p}(\1)\to p(\1)$ from \cref{exc.deriv_directions}.
That function sends every direction of $p$ to the position at which it is located.
So a section $\gamma\colon p(\1)\to\dot{p}(\1)$ of $\pi_p$ would send each $p$-position to a direction of $p$ at that position, which is also exactly what a situation for $p$ does.
Alternatively, we can directly apply \cref{exc.product_as_sections} to deduce that the dependent product $\Gamma(p)\iso\prod_{i \in p(\1)}p[i]$ is isomorphic to the set of sections of the projection $\sum_{i\in p(\1)}p[i]\to p(\1)$ defined by $(i,x)\mapsto i$, which is exactly $\pi_p$.
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.gamma_pres_coproduct}
The situations functor $\Gamma\colon\poly\to\smset\op$ sends $(0,+)$ to $(1,\times)$:
\[
	\Gamma(\0)\iso\1
	\qqand
	\Gamma(p+q)\iso\Gamma(p)\times\Gamma(q).
\]
\end{proposition}
Technically, one could say that $\Gamma$ preserves coproducts, since coproducts in $\smset\op$ are products in $\smset$.

\begin{exercise}
Prove \cref{prop.gamma_pres_coproduct}.
\begin{solution}
\cref{prop.gamma_pres_coproduct} follows directly from \cref{prop.poly_coprods}: we have that $\Gamma(\0) = \poly(\0,\yon) \iso \1$ since $\0$ is initial in $\poly$, and $\Gamma(p + q) = \poly(p+q,\yon) \iso \poly(p+q,\yon) = \Gamma(p) \times \Gamma(q)$ since $+$ gives coproducts in $\poly$.
\end{solution}
\end{exercise}

\begin{remark}
The situations functor $\Gamma\colon\poly\to\smset\op$ is also normal lax monoidal in the sense that there are canonical functions
\[
	\1\cong\Gamma(\yon)
	\qqand
	\Gamma(p)\times\Gamma(q)\to\Gamma(p\otimes q)
\]
satisfying certain well-known laws. But we won't need this, so we omit its proof.
\end{remark}

% \subsubsection{The polynomial $S\yon^S$ as a comonad on $\smset$}\label{page.poly_comonad}

% A \emph{comonad} on $\smset$ is a functor $F\colon\smset\to\smset$, equipped with two natural transformations $\epsilon\colon F\to\id$ and $\delta\colon F\to F\circ F$, satisfying three equations. We don't need this now, so we won't get into it here. But we will note that every comonad comes from an adjunction, and the adjunction corresponding to $S\yon^S$ is
% \[
% \adj{\smset}{-\times S}{-^S}{\smset}
% \]
% the ``curry/uncurry'' adjunction. In functional programming, the comonad $S\yon^S$ is called the \emph{state comonad},%
% \footnote{The comonad $S\yon^S$ is sometimes called the \emph{store} comonad.}
% and the elements of $S$ are called states. \niu{Are they??} It is no coincidence that we also refer to elements of $S$ as states. \niu{How does the interpretation of the store comonad as a data structure indexed by $S$ along with a ``current location'' element in $S$ actually relate to this?}

% Again, we will be \emph{very} interested in polynomial comonads later---as mentioned in \cref{prop.ahman_uustalu1}, they are exactly categories!!---but for now we move on to things we can use right away in our story about dynamical systems.%
% \footnote{If you're curious what category the comonad $S\yon^S$ corresponds to, it's the one with object set $S$ and a unique morphism $s_1\to s_2$ for every pair of objects $s_1,s_2\in S$.}

%-------- Section --------%
\section{General interaction}\label{sec.poly.dyn_sys.interact}

We now have all the pieces we need to fulfill the promises of \cref{sec.poly.intro.dyn_sys} by modeling interactions between dependent dynamical systems, which can change their interfaces and interaction patterns, using $\poly$.

\subsection{Wrapping juxtaposed dynamical systems together}

When wrapper interaces are used in conjunction with parallel products, they may encode multiple interacting dynamical systems as a single system.
Explicitly, given $n\in\nn$ and $n$ dynamical systems, $\varphi_i\colon S_i\yon^{S_i}\to p_i$ for every $i\in\ord{n}$, we can first juxtapose them into a single dynamical system
\[\varphi\colon \left(\prod_{i\in\ord{n}}S_i\right)\yon^{\prod_{i\in\ord{n}}S_i}\to\bigotimes_{i\in\ord{n}}p_i\]
by taking their parallel product.
Then we can wrap an interface $q$ around $\varphi$ using a wrapper $f$, yielding a new dynamical system
\[\left(\prod_{i\in\ord{n}}S_i\right)\yon^{\prod_{i\in\ord{n}}S_i}\To{\varphi}\bigotimes_{i\in\ord{n}}p_i\To{f}q.\]
On positions, $f$ gives a way of combining all the outputs of the constituent interfaces into a single output of the wrapper interace.
On directions, $f$ takes into account the current outputs of each of the constituent interfaces, as well as any new output given to the wrapper interface, then uses all of this to give input to each of the constituent interfaces.
In particular, a judiciously chosen on-directions function could feed output from some interfaces as inputs to others.
When $f$ is a wrapper around a parallel product of interfaces, we call $f$ the \emph{interaction pattern} between those interfaces.

\begin{example}[Repeater]
Suppose we have a dependent dynamical system $\varphi\colon S\yon^S\to A\yon+\yon$, which takes unchanging input and sometimes returns elements of $A$ as output while other times returning only silence.
What if we wanted to construct a system $\psi$ that operates just like $\varphi$, but \emph{always} returns elements of $A$ as output?
Where $\varphi$ would have returned silence, we want $\psi$ to instead \emph{repeat} the last element of $A$ that it returned. (We allow $\psi$ to repeat an arbitrary element of $A$ if $\varphi$ returns silence before it has returned any elements of $A$ yet.)

Let's think like a programmer.
What we need is a way to store an element of $A$ and then retrieve it.
So whenever $\varphi$ returns an element of $A$ as output, we store it; then when $\varphi$ returns silence, we retrieve the last element of $A$ we stored and return that instead.

What should this storage-retrieval dynamical system look like?
It needs to take elements of $A$ as input, return elements of $A$ as output, and store elements of $A$ as states.
In fact, the identity lens $\iota\colon A\yon^A\to A\yon^A$ works perfectly: it returns the element of $A$ currently stored as output and updates its state to the input it receives.

Now we can juxtapose our original system $\varphi$ with the storage-retrieval system $\iota$ by taking their parallel product, yielding a dynamical system
\[
    \varphi\otimes\iota\colon SA\yon^{SA}\to (A\yon+\yon)\otimes A\yon^A
\]
that runs both systems simultaneously---and independently.
But what we want is for $\varphi$ and $\iota$ to interact with each other, and for the resulting system to only output elements of $A$.
To do so, we need to wrap an interface $A\yon$ around $\varphi\otimes\iota$ by composing it with some lens
\[
    f\colon(A\yon+\yon)\otimes A\yon^A\to A\yon,
\]
the interaction pattern between the interfaces $A\yon+\yon$ and $A\yon^A$, which we must define.

Since $\otimes$ distributes over $+$, it suffices to give lenses
\[
    g\colon A\yon\otimes A\yon^A\to A\yon \qqand h\colon\yon\otimes A\yon^A\to A\yon.
\]
The former corresponds to the case where $\varphi$ outputs an element of $A$, while the latter corresponds to the case where $\varphi$ is silent.

When $\varphi$ outputs an element of $A$, we want to return that output, but we also want to give that output as input to $\iota$ so that it can be stored.
We don't need to do anything with the output of $\iota$; we can simply discard it.
So $g$ should send $(a,a')\mapsto a$ on positions, returning the output of $\varphi$ and discarding the output of $\iota$; and the on-directions function $g^\sharp_{(a,a')}\colon\1\to A$ should specify the direction $a\in A$, feeding the output of $\varphi$ as input to $\iota$.

Meanwhile, when $\varphi$ outputs silence, we want to return the output of $\iota$ instead.
We also need to feed the output of $\iota$ back into $\iota$ as input so that it can continue to be stored.
So $h$ should be the identity on positions as well as the identity on directions.
\end{example}

\begin{example}[Paddling]\label{ex.paddler}
Say we wanted to build a Moore machine with interface $\nn\yon$; we may interpret its natural number output as the machine's current location.
What if we don't want this machine to jump around wildly?
Instead, suppose we want to be very strict about what how far the machine can move and what makes it move.

To accomplish this, we introduce two intermediary systems, which we call the \emph{paddler} and the \emph{tracker}:%
\footnote{Perhaps one could refer to the tracker as the \emph{demiurge}; it is responsible for maintaining the material universe.}
\[
  \text{paddler}\colon S\yon^S\to\2\yon
  \qqand
  \text{tracker}\colon T\yon^T\to\nn\yon^\2
\]
The paddler has interface $\2\yon$ because it is blind (i.e.\ takes no inputs) and can only move (i.e.\ output) its paddle to the left side or the right side: $\2\cong\{\text{left, right}\}$. The tracker has interface $\nn\yon^\2$ because it will announce the location of the machine (as an element $n\in\nn$) and watch what side the paddler is on (as an element of $\2$).
We can wrap an interface $\nn\yon$ around them both using an interaction pattern
\[
    \2\yon\otimes\nn\yon^\2\to\nn\yon
\]
whose on-positions function is the projection $\2\nn\to\nn$, returning the location returned by the tracker, and whose on-directions function is the projection $\2\nn\to\2$, passing the output of the paddler as input to the tracker.

Let's leave the paddler's dynamics alone---how you make that paddler behave is totally up to you---and instead focus on the dynamics of the tracker.
We want it to watch for when the paddle switches from left to right or from right to left; at that moment it should push the paddler forward one unit. Thus the states of the tracker are given by $T\coloneqq\2\nn$, storing what side the paddler is on and the current location.
The on-positions function of the tracker is the projection $\2\nn\to\nn$ that returns the current location; then at each $(d,i)\in\2\nn$, the on-directions function of the tracker $\2\to\2\nn$ sends
\[
  d'\mapsto
	\begin{cases}
		(d',i)&\tn{if }d=d'\\
		(d',i+1)&\tn{if }d\neq d',
	\end{cases}
\]
storing the new direction of the paddler as well as moving the machine forward one unit if the paddle switches while keeping the machine still if the paddle stays still.
\end{example}

\begin{exercise}
Change the dynamics and state system of the tracker in \cref{ex.paddler} so that it exhibits the following behavior.

When the paddle switches once and stops, the tracker increases its location by one unit and stops, as before in \cref{ex.paddler}. But when the paddle switches twice in a row, the tracker increases its location by two units on the second switch! So if it is quiet for a while and then switches three times in a row, the tracker will increase its location by one then two then two.
\begin{solution}
We define a new tracker $T'\yon^{T'}\to\nn\yon^\2$ based on the one from \cref{ex.paddler} to watch for when the paddle switches sides once, at which point the tracker should increase its location by one, and watch for when the paddle switches sides twice in a row, at which point the tracker should increase its location by two.
To do this, we need the tracker to remember not just the current side the paddle is on, but the previous side the paddle was on as well.
The tracker should still remember the current location.
Thus the states of the tracker are given by $T\coloneqq\2\times\2\nn$, storing the previous side the paddler was on, the current side the paddler is on, and the current location.
The on-positions function of the tracker is the projection $\2\times\2\nn\to\nn$ that returns the current location; then at each $(d,d',i)\in\2\nn$, the on-directions function of the tracker $\2\to\2\times\2\nn$ sends
\[
  d''\mapsto
	\begin{cases}
		(d',d'',i)&\tn{if }d'=d''\\
		(d',d'',i+1)&\tn{if }d'\neq d''\tn{ and }d=d'\\
		(d',d'',i+2)&\tn{if }d'\neq d''\tn{ and }d\neq d'
	\end{cases}
\]
storing both the last side the paddle was on and the new side the paddle is on as well as moving the machine forward one unit if the paddle switches after not switching and two units if the paddle switches after just switching.
\end{solution}
\end{exercise}

\begin{example}
Suppose you have two systems with the same interface $p\coloneqq q\coloneqq\rr^\2\yon^{\rr^\2-\{(0,0)\}}$.
\[
\begin{tikzpicture}
	\node (m1) {\faMotorcycle};
	\node[above=-.15 of m1] (e1) {\faEye};
	\node[draw, thick, blue!10, fit = (m1) (e1)] {};
	\node[below right=0 and 1 of m1] (m2) {\scalebox{-1}[1]{\faMotorcycle}};
	\node[above=-.15 of m2] (e2) {\faEye};
	\node[draw, thick, blue!10, fit = (m2) (e2)] {};
\end{tikzpicture}
\]
The output of each interface indicates the location of the system, while the range of possible inputs indicate the locations that the system could observe, relative to the location of the system itself.
Taking all pairs of reals except $(0,0)$ corresponds to the fact that the eye cannot see that which is at the same position as the eye.

Let's have the two systems constantly approaching each other with a force equal to the reciprocal of the squared distance between them.
If they finally collide, let's have the whole thing come to a halt.
To do this, we want the wrapper interface to be $\{\text{`go'}\}\yon+\{\text{`stop'}\}$, so that if the system returns `go' it can still advance to the next state, but if it returns `stop' it halts.
The wrapper $\rr^\2\yon^{\rr^\2-\{(0,0)\}}\otimes\rr^\2\yon^{\rr^\2-\{(0,0)\}}\to\{\text{`go'}\}\yon+\{\text{`stop'}\}$ is given on positions by
\[
  \big((x_\1,y_1),(x_2,y_2)\big)\mapsto
	\begin{cases}
		\text{`stop'}&\mbox{ if $x_1=x_2$ and $y_1=y_2$}\\
		\text{`go'}&\mbox{ otherwise}.
	\end{cases}
\]
On directions, we use the function
\[
  \big((x_1,y_1),(x_2,y_2)\big)\mapsto \big((x_2-x_1,y_2-y_1),(x_1-x_2,y_1-y_2)\big),
\]
so that each system is able to see the location of the other system relative to its own, i.e.\ the vector pointing from itself to the other system (unless that vector is zero, in which case the whole thing should have already halted).

We can use these vectors to define the internal dynamics of each system so that they move the way we want them to.
Each system will hold as its internal state its current location and velocity, i.e.\ $S=\rr^\2\times\rr^\2$.
To define a lens $S\yon^S\to\rr^\2\yon^{\rr^\2-\{(0,0)\}}$ we simply return the current location, update the current location by adding the current velocity, and update the current velocity by adding a vector with appropriate magnitude pointing to the other system:
\begin{align*}
	\rr^\2\times\rr^\2&\To{\text{return}}\rr^\2\\
	\big((x,y),(v_x, v_y)\big)&\Mapsto{\text{return}}(x,y)
\end{align*}
\begin{align*}
	\rr^\2\times\rr^\2\times(\rr^\2-\{(0,0)\})&\To{\text{update}}\rr^\2\times\rr^\2\\
	\big((x,y),(v_x,v_y),(a,b)\big)&\Mapsto{\text{update}}\left(x+v_x,y+v_y,v_x+\frac{a}{(a^2+b^2)^{3/2}},v_y+\frac{b}{(a^2+b^2)^{3/2}}\right)
\end{align*}
\end{example}

\begin{exercise}
Suppose $(X,d)$ is a metric space, i.e.\ $X$ is a set of points and $d\colon X\times X\to\rr_{\geq0}$ is a distance function satisfying the usual laws.
Let's have robots interact in this space.

Let $A,A'$ be sets, each thought of as a set of signals, and let $a_0\in A$ and $a_0'\in A'$ be elements, each thought of as a default value. Let $p\coloneqq AX\yon^{A'X}$ and $p'\coloneqq A'X\yon^{AX}$, and imagine there are two robots, one with interface $p$, returning a signal as an element of $A$ and its location as a point in $X$, and one with interface $p'$, returning a signal as an element of $A'$ and also its location as a point in $X$.
\begin{enumerate}
	\item Write down an interaction pattern $p\otimes p'\to\yon$ such that each robot receives the other's location, but that it only receives the other's signal when their locations $x,x'$ are sufficiently close, namely when $d(x,x')<1$.
	Otherwise, it receives the default signal.
	\item Write down an interaction pattern $p\otimes p'\to\yon^{[0,5]}$ where the value $s\in [0,5]$ is a scalar, allowing the signal to travel $s$ times further.
	\item Suppose that each robot has a set $S,S'$ of possible private states in addition to their locations.
	What functions are involved in providing a dynamical system $\varphi\colon SX\yon^{SX}\to AX\yon^{A'X}$, if the location state $x\in X$ is directly returned without modification?
	\item Change the setup in any way so that each robot only extends a port to hear the other's signal when the distance between them is less than $s$. Otherwise, they can only detect the position (element of $X$) that the other currently inhabits.
	(Don't worry too much about timing---one missed signal when the robots first get close or one extra signal when the robots first get far is okay.)
\qedhere
\end{enumerate}
\begin{solution}
Here $(X,d)$ is a metric space, $A,A'$ are sets of signals with default signals $a_0\in A$ and $a'_0\in A'$, and there are two robots, one with interface $p\coloneqq AX\yon^{A'X}$ returning a signal in $A$ and a location in $X$ and another with interface $p'\coloneqq A'X\yon^{AX}$ returning a signal in $A'$ and a location in $X$.
\begin{enumerate}
    \item An interaction pattern $p\otimes p'\to\yon$ consists of a trivial on-positions function $AX\times A'X\to\1$ (indicating that no outputs leave the system) and an on-directions function $AX\times A'X\to A'X\times AX$ indicating what inputs the robots should receive according to the outputs they return.
    To model the fact that the robots receive each others' locations, but only receive each others' signals rather than the default signals when the distance between their locations is less than $1$ according to the distance function $d$, this on-directions function should send
    \[
        ((a,x),(a',x'))\mapsto
          \begin{cases}
          	((a',x'),(a,x))&\tn{ if }d(x,x')<1\\
          	((a'_0,x'),(a_0,x))&\tn{ otherwise}.
          \end{cases}
    \]
    \item An interaction pattern $p\otimes p'\to\yon^{[0,5]}$ that allows the signal to travel $s\in[0,5]$ times further consists of a still trivial on-positions function and an on-directions function $AX\times A'X\times[0,5]\to A'X\times AX$ indicating what inputs the robots should receive according to the external input $s\in[0,5]$ as well as the outputs they return.
    To model the fact that the robots receive each others' locations, but only receive each others' signals rather than the default signals when the distance between their locations is less than $s$ according to the distance function $d$, this on-directions function should send
    \[
        ((a,x),(a',x'),s)\mapsto
          \begin{cases}
          	((a',x'),(a,x))&\tn{ if }d(x,x')<s\\
          	((a'_0,x'),(a_0,x))&\tn{ otherwise}.
          \end{cases}
    \]
    \item To provide a dynamical system $\varphi\colon SX\yon^{SX}\to AX\yon^{A'X}$ under the condition that the on-positions function preserves the second coordinate $x\in X$, we must provide the first projection $SX\to A$ of an on-positions function that turns the robot's private state and current location into the signal it returns, as well as an on-directions function $SX\times A'X\to SX$ that provides a new private state and location for the robot given its old private state, old location, and the signal and location it receives from the other robot.

    \item To have the robots listen for each others' signals only when they are sufficiently close, we must move away from monomial interfaces and Moore machines to leverage dependency.
    There are several ways of doing this; we give just one method below.
    With $D\coloneqq\{\text{`close'},\text{`far'}\}$, let the robots' new interfaces be
    \[
        p\coloneqq \{\text{`close'}\}AX\yon^{DA'X}+\{\text{`far'}\}AX\yon^{DX} \qqand p'\coloneqq \{\text{`close'}\}A'X\yon^{DAX}+\{\text{`far'}\}A'X\yon^{DX},
    \]
    so that they may receive input telling them whether they are close or far, but cannot receive signals in $A$ or $A'$ when they are `far.'

    Then by the distributivity of $\otimes$ over $+$, their new interaction pattern $p\otimes p'\to\yon^{[0,5]}$ can be specified by four lenses, all trivial on positions: the lens
    \[
        \{\text{`close'}\}AX\yon^{DA'X}\otimes\{\text{`close'}\}A'X\yon^{DAX}\to\yon^{[0,5]},
    \]
    given by the on-directions function
    \[
        ((\text{`close'},a,x),(\text{`close'},a',x'),s)\mapsto
          \begin{cases}
          	((\text{`close'},a',x'),(\text{`close'},a,x))&\tn{ if }d(x,x')<s\\
          	((\text{`far'},a'_0,x'),(\text{`far'},a_0,x))&\tn{ otherwise};
          \end{cases}
    \]
    the lens
    \[
        \{\text{`far'}\}AX\yon^X\otimes\{\text{`far'}\}A'X\yon^X\to\yon^{[0,5]},
    \]
    given by the on-directions function
    \[
        ((\text{`far'},a,x),(\text{`far'},a',x'),s)\mapsto
          \begin{cases}
          	((\text{`close'},x'),(\text{`close'},x))&\tn{ if }d(x,x')<s\\
          	((\text{`far'},x'),(\text{`far'},x))&\tn{ otherwise};
          \end{cases}
    \]
    and two other lenses that can be defined arbitrarily, as they should never come up in practice.

    Finally, in order for each robot to properly remember whether the other is close or far, we record an element of $D$ in its state that is returned and updated: one robot is a lens
    \[
        \varphi\colon DSX\yon^{DSX}\to \{\text{`close'}\}AX\yon^{DA'X}+\{\text{`far'}\}AX\yon^{DX}
    \]
    whose on-positions function preserves not just the third coordinate $x\in X$ but also the first coordinate $d\in D$, while the on-directions function also preserves the first coordinate $d\in D$; and the other robot is constructed similarly.
\end{enumerate}
\end{solution}
\end{exercise}

\subsection{Enclosing juxtaposed dynamical systems together}

We saw in \cref{subsec.poly.dyn_sys.new.sit_encl} that a situation (i.e.\ lens into $\yon$) for the interface of a dynamical system encloses that dynamical system in a closed system.
So it should not come as a surprise that a situation for a parallel product of interfaces yields an interaction pattern between the interfaces that only allows the interfaces to interact with each other, cutting off any other interaction with the outside world.

\begin{example}[Picking up the chalk]\label{ex.pickup_chalk}
Imagine that you see some chalk and you pinch it between your thumb and forefinger.
An amazing thing about reality is that you will then have the chalk, in the sense that you can move it around.
How might we model this in $\poly$?
We will construct a closed dynamical system---one with interface $\yon$---consisting of only you and the chalk.
To do so, we will provide an interface for you, and interface for the chalk, and an enclosure for your juxtaposition.

Let's say that your hand can be at one of two heights, down or up, and that you can either press (apply pressure between your thumb and forefinger) or not press. Let's also say that you take in information about the chalk's height. Here are the two sets we'll be using:
\[
	H\coloneqq\{\text{`down', `up'}\}
	\qqand
	P\coloneqq\{\text{`press', `no press'}\}.
\]
Your interface is $HP\yon^H$: returning your own height and pressure, and receiving the chalk's height.

As for the chalk, it is either `in' your possession or `out' of it.
Either way, it also returns its height, which is either `down' or `up' in the air.
The chalk always takes in information about whether pressure is being applied or not.
When it's `out' of your possession, that's the whole story, but when it is `in' your possession, it also receives your hand's height.
All together, here are the two interfaces:
\[
	\const{You}\coloneqq HP\yon^H
	\qqand
	\const{Chalk}\coloneqq \{\text{`out'}\}H\yon^P + \{\text{`in'}\}H\yon^{HP}.
\]

Now we want to give the interaction pattern between you and the chalk.
As we said before, you see the chalk's height.
If your hand is not at the height of the chalk, the chalk receives no pressure.
Otherwise, your hand is at the height of the chalk, so the chalk receives your pressure (or lack thereof).
Furthermore, if the chalk is in your possession, it also receives your hand's height.

To provide a lens $\gamma\colon\const{You}\otimes\const{Chalk}\to\yon$, we use the fact that $\const{Chalk}$ is a sum and that $\otimes$ distributes over $+$.
Thus we need to give two lenses
\[
	\alpha\colon HP\yon^H\otimes H\yon^P\to\yon
	\qqand
	\beta\colon HP\yon^H\otimes H\yon^{HP}\to\yon
\]
The lens $\beta$, corresponding to when the chalk is in your possession, is quite easy to describe; it can be unfolded to a function
$HPH\to HHP$, and we take it to be the obvious map sending your height and pressure to the chalk and the chalk's height to you; see \cref{exc.pickup_chalk}. But $\alpha$ is more semantically interesting: it is given by
\[
  (h_\const{You},p_\const{You},h_\const{Chalk})\mapsto
  \begin{cases}
  	(h_\const{Chalk},\text{`no press'}) & \tn{ if } h_\const{You} \neq h_\const{Chalk} \\
  	(h_\const{Chalk},p_\const{You}) & \tn{ if } h_\const{You} = h_\const{Chalk}.
  \end{cases}
\]

So now we've got you and the chalk in enclosed together by $\gamma$, so we are ready to add some dynamics.
Your dynamics can be whatever you want, so let's just add some dynamics to the chalk (you'll get to give yourself some dynamics in \cref{exc.pickup_chalk}).
The chalk has only four states $C\coloneqq \{\text{`out'}, \text{`in'}\} \times H \cong\4$: the $H$ coordinate is its current height, and the other coordinate is whether or not it is in your possession.
We will give a dynamical system $C\yon^C\to\const{Chalk}$ with states $C$ and interface $\const{Chalk}$, i.e.\ a lens
\begin{equation}\label{eqn.chalk_dynamics}
	\{\text{`out'}, \text{`in'}\} \times H\yon^{\{\text{`out'}, \text{`in'}\} \times H}\to \{\text{`out'}\}H\yon^P + \{\text{`in'}\}H\yon^{HP}.
\end{equation}
On positions, as you might guess, the chalk returns its height and whether it is in your possession directly.
On directions, if it's not in your possession, it falls down unless you catch it (i.e.\ apply pressure to it so that it enters your possession); if it is in your possession, it takes whatever height you give it.
So we can express the on-directions function of \eqref{eqn.chalk_dynamics} at $(\text{`out'}, h_\const{Chalk})$ as
\begin{align*}
	\text{`no press'} &\mapsto (\text{`out', `down'}) \\
	\text{`press'} &\mapsto (\text{`in'}, h_\const{Chalk})
\end{align*}
and the on-directions function of \eqref{eqn.chalk_dynamics} at $(\text{`in'}, h_\const{Chalk})$ as
\begin{align*}
	(h_\const{You}, \text{`no press'}) &\mapsto (\text{`out'}, h_\const{You}) \\
	(h_\const{You}, \text{`press'}) &\mapsto (\text{`in'}, h_\const{You}).
\end{align*}
Obviously, this is all quite complicated, intricate, and contrived.
Our goal here is to show that you can define interactions in which one system can engage with or disengage from another, where one system controls the behavior of the other when the two are engaged.
\end{example}

\begin{exercise}\label{exc.pickup_chalk}
\begin{enumerate}
	\item In \cref{ex.pickup_chalk}, we said that $\beta\colon HP\yon^H\otimes H\yon^{HP}\to\yon$ was easy to describe and given by a function $HPH\to HHP$. Explain what's being said, and provide the function.
	\item Provide dynamics to the $\const{You}$ interface (i.e.\ specify a dynamical system with interface $\const{You}$) so that you repeatedly reach down and grab the chalk, lift it with your hand, and drop it.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item A lens $HP\yon^H \otimes H\yon^{HP} \to \yon$ consists of an on-positions function $HPH \to \1$ and an on-directions function $HPH \times \1 \to HHP$.
    This amounts to a function $HPH \to HHP$.
    We can easily define this function to be the isomorphism that sends $(h,p,h') \in HPH$ to $(h,h',p)$.
    \item To model the way in which you cycle through three possible actions---reaching down and grabbing the chalk, lifting it with your hand, and dropping it---it is simplest to work with a set of $\3$ possible states.
    So we will give your dynamics as a lens $\3\yon^\3 \to HP\yon^H$, where the return function $\3 \to HP$ indicates what happens at each state, sending $1 \mapsto (\text{down, press}), 2 \mapsto (\text{up, press})$, and $3 \mapsto (\text{up, no press})$.
    Then the update function $\3H \to \3$ always goes to the next state, regardless of input: it ignores the $H$ coordinate and sends $1$ to $2$, $2$ to $3$, and $3$ to $1$.
\end{enumerate}
\end{solution}
\end{exercise}

Given $n\in\nn$ and polynomials $p_1,\ldots,p_n$ as interfaces, a situation $p_1\otimes\cdots\otimes p_n\to\yon$ puts these $n$ interfaces in an enclosure together.
The following proposition provides an alternative perspective on such situations.

\begin{proposition}\label{prop.situations2}
Given polynomials $p,q\in\poly$, there is a bijection
\begin{equation} \label{eqn.situations2}
\Gamma(p\otimes q)\cong\smset\big(q(\1),\Gamma(p)\big)\;\times\;\smset\big(p(\1),\Gamma(q)\big).
\end{equation}
\end{proposition}
The idea is that specifying an enclosure for interfaces $p$ and $q$ together is equivalent to specifying an enclosure for $p$ for every output $q$ might return and specifying an enclosure for $q$ for every output $p$ might return.
\begin{proof}[Proof of \cref{prop.situations2}]
This is a direct calculation:
\begin{align*}
	\Gamma(p\otimes q) &\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}(p[i]\times q[j]) \\
	&\iso
	\left(\prod_{j\in q(\1)}\prod_{i\in p(\1)}p[i]\right)\times
		 \left(\prod_{i\in p(\1)}\prod_{j\in q(\1)}q[j]\right) \\
	&\iso
	\smset(q(\1),\Gamma(p))\times\smset(p(\1),\Gamma(q)).
\end{align*}
\end{proof}

\begin{example}
An enclosure $f\colon B\yon^A\otimes B'\yon^{A'}\to \yon$, corresponds to a map $BB'\to AA'$. In other words, for every pair of outputs $(b,b')\in BB'$, the enclosure $f$ specifies a pair of inputs $(a,a')\in AA'$.

Let's think of elements of $B$ and $B'$ not as outputs, but as locations that two machines may occupy.
\[
\begin{tikzpicture}[oriented WD, bb port length=0]
	\node[bb={1}{0}, fill=blue!10, dotted] (p) {$b$};
	\node[bb={1}{0}, fill=blue!10, dotted, below right=-0.5 and 0.5 of p] (q) {$b'$};
	\node[bb={0}{0}, inner sep=10pt, fit=(p) (q)] {};
	\node at (p_in1) {\faEye};
	\node at (q_in1) {\faEye};
\end{tikzpicture}
\hspace{.5in}
\begin{tikzpicture}[oriented WD, bb port length=0]
	\node[bb={1}{0}, fill=blue!10, dotted] (p) {$b$};
	\node[bb={1}{0}, fill=blue!10, dotted, below left=-0.5 and 0.5 of p] (q) {$b'$};
	\node[bb={0}{0}, inner sep=10pt, fit=(p) (q)] {};
	\node at (p_in1) {\faEye};
	\node at (q_in1) {\faEye};
\end{tikzpicture}
\]
Then given a pair of locations $(b,b')$, the interaction pattern $f$ tells us what the two eyes see, i.e.\ what values of $(a,a')$ they get.
Equivalently, \eqref{eqn.situations2} says that the interaction pattern tells us what values the first eye sees at any location when the second eye's location is fixed at $b'$, as well as what values the second eye sees at any location when the first eye's location is fixed at $b$.

Here we see that \eqref{eqn.situations2} provides two ways to interpret the interaction pattern between two interfaces in a closed system: either as an enclosure around each interface that the other is part of, or as a single enclosure around them both.
\end{example}

\begin{exercise}
Let $p\coloneqq q\coloneqq\nn\yon^\nn$.
We wish to specify an enclosure around their juxtaposition.
\begin{enumerate}
    \item Say we wanted to feed the output of $q$ as input to $p$.
    What function $f\colon q(\1)\to\Gamma(p)$ captures this behavior?
    \item Say we wanted to feed the sum of the outputs of $p$ and $q$ as input to $q$.
    What function $g\colon p(\1)\to\Gamma(q)$ captures this behavior?
    \item What enclosure $\gamma\colon p\otimes q\to\yon$ does the pair of functions $(f,g)$ correspond to via \eqref{eqn.situations2}?
	\item Let dynamical systems $\varphi\colon\nn\yon^\nn\to p$ and $\psi\colon\nn\yon^\nn\to q$ both be the identity on $\nn\yon^\nn$.
	Suppose $\varphi$ starts in the state $0\in\nn$ and $\psi$ starts in the state $1\in\nn$.
	Describe the behavior of the system obtained by enclosing $\varphi$ and $\psi$ together with $\gamma$, i.e.\ the system $(\varphi\otimes\psi)\then\gamma$.
\qedhere
\end{enumerate}
\begin{solution}
We have $p\coloneqq q\coloneqq\nn\yon^\nn$.
\begin{enumerate}
    \item If we want to feed the output of $q$ as input to $p$, the corresponding function $f\colon q(\1)\to\Gamma(p)$ should send any output $b\in q(\1)$ to the enclosure $\nn\to\nn$ of $p$ that sends any natural number output of $p$ to $b$ itself.
    That is, $f$ is the function $b\mapsto(\_\mapsto b)$.
    \item If we want to feed the sum of the outputs of $p$ and $q$ as input to $q$, the corresponding function $g\colon p(\1)\to\Gamma(q)$ should send any output $a\in q(\1)$ to the enclosure $\nn\to\nn$ of $q$ that sends every natural number output $b$ of $q$ to the sum $a+b$.
    That is, $g$ is the function $a\mapsto(b\mapsto a+b)$.
    \item Together, $f$ and $g$ form a function $\nn\times\nn\to\nn\times\nn$ mapping $(a,b)\mapsto((f(b))(a),(g(a))(b))=(b,a+b)$, which is the enclosure $\gamma\colon p\otimes q\to\yon$ that $(f,g)$ corresponds to via \eqref{eqn.situations2}.
    \item As $\varphi$ and $\psi$ are both the identity, the system $(\varphi\otimes\psi)\then\gamma$ is really just the system $\gamma\colon p\otimes q\to\yon$.
    When it is at state $(a,b)$, its next state will be $(b,a+b)$.
    So if its initial state is $(0,1)$, its following states will be $(1,1),(1,2),(2,3),(3,5),(5,8),(8,13),\ldots$, forming the familiar Fibonacci sequence.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
We will use \eqref{eqn.situations2} to consider the interaction pattern $\gamma$ between \const{You} and \const{Chalk} from \cref{ex.pickup_chalk} as a pair of functions $\const{You}(\1)\to\Gamma(\const{Chalk})$ and $\const{Chalk}(\1)\to\Gamma(\const{You})$.
\begin{enumerate}
	\item How does the chalk's output specify an enclosure for you? That is, write the map $\const{Chalk}(\1)\to\Gamma(\const{You})$.
	\item How does your output specify an enclosure for the chalk? That is, write the map $\const{You}(\1)\to\Gamma(\const{Chalk})$.
\qedhere
\end{enumerate}
\begin{solution}
We wish to write the enclosure $\gamma\colon\const{You}\otimes\const{Chalk}\to\yon$ from \cref{ex.pickup_chalk} as a pair of functions $\const{You}(\1)\to\Gamma(\const{Chalk})$ and $\const{Chalk}(\1)\to\Gamma(\const{You})$ via \eqref{eqn.situations2}.
\begin{enumerate}
    \item Fix an output $(s_\const{Chalk}, h_\text{chalk})\in\const{Chalk}(\1)=\{\text{`out'},\text{`in'}\}H$ of the chalk.
    If $s_\const{Chalk}=\text{`out'}$, then the corresponding enclosure $\const{You}\iso HP\yon^H\to\yon$ given by $f$ via \eqref{eqn.situations2} can be thought of as the function $HP\to H$ sending
    \[
        (h_\const{You},p_\const{You})\mapsto h_\const{Chalk},
    \]
    according to the behavior of $\alpha\colon HP\yon^H\otimes H\yon^P\to\yon$ when we fix $h_\text{chalk}$ to be position in the rightmost $H$ and focus on the result in the exponent $H$ on the left.
    Meanwhile, if $s_\const{Chalk}=\text{`in'}$, then the corresponding enclosure $\const{You}\iso HP\yon^H\to\yon$ can also be thought of as the function $HP\to H$ sending
    \[
        (h_\const{You},p_\const{You})\mapsto h_\const{Chalk},
    \]
    according to the behavior of $\beta\colon HP\yon^H\otimes H\yon^{HP}\to\yon$ when we fix $h_\text{chalk}$ to be position in the rightmost $H$ and focus on the result in the exponent $H$ on the left.
    So overall, the map $\const{Chalk}(\1)\to\Gamma(\const{You})$ sends
    \[
        (\_,h_\text{chalk})\mapsto((\_,\_)\mapsto h_\const{Chalk}).
    \]

    \item Fix an output $(h_\const{You},p_\const{You})\in\const{You}(\1)=HP$ of the chalk.
    Then the corresponding enclosure $\const{Chalk}\iso\{\text{`out'}\}H\yon^P + \{\text{`in'}\}H\yon^{HP}\to\yon$ can be thought of as a pair of functions: one $\{\text{`out'}\}H\to P$ sending
    \[
        (\text{`out'},h_\const{Chalk})\mapsto
            \begin{cases}
  	            \text{`no press'} & \tn{ if } h_\const{You} \neq h_\const{Chalk} \\
  	            p_\const{You} & \tn{ if } h_\const{You} = h_\const{Chalk}.
            \end{cases}
    \]
    according to the behavior of $\alpha\colon HP\yon^H\otimes H\yon^P\to\yon$ when we fix $(h_\const{You},p_\const{You})$ to be position in $HP$ on the left and focus on the result in the exponent $P$ on the right; and another $\{\text{`in'}\}H\to HP$ sending
    \[
        (\text{`in'},h_\const{Chalk})\mapsto(h_\const{You},p_\const{You})
    \]
    according to the behavior of $\beta\colon HP\yon^H\otimes H\yon^{HP}\to\yon$ when we fix $(h_\const{You},p_\const{You})$ to be position in $HP$ on the left and focus on the result in the exponent $HP$ on the right.
    So overall, the map $\const{You}(\1)\to\Gamma(\const{Chalk})$ sends
    \[
        (h_\const{You},p_\const{You})\mapsto\left(
            \begin{aligned}
                (\text{`out'},h_\const{Chalk})&\mapsto
                    \begin{cases}
          	            \text{`no press'} & \tn{ if } h_\const{You} \neq h_\const{Chalk} \\
          	            p_\const{You} & \tn{ if } h_\const{You} = h_\const{Chalk}
                    \end{cases} \\
                (\text{`in'},h_\const{Chalk})&\mapsto(h_\const{You},p_\const{You})
            \end{aligned}
        \right).
    \]
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
\begin{enumerate}
	\item State and prove a generalization of \eqref{eqn.situations2} from \cref{prop.situations2} for $n$-many polynomials $p_1,\ldots,p_n\in\poly$.
	\item Generalize the ``idea'' statement between \cref{prop.situations2} and its proof.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We generalize \eqref{eqn.situations2} for $n$ polynomials as follows.
    Given polynomials $p_1,\ldots,p_n\in\poly$, we claim there is a bijection
    \[
        \Gamma\left(\bigotimes_{i=1}^n p_i \right) \iso \prod_{i=1}^n \smset\left(\prod_{\substack{1 \leq j \leq n, \\ j \neq i}} p_j(\1), \Gamma(p_i)\right).
    \]
    The $n=1$ case is clear, and the $n=2$ case is given by \eqref{eqn.situations2}.
    Then by induction on $n$, we have
    \begin{align*}
        \Gamma\left(\bigotimes_{i=1}^n p_i \right) &\iso \smset\left(p_n(\1), \Gamma\left(\bigotimes_{i=1}^{n-1} p_i \right)\right) \times \smset\left(\prod_{i=1}^{n-1} p_i(\1), \Gamma(p_n)\right) \tag*{\eqref{eqn.situations2}} \\
        &\iso \smset\left(p_n(\1), \prod_{i=1}^{n-1} \smset\left(\prod_{\substack{1 \leq j \leq n-1, \\ j \neq i}} p_j(\1), \Gamma(p_i)\right)\right) \times \smset\left(\prod_{i=1}^{n-1} p_i(\1), \Gamma(p_n)\right) \tag{Inductive hypothesis} \\
        &\iso \prod_{i=1}^{n-1} \smset\left(\prod_{\substack{1 \leq j \leq n, \\ j \neq i}} p_j(\1), \Gamma(p_i)\right) \times \smset\left(\prod_{i=1}^{n-1} p_i(\1), \Gamma(p_n)\right) \tag{Universal properties of products and internal homs},
    \end{align*}
    and the result follows.
    \item The general idea is that specifying an enclosure for interfaces $p_1,\ldots,p_n$ together is equivalent to specifying an enclosure for $p_i$ for every output all the other interfaces might return, for each $i\in\ord{n}$.
\end{enumerate}
\end{solution}
\end{exercise}

\subsection{Wiring diagrams as interaction patterns}

We first saw interactions between systems drawn as wiring diagrams in \cref{sec.poly.intro.dyn_sys}.
They depict systems as boxes, showing how they can send inputs and outputs to each other through the wires between them, as well as how multiple systems can combine to form a larger system whenever smaller boxes are nested within a larger box.

Formally, and more precisely, we can think of each box in a wiring diagram as an interface given by some monomial.
The box itself is not, per se, a dynamical system as we have defined one; but it becomes a dynamical system once we equip it with a lens from a state system.
Then the entire wiring diagram---specifying how these boxes nest within a larger box---is just an interaction pattern between the interfaces, with the larger box playing the role of the wrapper interface.
Once every nested box is equipped with a lens from a state system, we obtain a dynamical system whose interface is the larger box.

\begin{example}
Here is a simple wiring diagram.
\begin{equation}\label{eqn.control_diag}
\begin{tikzpicture}[oriented WD, baseline=(B)]
	\node[bb={2}{1}, fill=blue!10] (plant) {\texttt{Plant}};
	\node[bb={1}{1}, below left=-1 and 1 of plant, fill=blue!10]  (cont) {\texttt{Controller}};
	\node[circle, inner sep=1.5pt, fill=black, right=.1] at (plant_out1) (pdot) {};
	\node[bb={0}{0}, inner ysep=25pt, inner xsep=1cm, fit=(plant) (pdot) (cont)] (outer) {};
	\coordinate (outer_out1) at (outer.east|-plant_out1);
	\coordinate (outer_in1) at (outer.west|-plant_in1);
	\begin{scope}[above, font=\footnotesize]
  	\draw (outer_in1) -- node {$A$} (plant_in1);
  	\draw (cont_out1) to node (B) {$B$} (plant_in2);
  	\draw (plant_out1) to node {$C$} (outer_out1);
  	\draw
  		let
  			\p1 = (cont.south west-| pdot),
  			\p2 = (cont.south west),
  			\n1 = \bby,
  			\n2 = \bbportlen
  		in
  			(pdot) to[out=0, in=0]
  			(\x1+\n2, \y1-\n1) --
  			(\x2-\n2, \y2-\n1) to[out=180, in=180]
  			(cont_in1);
		\end{scope}
	\node[below=0of outer.north] {\texttt{System}};
\end{tikzpicture}
\end{equation}
The plant is receiving information from the world outside the system, as well as from the controller. It's also producing information for the outside world which is being monitored by the controller.

There are three boxes shown in \eqref{eqn.control_diag}: the controller, the plant, and the system. Each has a fixed set of inputs and outputs, and so we can consider the box as a monomial interface.
\begin{equation}\label{eqn.basic_diagram}
	\const{Plant}\coloneqq C\yon^{AB}
	\qquad\quad
	\const{Controller}\coloneqq B\yon^C
	\qquad\quad
	\const{System}\coloneqq C\yon^A.
\end{equation}
The wiring diagram itself is a wrapper
\[
	w\colon\const{Plant}\otimes\const{Controller}\to\const{System},
\]
specifying an interaction pattern between $\const{Plant}$ and $\const{Controller}$ with wrapper interface $\const{System}$.
Concretely, $w$ is a lens $CB\yon^{ABC}\to C\yon^A$ that dictates how wires are feeding from outputs to inputs.
Like all lenses between monomials, $w$ consists of an on-positions function $CB\to C$ and an on-directions function $CBA\to ABC$.

The wiring diagram is a picture that tells us which maps to use.
The on-positions function says ``inside the system you have boxes outputting values of type $C$ and $B$.
The system needs to produce an output of type $C$; how shall I obtain it?''
The answer, according to the wiring diagram, is to send $(c,b)\mapsto c$.

Meanwhile, the on-directions function says ``inside the system you have boxes outputting values of type $C$ and $B$, and the system itself is receiving an input value of type $A$.
The boxes inside need input values of type $A$, $B$, and $C$; how shall I obtain them?''
Again, we can read the answer off the wiring diagram: send $(c,b,a)\mapsto (a,b,c)$.

Note that neither the wiring diagram nor any of the boxes within it are dynamical systems on their own.
Rather, each box is a monomial that could be the interface of a dynamical system.
When we assign to a box a dynamical system having that box as its interface, we say that \emph{give dynamics} to the box.
So the entire wiring diagram is a wrapper that tells us how, given the dynamics for each inner box,
\[
\varphi\colon S\yon^S\to\const{Plant}
\qqand
\psi\colon T\yon^T\to\const{Controller},
\]
we can obtain the dynamics for the outer box:
\[
ST\yon^{ST}\To{\varphi\otimes\psi}\const{Plant}\otimes\const{Controller}\To{w}\const{System}.
\]
\end{example}

\begin{exercise}
\begin{enumerate}
	\item Make a new wiring diagram like \eqref{eqn.control_diag} except where the controller also receives information of type $A'$ from the outside world.
	\item What are the monomials represented by the boxes in your diagram (replacing \eqref{eqn.basic_diagram})?
	\item What is the interaction pattern represented by this wiring diagram?
	Give the corresponding lens, including its on-positions and on-directions functions.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item Here is the wiring diagram \eqref{eqn.control_diag} modified so that the controller also receives information of type $A'$ from the outside world.
\[
\begin{tikzpicture}[oriented WD, baseline=(B)]
	\node[bb={2}{1}, fill=blue!10] (plant) {\texttt{Plant}};
	\node[bb={2}{1}, below left=-1 and 1 of plant, fill=blue!10]  (cont) {\texttt{Controller}};
	\node[circle, inner sep=1.5pt, fill=black, right=.1] at (plant_out1) (pdot) {};
	\node[bb={0}{0}, inner ysep=25pt, inner xsep=1cm, fit=(plant) (pdot) (cont)] (outer) {};
	\coordinate (outer_out1) at (outer.east|-plant_out1);
	\coordinate (outer_in1) at (outer.west|-plant_in1);
	\coordinate (outer_in2) at (outer.west|-cont_in1);
	\begin{scope}[above, font=\footnotesize]
  	\draw (outer_in1) -- node {$A$} (plant_in1);
  	\draw (outer_in2) -- node {$A'$} (cont_in1);
  	\draw (cont_out1) to node (B) {$B$} (plant_in2);
  	\draw (plant_out1) to node {$C$} (outer_out1);
  	\draw
  		let
  			\p1 = (cont.south west-| pdot),
  			\p2 = (cont.south west),
  			\n1 = \bby,
  			\n2 = \bbportlen
  		in
  			(pdot) to[out=0, in=0]
  			(\x1+\n2, \y1-\n1) --
  			(\x2-\n2, \y2-\n1) to[out=180, in=180]
  			(cont_in2);
		\end{scope}
	\node[below=0of outer.north] {\texttt{System}};
\end{tikzpicture}
\]
    \item The monomials represented by the boxes in this diagram are the same, except that $\const{Controller}$ and $\const{System}$ each have an extra $A'$ exponent:
    \[
	\const{Plant}\coloneqq C\yon^{AB}
	\qquad\quad
	\const{Controller}\coloneqq B\yon^{A'C}
	\qquad\quad
	\const{System}\coloneqq C\yon^{AA'}.
    \]

    \item The interaction pattern represented by this wiring diagram is the lens
    \[
    	w'\colon \const{Plant}\otimes\const{Controller}\to\const{System}
    \]
    consisting of an on-positions function $CB\to C$ given by $(c,b)\mapsto c$ and an on-directions function $CBAA'\to ABA'C$ given by $(c,b,a,a')\mapsto(a,b,a',c)$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Consider the following wiring diagram.
\[
\begin{tikzpicture}[oriented WD, font=\footnotesize, bb port sep=1, bb port length=2.5pt, bb min width=.4cm, bby=.2cm, inner xsep=.2cm, x=.5cm, y=.3cm, text height=1.5ex, text depth=.5ex]
  	\node[bb={2}{1}, fill=blue!10] (Trf) {$\const{Alice}$};
  	\node[bb={1}{2}, fill=blue!10, below=1 of Trf] (Trg) {$\const{Bob}$};
		\node[bb={2}{2}, fill=blue!10] at ($(Trf)!.5!(Trg)+(1.5,0)$) (Trh) {$\const{Carl}$};
  	\node[bb={0}{0}, fit={($(Trf.north west)+(-.25,4)$) (Trg) ($(Trh.north east)+(.25,0)$)}] (Tr) {};
		\node[below] at (Tr.north) {$\const{Team}$};
  	\node[coordinate] at (Tr.west|-Trf_in2) (Tr_in1) {};
  	\node[coordinate] at (Tr.west|-Trg_in1) (Tr_in2) {};
  	\node[coordinate] at (Tr.east|-Trh_out2) (Tr_out1) {};
  	\node at ($(Trg_out2)+(5pt,0)$) (dot) {$\bullet$};
\begin{scope}[font=\tiny]
  	\draw[shorten <=-2pt] (Tr_in1) -- node[below=-3pt] {$A$} (Trf_in2);
  	\draw[shorten <=-2pt] (Tr_in2) -- node[below=-3pt] {$B$} (Trg_in1);
		\draw (Trf_out1) to node[above=-3pt] {$D$} (Trh_in1);
		\draw (Trg_out1) to node[above=-3pt] {$E$} (Trh_in2);
  	\draw (Trg_out2) -- node[below=-3pt] {$F$} (dot.center);
  	\draw[shorten >=-2pt] (Trh_out2) -- node[below=-3pt] {$G$} (Tr_out1);
  	\draw let \p1=(Trh.east), \p2=(Trf.north west), \n1=\bbportlen, \n2=\bby in
  		(Trh_out1) to[in=0] (\x1+\n1,\y2+\n2) -- node[pos=.3, below=-3pt] {$H$} (\x2-\n1,\y2+\n2) to[out=180] (Trf_in1);
	\end{scope}
\end{tikzpicture}
\]
\begin{enumerate}
	\item Write out the polynomials for each of $\const{Alice}$, $\const{Bob}$, and $\const{Carl}$.
	\item Write out the polynomial for the outer box, $\const{Team}$.
	\item The wiring diagram constitutes a lens $f$ in $\poly$; what is its domain and codomain?
	\item What lens is it?
	\item Suppose we have dynamical systems $\alpha\colon A\yon^A\to\const{Alice}$, $\beta\colon B\yon^B\to\const{Bob}$, and $\gamma\colon C\yon^C\to\const{Carl}$. What is the induced dynamical system with interface $\const{Team}$?
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item According to the wiring diagram, we have that $\const{Alice}\coloneqq D\yon^{HA},$ that $\const{Bob}\coloneqq EF\yon^B,$ and that $\const{Carl}\coloneqq HG\yon^{DE}.$
    \item According to the wiring diagram, we have that $\const{Team}\coloneqq G\yon^{AB}.$
    \item The wiring diagram constitutes a wrapper
    \[
        f\colon\const{Alice}\otimes\const{Bob}\otimes\const{Carl}\to\const{Team}.
    \]
    Its domain is $\const{Alice}\otimes\const{Bob}\otimes\const{Carl}\iso DEFHG\yon^{HABDE}$, while its codomain is $\const{Team}=G\yon^{AB}$.
    \item On positions, the lens $f$ is a function $DEFHG\to G$ that sends $(d,e,f,h,g)\mapsto g$.
    On directions, $f$ is a function $DEFHGAB\to HABDE$ that sends $(d,e,f,h,g,a,b)\mapsto(h,a,b,d,e)$.
    \item Given dynamical systems $\alpha\colon A\yon^A\to\const{Alice}$, $\beta\colon B\yon^B\to\const{Bob}$, and $\gamma\colon C\yon^C\to\const{Carl}$, the dynamical system induced by the wiring diagram is given by the composite lens
    \[
        ABC\yon^{ABC}\To{\alpha\otimes\beta\otimes\gamma}\const{Alice}\otimes\const{Bob}\otimes\const{Carl}\To{f}\const{Team}.
    \]
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}[Long division] \label{exc.long_div}
\begin{enumerate}
	\item Let $\fun{divmod}\colon\nn\times\nn_{\geq1}\to\nn\times\nn$ send $(a,b)\mapsto(a\bdiv b, a\bmod b)$; for example, it sends $(10,7)\mapsto(1,3)$ and $(30,7)\mapsto(4,2)$.
	Use \cref{exc.funs_to_moore} to turn it into a dynamical system.
	\item Interpret the following wiring diagram, where we have already given dynamics to each box as indicated by their labels:
\[
\begin{tikzpicture}[oriented WD, bb small]
	\node[bb port sep=3, fill=blue!10, bb={2}{2}] (divmod) {divmod};
	\node[bb={0}{1}, fill=blue!10, left=of divmod_in2] (7) {$7$};
	\node[bb port sep=2, bb={2}{1}, fill=blue!10, below right=-1 and 3 of divmod_out2] (times) {$*$};
	\node[bb={0}{1}, fill=blue!10, below left=-1 and 1 of times_in2] (10) {$10$};
	\node[bb={0}{0}, inner xsep=\bbx, fit=(divmod) (times)(7) (10)] (outer) {};
	\coordinate (outer_in1) at (outer.west|-divmod_in1);
	\coordinate (outer_out1) at (outer.east|-divmod_out1);
	\coordinate (outer_out2) at (outer.east|-times_out1);
	\draw (outer_in1) -- (divmod_in1);
	\draw (7_out1) -- (divmod_in2);
	\draw (10_out1) -- (times_in2);
	\draw (divmod_out1) -- (outer_out1);
	\draw (divmod_out2) to (times_in1);
	\draw (times_out1) -- (outer_out2);
\end{tikzpicture}
\]
	\item Use the above and a diagram of the following form to create a dynamical system that alternates between spitting out $0$'s and the base-$10$ digits of $1/7$ after the decimal point, like so:
\[
\begin{tikzpicture}[oriented WD]
	\node[bb={1}{2}, fill=blue!10] (inner) {};
	\node[bb={0}{0}, inner xsep=1cm, inner ysep=1cm] (outer) {};
	\coordinate (outer_out1) at (outer.east|-inner_out1);
	\draw[shorten >=-3pt] (inner_out1) -- (outer_out1);
	\draw
		let \p1=(inner.south east), \p2=(inner.south west), \n1=\bbportlen, \n2=\bby in
		(inner_out2) to[in=0] (\x1+\n1,\y1-\n2) -- (\x2-\n1,\y1-\n2) to[out=180] (inner_in1);
		\node[right, font=\footnotesize] at (outer_out1) {$0,1,0,4,0,2,0,8,0,5,0,7,0,1,0,4,0,2,0,8,0,5,0,7,0,1,0,4,0,2,0,8,0,5,0,7,\ldots$};
\end{tikzpicture}
\]
We will see in \cref{subsec.comon.sharp.state.run} how to make a dynamical system run twice as fast, then apply this to the above system in \cref{ex.long_div_skip} so that it skips the $0$'s.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item Using \cref{exc.funs_to_moore}, we can turn $\fun{divmod}$ into the dynamical system $\fun{divmod}\colon\nn\times\nn\yon^{\nn\times\nn}\to\nn\times\nn\yon^{\nn\times\nn_{\geq1}}$ whose return function is the identity on $\nn\times\nn$ and whose update function $\nn\times\nn\times\nn\times\nn_{\geq1}\to\nn\times\nn$ sends $(\_,\_,a,b)\mapsto(a\bdiv b,a\bmod b)$.
    \item From left to right, the inner boxes represent monomial interfaces $\nn_{\geq1}\yon, \nn\times\nn\yon^{\nn\times\nn_{\geq1}}, \nn\yon,$ and $\nn\yon^{\nn\times\nn}$.
    The box labeled $7$ is given dynamics $7\colon\yon\to\nn_{\geq1}\yon$ so that it always returns the output $7$; similarly, the box labeled $10$ is given dynamics $10\colon\yon\to\nn\yon$ so that it always returns the output $10$.
    Meanwhile, the box labeled is given dynamics $\fun{divmod}\colon\nn\times\nn\yon^{\nn\times\nn}\to\nn\times\nn\yon^{\nn\times\nn_{\geq1}}$ from the previous part; while we can again apply \cref{exc.funs_to_moore} to the standard multiplication function $*\colon\nn\times\nn\to\nn$ to give the box labeled $*$ dynamics as well, yielding a dynamical system $*\colon\nn\yon^\nn\to\nn\yon^{\nn\times\nn}$ whose return function is the identity on $\nn$ and whose update function $\nn\times\nn\times\nn\to\nn$ sends $(\_,m,n)\mapsto m*n$.

    Then the outer box is the monomial interface $\nn\times\nn\yon^\nn$, and the wiring diagram is the interaction pattern
    \[
        w\colon\nn_{\geq1}\yon\otimes\left(\nn\times\nn\yon^{\nn\times\nn_{\geq1}}\right)\otimes\nn\yon\otimes\nn\yon^{\nn\times\nn}\to\nn\times\nn\yon^\nn
    \]
    with on-positions function $(s,q,r,t,p)\mapsto(q,p)$ and on-directions function $(s,q,r,t,p,a)\mapsto(a,s,r,t)$.
    So the dynamical system induced by the wiring diagram is the composite lens $\varphi$ given by
    \[
        \yon\otimes\left(\nn\times\nn\yon^{\nn\times\nn}\right)\otimes\yon\otimes\nn\yon^\nn\To{7\otimes\fun{divmod}\otimes10\otimes*}\nn_{\geq1}\yon\otimes\left(\nn\times\nn\yon^{\nn\times\nn_{\geq1}}\right)\otimes\nn\yon\otimes\nn\yon^{\nn\times\nn}\To{w}\nn\times\nn\yon^\nn,
    \]
    whose return function is given by the composite map $(q,r,p)\mapsto(7,q,r,10,p)\mapsto(q,p)$ and whose update function at state $(q,r,p)$ is given by the composite map $a\mapsto(a,7,r,10)\mapsto(a\bdiv7,a\bmod7,r*10)$.

    In other words, the dynamical system $\varphi$ behaves as follows: its state consists of a quotient $q$, a remainder $r$, and a product $p$, of which it returns the quotient and the product.
    Then it is fed a dividend $a$ and evaluates $a\bdiv7$ to obtain the new quotient and $a\bmod7$ to obtain the new remainder.
    Meanwhile, the new product is given by the previous remainder multiplied by $10$.

    \item This second wiring diagram specifies an interaction pattern
    \[
        w'\colon\nn\times\nn\yon^\nn\to\nn\yon
    \]
    with on-positions function $(q,p)\mapsto q$ and on-directions function $(q,p)\mapsto p$.
    So the dynamical system induced by nesting the first wiring diagram within the inner box in this second wiring diagram is a composite lens
    \[
        \left(\nn\times\nn\yon^{\nn\times\nn}\right)\otimes\nn\yon^\nn\To{\varphi}\nn\times\nn\yon^\nn\To{w'}\nn\yon
    \]
    whose return function is given by the composite map $(q,r,p)\mapsto(q,p)\mapsto q$ and whose update function at state $(q,r,p)$ is specifies the new state $(p\bdiv7,p\bmod7,r*10)$.

    In other words, the dynamical system $\varphi$ behaves as follows: its state consists of a quotient $q$, a remainder $r$, and a product $p$, of which it returns just the quotient.
    Then it advances to a new state by evaluating $p\bdiv7$ to obtain the new quotient and $p\bmod7$ to obtain the new remainder.
    Meanwhile, the new product is given by the previous remainder multiplied by $10$.

    If the initial state is $(q,r,p)=(0,0,10)$, then all the states will be as follows, with the values of $q$ in the left column giving us the outputs:
    \begin{table}[hbt!]
        \centering
        \footnotesize
        \begin{tabular}{c|c|c}
            $q$ $(p\bdiv7)$ & $r$ $(p\bmod7)$ & $p$ $(10r)$ \\
            \hline
            0 & 0 & 10 \\
            1 & 3 & 0 \\
            0 & 0 & 30 \\
            4 & 2 & 0 \\
            0 & 0 & 20 \\
            2 & 6 & 0 \\
            0 & 0 & 60 \\
            8 & 4 & 0 \\
            0 & 0 & 40 \\
            5 & 5 & 0 \\
            0 & 0 & 50 \\
            7 & 1 & 0 \\
            0 & 0 & 10 \\
            $\cdots$ & $\cdots$ & $\cdots$
        \end{tabular}
    \end{table}
\end{enumerate}
\end{solution}
\end{exercise}

\begin{example}[Graphs as wiring diagrams and cellular automata]\label{ex.graph_interaction}
Suppose we have a graph $G=(E\tto V)$ as in \cref{def.graph} and a set $\tau(v)$ associated with each vertex $v\in V$:
\[
\begin{tikzcd}
	E\ar[r, shift left=3pt, "s"]\ar[r, shift right=3pt, "t"']&
	V\ar[r, "\tau"]&
	\smset
\end{tikzcd}
\]
We can think of $G$ as an alternative representation of a specific kind of wiring diagram, one where each inner box has exactly one output wire and the outer box is closed.
The vertices $v\in V$ are the inner boxes, the set $\tau(v)$ is the set associated with $v$'s output wire, and each edge $e$ is a wire connecting the output wire of its target $t(e)$ to an input wire of its source $t(e)$.
An edge from a vertex $v_0$ to a vertex $v_1$ indicates that the inputs to $v_0$ depend on the outputs of $v_1$.\footnote{We could have defined the edges in the opposite directions, so that they would point in the direction of data flow rather than in the direction of data dependencies; this was an arbitrary choice.}

In other words, we can associate each vertex $v\in V$ with the monomial
\[
	p_v\coloneqq\tau(v)\yon^{\prod_{e\in E_v}\tau(t(e))}
\]
specifying its inputs and outputs, where $E_v\coloneqq s\inv(v)\ss E$ denotes the set of edges emanating from $v$.
The graph then determines an enclosure
\[
\gamma\colon\bigotimes_{v\in V}p_v\to\yon
\]
given by a function
\[
    \prod_{v\in V}\tau(v)\too\prod_{e\in E}\tau(t(e))
\]
that sends each dependent function $o\colon(v\in V)\to\tau(v)$ to the dependent function $(e\in E)\to\tau(t(e))$ sending $e\mapsto o(t(e))$.
In other words, given the output $o(v)\in\tau(v)$ for every vertex $v\in V$, we know for each edge $e\in E$ that the input $s(e)$ receives is the output of $t(e)$.

Hence, once we give dynamics to each $p_v$, namely by specifying a dynamical system $S_v\yon^{S_v}\to p_v$ with outputs in $\tau(v)$ and inputs in $\prod_{e\in E_v}\tau(t(e))$, we will obtain a closed dynamical system that transitions from each vertex's state to the next according to the information that they pass each other along their edges.

Effectively, by interpreting a graph as a wiring diagram and giving each vertex dynamics, we have created what is known as a \emph{cellular automaton}---a network of vertices (or \emph{cells}) with states, such that each vertex $v\in V$ ``listens'' to the signals its \emph{neighbors} in $E_v$ send based on their states, then responds accordingly by updating its own state.

For example, a common graph found in cellular automata is a 2-dimensional integer lattice, with vertices $V\coloneqq\zz\times\zz$. The edges indicate which vertices are neighbors and thus ``hear'' which other vertices. One might use
\[E\coloneqq(\{-1,0,1\}\times\{-1,0,1\}-\{(0,0)\})\times V\]
with $s(i,j,m,n)=(m,n)$ and $t(i,j,m,n)=(m+i, n+j)$, so that the neighbors of each vertex are the eight vertices that surround it.
\end{example}

\begin{exercise}[Conway's Game of Life]\label{exc.conway}
Conway's Game of Life is played on a 2-dimensional integer lattice as follows.
Each lattice point is either \emph{live} or \emph{dead}, and each point observes its eight \emph{neighbors} to which it is horizontally, vertically, or diagonally adjacent.
The following occurs at every time step:
\begin{itemize}
    \item Any live point with 2 or 3 live neighbors remains live.
    \item Any dead point with 3 live neighbors becomes live.
    \item All other points either become or remain dead.
\end{itemize}
We can use \cref{ex.graph_interaction} to model Conway's Game of Life as a closed dynamical system.
\begin{enumerate}
	\item What is the appropriate graph $E\tto V$?
	\item What is the appropriate assignment of sets $\tau\colon V\to\smset$?
	\item What are the polynomials $p_v$ from \cref{ex.graph_interaction}?
	\item What is the appropriate state set $S_v$ for each interface $p_v$?
	\item What is the appropriate dynamical system lens $S_v\yon^{S_v}\to p_v$?
\qedhere
\end{enumerate}
\begin{solution}
We seek to model Conway's Game of Life as a closed dynamical system using \cref{ex.graph_interaction}.
\begin{enumerate}
    \item Following the suggestion from the end of \cref{ex.graph_interaction}, we can use a graph with $V\coloneq\zz\times\zz$ and $E\coloneqq(\{-1,0,1\}\times\{-1,0,1\}-\{(0,0)\})\times V$ with $s(i,j,m,n)=(m,n)$ and $t(i,j,m,n)=(m+i,n+j)$ to model cellular automata like Conway's Game of Life on a 2-dimensional integer lattice in which each point listens only to its eight immediate neighbors.
    \item Each vertex needs only return whether it is live or dead, so we assign $\tau(v)\coloneqq\{\text{live},\text{dead}\}$ for every $v\in V$.
    \item For each $v\in V$, the monomial represented by $v$ from \cref{ex.graph_interaction} can be written as
    \[
        p_v\iso\{\text{live},\text{dead}\}\yon^{\smset(\{-1,0,1\}\times\{-1,0,1\}-\{(0,0)\},\, \{\text{live},\text{dead}\})}.
    \]
    Every vertex returns as output whether it is live or dead and receives as input whether each of its eight neighbors is alive or dead.
    \item Each vertex $v\in V$ only needs to record whether it is live or dead, so $S_v\coloneqq\{\text{live},\text{dead}\}$.
    \item The appropriate dynamical system lens $S_v\yon^S_v\to p_v$ for each vertex $v\in V$ should have the identity function on $\{\text{live},\text{dead}\}$ as its return function, while its update function should be a map
    \[
        \{\text{live},\text{dead}\}\smset(\{-1,0,1\}\times\{-1,0,1\}-\{(0,0)\},\, \{\text{live},\text{dead}\})\to\{\text{live},\text{dead}\}
    \]
    that executes the rules from Conway's Game of Life, using the data of whether the vertex itself is live or dead as well as whether each of its eight neighbors is live or dead to determine whether it should be live or dead in the next time step.
\end{enumerate}
\end{solution}
\end{exercise}

\subsection{More examples of general interaction}

While wiring diagrams are a handy visualization tool for certain simple interaction patterns, there are more general interaction patterns that cannot be captured by such a diagram.
For example, here we generalize our previous cellular automata example.

\begin{example}[Generalized cellular automata: voting on who your neighbors are]\label{ex.cell_auto_vote_interaction}
Recall from \cref{ex.graph_interaction} how we constructed a cellular automaton on a graph $G=(E\tto V)$.
For each $v\in V$, the graph specifies the set $N(v)\coloneqq t(E_v)$ of vertices at the ends of edges coming out of $v$.
These vertices are the \emph{neighbors} of $v$, or the vertices that $v$ can ``listen'' to.
We call the map $N\colon V\to\2^V$ from each vertex to the set of its neighbors the \emph{neighbor function}.
For simplicitly, we let each vertex store and return one of two states, so $S_v\coloneqq\tau(v)\coloneqq\2$.

Now just take the vertices and forget the edges of our graph.
Suppose instead that we are given a function $n\colon V\to\nn$ that we think of as specifying the number $n(v)$ of neighbors each $v\in V$ could potentially have.
Let $\ord{n}(v)\coloneqq\{1,2,\ldots,n(v)\}$.
Then we can think of the monomial that each vertex represents as
\[
    p_v\iso\2\yon^{\2^{\ord{n}(v)}},
\]
returning its own state as output and receiving its potential neighbors' states as input.

Say that a neighbor function $N\colon V\to\2^V$ \emph{respects} $n$ if we have an isomorphism $N(v)\iso\ord{n}(v)$ for each $v\in V$.
Now suppose we have a function $N'_-\colon \2^V\to (\2^V)^V$ that sends each set of vertices $S\in\2^V$ to a neighbor function $N'_S\colon V\to \2^V$ that respects $n$.
In other words, each possible state configuration $S$ of all the vertices in $V$ determines a neighbor function $N'_S$.
In the case of \cref{ex.graph_interaction}, when we had a graph, it told us what the neighbor function should always be.
Now we can think of it like all the vertices are voting, via $N'$, on what neighbor function to use to determine which vertices are listening to which others.

We can put this all together by providing an enclosure for all the vertices,
\begin{equation}\label{eqn.polymap_misc9237}
    \bigotimes_{v\in V}p_v\cong\2^V\yon^{\2^{\sum_{v\in V}\ord{n}(v)}}\too\yon.
\end{equation}
Specifying such an enclosure amounts to specifying a function $g\colon \2^V\to\2^{\sum_{v\in V}\ord{n}(v)}$ that sends each possible state configuration $S\in\2^V$ of all the vertices in $V$ to a function $g(S)\colon\sum_{v\in V}\ord{n}(v)\to\2$ specifying the states every vertex hears.
But we already have a neighbor function assigned to $S$ that respects $\ord{n}$, namely $N'_S$, for which $N'_S(v)\iso\ord{n}(v)$ for all $v\in V$.
So we can think of $g(S)$ equivalently as a function $g(S)\colon\sum_{v\in V}N'_S(v)\to\2$ that says for each $v\in V$ what signal in $\2$ it should receive from its neighbor $w\in N'_S(v)$.
But we can just have it receive the current state of its neighbor, as given by $S$:
\[
    g(S)(v,w)\coloneqq S(w).
\]

We have accomplished our goal: the vertices ``vote'' on how they ought to be connected, in that their states together determine the neighbor function.
Of course, we don't mean to imply that this vote needs to be democratic or fair in any way: it is an arbitrary function $N'_-\colon \2^V\to(\2^V)^V$.
It could be dictated by a given vertex $v_0\in V$ in the sense that its state completely determines the neighbor function $V\to\2^V$; this would be expressed by saying that $N'_-$ factors as $\2^V\to\2^{\{v_0\}}\cong\2\To{I_0}(\2^V)^V$ for some $I_0$.
\end{example}

\begin{exercise}
We can change \cref{ex.cell_auto_vote_interaction} slightly by replacing the wrapper interface $\yon$ with some other interface.
\begin{enumerate}
	\item First change it to $A\yon$ for some set $A$ of your choice, and update \eqref{eqn.polymap_misc9237} so that the system outputs some aspect of the current state configuration of all the vertices $S\in\2^V$.
	\item What would it mean to change \eqref{eqn.polymap_misc9237} to a map $\bigotimes_{v\in V}p_v\to\yon^A$ for some $A$?
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item
    \item
\end{enumerate}
\end{solution}
\end{exercise}

Finally, we are ready to formalize the examples we previewed in \cref{sec.poly.intro.dyn_sys}.

\begin{example}\label{ex.bonds_break}
Recall the first picture from \cref{ex.changing_wiring_bonds_supplier_assemble}. We said that when too much force is applied to a material, bonds can break. Let's simplify the picture a bit.
\[
\begin{tikzpicture}[oriented WD, bb small, bb port length=0]
	\node[bb={1}{1}, fill=blue!10] (x1) {$\varphi_1$};
	\node[bb={1}{1}, fill=blue!10, right=of x1] (x2) {$\varphi_2$};
	\node[bb={1}{1}, fit= (x1) (x2)] (outer) {};
	\draw[->, shorten >= -4mm] (x1_in1) -- (outer_in1) node[left=4.5mm, font=\tiny] {Force};
	\draw (x1_out1) -- (x2_in1);
	\draw[->, shorten >= -4mm] (x2_out1) -- (outer_out1) node[right=4.5mm, font=\tiny] (L) {Force};
%
	\node[bb={1}{1}, fill=blue!10, right=2in of L] (y1) {$\varphi_1$};
	\node[bb={1}{1}, fill=blue!10, right=of y1] (y2) {$\varphi_2$};
	\node[bb={1}{1}, fit= (y1) (y2)] (outer) {};
	\draw[->, shorten >= -4mm] (y1_in1) -- (outer_in1) node[left=4.5mm, font=\tiny] (R){Force};
	\draw[->, shorten >= -4mm] (y2_out1) -- (outer_out1) node[right=4.5mm, font=\tiny] {Force};
	\node[starburst, draw, minimum width=2cm, minimum height=1.5cm,red,fill=orange,line width=1.5pt] at ($(L)!.5!(R)$)
{Snap!};
\end{tikzpicture}
\]
We will imagine the dependent dynamical systems $\varphi_1\colon S\yon^S\to p_1$ and $\varphi_2\colon S\yon^S\to p_2$ as initially connected in space.
They experience forces from the outside world, and---for as long as they are connected---they experience forces from each other.
More precisely, each interface is defined by
\[
	p_1\coloneqq p_2\coloneqq F\yon^{FF}+\{\text{`snapped'}\}\yon^F.
\]
Elements of $F$ will be called \emph{forces}.
We need to be able to add and compare forces, i.e.\ we need $F$ to be an ordered monoid; let's say $F=\nn$ for simplicity.
The idea is that the interface has two kinds of output it can return: either a force $f_i\in F$ on the other system, at which point it can receive an input in $FF$ indicating a force acting on the system from its left and another force acting on it from its right; or `snapped,' indicating that the system is no longer connected to the other system, at which point it only receives a single force in $F$ from the outside.

The wrapper interface is defined to be
\[
    p\coloneqq\yon^{FF};
\]
it takes as input two forces $(f_L, f_R)$ and returns unchanging output.

Though the systems $\varphi_1$ and $\varphi_2$ may be initially connected, if the forces on either one surpass a threshold, that system stops sending and receiving forces from the other. The connection is broken and neither system ever receives forces from the other again. This is what we will implement explicitly below.

To do so, we need to define an interaction pattern $p_1\otimes p_2\to p$ that wraps $p$ around $\varphi_1$ and $\varphi_2$.
That is, we need to give a lens
\[
    \kappa\colon (F\yon^{FF}+\{\text{`snapped'}\}\yon^F)\otimes (F\yon^{FF}+\{\text{`snapped'}\}\yon^F)\to\yon^{FF}.
\]
By the distributivity of $\otimes$ over $+$, it suffices to give four lenses:
\begin{equation}\label{eqn.snapped_maps}
\arraycolsep=1.4pt
\begin{array}{lll}
	\kappa_{11}\colon&~ FF\yon^{(FF)(FF)}&\to\yon^{FF}\\
	\kappa_{12}\colon&~ F\{\text{`snapped'}\}\yon^{(FF)F}&\to\yon^{FF}\\
	\kappa_{21}\colon&~ \{\text{`snapped'}\}F\yon^{F(FF)}&\to\yon^{FF}\\
	\kappa_{22}\colon&~ \{\text{`snapped'}\}\{\text{`snapped'}\}\yon^{FF}&\to\yon^{FF}
\end{array}
\end{equation}
The middle two lenses $\kappa_{12}$ and $\kappa_{21}$ won't actually occur in our dynamics, so we take them to be arbitrary.
We take the last lens $\kappa_{22}$ to be the obvious isomorphism, passing the forces from outside to the two internal interfaces.
The first lens $\kappa_{11}$ is equivalent to a function $(FF)(FF)\to (FF)(FF)$ which we take to be $((f_1,f_2),(f_L,f_R))\mapsto((f_L,f_2),(f_1,f_R))$.
While the multiple $F$'s may be a little hard to keep track of, what this map says is that if $\varphi_1$ returns the force $f_1$ on $\varphi_2$ as output and $\varphi_2$ returns the force $f_2$ on $\varphi_1$ as output, then $\varphi_1$ receives the force $f_2$ from the right as input and $\varphi_2$ receives the force $f_1$ from the left as input; and in the meantime the left external force $f_L$ is given to $\varphi_1$ on the left, while the right external force is given to $\varphi_2$ on the right.

Now that we have the interfaces wrapped together, it remains to specify each dynamical system.
The states in the two cases will be identical, namely $S\coloneqq F+\{`snapped'\}$, meaning that at any point the system will either be in the state of applying a force to the other system or not.
The dynamical systems themselves will be identical as well, up to a symmetry swapping left and right; let's just define the left system for now.
It is given by a lens
\[\varphi_1\colon (F+\{\text{`snapped'}\})\yon^{F+\{\text{`snapped'}\}}\to F\yon^{FF}+\{\text{`snapped'}\}\yon^F\]
which we write as the sum of two lenses
\[F\yon^{F+\{\text{`snapped'}\}}\to F\yon^{FF} \qqand \{\text{`snapped'}\}\yon^{F+\{\text{`snapped'}\}}\to\{\text{`snapped'}\}\yon^F.\]
Both lenses are the identity on positions, directly returning their current state.
The second lens corresponds to when the connection is broken, after which the connection should remain broken, so its on-directions function is constant, sending any input to `snapped.'
Meanwhile, the first lens corresponds to the case where the systems are still connected; this system receives two input forces and must update its state---the force it applies---accordingly.
We let the on-directions function $F(FF)\to F+\{\text{`snapped'}\}$ send
\[
(f_1,(f_L,f_2))\mapsto
\begin{cases}
	f_L&\tn{ if }f_1+f_2<100\\
	\text{`snapped'}&\tn{ otherwise}
\end{cases}
\]
Thus, when the sum of forces is high enough, the internal state is updated to the `snapped' state; otherwise, it is sent to the force it receives from outside, which it is now ready to transfer to the other system.
\end{example}

\begin{example}\label{ex.supplier_change}
Recall the second picture from \cref{ex.changing_wiring_bonds_supplier_assemble}.
We want to consider the case of a company that may change its supplier based on its internal state. The company returns two possible outputs, corresponding to who it wants to receive widgets $W$ from:
\[
\begin{tikzpicture}[oriented WD, every node/.style={fill=blue!10}]
	\node[bb={0}{1}] (s1) {Supplier 1};
	\node[bb={0}{1}, below=of s1] (s2) {Supplier 2};
	\node[bb={1}{0}, right=0.5 of s1] (c) {Company};
	\draw (s1_out1) to node[above, fill=none, font=\tiny] {$W$} (c_in1);
	\draw (s2_out1) to +(5pt,0) node[fill=none] {$\bullet$};
\begin{scope}[xshift=3.5in]
	\node[bb={0}{1}] (s1') {Supplier 1};
	\node[bb={0}{1}, below=of s1'] (s2') {Supplier 2};
	\node[bb={1}{0}, right=0.5 of s2'] (c') {Company};
	\draw (s2'_out1) to node[above, fill=none, font=\tiny] {$W$} (c'_in1);
	\draw (s1'_out1) to +(5pt,0) node[fill=none] {$\bullet$};
\end{scope}
	\node[starburst, draw, minimum width=2cm, minimum height=2cm,align=center,fill=white, font=\small,line width=1.5pt] at ($(c.east)!.5!(s2'.west)$)
{Change\\supplier!};
\end{tikzpicture}
\]
So the company has interface $\2\yon^W$, and each supplier has interface $W\yon$.
Then an enclosure for the company and the suppliers is just a lens $\2\yon^W\otimes W\yon\otimes W\yon\to\yon$, corresponding to a function $\2W^\2\to W$ given by evaluation.
In other words, the company's output determines its supplier.
\end{example}

\begin{example}\label{ex.assemble_machine}
Recall the third picture from \cref{ex.changing_wiring_bonds_supplier_assemble}.
When someone assembles a machine, their own outputs dictate the interaction pattern of the machine's components.
\begin{equation*}%\label{eqn.someone2}
\begin{tikzpicture}[oriented WD, font=\ttfamily, bb port length=0, every node/.style={fill=blue!10}, baseline=(someone.north)]
	\node[bb port sep=.5, bb={0}{1}] (A) {unit A};
	\node[bb port sep=.5, bb={1}{0}, right=of A] (B) {unit B};
	\coordinate (helper) at ($(A)!.5!(B)$);
	\node[bb={1}{1}, below=2 of helper] (someone) {\tikzsymStrichmaxerl[3]};
	\draw[->, dashed, blue] (someone_in1) to[out=180, in=270] (A.270);
	\draw[->, dashed, blue] (someone_out1) to[out=0, in=270] (B.270);
	\draw[->] (A_out1) -- +(10pt,0);
	\draw (B_in1) -- +(-10pt,0);
%
\begin{scope}[xshift=3.5in]
	\node[bb port sep=.5, bb={0}{1}] (A') {unit A};
	\node[bb port sep=.5, bb={1}{0}, right=.5of A'] (B') {unit B};
	\coordinate (helper') at ($(A')!.5!(B')$);
	\node[bb={1}{1}, below=2 of helper'] (someone') {\tikzsymStrichmaxerl[3]};
	\draw[->, dashed, blue] (someone'_in1) to[out=180, in=270] (A'.270);
	\draw[->, dashed, blue] (someone'_out1) to[out=0, in=270] (B'.270);
	\draw[->] (A'_out1) -- (B'_in1);
\end{scope}
%
	\node[starburst, draw, minimum width=2cm, minimum height=2cm,fill=blue!50,line width=1.5pt, align=center, font=\upshape] at ($(B)!.5!(A')-(0,.6cm)$)
{Attach!};
\end{tikzpicture}
\end{equation*}
In order for the above picture to make sense, the output set of \texttt{unit A} should be the same as the input set of \texttt{unit B}.
Call this set $X$, so that \texttt{unit A} has interface $X\yon$ and \texttt{unit B} has interface $\yon^X$.
We fix a default value $x_0\in X$ for the input to \texttt{unit B} when it is not connected to \texttt{unit A}.
Meanwhile, the person takes no input and dictates whether the units are attached or not, so we give it interface $\2\yon$.

Then an enclosure for the person and the units is a lens $\2\yon\otimes X\yon\otimes \yon^X\to\yon$. The lens $\2X\yon^X\to\yon$, corresponding to a function $\2X\to X$ that maps $(1,x)\mapsto x_0$ and $(2,x)\mapsto x$.
\end{example}

We can easily generalize \cref{ex.assemble_machine}.
Indeed, we will see in the next section that there is an interface $\ihom{q_1\otimes\cdots\otimes q_k\,,\,r}$ that represents all the interaction patterns between $q_1,\ldots,q_k$ with wrapper interface $r$, and that wrapping it around $p$ to it is just a larger interaction pattern:
\[
\poly(p,[q_1\otimes\cdots\otimes q_k\,,\,r])\cong\poly(p\otimes q_1\otimes\cdots\otimes q_k\,,\,r).
\]
In other words, if $p$ is deciding the interaction pattern between $q_1,\ldots,q_k$ with wrapper interface $r$, and gets feedback from that interaction pattern itself, then this is equivalent to an interaction pattern with wrapper interface $r$ that $p$ is part of alongside $q_1,\ldots,q_k$ inside of $r$.

What it also means is that if you want, you can put a little dynamical system inside of $[q_1\otimes\cdots\otimes q_k,r]$ and have it be constantly choosing interaction patterns. Let's see how it works.


%-------- Section --------%
\section{Closure of $\otimes$}%[-,-]

The parallel monoidal product is closed---we have a monoidal closed structure on $\poly$---meaning that there is a closure operation, which we denote $\ihom{-,-}\colon\poly\op\times\poly\to\poly$, such that there is an isomorphism
\begin{equation}\label{eqn.monoidal_closure}
  \poly(p\otimes q,r) \iso \poly(p,\ihom{q,r})
\end{equation}
natural in $p,q,r$.
The closure operation is defined on $q,r$ as follows:
\begin{equation}\label{eqn.par_hom}
	\ihom{q,r} \coloneqq \prod_{j\in q(\1)}r\circ(q[j]\yon)
\end{equation}
Here $\circ$ denotes standard functor composition; informally, $r \circ (q[j]\yon)$ is the polynomial you get when you replace each appearance of $\yon$ in $r$ by $q[j]\yon$.
Composition, together with the unit $\yon$, is in fact yet another monoidal structure, as we will see in more depth in \cref{part.comon}.

Before we prove that the isomorphism \eqref{eqn.monoidal_closure} holds naturally, let us investigate the properties of the closure operation, starting with some simple examples.

\begin{exercise}
Calculate $\ihom{q,r}$ for $q,r\in\poly$ given as follows.
\begin{enumerate}
	\item $q\coloneqq \0$ and $r$ arbitrary.
	\item $q\coloneqq \1$ and $r$ arbitrary.
	\item $q\coloneqq\yon$ and $r$ arbitrary.
	\item $q\coloneqq A$ for $A\in\smset$ (constant) and $r$ arbitrary.
	\item $q\coloneqq A\yon$ for $A\in\smset$ (linear) and $r$ arbitrary.
	\item $q\coloneqq\yon^\2+\2\yon$ and $r\coloneqq\2\yon^\3+\3$.
\qedhere
\end{enumerate}
\begin{solution}
We compute $\ihom{q,r}$ for various values of $q, r \in \poly$ using \eqref{eqn.par_hom}.
\begin{enumerate}
    \item If $q \coloneqq \0$, then $q(\1) \iso \0$, so $\ihom{q,r}$ is an empty product.
    Hence $\ihom{q,r} \iso \1$.
    \item If $q \coloneqq \1$, then $q(\1) \iso \1$ and $q[1] \iso \0$, so $\ihom{q,r} \iso r \circ (\0\yon) \iso r(\0)$.
    \item If $q \coloneqq \yon$, then $q(\1) \iso \1$ and $q[1] \iso \1$, so $\ihom{q,r} \iso r \circ (\1\yon) \iso r$.
	\item If $q \coloneqq A$ for $A \in \smset$, then $q(\1) \iso A$ and $q[j] \iso \0$ for every $j \in A$, so $\ihom{q,r} \iso \prod_{j \in A} (r \circ (\0\yon)) \iso r(\0)^A$.
	\item If $q \coloneqq A\yon$ for $A\in\smset$, then $q(\1) \iso A$ and $q[j] \iso \1$ for every $j \in A$, so $\ihom{q,r} \iso \prod_{j \in A} (r \circ (\1\yon)) \iso r^A$.
	\item If $q\coloneqq\yon^\2+\2\yon$ and $r\coloneqq\2\yon^\3+\3$, then
	\begin{align*}
	    \ihom{q,r} &\iso (r \circ (\2\yon))(r \circ (\1\yon))^\2 \\
	    &\iso \left(\2(\2\yon)^\3 + \3\right)\left(\2\yon^\3 + \3\right)^\2 \\
	    &\iso \6\4\yon^\9 + \2\0\4\yon^\6 + \1\8\0\yon^\3 + \2\7 \\
	\end{align*}
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.sum_times_closure}
Show that for any polynomials $p_1,p_2,q$, we have an isomorphism
\[
\ihom{p_1 + p_2, q} \iso \ihom{p_1, q} \times \ihom{p_2, q}.
\]
\begin{solution}
We wish to show that for all $p_1, p_2, q \in \poly$, we have $\ihom{p_1 + p_2, q} \iso \ihom{p_1, q} \times \ihom{p_2, q}$.
By \eqref{eqn.par_hom},
\[
    \ihom{p_1 + p_2, q} \iso \left(\prod_{i \in p_1(\1)} q \circ (p_1[i]\yon)\right) \left(\prod_{i \in p_2(\1)} q \circ (p_2[i]\yon)\right) \iso \ihom{p_1, q} \times \ihom{p_2, q}.
\]
\end{solution}
\end{exercise}

\begin{exercise} \label{exc.par_hom_sum}
Show that there is an isomorphism
\begin{equation} \label{eqn.par_hom_sum}
\scalebox{1.3}{$\displaystyle
\ihom{q,r} \iso \sum_{f\colon q\to r}\yon^{\sum_{j\in q(\1)}r[f_\1(j)]}
$}
\end{equation}
where the sum is indexed by $f\in\poly(q,r)$.
\begin{solution}
We may compute
\begin{align*}
    \ihom{q, r} &\iso \prod_{j \in q(\1)} r \circ (q[j]\yon) \tag*{\eqref{eqn.par_hom}} \\
    &\iso \prod_{j \in q(\1)} \, \sum_{k \in r(\1)} (q[j]\yon)^{r[k]} \tag{Replacing each $\yon$ in $r$ by $q[j]\yon$} \\
    &\iso \sum_{f_\1 \colon q(\1) \to r(\1)} \, \prod_{j \in q(\1)} (q[j]\yon)^{r[f_\1(j)]} \tag*{\eqref{eqn.push_prod_sum_set_indep}} \\
    &\iso \sum_{f_\1 \colon q(\1) \to r(\1)} \, \left(\prod_{j \in q(\1)} q[j]^{r[f_\1(j)]} \right)\left(\prod_{j \in q(\1)} \yon^{r[f_\1(j)]} \right) \\
    &\iso \sum_{f_\1 \colon q(\1) \to r(\1)} \; \sum_{f^\sharp \in \prod_{j \in q(\1)} q[j]^{r[f_\1(j)]}} \yon^{\sum_{j \in q(\1)} r[f_\1(j)]} \\
    &\iso \sum_{f \colon q \to r} \yon^{\sum_{j \in q(\1)} r[f_\1(j)]}. \tag*{\eqref{eqn.main_formula}}
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise} \label{exc.dir_hom_p_yon_dir_p}
Verify that \eqref{eqn.dir_hom_p_yon_dir_p} holds.t
\begin{solution}
We verify \eqref{eqn.dir_hom_p_yon_dir_p} as follows:
\begin{align*}
    \ihom{p, \yon} \otimes p
    &\iso
    \left(\sum_{f \colon p \to \yon} \yon^{\sum_{i \in p(\1)} \yon[f_1(i)]}\right) \otimes p
    \tag*{\eqref{eqn.par_hom_sum}} \\
    &\iso
    \sum_{f \in \Gamma(p)} \yon^{p(\1)} \otimes \sum_{i \in p(\1)} \yon^{p[i]} \\
    &\iso
    \sum_{f \in \Gamma(p)} \; \sum_{i \in p(\1)} \yon^{p(\1) \times p[i]}
    \tag*{\eqref{eqn.parallel_def}} \\
    &\iso
    \sum_{f \in \prod_{i \in p(\1)} p[i]} \; \sum_{i \in p(\1)} \yon^{p(\1) \times p[i]}.
    \tag*{\eqref{eqn.gamma_prod}}
\end{align*}
\end{solution}
\end{exercise}

\begin{example}\label{ex.parallel_dual}
For any $A\in\smset$ we have
\[
  \ihom{\yon^A,\yon} \iso A\yon
  \qqand
  \ihom{A\yon,\yon} \iso \yon^A.
\]
More generally, for any polynomial $p\in\poly$ we have
\begin{equation}\label{eqn.dir_dual}
  \ihom{p,\yon} \iso \Gamma(p)\yon^{p(\1)}.
\end{equation}
All these facts follow directly from \eqref{eqn.par_hom}.
\end{example}

\begin{exercise}
Verify the three facts above.
\begin{solution}
We have that
\[
    \ihom{\yon^A, \yon} \iso \prod_{j \in \yon^A(\1)} \yon \circ (\yon^A[j]\yon) \iso \prod_{j \in \1} A\yon \iso A\yon,
\]
that
\[
    \ihom{A\yon, \yon} \iso \prod_{j \in A\yon(\1)} \yon \circ ((A\yon)[j]\yon) \iso \prod_{j \in A} \yon \iso \yon^A,
\]
and that
\begin{align*}
    \ihom{p, \yon} &\iso \sum_{f \colon p \to \yon} \yon^{\sum_{i \in p(\1)} \yon[f_1(i)]} \tag{\cref{eqn.par_hom_sum}} \\
    &\iso \sum_{f \in \Gamma(p)} \yon^{\sum_{i \in p(\1)} \1} \\
    &\iso \Gamma(p)\yon^{p(\1)}.
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}
Show that for any $p\in\poly$, if there is an isomorphism $\ihom{\ihom{p,\yon},\yon} \iso p$, then $p$ is either linear $A\yon$ or representable $\yon^A$ for some $A$. Hint: first show that $p$ must be a monomial.
\begin{solution}
Given $p \in \poly$ and an isomorphism $\ihom{\ihom{p,\yon},\yon} \iso p$, we wish to show that $p$ is either linear or representable.
Applying \eqref{eqn.dir_dual} twice, we have that
\[
    p \iso \ihom{\ihom{p,\yon},\yon} \iso \Gamma\left(\Gamma(p)\yon^{p(\1)}\right)\yon^{\Gamma(p)}.
\]
We can compute the coefficient of $p$ via \eqref{eqn.gamma_prod} to obtain
\[
    \Gamma\left(\Gamma(p)\yon^{p(\1)}\right) \iso \prod_{\gamma \in \Gamma(p)} p(\1) \iso p(\1)^{\Gamma(p)}.
\]
Hence
\begin{equation} \label{eqn.p_as_gamma_monomial}
    p \iso p(\1)^{\Gamma(p)}\yon^{\Gamma(p)}.
\end{equation}
In particular, $p$ is a monomial, so we can write $p \coloneqq B\yon^A$ for some $A,B \in \smset$.
Then $p(\1) \iso B$ and \eqref{eqn.gamma_prod} tells us that $\Gamma(p) \iso A^B$.
It follows from \eqref{eqn.p_as_gamma_monomial} that $A \iso A^B$ and that $B \iso B^A$.

We conclude with some elementary set theory.
If either one of $A$ or $B$ were $\1$, then $p$ would be either linear or representable, and we would be done.
Meanwhile, if either one of $A$ or $B$ were $\0$, then the other would be $\1$, and we would again be done.
Otherwise, $|A|,|B| \geq 2$.
But by Cantor's theorem,
\[
    |B| < \big|\2^B\big| \leq \big|A^B\big| = |A| \qqand |A| < \big|\2^A\big| \leq \big|B^A\big| = |B|,
\]
a contradiction.
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.parallel_closure}
With $\ihom{-,-}$ as defined in \eqref{eqn.par_hom}, there is a natural isomorphism
\begin{equation}\label{eqn.poly_closure_brackets}
	\poly(p\otimes q,r)\cong\poly(p,\ihom{q,r}).
\end{equation}
\end{proposition}
\begin{proof}
We have the following chain of natural isomorphisms:
\begin{align*}
	\poly(p\otimes q,r)
	&\iso
	\poly\Big(\sum_{i\in p(\1)}\sum_{j\in q(\1)}\yon^{p[i]q[j]},r\Big) \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}\poly(\yon^{p[i]q[j]},r)
	\tag{Universal property of coproducts} \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}r(p[i]q[j])
	\tag{Yoneda lemma} \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}\poly(\yon^{p[i]},r\circ(q[j]\yon))
	\tag{Yoneda lemma} \\
	&\iso
	\poly\Big(\sum_{i\in p(\1)}\yon^{p[i]},\prod_{j\in q(\1)}r\circ(q[j]\yon)\Big)
	\tag{Universal property of (co)products} \\
	&\iso
	\poly(p,\ihom{q,r}).
\end{align*}
\end{proof}

\begin{exercise}\label{exc.poly_plug_1}
Show that for any $p,q$ we have an isomorphism of sets
\[
\poly(p,q) \iso \ihom{p,q}(\1).
\]
Hint: you can either use the formula \eqref{eqn.par_hom}, or just use
\eqref{eqn.poly_closure_brackets} with the Yoneda lemma and the fact that $\yon\otimes p \iso p$.
\begin{solution}
The isomorphism $\poly(p,q) \iso \ihom{p,q}(\1)$ follows directly from \cref{exc.par_hom_sum} when both sides are applied to $\1$.
Alternatively, we can apply \eqref{eqn.poly_closure_brackets}.
Since $p \iso \yon \otimes p$, we have that
\begin{align*}
    \poly(p, q) &\iso \poly(\yon \otimes p, q) \\
    &\iso \poly(\yon, \ihom{p,q}) \tag*{\eqref{eqn.poly_closure_brackets}} \\
    &\iso \ihom{p,q}(\1). \tag{Yoneda lemma}
\end{align*}
\end{solution}
\end{exercise}

The closure of $\otimes$ implies that for any $p,q\in\poly$, there is a canonical \emph{evaluation} lens
\begin{equation}\label{eqn.eval_parallel}
  \fun{eval}\colon \ihom{p,q}\otimes p\too q.
\end{equation}
As in any closed monoidal category, such an evaluation lens has the universal property that for any $r\in\poly$ and lens $f\colon p\otimes q\to r$, there is a unique lens $f'\colon p\to\ihom{q,r}$ such that the following diagram commutes:
\[
    \begin{tikzcd}
    	p\otimes q\ar[r, "f'\otimes q"]\ar[rr, bend right, "f"']&
    	{\ihom{q,r}}\otimes q\ar[r, "\fun{eval}"]&
    	r
    \end{tikzcd}
\]

\begin{exercise} \label{exc.eval_parallel}
Obtain the evaluation lens $\fun{eval}\colon \ihom{p,q}\otimes p\too q$ from \eqref{eqn.eval_parallel}.
\begin{solution}
To obtain the evaluation lens $\fun{eval}\colon \ihom{p,q}\otimes p\too q$, we consider the following special case of the isomorphism \eqref{eqn.poly_closure_brackets}:
\[
    \poly(\ihom{p,q} \otimes p, q) \iso \poly(\ihom{p,q}, \ihom{p,q}).
\]
Then the evaluation lens is the lens corresponding to the identity lens on $\ihom{p,q}$ under the above isomorphism.
To recover this lens, we can start from the identity lens on $\ihom{p,q}$ and work our way along a chain of natural isomorphisms from $\poly(\ihom{p,q}, \ihom{p,q})$ until we get to $\poly(\ihom{p,q} \otimes p, q)$.
To start, \cref{exc.par_hom_sum} implies that
\begin{align*}
    \poly(\ihom{p,q}, \ihom{p,q})
    &\iso
    \poly\left(\sum_{f \colon p \to q} \, \prod_{i' \in p(\1)} \yon^{q[f_\1(i')]}, \prod_{i \in p(\1)} \, \sum_{j \in q(\1)} (p[i]\yon)^{q[j]}\right) \\
    &\iso
    \prod_{f \colon p \to q} \, \prod_{i \in p(\1)} \poly\left(\prod_{i' \in p(\1)} \yon^{q[f_\1(i')]}, \sum_{j \in q(\1)} (p[i]\yon)^{q[j]}\right),
\end{align*}
where the second isomorphism follows from the universal properties of products and coproducts.
In particular, under this isomorphism, the identity lens on $\ihom{p,q}$ corresponds to a collection of lenses, namely for each $f \colon p \to q$ and each $i \in p(\1)$ the composite
\[
    \prod_{i' \in p(\1)} \yon^{q[f_\1(i')]} \to \yon^{q[f_\1(i)]} \to \sum_{g \colon q[f_\1(i)] \to p[i]} \yon^{q[f_\1(i)]} \iso (p[i]\yon)^{q[f_\1(i)]} \to \sum_{j \in q(\1)} (p[i]\yon)^{q[j]}
\]
of the canonical projection with index $i' = i$, the canonical inclusion with index $g = f^\sharp_i$, and the canonical inclusion with index $j = f_\1(i)$.
On positions, this lens picks out the position of $\sum_{j \in q(\1)} (p[i]\yon)^{q[j]}$ corresponding to $j = f_\1(i) \in q(\1)$ and $f^\sharp_i \colon q[f_\1(i)] \to p[i]$; on directions, the lens is the canonical inclusion $q[f_\1(i)] \to \sum_{i' \in p(\1)} q[f_\1(i')]$ with index $i' = i$.

We can reinterpret each of these lenses as a lens
\[
    \yon^{p[i] \times \sum_{i' \in p(\1)} q[f_\1(i')]} \to \sum_{j \in q(\1)} \yon^{q[j]} \iso q
\]
that, on positions, picks out the position $f_\1(i) \in q(\1)$ of $q$ and, on directions, is the map $q[f_\1(i)] \to p[i] \times \sum_{i' \in p(\1)} q[f_\1(i')]$ induced by the universal property of products applied to the map $f^\sharp_i \colon q[f_\1(i)] \to p[i]$ and the inclusion $q[f_\1(i)] \to \sum_{i' \in p(\1)} q[f_\1(i')]$.
Then by the universal property of coproducts, this collection of lenses induces a single lens $\fun{eval} \colon \ihom{p, q} \otimes p \to q$ that sends each position $f \colon p \to q$ of $\ihom{p,q}$ and position $i \in p(\1)$ of $p$ to the position $f_\1(i)$ of $q$, with the same behavior on directions as the corresponding lens described previously.
\end{solution}
\end{exercise}

\begin{exercise}
\begin{enumerate}
	\item For any set $S$, obtain the lens $S\yon^S\to\yon$ whose on-directions  is the identity on $S$ using eval and \cref{ex.parallel_dual}.
	\item Show that four lenses in \eqref{eqn.snapped_maps} from \cref{ex.bonds_break}, written equivalently as
	\begin{equation} \label{eqn.snapped_maps2}
	\arraycolsep=1.4pt
    \begin{array}{lll}
    	\kappa_{11}\colon&~ F\yon^{FF}\otimes F\yon^{FF}&\to\yon^F\otimes\yon^F\\
    	\kappa_{12}\colon&~ F\yon^{FF}\otimes \yon^F&\to\yon^F\otimes\yon^F\\
    	\kappa_{21}\colon&~ \yon^F\otimes F\yon^{FF}&\to\yon^F\otimes\yon^F\\
    	\kappa_{22}\colon&~ \yon^F\otimes\yon^F&\to\yon^F\otimes\yon^F,
    \end{array}
	\end{equation}
	can be obtained by taking the parallel product of identity lenses and evaluation lenses.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item Given a set $S$, we wish to obtain the lens $S\yon^S \to \yon$ whose on-directions function is the identity by using eval and \cref{ex.parallel_dual}.
    The example shows that
    \[
        \ihom{S\yon, \yon} \otimes (S\yon) \iso \yon^S \otimes (S\yon) \iso S\yon^S,
    \]
    so by setting $p \coloneqq S\yon$ and $q \coloneqq S$ in \eqref{eqn.eval_parallel}, we obtain an evaluation lens $\fun{eval} \colon S\yon^S \to \yon$.
    By the solution to \cref{exc.eval_parallel}, given a position $s \in S$ of $S\yon^S$, the evaluation lens on directions is the map $\1 \to S$ that picks out $s$.
    In other words, it is indeed the identity on directions.
    \item We wish to write the four lenses in \eqref{eqn.snapped_maps2} from \cref{ex.bonds_break} as the parallel product of identity lenses and evaluation lenses.
    By the solution to \cref{exc.eval_parallel}, the evaluation lens $\ihom{F,\yon^F}\otimes F\to\yon^F$
    is a lens from
    \[
        \ihom{F,\yon^F}\otimes F\iso F\left(\sum_{f\colon F\to\yon^F}\prod_{i\in F}\yon^F\right)\iso F\yon^{FF}
    \]
    to $\yon^F$ that is unique on positions and has the on-directions function $FF\to FF$ given by the identity.
    Then we can verify that $\kappa_{11}$ is equivalent to the parallel product of this evaluation lens with itself.
    We can define $\kappa_{12}$ and $\kappa_{21}$ to be the parallel product of this evaluation lens with the identity on $\yon^F$, while $\kappa_{22}$ is the parallel product of the identity on $\yon^F$ with itself.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{example}[Modeling your environment without knowing what it is]
Let's imagine a robot whose interface is an arbitrary polynomial $p$. Let's imagine it is part of an interaction pattern
\[
	f\colon (q_1\otimes\cdots\otimes q_n)\otimes p\to r
\]
with some other robots whose interfaces are $q_1,\ldots,q_n$; let $q\coloneqq(q_1\otimes\cdots\otimes q_n)$. The interaction pattern induces a lens $f'\colon q\to \ihom{p,r}$ such that the original system $f$ factors through the evaluation $\ihom{p,r}\otimes p\to r$.

In other words, $\ihom{p,r}$ holds within it all of the possible ways $p$ can interact with other systems when they are all wrapped in $r$.
For example, in the case of $r\coloneqq\yon$, note that $\ihom{p,\yon}\cong\prod_{i\in p(\1)}p[i]\yon$.
That is, for each $p$-position it produces a direction there, which is just what $p$ needs as input in a closed system.

Now suppose we were to populate the interface $p$ with dynamics, a lens $S\yon^S\to p$. One could aim to choose a set $S$ along with an interesting map $g\colon S\to\poly(p,r)$. Then each state $s$ would include a guess $g(s)$ about the state of its environment. This is not the real environment $q$, but just the environment as it affects $p$, namely $\ihom{p,r}$. The robot's states model environmental conditions.
\end{example}

\begin{example}[Chu $\&$]
Suppose we have polynomials $p_1,p_2,q_1,q_2,r\in\poly$ and lenses
\[
	\varphi_1\colon p_1\otimes q_1\to r
	\qqand
	\varphi_2\colon p_2\otimes q_2\to r.
\]
One might call these ``$r$-Chu spaces.'' One operation you can do with these as Chu spaces is to return something denoted $\varphi_1\&\varphi_2$, or ``$\varphi_1$ \emph{with} $\varphi_2$'' of the following type:
\[
\varphi_1\&\varphi_2\colon (p_1\times p_2)\otimes (q_1+q_2)\to r
\]
Suppose we are given a position in $p_1$ and a position in $p_2$. Then given a position in either $q_1$ or $q_2$, one evaluates either $\varphi_1$ or $\varphi_2$ respectively to get a position in $r$; given a direction there, one returns the corresponding direction in $q_1$ or $q_2$ respectively, as well as a direction in $p_1\times p_2$ which is either a direction in $p_1$ or in $p_2$.

This sounds complicated, but it can be done formally, once we have monoidal closure. We first rearrange both $\varphi_1,\varphi_2$ to be $p$-centric, using monoidal currying:
\[
\psi_1\colon p_1\to \ihom{q_1,r}
\qqand
\psi_2\colon p_2\to \ihom{q_2,r}
\]
Now we multiply to get $\psi_1\times\psi_2\colon p_1\times p_2\to\ihom{q_1,r}\times\ihom{q_2,r}$. Then we apply \cref{exc.sum_times_closure} to see that $\ihom{q_1,r}\times\ihom{q_2,r}\cong\ihom{q_1+q_2,r}$, and finally monoidal-uncurry to obtain $(p_1\times p_2)\otimes(q_1+q_2)\to r$ as desired.
\end{example}

%-------- Section --------%
\section{Summary and further reading}

In this chapter we explained how discrete dynamical systems can be expressed as certain lenses between polynomial functors. For example, a Moore machine has an input set $A$, an output set $B$, a set of states $S$, a return function $S\to B$, and an update function $A\times S\to S$. All this is captured in a depedent lens
\[S\yon^S\to B\yon^A.\]
We discussed a generalization $S\yon^S\to p$, where the output is an arbitrary polynomial $p\in\poly$. We also talked about how to wire machines in parallel by using the parallel product $\otimes$ and how to add wrapper interfaces by composing with lenses $p\to q$.

Throughout the chapter we gave quite a few different examples. For example, we discussed how every function $A\to B$ counts as a memoryless dynamical system. In fact, it was shown in \cite{beurier2019memoryless} that every dynamical system can be obtained by wiring together memoryless ones. We discussed examples such as file readers, moving robots, colliding particles, companies that change their suppliers, materials that break when too much force is applied, etc.

For further reading on the mathematics of Moore machines, see \cite{conway2012regular}. For more on mode-dependent interaction, see \cite{spivak2017nesting}. For a similar and complementary categorical approach to dynamical systems, we recommend David Jaz Myers' \emph{Categorical Systems Theory} book, currently in draft form here: \url{http://davidjaz.com/Papers/DynamicalBook.pdf}.

%-------- Section --------%
\section{Exercise solutions}
\Closesolutionfile{solutions}
{\footnotesize
\input{solution-file4}}

\Opensolutionfile{solutions}[solution-file5]

%------------ Chapter ------------%
\chapter{More categorical properties of polynomials} \label{ch.poly.bonus}

The category $\poly$ has very useful formal properties, including completion under colimits and limits, various adjunctions with $\smset$, factorization systems, and so on. Most of the following material is not necessary for the development of our main story, but we collect it here for reference. The reader can skip directly to \cref{part.comon} if so inclined, and check back here when needed. Better yet might be to just gently leaf through this chapter, to see how well-behaved and versatile the category $\poly$ really is.

%-------- Section --------%
\section{Special polynomials and adjunctions} \label{sec.poly.bonus.adj}

There are a few special classes of polynomials that are worth discussing:
\begin{enumerate}[label=\alph*)]
	\item constant polynomials $\0,\1,\2,A$;
	\item linear polynomials $\0,\yon, \2\yon, A\yon$;
	\item representable (or pure power) polynomials $\1, \yon, \yon^\2, \yon^A$; and
	\item monomials $\0, A, \yon, \2\yon^\3, B\yon^A$.
\end{enumerate}
The first two classes, constant and linear polynomials, are interesting because they both put a copy of $\smset$ inside $\poly$, as we'll see in \cref{prop.ff_const_set_to_poly,prop.ff_lin_set_to_poly}.
The third puts a copy of $\smset\op$ inside $\poly$: it is the Yoneda embedding that we saw way back in \cref{exc.finish_proof_yoneda}.
Finally, the fourth puts a copy of bimorphic lenses inside $\poly$, as we saw in \cref{subsec.poly.cat.morph.bimorphic-lens}.

\begin{exercise}
Which of the four classes above are closed under
\begin{enumerate}
	\item the cocartesian monoidal structure $(\0,+)$ (i.e.\ addition)?
	\item the cartesian monoidal structure $(\1,\times)$ (i.e.\ multiplication)?
	\item the parallel monoidal structure $(\yon,\otimes)$ (i.e.\ taking the parallel product)?
	\item composition of polynomials $p\circ q$? (We have not discussed this yet, so feel free to skip it.)
\qedhere
\end{enumerate}
\begin{solution}
Here $A, B, A', B' \in \smset$.
\begin{enumerate}
    \item We determine whether various classes of polynomials are closed under addition.
    \begin{enumerate}
        \item Constant polynomials are closed under addition: given constants $A, B$, their sum $A + B$ is also a constant polynomial.
        \item Linear polynomials are closed under addition: given linear polynomials $A\yon, B\yon$, their sum $A\yon + B\yon \iso (A + B)\yon$ is also a linear polynomial.
        \item Representable polynomials are \emph{not} closed under addition: for example, $\yon$ is a representable polynomial, but the sum of $\yon$ with itself, $\2\yon$, is not.
        \item Monomials are \emph{not} closed under addition: for example, $\yon$ and $\2\yon^\3$ are monomials, but their sum $\yon + \2\yon^\3$ is not.
    \end{enumerate}
    \item We determine whether various classes of polynomials are closed under multiplication.
    The results below follow from \cref{exc.general_poly_times} \cref{exc.general_poly_times.monomial}.
    \begin{enumerate}
        \item Constant polynomials are closed under multiplication: given constants $A, B$, their product $AB$ is also a constant polynomial.
        \item Linear polynomials are \emph{not} closed under multiplication: for example, $\yon$ and $\2\yon$ are linear polynomials, but their product $\2\yon^\2$ is not.
        \item Representable polynomials are closed under multiplication: given representables $\yon^A, \yon^B$, their product $\yon^{A+B}$ is also a representable polynomial.
        \item Monomials are closed under multiplication: given monomials $B\yon^A, B'\yon^{A'}$, their product $BB'\yon^{A+A'}$ is also a monomial.
    \end{enumerate}
    \item We determine whether various classes of polynomials are closed under taking parallel products.
    The results below follow from \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial}.
    \begin{enumerate}
        \item Constant polynomials are closed under taking parallel products: given constants $A, B$, their parallel product $AB$ is also a constant polynomial.
        \item Linear polynomials are closed under taking parallel products: given linear polynomials $A\yon, B\yon$, their parallel product $AB\yon$ is also a linear polynomial.
        \item Representable polynomials are closed under taking parallel products: given representables $\yon^A, \yon^B$, their parallel product $\yon^{AB}$ is also a representable polynomial.
        \item Monomials are closed under taking parallel products: given monomials $B\yon^A, B'\yon^{A'}$, their parallel product $BB'\yon^{AA'}$ is also a monomial.
    \end{enumerate}
    \item We determine whether various classes of polynomials are closed under composition. (Recall that we can think of computing the composite $p \circ q$ of $p, q \in \poly$ as replacing each appearance of $\yon$ in $p$ with $q$.)
    \begin{enumerate}
        \item Constant polynomials are closed under composition: given constants $A, B$, their composite $A \circ B \iso A$ is also a constant polynomial.
        \item Linear polynomials are closed under composition: given linear polynomials $A\yon, B\yon$, their composite $A\yon \circ B\yon \iso A(B\yon) \iso AB\yon$ is also a linear polynomial.
        \item Representable polynomials are closed under composition: given representables $\yon^A, \yon^B$, their composite $\yon^A \circ \yon^B \iso (\yon^B)^A \iso \yon^{BA}$ is also a representable polynomial.
        \item Monomials are closed under taking parallel products: given monomials $B\yon^A, B'\yon^{A'}$, their composite $B\yon^A \circ B'\yon^{A'} \iso B(B'\yon^{A'})^A \iso BB'^A\yon^{A'A}$ is also a monomial.
    \end{enumerate}
\end{enumerate}
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.ff_const_set_to_poly}
There is a fully faithful functor $\smset\to\poly$ sending $A\mapsto A\yon^\0=A$.
\end{proposition}
\begin{proof}
By \eqref{eqn.colax_poly_map}, a lens $f\colon A\yon^\0\to B\yon^\0$ consists of a function $f\colon A\to B$ and, for each $a\in A$, a function $\0\to\0$. There is only one function $\0\to\0$, so $f$ can be identified with just a map of sets $A\to B$.
\end{proof}

\begin{proposition}\label{prop.ff_lin_set_to_poly}
There is a fully faithful functor $\smset\to\poly$ sending $A\mapsto A\yon$.
\end{proposition}
\begin{proof}
By \eqref{eqn.colax_poly_map}, a lens $f\colon A\yon^\1\to B\yon^\1$ consists of a function $f\colon A\to B$ and for each $a\in A$ a function $\1\to\1$. There is only one function $\1\to\1$, so $f$ can be identified with just a map of sets $A\to B$.
\end{proof}

\begin{theorem}\label{thm.adjoint_quadruple}
$\poly$ has an adjoint quadruple with $\smset$:
\begin{equation}\label{eqn.adjoints_galore}
\begin{tikzcd}[column sep=60pt, background color=theoremcolor]
  \smset
  	\ar[r, shift left=7pt, "A" description]
		\ar[r, shift left=-21pt, "A\yon"']&
  \poly
  	\ar[l, shift right=21pt, "p(\0)"']
  	\ar[l, shift right=-7pt, "p(\1)" description]
	\ar[l, phantom, "\scriptstyle\Leftarrow"]
	\ar[l, phantom, shift left=14pt, "\scriptstyle\Rightarrow"]
	\ar[l, phantom, shift right=14pt, "\scriptstyle\Rightarrow"]
\end{tikzcd}
\end{equation}
where the functors have been labeled by where they send $A\in\smset$ and $p\in \poly$.

Both rightward functors $\smset\to\poly$ are fully faithful.
\end{theorem}
\begin{proof}
For any set $A$, there is a functor $\poly\to\smset$ given by sending $p$ to $p(A)$; by the Yoneda lemma, it is the functor $\poly(\yon^A,-)$. This, together with \cref{prop.ff_const_set_to_poly,prop.ff_lin_set_to_poly}, gives us the four functors and the fact that the two rightward functors are fully faithful. It remains to provide the following three natural isomorphisms:
\[
\poly(A,p)\iso\smset(A,p(\0))\qquad
\poly(p,A)\iso\smset(p(\1),A)\qquad
\poly(A\yon,p)\iso\smset(A,p(\1)).
\]
All three come from our formula \eqref{eqn.main_formula} for computing general hom-sets in $\poly$; we leave the details to the reader in \cref{exc.adjoint_quadruple}.
\end{proof}

\begin{exercise}\label{exc.adjoint_quadruple}
Here we prove the remainder of \cref{thm.adjoint_quadruple} using \eqref{eqn.main_formula}:
\begin{enumerate}
	\item Provide a natural isomorphism $\poly(A,p)\iso\smset(A,p(\0))$.
	\item \label{exc.adjoint_quadruple.pos_const} Provide a natural isomorphism $\poly(p,A)\iso\smset(p(\1),A)$.
	\item \label{exc.adjoint_quadruple.linear_pos} Provide a natural isomorphism $\poly(A\yon,p)\iso\smset(A,p(\1))$.
\qedhere
\end{enumerate}
\begin{solution}
We complete the proof of \cref{thm.adjoint_quadruple} by exhibiting three natural isomorphisms, all special cases of \eqref{eqn.main_formula}, as follows.
\begin{enumerate}
    \item By \eqref{eqn.main_formula}, we have the natural isomorphism
    \[
        \poly(A, p) % \iso \prod_{a \in A} \sum_{i \in p(\1)} A[a]^{p[i]}
        \iso \prod_{a \in A} \sum_{i \in p(\1)} \0^{p[i]}.
    \]
    As $\0^{p[i]}$ is $\1$ if $p[i] \iso \0$ and $\0$ otherwise, it follows that
    \[
        \poly(A, p) \iso \prod_{a \in A} \{ i \in p(\1) \mid p[i] \iso \0 \} \iso \prod_{a \in A} p(\0) \iso \smset(A, p(0)).
    \]
    \item By \eqref{eqn.main_formula}, we have the natural isomorphism
    \begin{align*}
        \poly(p, A) %&\iso \prod_{i \in p(\1)} \sum_{a \in A} p[i]^{A[a]} \\
        &\iso \prod_{i \in p(\1)} \sum_{a \in A} p[i]^\0 \\
        &\iso \prod_{i \in p(\1)} \sum_{a \in A} \1 \\
        &\iso \prod_{i \in p(\1)} A \\
        &\iso \smset(p(\1), A).
    \end{align*}
    \item By \eqref{eqn.main_formula}, we have the natural isomorphism
    \begin{align*}
        \poly(A\yon, p) %&\iso \prod_{a \in A} \sum_{i \in p(\1)} (A\yon)[a]^{p[i]} \\
        &\iso \prod_{a \in A} \sum_{i \in p(\1)} \1^{p[i]} \\
        &\iso \prod_{a \in A} \sum_{i \in p(\1)} \1 \\
        &\iso \prod_{a \in A} p(\1) \\
        &\iso \smset(A, p(\1)).
    \end{align*}
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.positions_maps_yon}
Show that for any polynomial $p$, its set $p(\1)$ of positions is in bijection with the set of functions $\yon\to p$.
\begin{solution}
Given $p \in \poly$, we wish to show that $p(\1)$ is in bijection with the set of functions $\yon \to p$.
In fact, this follows directly from the Yoneda lemma, but we can also invoke the isomorphism from \cref{exc.adjoint_quadruple} \cref{exc.adjoint_quadruple.linear_pos} with $A \coloneqq \1$ to observe that
\[
    p(1) \iso \smset(\1, p(\1)) \iso \poly(\yon, p).
\]
\end{solution}
\end{exercise}

In \cref{thm.adjoint_quadruple} we see that $p\mapsto p(\0)$ and $p\mapsto p(\1)$ have left adjoints. This is true more generally for any set $A$ in place of $\0$ and $\1$, as we show in \cref{cor.substituting_adj}. However, the fact that $p\mapsto p(\1)$ is itself the left adjoint of the left adjoint of $p\mapsto p(\0)$---and hence that we have the \emph{quadruple} of adjunctions in \eqref{eqn.adjoints_galore}---is special to $A=\0,\1$.

We also have a copower-hom-power two-variable adjunction between $\poly,\smset,$ and $\poly$.

\begin{proposition}\label{prop.two_var_adj}
There is a two-variable adjunction between $\poly$, $\smset$, and $\poly$:
\begin{equation}\label{eqn.two_var_adj}
\poly(Ap,q) \iso \smset(A,\poly(p,q)) \iso \poly(p,q^A).
\end{equation}
\end{proposition}
\begin{proof}
Since $Ap$ is the $A$-fold coproduct of $p$ and $q^A$ is the $A$-fold product of $q$, the universal properties of coproducts and products give natural isomorphisms
\[\poly(Ap,q)\cong\prod_{a\in A}\poly(p,q)\cong\poly(p,q^A).\]
The middle set is naturally isomorphic to $\smset(A,\poly(p,q))$, completing the proof.
\end{proof}

Replacing $p$ with $\yon^B$ in \eqref{eqn.two_var_adj}, we obtain the following using the Yoneda lemma.

\begin{corollary}\label{cor.substituting_adj}
For any set $B$ there is an adjunction
\[
\adj{\smset}{A\yon^B}{q(B)}{\poly}
\]
where the functors are labeled by where they send $q\in\poly$ and $A\in\smset$.
\end{corollary}

\begin{exercise}
Prove \cref{cor.substituting_adj} from \cref{prop.two_var_adj}.
\begin{solution}
To prove \cref{cor.substituting_adj}, it suffices to exhibit a natural isomorphism
\[
    \poly(A\yon^B, q) \iso \smset(A, q(B)).
\]
Replacing $p$ with $\yon^B$ in \eqref{eqn.two_var_adj} from \cref{prop.two_var_adj}, we obtain the natural isomorphism
\[
    \poly(A\yon^B, q) \iso \smset(A, \poly(\yon^B, q)).
\]
By the Yoneda lemma, $\poly(\yon^B, q)$ is naturally isomorphic to $q(B)$, yielding the desired result.
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.yoneda_left_adjoint}
The Yoneda embedding $A\mapsto \yon^A$ has a left adjoint
\[
\adjr{\smset\op}{\yon^-}{\Gamma}{\poly}
\]
where $\Gamma(p) \coloneqq \poly(p, \yon) \iso \prod_{i\in p(\1)}p[i]$, as in \eqref{eqn.gamma_def} and \eqref{eqn.gamma_prod}.
That is, there is a natural isomorphism
\begin{equation} \label{eqn.yoneda_left_adjoint}
    \poly(p, \yon^A) \iso \smset(A, \Gamma(p)).
\end{equation}
\end{proposition}
\begin{proof}
By \eqref{eqn.main_formula}, we have the natural isomorphism
\[
    \poly(p, \yon^A) \iso \prod_{i \in p(\1)} p[i]^A,
\]
which in turn is naturally isomorphic to $\smset(A, \Gamma(p))$ by \eqref{eqn.gamma_prod}.
\end{proof}

% \begin{exercise} % Already basically done
% Show that $\Gamma(p)\cong\ihom{p,\yon}(\1)$ where $\ihom{-,-}$ is as in \cref{prop.parallel_closure}.
% \end{exercise}

\begin{corollary}[Principal monomial]\label{cor.principal_monomial}
There is an adjunction
\[
    \adj{\poly}{{(p(\1),\Gamma(p))}}{A\yon^B}{\smset\times\smset\op}
\]
where the functors are labeled by where they send $p\in\poly$ and $(A,B)\in\smset\times\smset\op$.
That is, there is a natural isomorphism
\begin{equation} \label{eqn.principal_monomial}
    \poly(p, A\yon^B) \iso \smset(p(1), A) \times \smset(B, \Gamma(p)).
\end{equation}
\end{corollary}
\begin{proof}
By the universal property of the product of $A$ and $\yon^B$, we have a natural isomorphism
\[
    \poly(p, A\yon^B) \iso \poly(p, A) \times \poly(p, \yon^B).
\]
Then the desired natural isomorphism follows from \cref{exc.adjoint_quadruple} \cref{exc.adjoint_quadruple.pos_const} and \eqref{eqn.yoneda_left_adjoint}.
\end{proof}

\begin{exercise}
Use \eqref{eqn.principal_monomial} together with \eqref{eqn.dir_dual} and \eqref{eqn.poly_closure_brackets} to find an alternative proof for \cref{prop.situations2}, i.e.\ that there is an isomorphism
\[
    \Gamma(p\otimes q) \iso \smset\big(q(\1),\Gamma(p)\big) \times \smset\big(p(\1),\Gamma(q)\big).
\]
for any $p,q\in\poly$.
\begin{solution}
% The universal properties of the adjunctions
% \[
%       \adj[40pt]{\poly}{{(-(\1),\Gamma(-))}}{-\yon^-}{\smset\times\smset\op}
%       \qqand
%       \adj{\poly}{-\otimes q}{{\ihom{q,-}}}{\poly}
% \]
% from \cref{cor.principal_monomial,prop.parallel_closure} together with the isomorphism $\ihom{p,\yon} \iso \Gamma(p)\yon^{p(\1)}$ from \eqref{eqn.dir_dual} give
We have the following chain of natural isomorphisms:
\begin{align*}
	\Gamma(p \otimes q) &=
	\poly(p \otimes q,\yon)
	\tag*{\eqref{eqn.gamma_def}} \\
	&\iso
	\poly(p, \ihom{q,\yon})
	\tag*{\eqref{eqn.poly_closure_brackets}} \\
	&\iso
	\poly(p, \Gamma(q)\yon^{q(\1)})
	\tag*{\eqref{eqn.dir_dual}} \\
% 	&\iso
% 	(\smset\times\smset\op)\Big(\big((p(\1),\Gamma(p)\big),\big(\Gamma(q),q(\1)\big)\Big) \\
	&\iso
	\smset\big(p(\1),\Gamma(q)\big) \times \smset\big(q(\1),\Gamma(p)\big).
	\tag*{\eqref{eqn.principal_monomial}}
\qedhere
\end{align*}
\end{solution}
\end{exercise}

%-------- Section --------%
\section{Epi-mono factorization of lenses}

\begin{proposition}\label{prop.monics_in_poly}
Let $f \colon p \to q$ be a lens in $\poly$. It is a monomorphism if and only if the on-positions function $f_\1 \colon p(\1) \to q(\1)$ is a monomorphism in $\smset$ and, for each $i \in p(\1)$, the on-directions function $f^\sharp_i \colon q[f_\1(i)]\to p[i]$ is an epimorphism in $\smset$.
\end{proposition}
\begin{proof}
To prove the forward direction, suppose that $f$ is a monomorphism.
Since $p\mapsto p(\1)$ is a right adjoint (\cref{thm.adjoint_quadruple}), it preserves monomorphisms, so the on-positions function $f_\1$ is also a monomorphism.

We now need to show that for any $i\in p(\1)$, the on-directions function $f^\sharp_i \colon q[f_\1(i)] \to p[i]$ is an epimorphism.
Suppose we are given a set $A$ and a pair of functions $g^\sharp,h^\sharp\colon p[i]\tto A$ with $f^\sharp_i \then g^\sharp = f^\sharp_i \then h^\sharp$.
Then there exist lenses $g,h \colon \yon^A \tto p$ whose on-positions functions both pick out $i$ and whose on-directions functions are $g^\sharp$ and $h^\sharp$, so that $g \then f = h \then f$.
As $f$ is a monomorphism, $g = h$; in particular, their on-directions functions $g^\sharp$ and $h^\sharp$ are equal, as desired.

Conversely, suppose that $f_\1$ is a monomorphism and that, for each $i\in p(\1)$, the function $f^\sharp_i$ is an epimorphism.
Let $r$ be a polynomial and $g,h\colon r\tto p$ be two lenses such that $g \then f = h \then f$.
Then $g_\1 \then f_\1 = h_\1 \then f_\1$, which implies $g_\1 = h_\1$; we'll consider $g_\1$ the default representation.
We also have that $f^\sharp_{g_\1(k)} \then g^\sharp_k = f^\sharp_{g_\1(k)} \then h^\sharp_k$ for any $k \in r(\1)$. But $f^\sharp_{g_\1(k)}$ is an epimorphism, so in fact $g^\sharp_k = h^\sharp_k$, as desired.
\end{proof}

\begin{example}\label{ex.clock_in_N}
Choose a finite nonempty set $\ord{k}$ for $1\leq k\in\nn$, e.g.\ $\ord{k}=\1\2$. There is a monomorphism
\[
f \colon\ord{k}\yon^{\ord{k}}\to\nn\yon^\nn
\]
such that the trajectory ``going around and around the $k$-clock'' comes from the usual counting trajectory $\nn\yon^\nn\to\yon$ from \cref{ex.counting_trajectory}.

On positions, we have $f_\1(i)=i$ for all $i \in \ord{k}$. On directions, for any $i \in \ord{k}$, we have $f^\sharp_i(n) = n \mod k$ for all $n \in \nn$.
\end{example}

\begin{exercise}
In \cref{ex.clock_in_N}, we gave a lens $\1\2\yon^{\1\2}\to\nn\yon^\nn$. This allows us to turn any dynamical system with $\nn$-many states into a dynamical system with 12 states, while keeping the same interface---say, $p$.

Explain how the behavior of the new system $\1\2\yon^{\1\2}\to p$ would be seen to relate to the behavior of the old system $\nn\yon^\nn\to p$.
\begin{solution}
We are given a monomorphism $f \colon \1\2\yon^{\1\2} \to \nn\yon^\nn$ from \cref{ex.clock_in_N}.
Let $g \colon \nn\yon^\nn \to p$ be a dynamical system with return function $g_\1 \colon \nn \to p(\1)$ and update functions $g^\sharp_n \colon p[g_\1(n)] \to \nn$ for each state $n \in \nn$.
Then the new composite dynamical system $h \coloneqq f \then g$ has a return function $h_\1 \colon \1\2 \to p(\1)$ which sends each state $i \in \1\2$ to the output $h_\1(i) = g_\1(f_\1(i)) = g_\1(i)$, the same output that the original system returned in the state $i \in \nn$.
Meanwhile, the update function for each state $i \in \1\2$ is a function $h^\sharp_i \colon p[g_\1(i)] \to \1\2$ which, given an input $a \in p[g_\1(i)]$, updates the state from $i$ to $h^\sharp_i(a) = f^\sharp_{g_\1(i)}(g^\sharp_i(a)) = g^\sharp_i(a) \mod 12$, which is where the original system would have taken the same state to, but reduced modulo 12.
In other words, the new system behaves like the old system but with only the states in $\1\2 \ss \nn$ retained, and on any input that would have caused the old system to move to a state outside of $\1\2$, the new system moves to the equivalent state (modulo 12) within $\1\2$ instead.
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.epis_in_poly}
Let $f \colon p \to q$ be a lens in $\poly$. It is an epimorphism if and only if the function $f_\1 \colon p(\1) \to q(\1)$ is an epimorphism in $\smset$ and, for each $j\in q(\1)$, the induced function
\[
    f^\flat_j \colon q[j] \to \prod_{\substack{i\in p(\1), \\ f_\1(i)=j}} p[i]
\]
from \eqref{eqn.useful_misc472} is a monomorphism.
\end{proposition}
\begin{proof}
To prove the forward direction, suppose that $f$ is an epimorphism. Since $p \mapsto p(\1)$ is a left adjoint (\cref{thm.adjoint_quadruple}), it preserves epimorphisms, so the on-positions function $f_\1$ is also a epimorphism.

We now need to show that for any $j\in q(\1)$, the induced function $f^\flat_j$ is a monomorphism.
Suppose we are given a set $A$ and a pair of functions $g',h'\colon A\tto q[j]$ with $g' \then f^\flat_j = h' \then f^\flat_j$.
They can be identified with lenses $g,h\colon q\tto \yon^A+\1$, which send the $j$-component to the first component, $\yon^A$, and send all other component to the second component, $\1$. It is easy to check that $fg=fh$, hence $g=h$, and hence $g^\sharp=h^\sharp$ as desired.

Then we can construct lenses $g,h \colon q \tto \yon^A+\1$ whose on-positions functions both send $j$ to the first position, corresponding to $\yon^A$, and all other positions to the second position, corresponding to $\1$.
In addition, we let the on-directions functions be $g^\sharp_j \coloneqq g'$ and $h^\sharp_j \coloneqq h'$.
Then $f \then g = f \then h$.
As $f$ is an epimorphism, $g = h$; in particular, their on-directions functions are equal, so $g' = h'$, as desired.

Conversely, suppose that $f_\1$ is an epimorphism and that, for each $j\in q(\1)$, the function $f^\flat_j$ is a monomorphism.
Let $r$ be a polynomial and $g,h\colon q\tto r$ be two lenses such that $f \then g = f \then h$.
Then $f_\1 \then g_\1 = f_\1 \then h_\1$, which implies $g_\1=h_\1$; we'll consider $g_\1$ the default representation.
We also have that $g^\sharp_{f_\1(i)} \then f^\sharp_i = h^\sharp_{f_\1(i)} \then f^\sharp_i$ for any $i\in p(\1)$.
It follows that, for any $j \in q(\1)$, the two composites
\[
\begin{tikzcd}
	r[g_\1(j)] \ar[r, shift left, "g^\sharp_j"] \ar[r, shift right, "h^\sharp_j"'] & q[j] \ar[r, "f^\flat_j"] & \displaystyle\prod_{\substack{i\in p(\1), \\ f_\1(i)=j}} p[i]
\end{tikzcd}
\]
are equal, which implies that $g^\sharp_j=h^\sharp_j$ as desired.
\end{proof}

% Insert exercise exploring the difference between the epi proposition and the one about monos

\begin{exercise}
Show that the only way for a lens $p\to\yon$ to \emph{not} be an epimorphism is when $p=0$.
\begin{solution}
Given $p \in \poly$ and a lens $f \colon p \to \yon$, we will use \cref{prop.epis_in_poly} to show that either $f$ is an epimorphism or $p = \0$.
First, note that $f_\1 \colon p(\1) \to \1$ must be an epimorphism unless $p(\1) \iso \0$, in which case $p = \0$.
Next, note that the induced function
\[
    f^\flat \colon \1 \to \prod_{i\in p(\1)} p[i]
\]
from \eqref{eqn.useful_misc472} must be a monomorphism.
So it follows from \cref{prop.epis_in_poly} that either $f$ is an epimorphism or $p = \0$.
\end{solution}
\end{exercise}

\begin{exercise}
Let $A$ and $B$ be sets and $AB$ their product. Find an epimorphism $\yon^A+\yon^B\surj\yon^{AB}$.
\begin{solution}
Given sets $A$ and $B$, by \cref{prop.epis_in_poly}, a lens $f \colon \yon^A + \yon^B \to \yon^{AB}$ is an epimorphism if its on-positions function $f_\1 \colon \2 \to \1$ is an epimorphism (which must be true) and if the induced function
\[
    f^\flat \colon AB \to \prod_{i \in \2} (\yon^A + \yon^B)[i] \iso AB
\]
is a monomorphism.
If we take the on-directions functions $AB \to A$ and $AB \to B$ of $f$ to be the canonical projections, then the induced function $f^\flat \colon AB \to AB$ would be the identity, which is indeed a monomorphism.
So $f$ would be an epimorphism.
\end{solution}
\end{exercise}

\begin{exercise}
Suppose a lens $f\colon p\to q$ is both a monomorphism and an epimorphism; it is then an isomorphism? (That is, is $\poly$ \emph{balanced}?)

Hint: You may use the following facts.
\begin{enumerate}
    \item A function that is both a monomorphism and an epimorphism in $\smset$ is an isomorphism.
    \item A lens is an isomorphism if and only if the on-positions function is an isomorphism and every on-directions function is an isomorphism.
\end{enumerate}
\begin{solution}
Let $f \colon p \to q$ be a lens in $\poly$ that is both a monomorphism and an epimorphism.
We claim that $f$ is an isomorphism.
By \cref{prop.monics_in_poly} and \cref{prop.epis_in_poly}, the on-positions function $f_\1 \colon p(\1) \to q(\1)$ is both a monomorphism and an epimorphism, so it is an isomorphism.
Meanwhile, \cref{prop.epis_in_poly} says that, for each $j \in q(\1)$, the induced function
\[
    f^\flat_j \colon q[j] \to \prod_{\substack{i \in p(\1), \\ f_\1(i) = j}} p[i]
\]
is a monomorphism.
As $f_\1$ is an isomorphism, it follows that for each $i \in p(\1)$, the function
\[
    f^\flat_{f_\1(i)} \colon q[f_\1(i)] \to p[i]
\]
is a monomorphism.
But this is just the on-directions function $f^\sharp_i$ of $f$.
From \cref{prop.monics_in_poly}, we also know that $f^\sharp_i$ is an epimorphism.
It follows that every on-directions function of $f$ is an isomorphism.
Hence $f$ itself is an isomorphism.
\end{solution}
\end{exercise}

We are often interested in whether epimorphisms and monomorphisms form what is called a \emph{factorization system} in a given category, which we define below.

\begin{definition}[Factorization system] \label{def.factor}
Given a category $\cat{C}$ and two classes of morphisms $E$ and $M$ in $\cat{C}$, we say that $(E, M)$ is a \emph{factorization system} of $\cat{C}$ if:
\begin{enumerate}
    \item every morphism $f$ in $\cat{C}$ factors uniquely (up to unique isomorphism) as a morphism $e \in E$ composed with a morphism $m\in M$, so that $f = e\then m$;
    \item $E$ and $M$ each contain every isomorphism; and
    \item $E$ and $M$ are each closed under composition.
\end{enumerate}
If $E$ is the class of epimorphisms and $M$ is the class of monomorphisms (in which case conditions 2 and 3 are automatically satisfied), we say that $\cat{C}$ has \emph{epi-mono factorization}.
\end{definition}

\begin{example}[Epi-mono factorization in $\smset$] \label{ex.epi_mono_set}
The category $\smset$ has epi-mono factorization: a function $f\colon X\to Y$ can be uniquely factored into an epimorphism (surjection) $e$ followed by a monomorphism (injection) $i$, as follows.
The epimorphism $e\colon X\to f(X)$ is given by restricting the codomain of $f$ to its image (also known as \emph{corestricting} $f$), so $e$ sends $x\mapsto f(x)$ for all $x\in X$.
The monomorphism $i\colon f(X)\to Y$ is then given by including the image into the codomain, so $i$ sends $y\mapsto y$ for all $y\in f(X)\ss Y$.
\end{example}

\begin{proposition}
$\poly$ has epi-mono factorization.
\end{proposition}
\begin{proof}
Take an arbitrary lens $\varphi\colon p\to q$.
It suffices to show that there exists a unique polynomial $r$ equipped with an epimorphism $\epsilon\colon p\to r$ and a monomorphism $\mu\colon r\to q$ such that $\varphi=\epsilon\then\mu$.

On positions, we must have $\varphi_\1 = \epsilon_\1\then\mu_\1$, with $\mu_\1$ a monomorphism and $\epsilon_\1$ an epimorphism per \cref{prop.monics_in_poly,prop.epis_in_poly}.
By \cref{ex.epi_mono_set}, since $\smset$ has epi-mono factorization, such $r(\1), \epsilon_\1,$ and $\mu_\1$ uniquely exist.
In particular, we must have that $r(\1)\iso \varphi_\1(p(\1))$, that $\epsilon_\1\colon p(\1)\to\varphi_\1(p(\1))$ is the corestriction of $\varphi_\1$ sending $i\mapsto\varphi_\1(i)$ for each $p$-position $i$, and that $\mu_\1\colon\varphi_\1(p(\1))\to q(\1)$ is the inclusion sending $j\mapsto j$ for each $r$-position $j$.

Then on directions, for any $i\in p(\1)$, we must have that
\[
\begin{tikzcd}
    q[\varphi_\1(i)] \ar[r, "\mu^\sharp_{\varphi_\1(i)}"] \ar[dr, "\varphi^\sharp_i"'] & r[\varphi_\1(i)] \ar[d, "\epsilon^\sharp_i"] \\
    & p[i]
\end{tikzcd}
\]
commutes---or, equivalently, for every $j\in r(\1)\iso\varphi_\1(p(\1))$,
\[
\begin{tikzcd}
    q[j] \ar[r, "\mu^\sharp_j"] \ar[dr, "\varphi^\flat_j"'] & r[j] \ar[d, "\epsilon^\flat_j"] \\
    & \prod\limits_{\substack{i\in p(\1), \\ \varphi_\1(i)=j}} p[i]
\end{tikzcd}
\]
commutes (here $\varphi^\flat_j$ and $\epsilon^\flat_j$ are the induced functions from \eqref{eqn.useful_misc472}), with $\mu_j^\sharp$ an epimorphism and $\epsilon^\flat_j$ a monomorphism per
\cref{prop.monics_in_poly,prop.epis_in_poly}.
So again since $\smset$ has epi-mono factorization, such $r[j], \mu^\sharp_j,$ and $\epsilon^\flat_j$ uniquely exist.
Hence such $p \To{\epsilon} r \To{\mu} q$ uniquely exists overall.
\end{proof}

%-------- Section --------%
\section{Cartesian closure}

We have already seen the closure operation $\ihom{-,-}$ for one monoidal structure on $\poly$, namely $(\yon,\otimes)$.
But this is not the only closed monoidal structure on $\poly$: in fact, we will show that $\poly$ is cartesian closed as well.

For any two polynomials $q,r$, define $r^q\in\poly$ by the formula
\begin{equation}\label{eqn.exponential}
  r^q\coloneqq\prod_{j\in q(\1)}r\circ(\yon+q[j])
\end{equation}
where $\circ$ denotes composition.

Before proving that this really is an exponential in $\poly$, which we do in \cref{thm.poly_cart_closed}, we first get some practice with it.

\begin{example}
Let $A$ be a set. We've been writing the polynomial $A\yon^\0$ simply as $A$, so it better be true that there is an isomorphism
\[
    \yon^A \iso \yon^{A\yon^\0}
\]
in order for the notation to be consistent.
Luckily, this is true.
By \eqref{eqn.exponential}, we have
\[
    \yon^{A\yon^\0} = \prod_{a\in A} \yon \circ (\yon+\0) \iso \yon^A
\]
\end{example}

\begin{exercise}
Compute the following exponentials in $\poly$ using \eqref{eqn.exponential}:
\begin{enumerate}
	\item $p^\0$ for an arbitrary $p\in\poly$.
	\item $p^\1$ for an arbitrary $p\in\poly$.
	\item $\1^p$ for an arbitrary $p\in\poly$.
	\item $A^p$ for an arbitrary $p\in\poly$ and $A\in\smset$.
	\item $\yon^\yon$.
	\item $\yon^{\4\yon}$.
	\item $(\yon^A)^{\yon^B}$ for arbitrary sets $A,B\in\smset$.
\qedhere
\end{enumerate}
\begin{solution}
We use \eqref{eqn.exponential} to compute various exponentials.
Here $p \in \poly$ and $A, B \in \smset$.
\begin{enumerate}
    \item We have that $p^\0$ is an empty product, so $p^\0 \iso \1$ as expected.
	\item We have that $p^\1 \iso p \circ (\yon + \0) \iso p$, as expected.
	\item We have that $\1^p \iso \prod_{i \in p(\1)} \1 \circ (\yon + p[i]) \iso \1$, as expected.
	\item We have that $A^p \iso \prod_{i \in p(\1)} A \circ (\yon + p[i]) \iso A^{p(\1)}$.
	\item We have that $\yon^\yon \iso \yon \circ (\yon + \1) \iso \yon + \1$.
	\item We have that $\yon^{\4\yon} \iso \prod_{j \in \4} \yon \circ (\yon + \1) \iso (\yon + \1)^\4 \iso \yon^\4 + \4\yon^\3 + \6\yon^\2 + \4\yon + \1$.
	\item We have that $(\yon^A)^{\yon^B} \iso (\yon^A) \circ (\yon + B) \iso (\yon + B)^A \iso \sum_{f \colon A \to \2} B^{f\inv(1)} \yon^{f\inv(2)}$.
\end{enumerate}
\end{solution}
\end{exercise}

% \begin{exercise} % Immediate from previous exercise
% Using \eqref{eqn.exponential}, show that the functor $\smset\to\poly$ that sends each set $A$ to the constant polynomial $A$ preserves exponentials.
% That is, given sets $A, B \in \smset$, the set $B^A$ as a constant polynomial coincides with the exponential in $\poly$ that is the constant polynomial $B$ raised to the constant polynomial $A$.
% \begin{solution}
% By \eqref{eqn.exponential}, the exponential that is the constant polynomial $B$ raised to the constant polynomial $A$ can be written as
% \[
%     \prod_{a \in A} B \circ (\yon + A[a]) \iso \prod_{a \in A} B \iso B^A.
% \]
% \end{solution}
% \end{exercise}

\begin{theorem}\label{thm.poly_cart_closed}
The category $\poly$ is cartesian closed. That is, we have a natural isomorphism
\[
    \poly(p,r^q) \iso \poly(p\times q,r),
\]
where $r^q$ is the polynomial defined in \eqref{eqn.exponential}.
\end{theorem}
\begin{proof}
We have the following chain of natural isomorphisms:
\begin{align*}
	\poly(p, r^q) &\iso
	\poly\Big(p, \prod_{j \in q(\1)} r \circ (\yon+q[j])\Big)
	\tag*{\eqref{eqn.exponential}} \\
% 	&\iso
% 	\prod_{j\in q(\1)}\poly(p,r\circ(\yon+q[j]))
% 	\tag{Universal property of products} \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}\poly\big(\yon^{p[i]},r\circ(\yon+q[j])\big)
	\tag{Universal property of (co)products} \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}r\circ(p[i]+q[j])
	\tag{Yoneda lemma} \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}\sum_{k\in r(\1)}(p[i]+q[j])^{r[k]}
	\\
	&\iso
	\prod_{(i,j) \in (p \times q)(\1)} \; \sum_{k\in r(\1)}(p \times q)[(i, j)]^{r[k]}
	\tag*{\eqref{eqn.poly_times}} \\
	&\iso
	\poly(p \times q,r).
	\tag*{\eqref{eqn.main_formula}}
\end{align*}
\end{proof}

\begin{exercise}
Use \cref{thm.poly_cart_closed} to show that for any polynomials $p,q$, there is a canonical evaluation lens
\begin{equation*}%\label{eqn.eval_times}
	\text{eval}\colon p^q \times q \to p.
\end{equation*}
\begin{solution}
By \cref{thm.poly_cart_closed}, there is a natural isomorphism
\[
    \poly(p^q, p^q) \iso \poly(p^q \times q, p).
\]
Under this isomorphism, there exists a lens $\text{eval} \colon p^q \times q \to p$ corresponding to the identity lens on $p^q$.
The lens $\text{eval}$ is the canonical evaluation lens.
\end{solution}
\end{exercise}


%-------- Section --------%
\section{Limits and colimits of polynomials}

We have already seen that $\poly$ has all coproducts (\cref{prop.poly_coprods}) and products (\cref{prop.poly_prods}).
We will now see that $\poly$ has all small limits and colimits.

\begin{theorem}\label{thm.poly_limits}
The category $\poly$ has all small limits.
\end{theorem}
\begin{proof}
A category has all small limits if and only if it has products and equalizers, so by \cref{prop.poly_prods}, it suffices to show that $\poly$ has equalizers.

We claim that equalizers in $\poly$ are simply equalizers on positions and coequalizers on directions.
More precisely, let $f,g \colon p \tto q$ be two lenses.
We construct the equalizer $p'$ of $f$ and $g$ as follows.\footnote{If we're being precise, a ``(co)equalizer'' is an object equipped with a morphism, but we will use the term to refer to either just the object or just the morphism when the context is clear.}
We define its position-set $p'(\1)$ to be the equalizer of $f_\1,g_\1 \colon p(\1) \tto q(\1)$ in $\smset$; that is,
\[
    p'(\1) \coloneqq \{i \in p(\1) \mid f_\1(i) = g_\1(i)\}.
\]
Then for each $i \in p'(\1)$, we can define the direction-set $p'[i]$ to be the coequalizer of $f^\sharp_i, g^\sharp_i \colon q[f_\1(i)] \tto p[i]$.
In this way, we obtain a polynomial $p'$ that comes equipped with a lens $e \colon p' \to p$.
One can check that $p'$ together with $e$ satisfies the universal property of the equalizer of $f$ and $g$; see \cref{exc.poly_limits}.
\end{proof}

\begin{exercise}\label{exc.poly_limits}
Complete the proof of \cref{thm.poly_limits} as follows:
\begin{enumerate}
	\item We said that $p'$ comes equipped with a lens $e \colon p' \to p$; what is it?
	\item Show that $e \then f = e \then g$.
	\item Show that $e$ is the equalizer of the pair $f,g$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item The lens $e \colon p' \to p$ can be characterized as follows.
    The on-positions function $e_\1 \colon p'(\1) \to p(\1)$ is the equalizer of $f_\1, g_\1 \colon p(\1) \tto q(\1)$ in $\smset$.
    In particular, $e_\1$ is the canonical inclusion that sends each element of $p'(\1)$ to the same element in $p(\1)$.
    Then for each $i \in p'(\1)$, the on-directions function $e^\sharp_i \colon p[i] \to p'[i]$ is the coequalizer of $f^\sharp_i, g^\sharp_i \colon q[f_\1(i)] \tto p[i]$ in $\smset$.

    \item To show that $e \then f = e \then g$, it suffices to show that both sides are equal on positions and on directions.
    On positions, $e_\1$ is defined to be the equalizer of $f_\1$ and $g_\1$, so $e_\1 \then f_\1 = e_\1 \then g_\1$.
    Then for each $i \in p'(\1)$, the on-directions function $e^\sharp_i$ is defined to be the coequalizer of $f^\sharp_i$ and $g^\sharp_i$, so $f^\sharp_i \then e^\sharp_i = g^\sharp_i \then e^\sharp_i$.

    \item To show that $e$ is the equalizer of $f$ and $g$, it suffices to show that for any $r \in \poly$ and lens $a \colon r \to p$ satisfying $a \then f = a \then g$, there exists a unique lens $h \colon r \to p'$ for which $a = h \then e$, so that the following diagram commutes.
    \begin{equation*} %\label{eqn.eq_univ_prop}
    \begin{tikzcd}
        p' \ar[r, "e"] & p \ar[r, "f", shift left] \ar[r, "g"', shift right] & q \\
        r \ar[u, "h", dashed] \ar[ur, "a"']
    \end{tikzcd}
    \end{equation*}
    In order for $a = h \then e$ to hold, we must have $a_\1 = h_\1 \then e_\1$ on positions.
    But we have that $a_\1 \then f_\1 = a_\1 \then g_\1$, so by the universal property of $p'(\1)$ and the map $e_\1$ as the equalizer of $f_\1$ and $g_\1$ in $\smset$, there exists a unique $h_\1$ for which $a_\1 = h_\1 \then e_\1$.
    Hence $h$ is uniquely characterized on positions.
    In particular, it must send each $k \in r(\1)$ to $a_\1(k) \in p'(\1)$.

    Then for $a = h \then e$ to hold on directions, we must have that $a^\sharp_k = e^\sharp_{a_\1(k)} \then h^\sharp_k$ for each $k \in r(\1)$.
    But we have that $f^\sharp_{a_\1(k)} \then a^\sharp_{a_\1(k)} = g^\sharp_{a_\1(k)} \then a^\sharp_{a_\1(k)}$, so by the universal property of $p'[a_\1(k)]$ and the map $e^\sharp_{a_\1(k)}$ as the coequalizer of $f^\sharp_{a_\1(k)}$ and $g^\sharp_{a_\1(k)}$ in $\smset$, there exists a unique $h^\sharp_k$ for which $a^\sharp_k = e^\sharp_{a_\1(k)} \then h^\sharp_k$, so that the diagram below commutes.
    \begin{equation*} %\label{eqn.eq_univ_prop_dir}
    \begin{tikzcd}[sep=large]
        p'[a_\1(k)] \ar[d, "h^\sharp_k"', dashed] & p[a_\1(k)] \ar[l, "e^\sharp_{a_\1(k)}"'] \ar[dl, "a^\sharp_k"] & q[f_\1(a_\1(k))] \ar[l, "f^\sharp_{a_\1(k)}"', shift right] \ar[l, "g^\sharp_{a_\1(k)}", shift left] \\
        r[k]
    \end{tikzcd}
    \end{equation*}
    Hence $h$ is also uniquely characterized on directions, so it is unique overall.
    Moreover, we have shown that we can define $h$ on positions so that $a_\1 = h_\1 \then e_\1$, and that we can define $h$ on directions such that $a^\sharp_k = e^\sharp_{a_\1(k)} \then h^\sharp_k$ for all $k \in r(\1)$.
    It follows that there exists $h$ for which $a = h \then e$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{example}[Computing general limits in $\poly$] \label{ex.compute_limits}
The proof of \cref{thm.poly_limits} justifies the following mnemonic for limits in $\poly$:
\slogan{The positions of a limit are the limit of the positions. \\ The directions of a limit are the colimit of the directions.}
We can make this precise as follows: the limit of a functor $p_-\colon\cat{J}\to\poly$ is the polynomial whose position-set is
\begin{equation} \label{eqn.lim_pos}
    \left(\lim_{j\in\cat{J}} p_j\right)(\1) \iso \lim_{j\in\cat{J}} p_j(\1),
\end{equation}
equipped with a canonical projection $\pi_j$ to each $p_j(\1)$, and whose direction-set for each position $i$ is
\begin{equation} \label{eqn.lim_dir}
    \left(\lim_{j\in\cat{J}} p_j\right)[i] \iso \colim_{j\in\cat{J}\op} p_j[\pi_j(i)].
\end{equation}
This notation obscures what is occuring on lenses, but in particular, each lens $\varphi\colon p_j\to p_{j'}$ in the diagram $p_-$ induces an on-positions function $\varphi_\1\colon p_j(\1)\to p_{j'}(\1)$ in the diagram whose limit we take in \eqref{eqn.lim_pos} and, for every position $i$ of the limit, an on-directions function $\varphi^\sharp_{\pi_j(i)}\colon p_{j'}[\pi_{j'}(i)]\to p_j[\pi_j(i)]$ in the diagram whose colimit we take in \eqref{eqn.lim_dir}.
(Note that, by the definition of a limit, $\varphi_\1(\pi_j(i)) = \pi_{j'}(i)$.)

We have seen \eqref{eqn.lim_pos} and \eqref{eqn.lim_dir} to be true for products: the position-set of the product is just the product of the original position-sets, while the direction-set at a tuple of the original positions is just the coproduct of the direction-sets at every position in the tuple.
We have also just shown \eqref{eqn.lim_pos} and \eqref{eqn.lim_dir} to be true for equalizers in the proof of \cref{thm.poly_limits}.
It follows from the construction of any limit as an equalizer of products that it is true for arbitrary limits.
\end{example}

\begin{example}[Pullbacks in $\poly$]\label{ex.pullbacks_in_poly}
Given $q,q',r \in \poly$ and lenses $q\To{f} r\From{f'} q'$, the pullback
\[
\begin{tikzcd}
	p\ar[r, "g'"]\ar[d, "g"']&
	q'\ar[d, "f'"]\\
	q\ar[r, "f"']&
	r\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
is given as follows.
The position-set of $p$ is the pullback of the position-sets of $q$ and $q'$ over that of $r$ in $\smset$.
Then at each position $(i, i') \in p(\1) \ss q(\1) \times q'(\1)$ with $f_\1(i)=f'_\1(i')$, we take the direction-set $p[(i, i')]$ to be the pushout of the direction-sets $q[i]$ and $q'[i']$ over $r[f_\1(i)]=r[f_\1'(i')]$ in $\smset$.
These pullback and pushout squares also give the lenses $g$ and $g'$ on positions and on directions:
\begin{equation}\label{eqn.pullback_poly}
\begin{tikzcd}
	p(\1)\ar[r, "g'_\1"]\ar[d, "g_\1"']&
	q'(\1)\ar[d, "f_\1'"]\\
	q(\1)\ar[r, "f_\1"']&
	r(\1)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\qqand
\begin{tikzcd}
	p[(i,i')]\ar[from=r, "(g')^\sharp_{(i,i')}"']\ar[from=d, "g^\sharp_{(i,i')}"]&
	q'[i']\ar[from=d, "(f')^\sharp_{i'}"']\\
	q[i]\ar[from=r, "f^\sharp_i"]&
	r[f_1(i)]\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\end{equation}
\end{example}

\begin{exercise}
Let $p$ be any polynomial.
\begin{enumerate}
	\item There is a canonical choice of lens $\eta\colon p\to p(\1)$; what is it?
	\item Given an element $i\in p(\1)$, i.e.\ a function (or lens between constant polynomials) $i\colon\1\to p(\1)$, let $p_i$ be the pullback
	\[
	\begin{tikzcd}
	p_i\ar[r, "g"]\ar[d, "f"']&
	p\ar[d, "\eta"]\\
	\1\ar[r, "i"']&
	p(\1)\ar[ul, phantom, very near end, "\lrcorner"]
	\end{tikzcd}
	\]
	What is $p_i$? What are the lenses $f \colon p_i \to \1$ and $g \colon p_i \to p$? \qedhere
\end{enumerate}
\begin{solution}
Here $p \in \poly$.
\begin{enumerate}
    \item The canonical lens $\eta \colon p \to p(\1)$ is the identity $\eta_\1 \colon p(\1) \to p(\1)$ on positions and the empty function on directions.

    \item On positions, we have that $p_i(\1)$ along with $f_\1$ and $g_\1$ form the following pullback square in $\smset$:
    \[
	\begin{tikzcd}
    	p_i(\1) \ar[r, "g_\1"] \ar[d, "f_\1"'] &
    	p(\1) \ar[d, equals] \\
    	\1 \ar[r, "i"'] &
    	p(\1) \ar[ul, phantom, very near end, "\lrcorner"]
	\end{tikzcd}
	\]
	So $p_i(\1) \coloneqq \{(a, i') \in \1 \times p(\1) \mid i = i' \} = \{(1, i)\}$, with $f_\1$ uniquely determined and $g_1$ picking out $i \in p(\1)$.
	Then on directions, we have that $p_i[(1,i)]$ along with $f^\sharp_{(1,i)}$ and $g^\sharp_{(1,i)}$ form the following pushout square in $\smset$:
	\[
	\begin{tikzcd}
    	p_i[(1,i)] \ar[from=r, "g^\sharp_{(1,i)}"'] \ar[from=d, "f^\sharp_{(1,i)}"] &
    	p[i] \ar[from=d, "!"'] \\
    	\0 \ar[from=r, "!"] &
    	\0 \ar[ul, phantom, very near end, "\lrcorner"]
    \end{tikzcd}
    \]
    So $p_i[(1,i)] \coloneqq p[i]$, with $f^\sharp_{(1,i)}$ uniquely determined and $g^\sharp_{(1,i)}$ as the identity.
    It follows that $p_i \coloneqq \{(1,i)\}\yon^{p[i]} \iso \yon^{p[i]}$, where $f \colon p_i \to \1$ is uniquely determined and $g \colon p_i \to p$ picks out $i \in p(\1)$ on positions and is the identity on $p[i]$ on directions.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Let $q\coloneqq \yon^\2+\yon$, $q'\coloneqq\2\yon^\3+\yon^\2$, and $r\coloneq\yon+\1$.
\begin{enumerate}
	\item Choose lenses $f\colon q\to r$ and $f'\colon q'\to r$ and write them down.
	\item Find the pullback of $q\To{f} r\From{f'} q'$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item There are many possible answers, but one lens $f \colon q \to r$, on positions, sends $1 \in q(\1)$ (corresponding to $\yon^\2$) to $2 \in r(\1)$ (corresponding to $\1$) and $2 \in q(\1)$ (corresponding to $\yon$) to $1 \in r(\1)$ (corresponding to $\yon$).
    Then the on-directions functions $f^\sharp_\1 \colon \0 \to \2$ and $f^\sharp_2 \colon \1 \to \1$ are uniquely determined.
    Another morphism $f' \colon q' \to r$, on positions, sends $1 \in q'(\1)$ (corresponding to one of the $\yon^\3$ terms) to $2 \in r(\1)$ and both $2 \in q'(\1)$ (corresponding to the other $\yon^\3$ term) and $3 \in q'(\1)$ (corresponding to the $\yon^\2$ term) to $1 \in r(\1)$.
    Then the on-directions function $(f')^\sharp_1 \colon \0 \to \3$ is uniquely determined, while we can let $(f')^\sharp_2 \colon \1 \to \3$ pick out $3$ and $(f')^\sharp_3 \colon \1 \to \2$ pick out $1$.

    \item We compute the pullback $p$ along with the lenses $g \colon p \to q$ and $g' \colon p \to q'$ of $q\To{f} r\From{f'} q'$ by following \cref{ex.pullbacks_in_poly}.
    We can compute $p(\1)$ by taking the pullback in $\smset$:
    \[
        p(\1) \coloneqq \{(i, i') \in \2 \times \3 \mid f_\1(i) = f'_\1(i)\} = \{(1,1), (2,2), (2,3)\}.
    \]
    Moreover, the on-positions functions $g_\1$ and $g'_\1$ send each pair in $p(\1)$ to its left component and its right component, respectively.

    To compute the direction-set at each $p$-position, we must compute a pushout.
    At $(1,1)$, we have $r[f_\1(1)] = r[f'_\1(1)] = r[2] = \0$, so the pushout $p[(1,1)]$ is just the sum $q[1] + q'[1] = \2 + \3 \iso \5$.
    Moreover, the on-directions functions $g^\sharp_{(1,1)}$ and $(g')^\sharp_{(1,1)}$ are the canonical inclusions $\2 \to \2 + \3$ and $\3 \to \2 + \3$.

    At $(2,2)$, we have $r[f_\1(2)] = r[f'_\1(2)] = r[1] = \1$, with $f^\sharp_2$ picking out $1 \in \1 = q[2]$ and $(f')^\sharp_2$ picking out $3 \in \3 = q'[2]$.
    So the pushout $p[(2,2)]$ is the set $\1 + \3 = \{(1,1), (2,1), (2,2), (2,3)\}$ but with $(1,1)$ identified with $(2,3)$; we can think of it as the set of equivalence classes $p[(2,2)] \iso \{\{(1,1), (2,3)\}, \{(2,1)\}, \{(2,2)\}\} \iso \3$.
    Moreover, the on-directions function $g^\sharp_{(2,2)}$ maps $1 \mapsto \{(1,1), (2,3)\}$, while the on-directions function $(g')^\sharp_{(2,2)}$ maps $1 \mapsto \{(2,1)\}, 2 \mapsto \{(2,2)\},$ and $3 \mapsto \{(1,1), (2,3)\}$.

    Finally, at $(2,3)$, we have $r[f_\1(2)] = r[f'_\1(3)] = r[1] = \1$, with $f^\sharp_2$ still picking out $1 \in \1 = q[2]$ and $(f')^\sharp_3$ picking out $1 \in \2 = q'[3]$.
    So the pushout $p[(2,3)]$ is the set $\1 + \2 = \{(1,1), (2,1), (2,2)\}$ but with $(1,1)$ identified with $(2,1)$; we can think of it as the set of equivalence classes $p[(2,3)] \iso \{\{(1,1), (2,1)\}, \{(2,2)\}\} \iso \2$.
    Moreover, the on-directions function $g^\sharp_{(2,3)}$ maps $1 \mapsto \{(1,1), (2,1)\}$, while the on-directions function $(g')^\sharp_{(2,3)}$ maps $1 \mapsto \{(1,1), (2,1)\}$ and $2 \mapsto \{(2,2)\}$.

    It follows that $p \iso \yon^\5 + \yon^\3 + \yon^\2$, with $g$ and $g'$ as described.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise} \label{exc.refl_limits}
An alternative way to prove \cref{thm.poly_limits} would have been to show that the equalizer of two natural transformations between polynomial functors in $\smset^\smset$ is still a polynomial functor---since the full subcategory inclusion $\poly \to \smset^\smset$ reflects these equalizers, it would follow that $\poly$ has equalizers.
But we already know what polynomial the equalizer should be from the proof of \cref{thm.poly_limits}.
So in this exercise, we will show that the equalizer of polynomials we found in $\poly$ is also the equalizer of those same functors in $\smset^\smset$.

Let $f,g\colon p\tto q$ be a pair of natural transformations $f,g\colon p\tto q$ between polynomial functors $p$ and $q$, and let $e\colon p'\to p$ be their equalizer in $\poly$ that we computed in the proof of \cref{thm.poly_limits}.
\begin{enumerate}
    \item Given a set $X$, show that $e_X\colon p'(X)\to p(X)$ is the equalizer of the $X$-components $f_X,g_X\colon p(X)\tto q(X)$ in $\smset$.
    \item Deduce that equalizers in $\poly$ coincide with equalizers in $\smset^\smset$.
    \item Conclude that limits in $\poly$ coincide with limits in $\smset^\smset$. \qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item By \cref{prop.morph_arena_to_func}, $f_X$ (resp.\ $g_X$) sends each $(i,h)\in p(X)$ with $i\in p(\1)$ and $h\colon p[i]\to X$ to $(f_\1(i), f^\sharp_i\then h)$ (resp.\ $(g_\1(i), g^\sharp_i\then h)$) in $q(X)$.
    So the equalizer of $f_X$ and $g_X$ is the set of all $(i,h)\in p(X)$ for which both $f_\1(i) = g_\1(i)$ and $f^\sharp_i\then h = g^\sharp_i\then h$.

    Indeed, by our construction of $p'$, the set $p'(X)$ consists of all pairs $(i,h')$ with $i\in p(\1)$ such that $f_\1(i) = g_\1(i)$ and $h'\colon p'[i]\to X$, where $p'[i]$ is the coequalizer of $f^\sharp_i,g^\sharp_i\colon q[f_\1(i)]\tto p[i]$.
    By the universal property of the coequalizer, functions $h'\colon p'[i]\to X$ precisely correspond to functions $h\colon p[i]\to X$ for which $f^\sharp_i\then h = g^\sharp_i\then h$.
    So $p'(X)$ is indeed the equalizer of $f_X$ and $g_X$.

    The equalizer natural transformation $e'\colon p'\to p$ has the inclusion $e'_X\colon p'(X)\to p(X)$ as its $X$-component, so by \cref{cor.morph_func_to_arena}, it is the lens whose on-positions function is the canonical equalizer inclusion $e'_\1\colon p'(\1)\to p(\1)$, while its on-directions function at $i\in p'(\1)$ is the map $p[i]\to p'[i]$ corresponding to the identity on $p'[i]$ given by the universal property of the coequalizer---which is just the canonical coequalizer map $p[i]\to p'[i]$.
    But this is exactly the lens $e\colon p'\to p$ constructed in the proof of \cref{prop.poly_prods}, as desired.
    \item By \cref{prop.presheaf_lim_ptwise}, limits---including equalizers---in $\smset^\smset$ are computed pointwise.
    So if $e_X\colon p'(X)\to p(X)$ is the equalizer of $f_X,g_X\colon p(X)\tto q(X)$ for every $X\in\smset$, then $e\colon p'\to p$ is the equalizer of $f,g\colon p(X)\tto q(X)$.
    \item We have just shown that equalizers in $\poly$ coincide with equalizers in $\smset^\smset$.
    We saw in the proof of \cref{prop.poly_prods} that products in $\poly$ also coincide with products in $\smset^\smset$.
    Since every limit can be computed as an equalizer of products, we can conclude that limits in $\poly$ coincide with limits in $\smset^\smset$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{theorem}\label{thm.poly_colimits}
The category $\poly$ has all small colimits.
\end{theorem}
\begin{proof}
A category has all small colimits if and only if it has coproducts and coequalizers, so by \cref{prop.poly_coprods}, it suffices to show that $\poly$ has coequalizers.

Let $s,t \colon p \tto q$ be two lenses.
We construct the coequalizer $q'$ of $s$ and $t$ as follows.
The pair of functions $s_\1, t_\1 \colon p(\1) \tto q(\1)$ define a graph $G \colon \fbox{$\bullet\tto\bullet$} \to \smset$ with vertices in $q(\1)$, edges in $p(\1)$, sources indicated by $s_\1$, and targets indicated by $t_\1$.
Then the set $C$ of connected components of $G$ is given by the coequalizer $g_\1 \colon q(\1) \to C$ of $s_\1$ and $t_\1$.
We define the position-set of $q'$ to be $C$.
Each direction-set of $q'$ will be a limit of a diagram of direction-sets of $p$ and $q$, but expressing this limit, as we proceed to do, is a bit involved.

For each connected component $c \in C$, we have a connected subgraph $G_c \ss G$ with vertices $V_c \coloneqq g_\1\inv(c)$ and edges $E_c \coloneqq s_\1\inv(g_\1\inv(c)) = t_\1\inv(g_\1\inv(c))$.
Note that $E_c\ss p(\1)$ and $V_c\ss q(\1)$, so to each $e\in E_c$ (resp.\ to each $v\in V_c$) we have an associated direction-set $p[e]$ (resp.\ $q[v]$).

The category of elements $\int G_c$ has objects $E_c+V_c$ and two kinds of (non-identity) morphisms, $e \to s_\1(e)$ and $e \to t_\1(e)$, associated to each $e \in E_c$, all pointing from an object in $E_c$ to an object in $V_c$.
There is a functor $F \colon (\int G_c)\op \to \smset$ sending every $v \mapsto q[v]$, every $e \mapsto p[e]$, and every morphism to a function between them, namely either $s^\sharp_e \colon q[s_\1(e)] \to p[e]$ or $t^\sharp_e \colon q[t_\1(e)] \to p[e]$.
So we can define $q'[c]$ to be the limit of $F$ in $\smset$.

We claim that $q'\coloneqq\sum_{c\in C}\yon^{q'[c]}$ is the coequalizer of $s$ and $t$. We leave the complete proof to the interested reader in \cref{exc.poly_colimits}.
\end{proof}

\begin{exercise}\label{exc.poly_colimits}
Complete the proof of \cref{thm.poly_colimits} as follows:
\begin{enumerate}
	\item Provide a lens $g \colon q \to q'$.
	\item Show that $s \then g = t \then g$.
	\item Show that $g$ is a coequalizer of the pair $s, t$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We define a lens $g \colon q \to q'$ as follows.
    The on-positions function $g_\1 \colon q(\1) \to q'(\1)$ is the coequalizer of $s_\1, t_\1 \colon p(\1) \tto q(\1)$.
    In particular, $g_\1$ sends each vertex in $q(\1)$ to its corresponding connected component in $q'(\1) = C$.
    Then for each $v \in q(\1)$, if we let its corresponding connected component be $c \coloneqq g_\1(v)$, we can define the on-directions function $g^\sharp_v \colon q'[c] \to q[v]$ to be the projection from the limit $q'[c]$ to its component $q[v]$.

    \item To show that $s \then g = t \then g$, we must show that both sides are equal on positions and on directions.
    The on-positions function $g_\1$ is defined to be the coequalizer of $s_\1$ and $t_\1$, so $s_\1 \then g_\1 = t_\1 \then g_\1$.
    So it suffices to show that for all $e \in p(\1)$, if we let its corresponding connected component be $c \coloneqq g_\1(s_\1(e)) = g_\1(t_\1(e))$, then the following diagram of on-directions functions commutes:
    \[
    \begin{tikzcd}[sep=small]
        & q[s_\1(e)] \ar[dl, "s^\sharp_e"'] \\
        p[e] & & q'[c] \ar[ul, "g^\sharp_{s_\1(e)}"'] \ar[dl, "g^\sharp_{t_\1(e)}"] \\
        & q[t_\1(e)] \ar[ul, "t^\sharp_e"]
    \end{tikzcd}
    \]
    But this is automatically true by the definition of $q'[c]$ as a limit---specifically the limit of a functor with $s^\sharp_e$ and $t^\sharp_e$ in its image---and the definitions of $g^\sharp_{s_\1(e)}$ and $g^\sharp_{t_\1(e)}$ as projections from this limit.

    \item To show that $g$ is the coequalizer of $s$ and $t$, it suffices to show that for any $r \in \poly$ and lens $f \colon q \to r$ satisfying $s \then f = t \then f$, there exists a unique lens $h \colon q' \to r$ for which $f = g \then h$, so that the following diagram commutes.
    \begin{equation*} %\label{eqn.eq_univ_prop}
    \begin{tikzcd}
        p \ar[r, "s", shift left] \ar[r, "t"', shift right] & q \ar[r, "g"] \ar[dr, "f"'] & q' \ar[d, "h", dashed] \\
        & & r
    \end{tikzcd}
    \end{equation*}
    In order for $f = g \then h$ to hold, we must have $f_\1 = g_\1 \then h_\1$ on positions.
    But we have that $s_\1 \then f_\1 = t_\1 \then f_\1$, so by the universal property of $q'(\1)$ and the map $g_\1$ as the coequalizer of $s_\1$ and $t_\1$ in $\smset$, there exists a unique $h_\1$ for which $f_\1 = g_\1 \then h_\1$.
    Hence $h$ is uniquely characterized on positions.
    In particular, it must send each connected component $c \in q'(\1)$ to the element in $r(\1)$ to which $f_\1$ sends every vertex $v \in V_c = g_\1\inv(c)$ that lies in the connected component $c$.

    Then for $f = g \then h$ to hold on directions, we must have that $f^\sharp_v = h^\sharp_{g_\1(v)} \then g^\sharp_v$ for each $v \in q(\1)$.
    Put another way, given $c \in q'(\1)$, we must have that $f^\sharp_v = h^\sharp_c \then g^\sharp_v$ for every $v \in V_c$.
    But $s \then f = t \then f$ implies that for each $e \in E_c = s_\1\inv(g_\1\inv(c)) = t_\1\inv(g_\1\inv(c)) \ss p(\1)$, the following diagram of on-directions functions commutes:
    \[
    \begin{tikzcd}[sep=small]
        & q[s_\1(e)] \ar[dl, "s^\sharp_e"'] \\
        p[e] & & r[f_\1(v)] \ar[ul, "f^\sharp_{s_\1(e)}"'] \ar[dl, "f^\sharp_{t_\1(e)}"] \\
        & q[t_\1(e)] \ar[ul, "t^\sharp_e"]
    \end{tikzcd}
    \]
    It follows that $r[f_\1(v)]$ together with the maps $(f^\sharp_v)_{v \in V_c}$ form a cone over the functor $F$.
    So by the universal property of the limit $q'[c]$ of $F$ with projection maps $(g^\sharp_v)_{v \in V_c}$, there exists a unique $h^\sharp_c \colon r[f_\1(v)] \to q'[c]$ for which $f^\sharp_v = h^\sharp_c \then g^\sharp_v$ for every $v \in V_c$.
    Hence $h$ is also uniquely characterized on directions, so it is unique overall.
    Moreover, we have shown that we can define $h$ on positions so that $f_\1 = g_\1 \then h_\1$, and that we can define $h$ on directions such that $f^\sharp_v = h^\sharp_c \then g^\sharp_v$ for all $c \in q'(\1)$ and $v \in V_c$.
    It follows that there exists $h$ for which $f = g \then h$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{example}
Given a diagram in $\poly$, one could either take its (co)limit as a diagram of \emph{polynomial} functors (i.e.\ its (co)limit in $\poly)$ or its (co)limit simply as a diagram of functors (i.e.\ its (co)limit in $\smset^\smset$).
We saw in \cref{exc.refl_limits} that in the case of limits, these yield the same result.
So, too, in the case of coproducts, per \cref{prop.poly_coprods}.

But in the case of general colimits, there are diagrams that yield different results: by the co-Yoneda lemma, \emph{every} functor $\smset \to \smset$---even those that are not polynomials---can be written as the colimit of representable functors in $\smset^\smset$, yet the colimit of the same representables in $\poly$ can only be another polynomial.

For a concrete example, consider the two distinct projections $\yon^\2\to\yon$, which form the diagram
\begin{equation} \label{eqn.2_projs}
    \yon^\2\tto\yon.
\end{equation}
According to \cref{thm.poly_colimits}, the colimit of \eqref{eqn.2_projs} in $\poly$ has the coequalizer of $\1 \tto \1$, namely $\1$, as its position-set, and the limit of the diagram $\1 \tto \2$ consisting of the two inclusions as its sole direction-set.
But this latter limit is just $\0$, so in fact the colimit of \eqref{eqn.2_projs} in $\poly$ is the constant functor $\1\yon^\0\iso\1$.

But as functors, by \cref{prop.presheaf_lim_ptwise}, the colimit of \eqref{eqn.2_projs} can be computed pointwise: it is the (nonconstant!) functor
\[
  X\mapsto
  \begin{cases}
  	\0&\tn{ if }X=\0\\
  	\1&\tn{ if }X\neq\0
  \end{cases}
\]
\end{example}

\begin{exercise}
By \cref{prop.adjoint_quadruple}, for any polynomial $p$, there are canonical lenses
\[
	\epsilon \colon p(\1)\yon\to p
	\qqand
	\eta \colon p\to \yon^{\Gamma(p)}.
\]
\begin{enumerate}
	\item Characterize the behavior of the canonical lens $\epsilon \colon p(\1)\yon\to p$.
	\item Characterize the behavior of the canonical lens $\eta \colon p\to \yon^{\Gamma(p)}$.
	\item Show that the following is a pushout in $\poly$:
    \begin{equation} \label{eqn.pushout_adjoint}
    \begin{tikzcd}
    	p(\1)\yon\ar[r, "!"]\ar[d, "\epsilon"']&
    	\yon\ar[d, "!"]\\
    	p\ar[r, "\eta"']&
    	\yon^{\Gamma(p)}\ar[ul, phantom, very near start, "\ulcorner"]
    \end{tikzcd}
    \end{equation}
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We characterize the lens $\epsilon \colon p(\1)\yon \to p$ as follows.
    On positions, it is the identity on $p(\1)$.
    Then for each $i \in p(\1)$, on directions, it is the unique map $p[i] \to \1$.

    \item We characterize the lens $\eta \colon p \to \yon^{\Gamma(p)}$ as follows.
    On positions, it is the unique map $p(\1) \to \1$.
    Then for each $i \in p(\1)$, on directions, it is the canonical projection $\Gamma(p) \iso \prod_{i' \in p(\1)} p[i'] \to p[i]$.

    \item Showing that \eqref{eqn.pushout_adjoint} is a pushout square is equivalent to showing that, in the diagram
    \begin{equation} \label{eqn.coeq_adjoint}
    \begin{tikzcd}[sep=large]
        & \yon \ar[d, "\iota"] \ar[dr, "!"] \\
        p(\1)\yon \ar[ur, "!"] \ar[dr, "\epsilon"'] \ar[r, "s", shift left] \ar[r, "t"', shift right] & \yon + p \ar[r, "g"] & \yon^{\Gamma(p)} \\
        & p \ar[u, "\iota'"'] \ar[ur, "\eta"']
    \end{tikzcd}
    \end{equation}
    in which $\iota, \iota'$ are the canonical inclusions and the four triangles commute, $\yon^{\Gamma(p)}$ equipped with the lens $g$ is the coequalizer of $s$ and $t$.
    To do so, we apply \cref{thm.poly_colimits} to compute the coequalizer $q'$ of $s$ and $t$.
    The position-set of $q'$ is the coequalizer of $s_\1 = (! \then \iota)_\1$, which sends every $i \in p(\1)$ to the position of $\yon + p$ corresponding to the summand $\yon$, and $t_\1 = (\epsilon \then \iota')_\1$, which sends each $i \in p(\1)$ to the corresponding position in the summand $p$ of $\yon + p$.
    It follows that the coequalizer of $s_\1$ and $t_\1$ is $\1$, so $q'(\1) \iso \1$.

    Then the direction-set of $q'$ at its sole position is the limit of the functor $F$ whose image consists of lenses of the form $\1 \to \1$ or $p[i] \to \1$ for every $i \in p(\1)$.
    It follows that the limit of $F$ is just a product, namely $\prod_{i \in p(\1)} p[i] \iso \Gamma(p)$.
    Hence $q' \iso \yon^{\Gamma(p)}$, as desired.

    It remains to check that the upper right and lower right triangles in \eqref{eqn.coeq_adjoint} commute.
    The upper right triangle must commute by the uniqueness of morphisms $\yon \to \yon^{\Gamma(p)}$; and the lower right triangle must commute on positions.
    Moreover, the on-directions function of the coequalizer morphism $g$ at each position $i \in p(\1) \ss (\yon+p)(\1)$ must be the canonical projection $\Gamma(p) \to p[i]$, which matches the behavior of the corresponding on-directions function of $\eta$; hence the lower right triangle also commutes on directions.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.tensor_as_pushout}
For polynomials $p,q$, the following is a pushout:
\[
\begin{tikzcd}
	p(\1)\yon\otimes q(\1)\yon\ar[r]\ar[d]&
	p(\1)\yon\otimes q\ar[d]\\
	p\otimes q(\1)\yon\ar[r]&
	p\otimes q\ar[ul, phantom,very near start, "\ulcorner"]
\end{tikzcd}
\]
\end{proposition}
\begin{proof}
All the lenses shown are identities on positions, so the displayed diagram is the coproduct over all $(i,j)\in p(\1)\times q(\1)$ of the diagram shown left
\[
\begin{tikzcd}
	\yon\ar[r]\ar[d]&
	\yon^{q[j]}\ar[d]\\
	\yon^{p[i]}\ar[r]&
	\yon^{p[i]\times q[j]}\ar[ul, phantom, very near start, "\ulcorner"]
\end{tikzcd}
\begin{tikzcd}
	\1\ar[dr, phantom, very near end, "\ulcorner"]&
	q[j]\ar[l]\\
	p[i]\ar[u]&
	p[i]\times q[j]\ar[l]\ar[u]
\end{tikzcd}
\]
where we used $(p\otimes q)[(i,j)]\cong p[i]\times q[j]$. This is the image under the Yoneda embedding of the diagram of sets shown right, which is clearly a pullback. The result follows by \cref{prop.yoneda_left_adjoint}.
\end{proof}

This means that to give a lens $\varphi\colon p\otimes q\to r$, it suffices to give two lenses $\varphi_p\colon p\otimes q(\1)\yon\to r$ and $\varphi_q\colon p(\1)\yon\otimes q\to r$ that agree on positions. The lens $\varphi_p$ says how information about $q$'s position is transferred to $p$, and the lens $\varphi_q$ says how information about $p$'s position is transferred to $q$.

\begin{corollary}\label{cor.tensor_as_pushout}
Suppose we have polynomials $p_1,\ldots,p_n\in\poly$. Then $p_1\otimes\cdots\otimes p_n$ is isomorphic to the wide pushout
\[
  \colim
  \left(
  \begin{tikzcd}[column sep=-10pt]
  	&
		p_1(\1)\yon\otimes\cdots\otimes  p_n(\1)\yon\ar[dl]\ar[dr]\\
		p_1\otimes p_2(\1)\yon\otimes\cdots\otimes p_n(\1)\yon&
		\cdots&
		p_1(\1)\yon\otimes\cdots\otimes p_{n-1}(\1)\yon \otimes p_n
  \end{tikzcd}
  \right)
\]
\end{corollary}
\begin{proof}
We proceed by induction on $n\in\nn$.
When $n=0$, the wide pushout has no legs and the empty parallel product is $\yon$, so the result holds.
If the result holds for $n$, then it holds for $n+1$ by \cref{prop.tensor_as_pushout}.
\end{proof}

%-------- Section --------%
\section{Vertical-cartesian factorization of lenses}

Aside from epi-mono factorization, there is another factorization system on $\poly$ that will show up frequently.

\begin{definition}[Vertical and cartesian lenses] \label{def.vert_cart}
Let $f\colon p\to q$ be a lens.
It is called \emph{vertical} if $f_\1\colon p(\1)\to q(\1)$ is an isomorphism.
It is called \emph{cartesian} if, for each $i\in p(\1)$, the function $f^\sharp_i\colon q[f(i)]\to p[i]$ is an isomorphism.
\end{definition}

\begin{proposition}\label{prop.vert_cart_factorization}
Vertical and cartesian lenses form a factorization system of $\poly$.
\end{proposition}
\begin{proof}
It is easy to check that isomorphisms are both vertical and cartesian, and that vertical and cartesian lenses are each closed under composition.
It remains to show that every lens in $\poly$ can be uniquely (up to unique isomorphism) factored as a vertical lens composed with a cartesian lens.

Recall from \eqref{eqn.colax_poly_map} that a lens in $\poly$ can be written as to the left; we can thus rewrite it as to the right:
\[
\begin{tikzcd}[column sep=small]
	p(\1)\ar[dr, bend right, "{p[-]}"']\ar[rr, "f_\1"]&~&
	q(\1)\ar[dl, bend left, "{q[-]}"]\\&
	\smset\ar[u, phantom, near end, "\overset{f^\sharp}{\Leftarrow}"]
\end{tikzcd}
\hspace{1in}
\begin{tikzcd}
	p(\1)\ar[dr, bend right, "{p[-]}"']\ar[r, equal, ""' name=equal]&
	p(\1)\ar[d, "{q[f_\1(-)]}"]\ar[r, "f_\1"]&
	q(\1)\ar[dl, bend left, "{q[-]}"]\\&
	|[alias=set]|\smset\ar[from=equal, to=set, pos=.3, phantom, "\overset{f^\sharp}{\Leftarrow}"]
\end{tikzcd}
\]
We can see that the intermediary object $\sum_{i\in p(\1)} \yon^{q[f_\1(i)]}$ is unique up to unique isomorphism.
\end{proof}

\begin{proposition}
Vertical lenses satisfy 2-out-of-3: given $p\To{f}q\To{g}r$ with $h = f \then g$, if any two of $f,g,h$ are vertical, then so is the third.

If $g$ is cartesian, then $h$ is cartesian if and only if $f$ is cartesian.
\end{proposition}
\begin{proof}
Given $h = f \then g$, we have that $h_\1 = f_\1 \then g_\1$.
Since isomorphisms satisfy 2-out-of-3, it follows that vertical lenses satisfy 2-out-of-3 as well.

Now assume $g$ is cartesian.
On directions, $h = f \then g$ implies that for every $i \in p(\1)$, we have $h^\sharp_i = g^\sharp_{f_\1(i)} \then f^\sharp_i$.
Since $g^\sharp_{f_\1(i)}$ is an isomorphism, it follows that every $h^\sharp_i$ is an isomorphism if and only if every $f^\sharp_i$ is an isomorphism, so $h$ is cartesian if and only if $f$ is cartesian.
\end{proof}

\begin{exercise}
Give an example of polynomials $p,q,r$ and lenses $p\To{f}q\To{g}r$ such that $f$ and $f \then g$ are cartesian but $g$ is not.
\begin{solution}
Consider the lenses $\yon \To{f} \yon^\2 + \yon \To{g} \yon$ where $f$ is the canonical inclusion and $g$ is uniquely determined on positions and picks out $1 \in \2$ and $1 \in \1$ on directions.
Then the only on-directions function of $f$ is a function $\1 \to \1$, an isomorphism, so $f$ is cartesian.
Meanwhile, one of the on-directions functions of $g$ is a function $\1 \to \2$, which is not an isomorphism, so $g$ is not cartesian.
Finally, $f \then g$ can only be the unique lens $\yon \to \yon$, namely the identity, which is cartesian.
\end{solution}
\end{exercise}

Here is an alternative characterization of a cartesian lens in $\poly$.
Recall from \cref{exc.deriv_directions} that for any polynomial $p$, there is a corresponding function $\pi_p\colon\dot{p}(\1)\to p(\1)$, i.e.\ the set of all directions mapping to the set of positions.
A lens $(f_\1,f^\sharp)\colon p\to q$ can then be described as a function $f_\1\colon p(\1)\to q(\1)$ along with a function $f^\sharp$ that makes the following diagram in $\smset$ commute:
\begin{equation}\label{eqn.poly_map_usu}
\begin{tikzcd}
	\dot{p}(\1)\ar[d, "\pi_p"']&
	\bullet\ar[l, "f^\sharp"']\ar[r]\ar[d]&
	\dot{q}(\1)\ar[d, "\pi_q"]\\
	p(\1)\ar[r, equal]&
	p(\1)\ar[r, "f_\1"']&
	q(\1)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\end{equation}
Here, the pullback denoted by the dot $\bullet$ is the set of pairs comprised of a $p$-position $i$ and a $q[f_\1(i)]$-direction $e$.
The function $f^\sharp$ sends each such pair to a direction $f^\sharp_i(e)$ of $p$, and the commutativity of the left square implies that $f^\sharp_i(e)$ is specifically a $p[i]$-direction.
So $f^\sharp_i$ is indeed our familiar on-directions function $q[f_\1(i)]\to p[i]$, and $f^\sharp$ is just the sum of all these on-directions functions over $i\in p(\1)$.

\begin{exercise} \label{exc.cart_pullbacks}
Show that a lens $f\colon p\to q$ in $\poly$ is cartesian if and only if the square on the left hand side of \eqref{eqn.poly_map_usu} is also a pullback:
\[
\begin{tikzcd}
	\dot{p}(\1)\ar[d, "\pi_p"']&
	\bullet\ar[l, "f^\sharp"']\ar[r]\ar[d]&
	\dot{q}(\1)\ar[d, "\pi_q"]\\
	p(\1)\ar[r, equal]\ar[ur, phantom, very near end, "\llcorner"]&
	p(\1)\ar[r, "f_\1"']&
	q(\1)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
\begin{solution}
We wish to show that a lens $f\colon p\to q$ in $\poly$ is cartesian if and only if the square on the left hand side of \eqref{eqn.poly_map_usu} is a pullback.
We already know that that square commutes, so it is a pullback if and only if $f^\sharp$ is an isomorphism.
The right pullback square tells us that the $\bullet$ is $\sum_{i\in p(\1)}q[f_\1(i)]$.
So $f^\sharp_i\colon q[f_\1(i)]\to p[i]$ is an isomorphism for every $i\in p(\1)$ if and only if their sum $f^\sharp\colon\sum_{i\in p(\1)}q[f_\1(i)]\to\sum_{i\in p(\1)}p[i]\iso\dot{p}(\1)$ is an isomorphism as well.
Hence $f$ is cartesian if and only if $f^\sharp$ is an isomorphism, as desired.
\end{solution}
\end{exercise}

\begin{exercise}
Is the pushout of a cartesian lens always cartesian?
\begin{solution}
The pushout of a cartesian lens is \emph{not} necessarily cartesian.
Take the pushout square \eqref{eqn.pushout_adjoint}.
The lens $!\colon p(\1)\yon\to\yon$ has $\1\to\1$ as every on-directions function, so it is cartesian, but its pushout $\eta\colon p\to\yon^{\Gamma(p)}$ is not going to be cartesian as long as there is some $i\in p(\1)$ for which $\Gamma(p)\not\iso p[i]$.
For instance, when $p\coloneqq\yon+\1$, we have that $\Gamma(p)\iso\0\not\iso\1\iso p[1]$, so $\eta$ is not cartesian.
\end{solution}
\end{exercise}

Why do we use the word \emph{cartesian} to describe cartesian morphisms? It turns out that, as natural transformations, cartesian morphisms are precisely what are known as cartesian natural transformations.

\begin{definition}[Cartesian natural transformation] \label{def.cart_nat_trans}
A \emph{cartesian natural transformation} is a natural transformation whose naturality squares are all pullbacks.
That is, given categories $\cat{C},\cat{D}$, functors $F,G$, and natural transformation $\alpha$, we say that $\alpha$ is \emph{cartesian} if for all morphisms $h\colon c\to c'$ in $\cat{C}$,
\[
\begin{tikzcd}
    Fc \ar[d, "Fh"'] \ar[r, "\alpha_c"] & Gc \ar[d, "Gh"] \\
    Fd \ar[r, "\alpha_d"'] & Gd \ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
is a pullback.
\end{definition}

\begin{proposition}\label{prop.cart_as_nt}
Let $f\colon p\to q$ be a morphism in $\poly$. The following are equivalent:
	\begin{enumerate}
		\item viewed as a lens, $f$ is cartesian in the sense of \cref{def.vert_cart}: for each $i\in p(\1)$, the on-directions function $f^\sharp_i$ is a bijection;
		\item the square on the left hand side of \eqref{eqn.poly_map_usu} is also a pullback:
\[
\begin{tikzcd}
	\dot{p}(\1)\ar[d, "\pi_p"']&
	\bullet\ar[l, "f^\sharp"']\ar[r]\ar[d]&
	\dot{q}(\1)\ar[d, "\pi_q"]\\
	p(\1)\ar[r, equal]\ar[ur, phantom, very near end, "\llcorner"]&
	p(\1)\ar[r, "f_\1"']&
	q(\1)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
		\item viewed as a natural transformation, $f$ is cartesian in the sense of \cref{def.cart_nat_trans}: for any sets $A,B$ and function $h\colon A\to B$, the naturality square
\begin{equation} \label{eqn.cart_nt_pullback}
\begin{tikzcd}
	p(A)\ar[r, "f_A"]\ar[d, "p(h)"']&
	q(A)\ar[d, "q(h)"]\\
	p(B)\ar[r, "f_B"']&
	q(B)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\end{equation}
is a pullback.
  \end{enumerate}
\end{proposition}
\begin{proof}
We already showed that the first two are equivalent in \cref{exc.cart_pullbacks}, and we will complete this proof in \cref{exc.cart_as_nt}.
\end{proof}

\begin{exercise} \label{exc.cart_as_nt}
In this exercise, you will complete the proof of \cref{prop.cart_as_nt}.

First, we will show that $1\Rightarrow3$.
In the following, let $f\colon p\to q$ be a cartesian lens in $\poly$ and $h\colon A\to B$ be a function.
\begin{enumerate}
    \item Using \cref{prop.morph_arena_to_func} to translate $f$ from a lens in $\poly$ to a natural transformation and \cref{prop.poly_on_functions} to interpret $q(h)$, characterize the pullback of $p(B)\To{f_B}q(B)\From{q(h)}q(A)$ in $\smset$.
    \item Show that this pullback coincides with the naturality square \eqref{eqn.cart_nt_pullback}, hence proving $1\Rightarrow3$.
\end{enumerate}
Next, we show that $3\Rightarrow1$.
In the following, let $f\colon p\to q$ be a lens in $\poly$ that is cartesian when viewed as a natural transformation, so that \eqref{eqn.cart_nt_pullback} is a pullback for any function $h\colon A\to B$.
Also fix $i\in p(\1)$.
\begin{enumerate}[resume]
    \item Show that the diagram
    \begin{equation} \label{eqn.cart_nt_pullback_cone}
        \begin{tikzcd}[column sep=50pt]
        	\1 \ar[r, "{(f_\1(i),\,\id_{q[f_\1(i)]})}"]\ar[d, "{(i,\,\id_{p[i]})}"']&
        	q(q[f_\1(i)])\ar[d, "q(f^\sharp_i)"]\\
        	p(p[i])\ar[r, "f_{p[i]}"']&
        	q(p[i])\ar[ul, phantom, very near end, "\lrcorner"]
        \end{tikzcd}
    \end{equation}
    commutes.
    Hint: Use \cref{prop.poly_on_functions}, \cref{prop.morph_arena_to_func}, and/or \cref{cor.morph_func_to_arena}.
    \item Apply the universal property of the pullback \eqref{eqn.cart_nt_pullback} to the diagram \eqref{eqn.cart_nt_pullback_cone} above to exhibit an element of $p(q[f_\1(i)])$.
    Conclude from the existence of this element that $f^\sharp_i$ is an isomorphism, hence proving $3\Rightarrow1$.\qedhere
\end{enumerate}
\begin{solution}
First, we will show that $1\Rightarrow3$ in \cref{prop.cart_as_nt}.
Here $f\colon p\to q$ is a cartesian lens in $\poly$ and $h\colon A\to B$ is a function.
\begin{enumerate}
    \item An element of $p(B)$ is a pair comprised of a $p$-position $i$ and a function $k\colon p[i]\to B$, and \cref{prop.morph_arena_to_func} tells us that $f_B\colon p(B)\to q(B)$ sends $(i,k)\mapsto(f_\1(i),f^\sharp_i\then k)$.
    Meanwhile, an element of $q(A)$ is a pair comprised of a $q$-position $j$ and a function $\ell\colon q[j]\to A$, and \cref{prop.poly_on_functions} tells us that $q(h)$ sends $(j,\ell)\mapsto(j,\ell\then h)$.
    So $((i,k),(j,\ell))$ is in the pullback of $p(B)\To{f_B}q(B)\From{q(h)}q(A)$ if and only if $f_\1(i)=j$ and $f^\sharp_i\then k=\ell\then h$.

    As $f$ is cartesian, $f^\sharp_i$ is an isomorphism, so we can rewrite the latter equation as $k=g_i\then\ell\then h$, where $g_i$ is the inverse of $f^\sharp_i$.
    In fact, if we let $\ell'\coloneqq g_i\then\ell$, we observe that the values of $j,k,$ and $\ell$ are all already determined by the values of $i$ and $\ell'$: we have that $j=f_\1(i)$, that $k=\ell'\then h$, and that $\ell=f^\sharp_i\then\ell'$
    It follows that the pullback is equivalently the set of pairs $(i,\ell')$ comprised of a $p$-position $i$ and a function $\ell'\colon p[i]\to A$ (with no other restrictions on $i$ and $\ell'$).
    The projection from the pullback to $p(B)$ sends $(i,\ell')\mapsto(i,\ell'\then h)$, and the projection from the pullback to $q(A)$ sends $(i,\ell')\mapsto(f_\1(i),f^\sharp_i\then\ell')$.
    \item The pullback described above---the set of pairs $(i,\ell')$ comprised of a $p$-position $i$ and a function $\ell'\colon p[i]\to A$---is exactly the set $p(A)$.
    Moreover, the projection to $p(B)$ sending $(i,\ell')\mapsto(i,\ell'\then h)$ is $p(h)$, and the projection to $q(A)$ sending $(i,\ell')\mapsto(f_\1(i),f^\sharp_i\then\ell')$ is $f_A$ by \cref{prop.morph_arena_to_func}.
    So \eqref{eqn.cart_nt_pullback} is a pullback, as desired.
\end{enumerate}
Next, we will show that $3\Rightarrow1$ in \cref{prop.cart_as_nt}, with $f\colon p\to q$ as a lens in $\poly$ that is a cartesian natural transformation and $i\in p(\1)$.
\begin{enumerate}[resume]
    \item By \cref{cor.morph_func_to_arena}, $f_{p[i]}$ sends $(i,\id_{p[i]})\mapsto(f_\1(i),f^\sharp_i)$, and by \cref{prop.poly_on_functions}, $q(f^\sharp_i)$ sends $(f_\1(i),\id_{q[f_\1(i)]})\mapsto(f_\1(i),f^\sharp_i)$ as well, so \eqref{eqn.cart_nt_pullback_cone} commutes.

    \item Taking $A\coloneqq q[f_\1(i)], B\coloneqq p[i],$ and $h\coloneqq f^\sharp_i$ in \eqref{eqn.cart_nt_pullback} and applying its universal property to \eqref{eqn.cart_nt_pullback_cone} induces an element $(i',g)$ of $p(q[f_\1(i)])$, with $i'\in p(\1)$ and $g\colon p[i']\to q[f_\1(i)]$, such that $p(f^\sharp_i)$ sends $(i',g)\mapsto(i,\id_{p[i]})$ and $f_{q[f_\1(i)]}$ sends $(i',g)\mapsto(f_\1(i),\id_{q[f_\1(i)]})$.
    It follows from the behavior of $p(f^\sharp_i)$ (by \cref{prop.poly_on_functions}) that $i'=i$ and $g\then f^\sharp_i=\id_{p[i]}$, and it follows from the behavior of $f_{q[f_\1(i)]}$ (by \cref{prop.morph_arena_to_func}) that $f^\sharp_i\then g=\id_{q[f_\1(i)}$.
    So $g$ is the inverse of $f^\sharp_i$, proving that $f^\sharp_i$ is an isomorphism, as desired.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.monoidal_pres_vert_cart}
The monoidal structures $+$, $\times$, and $\otimes$ preserve both vertical and cartesian morphisms.
\end{proposition}
\begin{proof}
Suppose that $f\colon p\to p'$ and $g\colon q\to q'$ are vertical, so that the on-positions functions $f_\1$ and $g_\1$ are isomorphisms.

We can obtain the on-positions function of a lens by passing it through the functor $\poly\To{p(\1)}\smset$ from \cref{thm.adjoint_quadruple}.
As this functor is both a left adjoint and a right adjoint, it preserves both sums and products, so $(f+g)_\1 = f_\1+g_\1$ and $(f\times g)_1 = f_1\times g_1$.
Hence $f+g$ and $f\times g$ are both vertical.
On-positions, the behavior of $\otimes$ is identical to the behavior of $\times$, so $f\otimes g$ must be vertical as well.

Now suppose that $f\colon p\to p'$ and $g\colon q\to q'$ are cartesian.

A position of $p+q$ is a position $i\in p(\1)$ or a position $j\in q(\1)$, and the map $(f+g)^\sharp$ at that position is either $f^\sharp_i$ or $g^\sharp_j$; either way it is an isomorphism, so $f+g$ is cartesian.

A position of $p\times q$ (resp.\ of $p\otimes q$) is a pair $(i,j)\in p(\1)\times q(\1)$. The lens $(f\times g)^\sharp_{(i,j)}$ (resp.\ $(f\otimes g)^\sharp_{(i,j)}$) is $f^\sharp_i+g^\sharp_j$ (resp.\ $f^\sharp_i\times g^\sharp_j$) which is again an isomorphism if $f^\sharp_i$ and $g^\sharp_j$ are. Hence $f\times g$ (resp.\ $f\otimes g$) is cartesian, completing the proof.
\end{proof}

\begin{proposition}\label{prop.pullback_vert_cart}
Pullbacks preserve vertical (resp.\ cartesian) lenses.
In other words, if $f\colon p\to q$ is a lens and $g\colon q'\to q$ a vertical (resp.\ cartesian) lens, then the pullback $g'$ of $g$ along $p$
\[
\begin{tikzcd}
	p\times_qq'\ar[r]\ar[d, "g'"']&
	q'\ar[d, "g"]\\
	p\ar[r, "f"']&
	q\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
is vertical (resp.\ cartesian).
\end{proposition}
\begin{proof}
This follows from \cref{ex.pullbacks_in_poly}, since the pullback (resp.\ pushout) of an isomorphism is an isomorphism.
\end{proof}

%-------- Section --------%
\section{Monoidal $*$-bifibration over $\smset$}

We conclude this chapter by showing that the functor $p\mapsto p(\1)$ has special properties that make it what \cite{shulman2008framed} refers to as a \emph{monoidal $*$-bifibration}.
Roughly speaking, this means that $\smset$ acts as a sort of remote controller on the category $\poly$, grabbing every polynomial by its positions and pushing or pulling it this way and that.
The material in this section is even more technical than the rest of this chapter, and we won't use it again in the book, so the reader may wish to skip to \cref{part.comon}.

As an example, suppose one has a set $A$ and a function $f\colon A\to p(\1)$, which we can also think of as a cartesian lens between constant polynomials.
From $f$, we can obtain a new polynomial $f^*p$ with position-set $A$ via a pullback
\begin{equation}\label{eqn.f^*_defined}
\begin{tikzcd}
	f^*p\ar[r, "\fun{cart}"]\ar[d]&
	p\ar[d, "\eta_p"]\\
	A\ar[r, "f"']&
	p(\1)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\end{equation}
Here $\eta_p$ is the unit of the adjunction $\adjr{\smset}{A}{p(\1)}{\poly}$; it is a vertical lens.
We could evaluate this pullback using \cref{ex.pullbacks_in_poly}.
Alternatively, we can use \cref{prop.pullback_vert_cart} to deduce that the top lens $f^*p\to p$ (which we presciently labeled $\fun{cart}$) is cartesian like $f$ and that the left lens $f^*p\to A$ is vertical like $\eta_p$. Furthermore, $\fun{cart}_1 = f$.
Hence
\[
    f^*p \iso \sum_{a \in A} \yon^{p[f(a)]}.
\]
We'll see this as part of a bigger picture in \cref{prop.basechange,thm.triple_adjoint_basechange}, but first we need the following definitions and a result about cartesian lenses.

\begin{definition}[Slice category] \label{def.slice}
Given an object $c$ in a category $\cat{C}$, the \emph{slice category} of $\cat{C}$ over $c$, denoted $\cat{C}/c$, is the category whose objects are morphisms in $\cat{C}$ with codomain $c$ and whose morphisms are commutative triangles in $\cat{C}$.
\end{definition}

\begin{definition}[Exponentiable morphism]
Given a category $\cat{C}$ with objects $c, d$ and morphism $f \colon c \to d$ such that all pullbacks along $f$ exist in $\cat{C}$, we say that $f$ is \emph{exponentiable} if the functor $f^* \colon \cat{C}/d \to \cat{C}/c$ given by pulling back along $f$ is a left adjoint.
\end{definition}

\begin{theorem}\label{thm.cart_exponentiable}
Cartesian lenses in $\poly$ are exponentiable.
That is, if $f\colon p\to q$ is cartesian, then the functor $f^*\colon\poly/q\to\poly/p$ given by pulling back along $f$ is a left adjoint:
\[
\begin{tikzcd}[column sep=50pt, background color=theoremcolor]
	\poly/p\ar[r, shift right=7pt, "f_*"']&
	\poly/q\ar[l, shift right=7pt, "f^*"']\ar[l, phantom, "\Leftarrow"]
\end{tikzcd}
\]
\end{theorem}
\begin{proof}
Fix $e\colon p'\to p$ and $g\colon q'\to q$.
\[
\begin{tikzcd}
	p'\ar[d, "e"']&q'\ar[d, "g"]\\
	p\ar[r, "f"']&q
\end{tikzcd}
\]
We need to define a functor $f_*\colon\poly/p\to\poly/q$ and prove the analogous isomorphism establishing it as right adjoint to $f^*$. We first establish some notation. Given a set $Q$ and sets $(P'_i)_{i\in I}$, each equipped with a map $Q\to P'_i$, let $Q/\sum_{i\in I}P'_i$ denote the coproduct in $Q/\smset$, or equivalently the wide pushout of sets $P'_i$ with apex $Q$. Then we give the following formula for $f_*p'$, which we write in larger font for clarity:
\begin{equation}\label{eqn.cart_exp}
f_*p'\coloneqq
\scalebox{1.3}{$\displaystyle
\sum_{j\in q(\1)}\;\sum_{i'\in\prod\limits_{i\in f_\1\inv(j)}e_\1\inv(i)}\;\yon^{q[j]/\sum_{i\in f_\1\inv(j)}p'[i'(i)]}
$}
\end{equation}
Again, $q[j]/\sum_{i\in f_\1\inv(j)}p'[i'(i)]$ is the coproduct of the $p'[i'(i)]$, taken in $q[j]/\smset$. Since $p[i]\cong q[f(i)]$ for any $i\in p(\1)$ by the cartesian assumption on $f$, we have the following chain of natural isomorphisms
\begin{align*}
	(\poly/p)(f^*q', p')&\cong
	\prod_{i\in p(\1)}\;\prod_{\{j'\in q'(\1)\,\mid\,g_\1(j')=f_\1(i)\}}\;\sum_{\{i'\in p'(\1)\,\mid\,e_\1(i')=i\}}\;(p[i]/\smset)(p'[i'],p[i]+_{q[f(i)]}q'[j'])\\&\cong
	\prod_{i\in p(\1)}\;\prod_{\{j'\in q'(\1)\,\mid\,g_\1(j')=f_\1(i)\}}\;\sum_{\{i'\in p'(\1)\,\mid\,e_\1(i')=i\}}\;(q[f(i)]/\smset)(p'[i'],q'[j'])\\&\cong
	\prod_{j\in q(\1)}\;\prod_{\{j'\in q'(\1)\,\mid\, g_\1(j')=j\}}\;\prod_{\{i\in p(\1)\,\mid\,f_\1(i)=j\}}\;\sum_{\{i'\in p'(\1)\,\mid\,e_\1(i')=i\}}\;(q[j]/\smset)(p'[i'],q'[j'])\\&\cong
	\prod_{j\in q(\1)}\;\prod_{\{j'\in q'(\1)\,\mid\, g_\1(j')=j\}}\;\sum_{i'\in\prod_{i\in f_\1\inv(j)}e_\1\inv(i)}\;\prod_{i\in f_\1\inv(j)}\;(q[j]/\smset)(p'[i'(i)],q'[j'])\\&\cong
	\prod_{j\in q(\1)}\;\prod_{\{j'\in q'(\1)\,\mid\, g_\1(j')=j\}}\;\sum_{i'\in\prod_{i\in f_\1\inv(j)}e_\1\inv(i)}\;(q[j]/\smset)\Big(\sum_{i\in f_\1\inv(j)}p'[i'(i)],q'[j']\Big)\\&\cong
	(\poly/q)(q',f_*p')
\end{align*}
\end{proof}

\begin{example}
Let $p\coloneqq\2\yon^\2$, $q\coloneqq\yon^\2+\yon^\0$, and $f\colon p\to q$ the unique cartesian lens between them.
Then for any $e\colon p'\to p$ over $p$, \eqref{eqn.cart_exp} provides the following description for the pushforward $f_*p'$.
%We use the isomorphisms $p(\1)\cong\2$ and $q(\1)\cong\2$ to talk about the positions of $p$ and $q$.

Over the $j=2$ position, $f_\1\inv(2)=\0$ and $q[2]=\0$, so $\prod_{i \in f_\1\inv(2)} e_\1\inv(i)$ is an empty product and $q[2]/\sum_{i\in f_\1\inv(2)} p'[i'(i)]$ is an empty pushout.
Hence the corresponding summand of \eqref{eqn.cart_exp} is simply $\yon^\0\cong\1$.

Over the $j=1$ position, $f_\1\inv(1)=\2$ and $q[1]=p[1]=p[2]=\2$, so $\prod_{i'\in f_\1\inv(1)} e_\1\inv(i) \iso e_\1\inv(1)\times e_\1\inv(2)$.
For $i' \in e_\1\inv(1) \times e_\1\inv(2)$, we have that $q[1]/\sum_{i\in f_\1\inv(2)} p'[i'(i)] \iso X_{i'}$ in the following pushout square:
\[
\begin{tikzcd}
	X_{i'} \ar[from=r] \ar[from=d] &
	p'[i'(2)] \ar[from=d, "e^\sharp_{i'(2)}"'] \\
	p'[i'(1)] \ar[from=r, "e^\sharp_{i'(1)}"] &
	\2 \ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
Then in sum we have
\[
    f_*p' \iso \left(\sum_{i' \in e_\1\inv(1) \times e_2\inv(2)} \yon^{X_{i'}}\right) + \1.
\]
\end{example}

\begin{exercise}
Prove that the unique lens $f\colon\yon\to\1$ is exponentiable.
\begin{solution}
Choose $p\in\poly$ and $q'\in\poly/\yon$. Then there is $q\in\poly$ such that $q'\cong q\yon$, equipped with the projection $q\yon\to\yon$. The pushforward is given by the exponential
\[f_*(q\yon)\coloneqq q^\yon\]
from the cartesian closure; see \eqref{eqn.exponential}. Indeed, we have
\begin{align*}
	\poly/\yon(f^*p,q\yon)&\cong
	\poly/\yon(p\yon,q\yon)\\&\cong
	\poly(p\yon,q)\\&\cong
	\poly(p,q^\yon).
\end{align*}
\end{solution}
\end{exercise}

For any set $A$, let $\poly[A.]$ denote the category whose objects are polynomials $p$ equipped with an isomorphism $A\cong p(\1)$, and whose morphisms are lenses respecting the isomorphisms with $A$.

\begin{proposition}[Base change]\label{prop.basechange}
For any function $f\colon A\to B$, pullback $f^*$ along $f$ induces a functor $\poly[B.]\to\poly[A.]$, which we also denote $f^*$.
\end{proposition}
\begin{proof}
This follows from \eqref{eqn.pullback_poly} with $q\coloneqq A$ and $r\coloneqq B$, since pullback of an iso is an iso.
\end{proof}

\begin{theorem}\label{thm.triple_adjoint_basechange}
For any function $f\colon A\to B$, the pullback functor $f^*$ has both a left and a right adjoint
\begin{equation}\label{eqn.adjoint_triple_monoidal_fib}
\begin{tikzcd}[column sep=large, background color=theoremcolor]
	\poly[A.]\ar[r, shift left=16pt, "f_!"]\ar[r, shift right=16pt, "f_*"']
	\ar[r, phantom, shift left=9pt, "\Rightarrow"]\ar[r, phantom, shift right=9pt, "\Leftarrow"]
&
	\poly[B.]\ar[l, "f^*" description]
\end{tikzcd}
\end{equation}
Moreover $\otimes$ preserves the op-cartesian arrows, making this a monoidal $*$-bifibration in the sense of \cite[Definition 12.1]{shulman2008framed}.
\end{theorem}
\begin{proof}
Let $p$ be a polynomial with $p(\1)\cong A$. Then the formula for $f_!p$ and $f_*p$ are given as follows:
\begin{equation}\label{eqn.f_!andf_*}
f_!p\cong\scalebox{1.3}{$\displaystyle\sum_{b\in B}\yon^{\;\prod\limits_{a\mapsto b}p[a]}$}
\qqand
f_*p\cong\scalebox{1.3}{$\displaystyle\sum_{b\in B}\yon^{\;\sum\limits_{a\mapsto b}p[a]}$}
\end{equation}
It may at first be counterintuitive that the left adjoint $f_!$ involves a product and the right adjoint $f_*$ involves a sum. The reason for this comes from the fact that $\poly$ is equivalent to the Grothendieck construction applied to the functor $\smset\op\to\smcat$ sending each set $A$ to the category $(\smset/A)\op$. The fact that functions $f\colon A\to B$ induces an adjoint triple between $\smset/A$ and $\smset/B$, and hence between $(\smset/A)\op$ and $(\smset/B)\op$ explains the variance in \eqref{eqn.f_!andf_*} and simultaneously establishes the adjoint triple \eqref{eqn.adjoint_triple_monoidal_fib}.

The functor $p\mapsto p(\1)$ is strong monoidal with respect to $\otimes$ and strict monoidal if we choose the lens construction as our model of $\poly$. By \cref{prop.monoidal_pres_vert_cart}, the monoidal product $\otimes$ preserves cartesian lenses; thus we will have established the desired monoidal $*$-bifibration in the sense of \cite[Definition 12.1]{shulman2008framed} as soon as we know that $\otimes$ preserves op-cartesian lenses.

Given $f$ and $p$ as above, the op-cartesian lens is the lens $p\to f_!p$ obtained as the composite $p\to f^*f_!p\to f_!p$ where the first lens is the unit of the $(f_!,f^*)$ adjunction and the second is the cartesian lens for $f_!p$. On positions $p\to f_!p$ acts as $f$, and on directions it is given by projection.

If $f\colon p(\1)\to B$ and $f'\colon p'(\1)\to B'$ are functions then we have
\begin{align*}
	f_!(p)\otimes f'_!(p')&\cong
	\sum_{b\in B}\sum_{b'\in B'}\yon^{\big(\prod_{a\mapsto b}p[a]\big)\times\big(\prod_{a'\mapsto b'}p'[a']\big)}\\&\cong
	\sum_{(b,b')\in B\times B'}\yon^{\big(\prod_{(a,a')\mapsto(b,b')}p[a]\times p[b]\big)}\\&
	\cong (f_!\otimes f'_!)(p\otimes p')
\end{align*}
and the op-cartesian lenses are clearly preserved since projections in the second line match with projections in the first.
\end{proof}

%-------- Section --------%
\section{Summary and further reading}

In this chapter we discussed several of the nice properties of the category $\poly$: it has various adjunctions to $\smset$ and $\smset\op$, is Cartesian closed, has limits and colimits, has an epi-mono factorization system, has a vertical-cartesian factorization system, and comes with a monoidal $*$-bifibration to $\smset$.

The principal monomial functor $p\mapsto p(1)\yon^{\Gamma p}$ discussed in \cref{cor.principal_monomial} is in fact distributive monoidal, and this comes up in work on entropy \cite{spivak2022polynomial} and on noncooperative strategic games \cite{capucci2022diegetic}.


%-------- Section --------%
\section{Exercise solutions}
\Closesolutionfile{solutions}
{\footnotesize
\input{solution-file5}}

\end{document}