% !TeX root = P1-Polynomials.tex
\documentclass[Book-Poly]{subfiles}
\begin{document}


\setcounter{chapter}{0}%Just finished 0.

%---------------- Part ----------------%
\part{The category of polynomial functors}\label{part.poly}

\Opensolutionfile{solutions}[solution-file1]

%------------ Chapter ------------%
\chapter{Representable functors from the~category of sets} \label{ch.poly.rep-sets}

In this chapter, we lay the categorical groundwork needed to define our category of interest, the category of polynomial functors.
We begin by examining a special kind of polynomial functor that you may already be familiar with---representable functors from the category $\smset$ of sets and functions.%
\index{functor!representable|(}
\index{functor!polynomial|see{polynomial functor}}
\index{polynomial functor}
\index{functor!set-valued}
We highlight the role these representable functors play in what is arguably the fundamental theorem of category theory, the Yoneda lemma.
We will also discuss sums and products of sets and of set-valued functors, which we will need to construct our polynomial functors.

%-------- Section --------%
\section{Representable functors and the Yoneda lemma} \label{sec.poly.rep-sets.yon}

\index{Yoneda lemma}

Representable functors---special functors to the category of sets---provide the foundation for the category $\poly$.
While much of the following theory applies to representable functors from any category, we need only focus on representable functors $\smset\to\smset$.

\begin{definition}[Representable functor] \label{def.representable}
    For a set $S$, we denote by $\yon^S\colon\smset\to\smset$ the functor that sends each set $X$ to the set $X^S=\smset(S,X)$ and each function $h\colon X\to Y$ to the function $h^S\colon X^S\to Y^S$ that sends $g\colon S\to X$ to $g\then h\colon S\to Y$.\footnote{Throughout this text, given morphisms $f \colon A \to B$ and $g \colon B \to C$ in a category, we will denote their composite morphism $A \to C$ interchangeably as $f \then g$ or $g \circ f$ (or even $gf$), whichever is more convenient.}

    We call a functor (isomorphic to one) of this form a \emph{representable functor}, or a \emph{representable}.
    In particular, we call $\yon^S$ the functor \emph{represented by} $S$, and we call $S$ the \emph{representing set of $\yon^S$}.
    As $\yon^S$ denotes raising a variable to the power of $S$, we will also refer to representables as \emph{pure powers}.%
    \index{pure power|see{functor, representable}}
\end{definition}

The symbol $\yon$ stands for Yoneda, for reasons we will explain in \cref{lemma.yoneda} and \cref{exc.finish_proof_yoneda} \cref{exc.finish_proof_yoneda.embed}.

Throughout this book, we will use the notation $\0\coloneqq\{\}=\varnothing,$ $\1\coloneqq\{1\},$ $\2\coloneqq\{1,2\},$ $\3\coloneqq\{1,2,3\}$, and so on, with $\ord{n}\coloneqq\{1,\ldots,n\}$
\begin{example}
    The functor that sends each set $X$ to $X\times X$ and each function $h\colon X\to Y$ to $(h\times h)\colon (X\times X)\to(Y\times Y)$ is representable.
    After all, $X\times X \iso X^\2$, so this functor is the pure power $\yon^\2$.
\end{example}

\begin{exercise}\label{exc.representable_fun}\index{functor!representable}\index{representable polynomial|seealso{functor!representable}}
    For each of the following functors $\smset\to\smset$, say if it is representable or not; if it is, give the set that represents it.
    \begin{enumerate}
        \item The identity functor $X\mapsto X$, which sends each function to itself.
        \item The constant functor $X\mapsto\2$, which sends every function to the identity on $\2$.
        \item The constant functor $X\mapsto\1$, which sends every function to the identity on $\1$.
        \item The constant functor $X\mapsto\0$, which sends every function to the identity on $\0$.
        \item A functor $X\mapsto X^\nn$.
        If it could be representable, where should it send each function?
        \item A functor $X\mapsto \2^X$.
        If it could be representable, where should it send each function? \qedhere
    \end{enumerate}
\index{functor!identity}\index{functor!constant}

    \begin{solution}
        \begin{enumerate}
            \item The identity functor $X\mapsto X$ is represented by the set $\1$: a function $\1 \to X$ can be identified with an element of $X$, so $\smset(\1,X)\iso X$.
            Alternatively, note that $X^\1 \iso X$.
            \item \label{sol.representable_fun.2} The constant functor $X\mapsto\2$ is not representable: it sends $\1$ to $\2$, but $\1^S \iso \1 \not\iso \2$ for any set $S$.
            \item The constant functor $X\mapsto\1$ is represented by $S=\0$: there is exactly one function $\0 \to X$, so $\smset(\0,X) \iso \1$.
            Alternatively, note that $X^0 \iso \1$.
            \item The constant functor $X\mapsto\0$ is not representable for the same reason as in \cref{sol.representable_fun.2}.
            \item The functor $\yon^\nn$ that sends $X\mapsto X^\nn$ is represented by $\nn$, by definition.
            It should send each function $h \colon X \to Y$ to the function $h^\nn \colon X^\nn \to Y^\nn$ that sends each $g \colon \nn \to X$ to $g \then h \colon \nn \to Y$.
            \item No $\smset \to \smset$ functor $X\mapsto \2^X$ is representable, for the same reason as in \cref{sol.representable_fun.2}.
            (There \emph{is}, however, a functor $\smset\op \to \smset$ sending $X \mapsto 2^X$ that is understood to be representable in a more general sense.)
        \end{enumerate}
    \end{solution}
\end{exercise}

Now that we have introduced representable functors, we study the maps between them.
As representables are functors, the maps between them are natural transformations.
\index{natural transformation!between representables}

\begin{proposition}\label{prop.representable_nt}
    For any function $f\colon R\to S$, there is an induced natural transformation $\yon^f\colon\yon^S\to \yon^R$; on any set $X$ its $X$-component $X^f\colon X^S\to X^R$ is given by sending $g\colon S\to X$ to $f\then g\colon R\to X$.
\end{proposition}

\begin{proof}
    See \cref{exc.representable_nt}.
\end{proof}

\begin{exercise} \label{exc.representable_nt}
    To prove \cref{prop.representable_nt}, show that for any function $f\colon R\to S$, the given construction $\yon^f\colon\yon^S\to\yon^R$ really is a natural transformation.
    That is, for any function $h\colon X\to Y$, show that the following naturality square commutes:
    \begin{equation} \label{diag.yon_embed_nat}
        \begin{tikzcd}%[bottom base]
            X^S\ar[r, "h^S"]\ar[d, "X^f"']&
            Y^S\ar[d, "Y^f"]\\
            X^R\ar[r, "h^R"']&
            Y^R\ar[ul, phantom, "?"]
        \end{tikzcd}
    \end{equation}
    \qedhere

    \begin{solution}
        To show that \eqref{diag.yon_embed_nat} commutes, we note that by the construction of the components of $\yon^f$ in the statement of \cref{prop.representable_nt}, both vertical maps in the diagram compose functions from $S$ with $f \colon R \to S$ on the left; and by \cref{def.representable}, both horizontal maps compose functions to $X$ with $h \colon X \to Y$ on the right.
        So by the associativity of composition, the diagram commutes: $(f\then g)\then h=f\then(g\then h)$ for all $g\colon S\to X$.
    \end{solution}
\end{exercise}\index{associativity}

\begin{exercise} \label{exc.representable_nt_components}
    Let $X$ be an arbitrary set. For each of the following sets $R,S$ and functions $f\colon R\to S$, describe the $X$-component $X^f\colon X^S\to X^R$ of the natural transformation $\yon^f\colon\yon^S\to\yon^R$.
    \begin{enumerate}
        \item \label{exc.representable_nt_components.id} $R=\5$, $S=\5$, $f=\id_\5$. (You should describe the function $X^{\id_\5}\colon X^\5\to X^\5$.)
        \item $R=\2$, $S=\1$, $f$ is the unique function.
        \item $R=\1$, $S=\2$, $f(1)=1$.
        \item $R=\1$, $S=\2$, $f(1)=2$.
        \item $R=\0$, $S=\5$, $f$ is the unique function.
        \item $R=\nn$, $S=\nn$, $f(n)=n+1$.
        \qedhere
    \end{enumerate}

    \begin{solution}
        In each case, given $f \colon R \to S$, we can find the $X$-component $X^f \colon X^S \to X^R$ of the natural transformation $\yon^f\colon\yon^S\to\yon^R$ by applying \cref{prop.representable_nt}, which says that $X^f$ sends each $g \colon S \to X$ to $f \then g \colon R \to X$.
        \begin{enumerate}
            \item Here $X^{\id_5}\colon X^\5\to X^\5$ is the identity function.
            \item If $f\colon\2\to\1$ is the unique function, then $X^f\colon X^\1\to X^\2$ sends each $g \in X$ (i.e.\ function $g \colon \1 \to X$) to the function that maps both elements of $\2$ to $g$.
            We can think of $X^f$ as the diagonal $X \to X \times X$.
            \item If $f\colon\1\to\2$ sends $1\mapsto1$, then $X^f\colon X^\2\to X^\1$ sends each $g \colon \2 \to X$ to $g(1)$, viewed as a function $\1 \to X$.
            We can think of $X^f$ as the left projection $X \times X \to X$.
            \item If $f\colon\1\to\2$ sends $1\mapsto2$, then $X^f\colon X^\2\to X^\1$ sends each $g \colon \2 \to X$ to $g(2)$, viewed as a function $\1 \to X$.
            We can think of $X^f$ as the right projection $X \times X \to X$.
            \item Here $X^f\colon X^\5\to X^\0\iso\1$ is the unique function.
            \item If $f\colon\nn\to\nn$ sends $n\mapsto n+1$, then $X^f\colon X^\nn\to X^\nn$ sends each $g \colon \nn \to X$ to the function $h \colon \nn \to X$ defined by $h(n)\coloneqq g(n+1)$ for all $n \in \nn$.
            We can think of $X^f$ as removing the first term of an infinite sequence of elements $(g(0),g(1),g(2),\ldots)$ of $X$ to obtain a new sequence $(g(1),g(2),g(3),\ldots)$.
        \end{enumerate}
    \end{solution}
\end{exercise}

These representable functors and natural transformations live in the larger category $\smset^\smset$, whose objects are functors $\smset\to\smset$ and whose morphisms are the natural transformations between them.

\begin{exercise} \label{exc.representable_nt_functorial}
    Show that the construction $f\mapsto\yon^f$ from \cref{prop.representable_nt} defines a functor
    \begin{equation} \label{eqn.yoneda_embedding}
        \yon^-\colon\smset\op\to\smset^\smset
    \end{equation}
    by verifying functoriality, as follows.
    \begin{enumerate}
        \item Show that for any set $S$, the natural transformation $\yon^{\id_S}\colon\yon^S\to\yon^S$ is the identity.
        \item Show that for functions $f\colon R\to S$ and $g\colon S\to T$, we have $\yon^g\then\yon^f=\yon^{f\then g}$. \qedhere
    \end{enumerate}

    \begin{solution}
        \begin{enumerate}
            \item The fact that $\yon^{\id_S}\colon\yon^S\to\yon^S$ is the identity is just a generalization of \cref{exc.representable_nt_components} \cref{exc.representable_nt_components.id}.
            For any set $X$, the $X$-component $X^{\id_S} \colon X^S \to X^S$ of $\yon^{\id_S}$ sends each $h \colon S \to X$ to $\id_S \then h = h$, so $X^{\id_S}$ is the identity natural transformation on $X^S$.
            Hence $\yon^{\id_S}$ is the identity on $\yon^S$.
            \item Fix $f \colon R \to S$ and $g \colon S \to T$; we wish to show that $\yon^g \then \yon^f = \yon^{f \then g}$.
            It suffices to show componentwise that $X^g \then X^f = X^{f \then g}$ for every set $X$.
            Indeed, $X^g$ sends each $h \colon T \to X$ to $g \then h$; then $X^f$ sends $g \then h$ to $f \then g \then h = X^{f \then g}(h)$.
        \end{enumerate}
    \end{solution}
\end{exercise}

We now have all the ingredients we need to state and prove the Yoneda lemma on the category of sets.
\index{Yoneda lemma}

\begin{lemma}[Yoneda lemma]\label{lemma.yoneda}
    Given a functor $F\colon\smset\to\smset$ and a set $S$, there is an isomorphism
    \begin{equation}\label{eqn.yoneda}
        F(S)\iso\smset^\smset(\yon^S,F)
    \end{equation}
    where the right hand side is the set of natural transformations $\yon^S\to F$.
    Moreover, \eqref{eqn.yoneda} is natural in both $S$ and $F$.
\end{lemma}
\begin{proof}[Proof]
    Given a natural transformation $m\colon\yon^S\to F$, consider its $S$-component $m_S\colon S^S\to F(S)$.
    Applying this function to $\id_S\in S^S$ yields an element $m_S(\id_S)\in F(S)$.

    Conversely, given an element $a\in F(S)$, there is a natural transformation we denote by $m^a\colon\yon^S\to F$ whose $X$-component is the function $X^S\to F(X)$ that sends $g\colon S\to X$ to $F(g)(a)$.
    In \cref{exc.finish_proof_yoneda} we ask you to show that this is indeed natural in $X$; that these two constructions, $m\mapsto m_S(\id_S)$ and $a\mapsto m^a$, are mutually inverse; and that the resulting isomorphism is natural.
\end{proof}

\index{natural transformation!and Yoneda embedding}

\begin{exercise}\label{exc.finish_proof_yoneda}
    In this exercise, we fill in the details of the preceding proof.
    \begin{enumerate}
        \item Show that for any $a\in F(S)$, the maps $X^S\to F(X)$ defined in the proof of \cref{lemma.yoneda} are natural in $X$.
        \item Show that the two mappings given in the proof of \cref{lemma.yoneda} are mutually inverse, thus defining the isomorphism \eqref{eqn.yoneda}.
        \item Show that \eqref{eqn.yoneda} as defined is natural in $F$.
        \item Show that \eqref{eqn.yoneda} as defined is natural in $S$.
        \item \label{exc.finish_proof_yoneda.embed} As a corollary of \cref{lemma.yoneda}, show that the functor $\yon^-\colon\smset\op\to\smset^\smset$ from \eqref{eqn.yoneda_embedding} is fully faithful---in particular, there is an isomorphism $S^T\iso \smset^\smset(\yon^S,\yon^T)$ for sets $S,T$.
        For this reason, we call $\yon^-$ the \emph{Yoneda embedding}.
        \qedhere
    \end{enumerate}

    \begin{solution}
        \begin{enumerate}
            \item To check that $X^S \to F(X)$ is natural in $X$, we verify that the naturality square
            \[
            \begin{tikzcd}[ampersand replacement=\&]
                X^S\ar[r, "h^S"]\ar[d, "F(-)(a)"']\&
                Y^S\ar[d, "F(-)(a)"]\\
                F(X)\ar[r, "F(h)"']\&
                F(Y)
            \end{tikzcd}
            \]
            commutes for all $h \colon X \to Y$.
            The top map $h^S$ sends any $g \colon S \to X$ to $g \then h$ (\cref{def.representable}), which is then sent to $F(g \then h)(a)$ by the right map.
            Meanwhile, the left map sends $g$ to $F(g)(a)$, which is then sent to $F(h)(F(g)(a))$ by the bottom map.
            So by the functoriality of $F$, the square commutes.

            \item We show that the maps $m\mapsto m_S(\id_S)$ and $a\mapsto m^a$ defined in the proof of \cref{lemma.yoneda} are mutually inverse.
            First, we show that for any natural transformation $m \colon \yon^S \to F$, we have $m^{m_S(\id_S)} = m$.
            Given a set $X$, the $X$-component of $m^{m_S(\id_S)}$ sends each $g \colon S \to X$ to $F(g)(m_S(\id_S))$; it suffices to show that this is also where the $X$-component of $m$ sends $g$.
            Indeed, by the naturality of $m$, the square
            \[
            \begin{tikzcd}[ampersand replacement=\&]
                S^S\ar[r, "g^S"]\ar[d, "m_S"']\&
                X^S\ar[d, "m_X"]\\
                F(S)\ar[r, "F(g)"']\&
                F(X)
            \end{tikzcd}
            \]
            commutes, so in particular, following $\id_S\in S^S$ around this diagram, we have
            \begin{equation} \label{eqn.finish_proof_yoneda}
                F(g)(m_S(\id_S)) = m_X(g^S(\id_S)) = m_X(\id_S \then g) = m_X(g).
            \end{equation}
            In the other direction, we show that for any $a \in F(S)$, we have $m^a_S(\id_S) = a$: by construction, $m^a_S \colon S^S \to F(S)$ sends $\id_S$ to $F(\id_S)(a) = a$.

            \item It suffices to show that given functors $F, G\colon\smset\to\smset$ and a natural transformation $\alpha \colon F \to G$, the naturality square
            \[
            \begin{tikzcd}[ampersand replacement=\&]
                \smset^\smset(\yon^S,F)\ar[d, "- \then \alpha"']\ar[r, "\sim"]\&
                F(S)\ar[d, "\alpha_S"]\\
                \smset^\smset(\yon^S,G)\ar[r, "\sim"]\&
                G(S)
            \end{tikzcd}
            \]
            commutes.
            The top map sends any $m \colon \yon^S \to F$ to $m_S(\id_S)$, which in turn is sent by the right map to $\alpha_S(m_S(\id_S)) = (m \then \alpha)_S(\id_S)$.
            This is also where the bottom map sends $m \then \alpha$, so the square commutes.

            \item It suffices to show that given a function $g \colon S \to X$, the naturality square
            \[
            \begin{tikzcd}[ampersand replacement=\&]
                \smset^\smset(\yon^S,F)\ar[d, "\yon^g \then -"']\ar[r, "\sim"]\&
                F(S)\ar[d, "F(g)"]\\
                \smset^\smset(\yon^X,F)\ar[r, "\sim"]\&
                F(X)
            \end{tikzcd}
            \]
            commutes.
            The left map sends any $m \colon \yon^S \to F$ to $\yon^g \then m$, which is sent by the bottom map to $(\yon^g \then m)_X(\id_X) = m_X(X^g(\id_X)) = m_X(g \then \id_X) = m_X(g)$.
            Meanwhile, the top map sends $m$ to $m_S(\id_S)$, which is sent by the right map to $F(g)(m_S(\id_S))$.
            So the square commutes by \eqref{eqn.finish_proof_yoneda}.

            \item To show that $\smset^\smset(\yon^S, \yon^T) \iso S^T$, just take $F\coloneqq\yon^T$ in \cref{lemma.yoneda} so that $F(S)\iso S^T$.
        \end{enumerate}
    \end{solution}
\end{exercise}

\index{functor!representable|)}


How will we go from these representable functors to polynomial ones?%
\index{polynomial functor}
Recall that, in algebra, a polynomial is just a sum of pure powers.
So we will define a \emph{polynomial functor} $\smset\to\smset$ to be a sum of pure power functors---that is, the representable functors $\yon^A$ for each set $A$ we just introduced.%
\footnote{This analogy isn't perfect: in algebra, polynomials are generally finite sums of pure powers, whereas our polynomial functors may be infinite sums of representables.
However, we are not the first to use the term ``polynomial'' this way, and the name stuck.}

All of our polynomials will be in one variable, $\yon$.
Every other letter or number that shows up in our notation for a polynomial will denote a set.
For example, in the polynomial
\begin{equation} \label{eqn.biz_poly}
    \rr\yon^\zz+\3\yon^{\3}+\yon^A+\sum_{i\in I}\yon^{R_i+Q_i^\2},
\end{equation}
$\rr$ denotes the set of real numbers, $\zz$ denotes the set of integers, $\2$ and $\3$ respectively denote the sets $\{1,2\}$ and $\{1,2,3\}$, and $A$, $I$, $Q_i$, and $R_i$ denote arbitrary sets.

To make sense of these polynomials, we need to define functor addition,%
\index{functor!sum of functors}
\index{functor!product of functors}
\index{polynomial functor!sum of polynomials}
\index{polynomial functor!product of polynomials}\index{product|seealso{polynomial functor, product of polynomials}}
 both in the binary case (i.e.\ what is $\yon^A+\yon^B$?) and more generally over arbitrary sets (i.e.\ what is $\sum_{i\in I}\yon^{A_i}$?).
This will allow us to interpret polynomials like \eqref{eqn.biz_poly}.
In particular, just as $3y^3=y^3+y^3+y^3$ in algebra, the summand $\3\yon^\3$ of \eqref{eqn.biz_poly} denotes the sum of representables $\yon^\3+\yon^\3+\yon^\3$, while the summand $\rr\yon^\zz$ denotes the sum over $\rr$ of copies of $\yon^\zz$.

While polynomial functors will be defined as sums, products of polynomials will turn out to be polynomials as well, again mimicking polynomials in algebra.
To make sense of these products, we will also define functor multiplication.
The construction of sums and products of functors $\smset\to\smset$ will rely on the construction of sets and products of sets themselves.

%-------- Section --------%
\section{Sums and products of sets} \label{sec.poly.rep-sets.sum-prod-set}

\index{set!sum of sets}
\index{set!product of sets}
\index{set!indexing}\index{category!discrete}\index{indexed set}

Let $I$ be a set, and let $X_i$ be a set for each $i\in I$.
Classically, we may denote this \emph{$I$-indexed family of sets} by $(X_i)_{i\in I}$.
Categorically, we may view this data as a specific kind of functor: if we identify the set $I$ with the \emph{discrete category} on $I$, whose objects are the elements of $I$ and whose morphisms are all identities, then $(X_i)_{i\in I}$ can be identified with a functor $X\colon I\to\smset$ with $X(i)\coloneqq X_i$.
\index{indexed family of objects}
To compromise, we will denote an indexed family of sets by $X\colon I\to\smset$ for a set $I$ viewed as a discrete category (although we will occasionally use the classical notation when convenient), but denote the set obtained by evaluating $X$ at each $i\in I$ by $X_i$ rather than $X(i)$.

To pick out an element of one of the sets in the indexed family $X\colon I\to\smset$, we need to specify both an index $i\in I$ and an element $x\in X_i$.
We call the set of such pairs $(i,x)$ the \emph{sum} of this indexed family, as below.

\begin{definition}[Sum of sets] \label{def.sum_sets}
    Let $I$ be a set and $X\colon I\to\smset$ be an $I$-indexed family of sets.
    The \emph{sum $\sum_{i\in I}X_i$ of the indexed family $X$} is the set
    \[
    \sum_{i\in I}X_i\coloneqq\{(i,x)\mid i\in I\text{ and }x\in X_i\}.
    \]
    When $I\coloneqq\{i_1,\ldots,i_n\}$ is finite, we may alternatively denote this sum as
    \[
    X_{i_1}+\cdots+X_{i_n}.
    \]
\end{definition}
\index{element!of a sum of sets}
\index{element!of a product of sets}
\index{dependent function}\index{indexed set}

Say instead we pick an element from \emph{every} set in the indexed family: that is, we construct an assignment $i\mapsto x_i$, where each $x_i\in X_i$.
If every $X_i$ were the same set $X$, then this would just be a function $I\to X$.
More generally, this assignment is what we call a \emph{dependent function}: its codomain $X_i$ \emph{depends} on its input $i$.
\index{dependent function}
We write the signature of such a dependent function as
\[
f \colon (i \in I) \to X_i.
\]
Note that the indexed family of sets $X\colon I\to\smset$ completely determines this signature.
The set of all dependent functions whose signature is determined by a given indexed family of sets is the \emph{product} of that indexed family, as below.

\begin{definition}[Product of sets] \label{def.prod_sets}
    Let $I$ be a set and $X\colon I\to\smset$ be an $I$-indexed family of sets.
    The \emph{product $\prod_{i\in I}X_i$ of the indexed family $X$} is the set of dependent functions
    \[
    \prod_{i\in I}X_i\coloneqq\{f \colon (i \in I) \to X_i\}.
    \]
    When $I\coloneqq\{i_1,\ldots,i_n\}$ is finite, we may alternatively denote this product as
    \[
    X_{i_1}\times\cdots\times X_{i_n} \qqor X_{i_1}\cdots X_{i_n}.
    \]
\end{definition}\index{dependent function}

For a dependent function $f\colon(i\in I)\to X_i$, we may denote the element of $X_i$ that $f$ assigns to $i\in I$ by $f(i), fi,$ or $f_i$.
When $I\coloneqq\{i_1,\ldots,i_n\}$ is finite, we may identify $f$ with the $n$-tuple $(f(i_1),\ldots,f(i_n))$; similarly, when $I\coloneqq\nn$, we may identify $f$ with the infinite sequence $(f_0,f_1,f_2,\ldots)$.

\index{set!cardinality of}
\begin{example}\label{ex.two_sums_and_prods}
    If $I\coloneqq\2=\{1,2\}$, then an $I$-indexed family $X\colon I\to \smset$ consists of two sets---say $X_1\coloneqq\{a,b,c\}$ and $X_2\coloneqq\{c,d\}$.
    Their sum is then the disjoint union
    \[
    \sum_{i\in \2}X_i=X_1+X_2=\{(1,a),(1,b),(1,c),(2,c),(2,d)\}.
    \]
    The cardinality\footnote{The \emph{cardinality} of a set is the number of elements it contains, at least when the set is finite; with care the notion can be extended to infinite sets as well.} of $X_1+X_2$ will always be the sum of the cardinalities of $X_1$ and $X_2$, justifying the use of the word ``sum.''

    Meanwhile, their product is the usual cartesian product
    \index{cartesian product|see{product}}\index{product}
    \[\prod_{i\in \2}X_i \cong X_1\times X_2=\{(a,c),(a,d),(b,c),(b,d),(c,c),(c,d)\}.\]
    The cardinality of $X_1\times X_2$ will always be the product of the cardinalities of $X_1$ and $X_2$, justifying the use of the word ``product.''
\end{example}

\begin{exercise}\label{exc.on_sums_prods_sets}\index{indexed set}
    Let $I$ be a set.
    \begin{enumerate}
        \item \label{exc.on_sums_prods_sets.sum} Show that there is an isomorphism of sets $I\iso\sum_{i\in I}\1$.
        \item \label{exc.on_sums_prods_sets.prod} Show that there is an isomorphism of sets $\1\iso\prod_{i\in I}\1$.
    \end{enumerate}
    As a special case, suppose that $I\coloneqq\0=\varnothing$ and that $X\colon \varnothing\to\smset$ is the unique empty indexed family of sets.
    \begin{enumerate}[resume]
        \item Is it true that $X_i=\1$ for each $i\in I$?
        \item Justify the statement ``the empty sum is $\0$'' by showing that there is an isomorphism of sets $\sum_{i\in\varnothing}X_i\iso\0$.
        \item Justify the statement ``the empty product is $\1$'' by showing that there is an isomorphism of sets $\prod_{i\in\varnothing}X_i\iso\1$.
        \qedhere
    \end{enumerate}

    \begin{solution}
        \begin{enumerate}
            \item \label{sol.on_sums_prods_sets.sum}
            To show that $I\iso\sum_{i \in I}\1$, observe that $x \in \1 = \{1\}$ if and only if $x = 1$, so $\sum_{i \in I} \1 = \{(i, 1) \mid i \in I\}$.
            Then the function $I \to \sum_{i \in I} \1$ that sends each $i \in I$ to $(i, 1)$ is clearly an isomorphism.

            \item \label{sol.on_sums_prods_sets.prod}
            To show that $\1 \iso \prod_{i \in I} \1$, it suffices to show that there is a unique dependent function $f \colon (i \in I) \to \1$.
            As $\1 = \{1\}$, such a function $f$ must always send $i \in I$ to $1$.
            This uniquely characterizes $f$, so there is indeed only one such dependent function.

            \item \label{sol.on_sums_prods_sets.vac} Yes: since $I$ is empty, there are no $i \in I$.
            So it is true that $X_i = 1$ holds whenever $i \in I$ holds, because $i \in I$ never holds.
            We say that this sort of statement is \emph{vacuously true}.

            \item As $I = \0 = \varnothing$, we have $\sum_{i\in\varnothing}X_i = \sum_{i\in I}\1 \iso I = \0$, where the equation on the left follows from \cref{sol.on_sums_prods_sets.vac} and the isomorphism in the middle follows from \cref{sol.on_sums_prods_sets.sum}.

            \item As $I = \varnothing$, we have $\prod_{i\in\varnothing}X_i = \prod_{i\in I}\1 \iso \1$, where the equation on the left follows from \cref{sol.on_sums_prods_sets.vac} and the isomorphism on the right follows from \cref{sol.on_sums_prods_sets.prod}.
        \end{enumerate}
    \end{solution}
\end{exercise}

The following standard fact describes the constructions from \cref{def.sum_sets,def.prod_sets} categorically and further justifies why we call them sums and products.
\index{product}
\index{coproduct}
\index{diagram}

\begin{proposition} \label{prop.set_prod_coprod}\index{functor!set-valued}
    Let $I$ be a set and $X\colon I\to\smset$ be an $I$-indexed family of sets. Then the sum $\sum_{i\in I}X_i$ is the categorical coproduct of these sets in $\smset$ (i.e.\ the colimit of the functor $X\colon I\to\smset$, viewed as a diagram), and the product $\prod_{i\in I}X_i$ is the categorical product of these sets in $\smset$ (i.e.\ the limit of the functor $X\colon I\to\smset$, viewed as a diagram).
\end{proposition}
\index{coproduct!inclusion map into}
\index{coproduct!universal property of}
\index{product!projection map out of}
\index{product!universal property of}\index{colimit}

\begin{proof}
    The sum $\sum_{i\in I}X_i$ comes equipped with an inclusion $\iota_j\colon X_j\to\sum_{i\in I}X_i$ for each $j\in I$ given by $x\mapsto(j,x)$.
    The product $\prod_{i\in I}X_i$ comes equipped with a projection $\pi_j\colon\prod_{i\in I}X_i\to X_j$ for each $j\in I$ sending each $f\colon(i\in I)\to X_i$ to $f(j)$.
    These satisfy the universal properties for categorical coproducts and products, respectively; see \cref{exc.set_prod_coprod}.
\end{proof}

\begin{exercise} \label{exc.set_prod_coprod}
    \begin{enumerate}
        \item Show that $\sum_{i\in I}X_i$ along with the inclusions $\iota_j\colon X_j\to\sum_{i\in I}X_i$ described in the proof of \cref{prop.set_prod_coprod} satisfy the universal property of a categorical coproduct: for any set $Y$ with functions $g_j\colon X_j\to Y$ for each $j\in I$, there exists a unique function $h\colon\sum_{i\in I}X_i\to Y$ for which $\iota_j\then h=g_j$ for all $j\in I$.
        \item Show that $\prod_{i\in I}X_i$ along with the projections $\pi_j\colon\prod_{i\in I}X_i\to X_j$ described in the proof of \cref{prop.set_prod_coprod} satisfy the universal property of a categorical product: for any set $Y$ with functions $g_j\colon Y\to X_j$ for each $j\in I$, there exists a unique function $h\colon Y\to\prod_{i\in I}X_i$ for which $h\then\pi_j=g_j$ for all $j\in I$. \qedhere
    \end{enumerate}
    \begin{solution}
        \begin{enumerate}
            \item Any function $h\colon\sum_{i\in I}X_i\to Y$ for which $\iota_j\then h=g_j$ for all $j\in I$ must satisfy $h(j,x)=h(\iota_j(x))=g_j(x)$ for all $j\in I$ and $x\in X_j$.
            This uniquely characterizes $h$, so if we define $h(j,x)\coloneqq g_j(x)$ we are done.

            \item Any function $h\colon Y\to\prod_{i\in I}X_i$ for which $h\then\pi_j=g_j$ for all $j\in I$ must satisfy $h(y)_j=\pi_j(h(y))=g_j(y)$ for all $y\in Y$ and $j\in I$.
            This uniquely characterizes $h(y)$ and thus $h$, so if we define $h(y)\colon(i\in I)\to X_i$ to be the dependent function given by $i\mapsto g_i(y)$ we are done.
        \end{enumerate}
    \end{solution}
\end{exercise}

Though we proved above explicitly that $\smset$ has all small products and coproducts, from here on out, we will assume the standard categorical fact that $\smset$ is complete (has all small limits) and cocomplete (has all small colimits). % TODO: add ref for this??
\index{limit}
\index{colimit}

We have constructed categorical sums and products of sets, but we can also construct categorical sums and products of the maps between them: functions.

\index{coproduct!of functions}
\index{product!of functions}

\begin{definition}[Categorical sum and product of functions] \label{def.sum-prod-func}
    Let $I$ be a set and $X,Y\colon I\to\smset$ be $I$-indexed families of sets.
    Given a natural transformation $f\colon X\to Y$, i.e.\ an $I$-indexed family of functions $(f_i\colon X_i\to Y_i)_{i\in I}$, its \emph{categorical sum} (or \emph{coproduct}) is the function
    \[
    \sum_{i\in I}f_i\colon\sum_{i\in I}X_i\to\sum_{i\in I}Y_i
    \]
    that, given $i\in I$ and $x\in X_i$, sends $(i,x)\mapsto(i,f_i(x))$; while its \emph{categorical product} is the function
    \[
    \prod_{i\in I}f_i\colon\prod_{i\in I}X_i\to\prod_{i\in I}Y_i
    \]
    that sends each $g\colon(i\in I)\to X_i$ to the \emph{composite dependent function} $(i\in I)\to Y_i$, denoted $g\then f$ or $f\circ g$, which sends $i\in I$ to $f_i(g(i))$.

    When $I\coloneqq\{i_1,\ldots,i_n\}$ is finite, we may alternatively denote this categorical sum and product of functions respectively as\footnote{We will take care to highlight when this notation may clash with a sum (resp.\ product) of functions with common domain and codomain whose codomain has an additive (resp.\ multiplicative) structure.}
    \[
    f_{i_1}+\cdots+f_{i_n} \qqand f_{i_1}\times\cdots\times f_{i_n}.
    \]
\end{definition}

\begin{exercise} \label{exc.sum-prod-func}
    \begin{enumerate}
        \item Show that the categorical sum of functions is the one induced by the universal property of the categorical sum of sets.
        That is, given a set $I$, two $I$-indexed families of sets $X,Y\colon I\to\smset$, and a natural transformation $f\colon X\to Y$, the function $\sum_{i\in I}f_i\colon\sum_{i\in I}X_i\to\sum_{i\in I}Y_i$ that we called the categorical sum is induced by the following composite maps for $j\in I$:
        \[
        X_j\To{f_j}Y_j\To{\iota'_j}\sum_{i\in I}Y_i,
        \]
        where $\iota'_j$ is the inclusion.
        It then follows by a standard categorical argument that the sum is functorial, i.e.\ that the sum of identities is an identity and that the sum of composites is the composite of sums.

        \item Similarly, show that the categorical product of functions is the one induced by the universal property of the categorical product of sets.
        That is, given the same setup as the previous part, the function $\prod_{i\in I}f_i\colon\prod_{i\in I}X_i\to\prod_{i\in I}Y_i$ that we called the categorical product is induced by the following composite maps for $j\in J$:
        \[
        \prod_{i\in I}X_i\To{\pi_j}X_j\To{f_j}Y_j.
        \]
        Again, this implies that the product is functorial. \qedhere
    \end{enumerate}
    \begin{solution}
        \begin{enumerate}
            \item It suffices to show that the following square, where the vertical maps are the inclusions, commutes for all $j\in I$:
            \[
            \begin{tikzcd}[column sep=large]
                X_j \ar[d, "\iota_j"] \ar[r, "f_j"] & Y_j \ar[d, "\iota'_j"] \\
                \sum_{i\in I}X_i \ar[r, "\sum_{i\in I}f_i"] & \sum_{i\in I}Y_i
            \end{tikzcd}
            \]
            Given $x\in X_j$, the left inclusion map sends $x$ to $(j,x)$, which the bottom sum of maps sends to $(j,f_j(x))$.
            Meanwhile, the top map sends $x$ to $f_j(x)$, which the right inclusion map again sends to $(j,f_j(x))$.

            \item It suffices to show that the following square, where the vertical maps are the projections, commutes for all $j\in I$:
            \[
            \begin{tikzcd}[column sep=large]
                \prod_{i\in I}X_i \ar[d, "\pi_j"] \ar[r, "\prod_{i\in I}f_i"] & \prod_{i\in I}Y_i \ar[d, "\pi'_j"] \\
                X_j \ar[r, "f_j"] & Y_j
            \end{tikzcd}
            \]
            Given $g\colon(i\in I)\to X_i$ in $\prod_{i\in I}X_i$, the top product of maps sends $g$ to $f\circ g$, which the right projection map sends to $f_j(g(j))$.
            Meanwhile, the left projection map sends $g$ to $g(j)$, which the bottom map again sends to $f_j(g(j))$.
        \end{enumerate}
    \end{solution}
\end{exercise}

We now highlight some tools and techniques to help us work with sum and product sets.

\begin{exercise}\label{exc.product_as_sections}
    Let $I$ be a set and $X \colon I \to \smset$ be an indexed family.
    There is a
    projection function
    $\pi_1 \colon \sum_{i \in I} X_i \to I$
    defined by $\pi_1(i, x) \coloneqq i$.
    \begin{enumerate}
        \item What is the signature of the second projection $\pi_2(i, x) \coloneqq x$?
        (Hint: it's a dependent function.)
        \item A \emph{section} of a function $r \colon A \to B$ is a function $s \colon B \to A$ such that $s \then r = \id_B$.
        Show that the product of the indexed family is isomorphic to the set of sections of $\pi_1$:
        \[\prod_{i \in I} X_i \cong \left\{s \colon I \to \sum_{i \in I} X_i \,\middle|\, s \then \pi_1 = \id_I\right\}.\]
        \qedhere
    \end{enumerate}
    \begin{solution}
        \begin{enumerate}
            \item The second projection $\pi_2(i, x) = x$ sends each pair $p \coloneqq (i, x) \in \sum_{i \in I} X_i$ to $x$, an element of $X_i$.
            Note that we can write $i$ in terms of $p$ as $\pi_1(p)$.
            This allows us to write the signature of $\pi_2$ as $\pi_2 \colon (p \in \sum_{i \in I} X_i) \to X_{\pi_1(p)}$.

            \item Let $S := \{s \colon I \to \sum_{i \in I} X_i \mid s \then \pi_1 = \id_I\}$ be the set of sections of $\pi_1$. To show that $\prod_{i \in I} X_i \cong S$, we will exhibit maps in either direction and show that they are mutually inverse.
            For each $f \colon (i \in I) \to X_i$ in $\prod_{i \in I} X_i$, we have $f(i) \in X_i$ for $i \in I$, so we can define a function $s_f \colon I \to \sum_{i \in I} X_i$ that sends $i\mapsto(i, f(i))$.
            Then $\pi_1(s_f(i)) = \pi_1(i, f(i)) = i$, so $s_f$ is a section of $\pi_1$.
            Hence $f \mapsto s_f$ is a map $\prod_{i \in I} X_i \to S$.

            In the other direction, for each section $s \colon I \to \sum_{i \in I} X_i$ we have $\pi_1(s(i)) = i$ for $i \in I$, so we can write $s(i)$ as an ordered pair $(i, \pi_2(s(i)))$ with $\pi_2(s(i)) \in X_i$.
            Hence we can define a dependent function $f_s \colon (i \in I) \to X_i$ sending $i\mapsto\pi_2(s(i))$.
            Then $s \mapsto f_s$ is a map $S \to \prod_{i \in I} X_i$.
            By construction $s_{f_s}(i) = (i, f_s(i)) = (\pi_1(s(i)), \pi_2(s(i))) = s(i)$ and $f_{s_f}(i) = \pi_2(s_f(i)) = \pi_2(i, f(i)) = f(i)$, so these maps are mutually inverse.
        \end{enumerate}
    \end{solution}
\end{exercise}\index{dependent function}

A helpful way to think about sum or product sets is to consider what choices must be made to specify an element of such a set.
In the following examples, say that we have a set $I$ and an $I$-indexed family $X \colon I \to \smset$.

Below, we give the instructions for choosing an element of $\sum_{i \in I} X_i$.

\begin{quote}
    To choose an element of $\sum_{i \in I} X_i$:
    \begin{enumerate}
        \item choose an element $i \in I$;
        \item choose an element of $X_i$.
    \end{enumerate}
\end{quote}
\index{element!of a dependent sum}


Then the projection $\pi_1$ from \cref{exc.product_as_sections} sends each element of $\sum_{i \in I} X_i$ to the element of $i \in I$ chosen in step 1, while the projection $\pi_2$ sends each element of $\sum_{i \in I} X_i$ to the element of $X_i$ chosen in step 2.

Next, we give the instructions for choosing an element of $\prod_{i \in I} X_i$.

\begin{quote}
    To choose an element of $\prod_{i \in I} X_i$:
    \begin{enumerate}
        \item for each element $i \in I$:
        \begin{enumerate}[label*=\arabic*.]
            \item choose an element of $X_i$.
        \end{enumerate}
    \end{enumerate}
\end{quote}

\index{element!of a dependent product}
\index{element!of a nested dependent set}

Armed with these interpretations, we can tackle more complicated expressions, including those with nested $\sum$'s and $\prod$'s such as
\begin{equation}\label{eqn.sum_prod_sum}
    A \coloneqq \sum_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k).
\end{equation}
The instructions for choosing an element of $A$ form a nested list, as follows.

\begin{quote}
    To choose an element of $A$:
    \begin{enumerate}
        \item choose an element $i \in I$;
        \item for each element $j \in J(i)$:
        \begin{enumerate}[label*=\arabic*.]
            \item choose an element $k \in K(i,j)$;
            \item choose an element of $X(i,j,k)$.
        \end{enumerate}
    \end{enumerate}
\end{quote}

Here the choice of $k\in K(i,j)$ may depend on $i$ and $j$: different values of $i$ and $j$ may lead to different sets $K(i,j)$.

By describing $A$ like this, we see that each $a \in A$ can be projected to an element $i\coloneqq\pi_1(a) \in I$, chosen in step 1, and a dependent function $\pi_2(a)$, chosen in step 2.
This dependent function in turn sends each $j \in J(i)$ to a pair that can be projected to an element $k\coloneqq\pi_1(\pi_2(a)(j)) \in K(i, j)$ chosen in step 2.1 and an element $\pi_2(\pi_2(a)(j)) \in X(i,j,k)$ chosen in step 2.2.

\begin{example}%[Notation for $\sum\prod$ stuff]
    \label{ex.notation_sum_prod}
    % Here we give notation for the elements of a set involving $\sum$'s and $\prod$'s such as that in \eqref{eqn.sum_prod_sum}.

    Let $I\coloneqq\{1,2\}$; let $J(1)\coloneqq\{j\}$ and $J(2)\coloneqq\{j,j'\}$; let $K(1,j)\coloneqq\{k_1,k_2\}$, $K(2,j)\coloneqq\{k_1\}$, and $K(2,j')\coloneqq\{k'\}$; and let $X(i,j,k)\coloneqq\{x,y\}$ for all $i,j,k$. Now the formula
    \[\sum_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k)\]
    from \eqref{eqn.sum_prod_sum} specifies a fixed set. Here is a list of all eight of its elements:
    \[
    \left\{
    \begin{gathered}
        \big(1, j\mapsto(k_1,x)\big),\qquad
        \big(1, j\mapsto(k_1,y)\big),\qquad
        \big(1, j\mapsto(k_2,x)\big),\qquad
        \big(1, j\mapsto(k_2,y)\big),\\
        \big(2, j\mapsto(k_1,x), j'\mapsto(k',x)\big),\qquad
        \big(2, j\mapsto(k_1,x), j'\mapsto(k',y)\big),\\
        \big(2, j\mapsto(k_1,y), j'\mapsto(k',x)\big),\qquad
        \big(2, j\mapsto(k_1,y), j'\mapsto(k',y)\big)
    \end{gathered}
    \right\}
    \]
    In each case, we first chose an element $i\in I$, either 1 or 2. Then for each $j\in J(i)$ we chose an element $k\in K(i,j)$ and an element of $X(i,j,k)$.
\end{example}

\begin{exercise}
    Consider the set
    \begin{equation}\label{eqn.prod_sum_prod}B \coloneqq \prod_{i\in I}\sum_{j\in J(i)}\prod_{k\in K(i,j)}X(i,j,k).\end{equation}
    \begin{enumerate}
        \item Give the instructions for choosing an element of $B$ as a nested list, like we did for $A$ just below \eqref{eqn.sum_prod_sum}.
        \item With $I$, $J$, $K$, and $X$ as in \cref{ex.notation_sum_prod}, how many elements are in $B$?
        \item Write out three of these elements in the style of \cref{ex.notation_sum_prod}.
        \qedhere
    \end{enumerate}
    \begin{solution}
        \begin{enumerate}
            \item Here are the instructions for choosing an element of $B$ as a nested list.
            \begin{quote}
                To choose an element of $B$:
                \begin{enumerate}[label=\arabic*.]
                    \item for each element $i \in I$:
                    \begin{enumerate}[label*=\arabic*.]
                        \item choose an element $j \in J(i)$;
                        \item for each element $k \in K(i, j)$:
                        \begin{enumerate}[label*=\arabic*.]
                            \item choose an element of $X(i,j,k)$.
                        \end{enumerate}
                    \end{enumerate}
                \end{enumerate}
            \end{quote}
            \item Given $I\coloneqq\{1,2\}$, $J(1)\coloneqq\{j\}$, $J(2)\coloneqq\{j,j'\}$, $K(1,j)\coloneqq\{k_1,k_2\}$, $K(2,j)\coloneqq\{k_1\}$, $K(2,j')\coloneqq\{k'\}$, and $X(i,j,k)\coloneqq\{x,y\}$ for all $i,j,k$, our goal is to count the number of elements in $B$.
            To compute the cardinality of $B$, we can use the fact that the cardinality of a sum (resp.\ product) is the sum (resp.\ product) of the cardinalities of the summands (resp.\ factors).
            So
            \begin{align*}
                |B| &= \prod_{i\in I}\sum_{j\in J(i)}\prod_{k\in K(i,j)}|X(i,j,k)| \\
                &= \prod_{i\in \{1,2\}}\sum_{j\in J(i)}\prod_{k\in K(i,j)}2 \\
                &= \left(\sum_{j\in J(1)} 2^{|K(1,j)|}\right)\left(\sum_{j\in J(2)} 2^{|K(2,j)|}\right) \\
                &= \left(2^2\right)\left(2^1 + 2^1\right) = 16.
            \end{align*}
            \item Here are three of the elements of $B$ (you may have written down others):
            \begin{itemize}
                \item $(1 \mapsto (j, k_1 \mapsto x, k_2 \mapsto y), 2 \mapsto (j', k' \mapsto x))$
                \item $(1 \mapsto (j, k_1 \mapsto y, k_2 \mapsto y), 2 \mapsto (j, k_1 \mapsto y))$
                \item $(1 \mapsto (j, k_1 \mapsto y, k_2 \mapsto x), 2 \mapsto (j', k' \mapsto y))$
            \end{itemize}
            \qedhere
        \end{enumerate}
    \end{solution}
\end{exercise}

%Henceforth, we will omit the full sequence of nested instructions corresponding to every sum or product of sets; we will assume you can read them for yourself.

%-------- Subsection --------%
\section{Expanding products of sums} \label{sec.poly.rep-sets.expand}

\index{type theoretic axiom of choice}
\index{distributive law}

We will often encounter sums of sets nested within products, as in \eqref{eqn.sum_prod_sum} and \eqref{eqn.prod_sum_prod}.
The following proposition helps us work with these; it is sometimes called the \emph{type-theoretic axiom of choice}, but it is perhaps more familiar as a set-theoretic analogue of the \emph{distributive property} of multiplication over addition.
While the identity may look foreign, it captures for sets the same process that you would use to multiply multi-digit numbers from grade school arithmetic or polynomials from high school algebra.

\begin{proposition}[Pushing $\prod$ past $\sum$]\label{prop.push_prod_sum_set}
    For any sets $I,(J(i))_{i\in I},$ and $(X(i,j))_{i\in I, j\in J(i)}$, we have a natural isomorphism\index{isomorphism!natural}
    \begin{equation}\label{eqn.set_completely_distributive}
        \prod_{i\in I}\sum_{j\in J(i)}X(i,j)
        \iso
        \sum_{\bar{j}\in \prod_{i\in I}J(i)}\;\prod_{i\in I}X(i,\bar{j}(i)).\footnote{We draw a bar over $j$ in $\bar{j}$ to remind ourselves that $\bar{j}$ is no longer just an index but a (dependent) function.}
    \end{equation}
\end{proposition}
\begin{proof}
    First, we construct a map from the left hand set to the right. An element of the set on the left is a dependent function $f \colon (i \in I) \to \sum_{j \in J(i)} X(i, j)$, which we can compose with projections from its codomain to yield $\pi_1(f(i)) \in J(i)$ and $\pi_2(f(i)) \in X(i, \pi_1(f(i)))$ for every $i \in I$.
    We can then form the following pair:\footnote{We omit parentheses for function application here and throughout the text for compactness whenever the meaning is clear.}
    \[
    (i \mapsto \pi_1 fi, i \mapsto \pi_2 fi).
    \]
    This is an element of the right hand set, because $i \mapsto \pi_1 fi$ is a dependent function in $\prod_{i\in I}J(i)$ and $i \mapsto \pi_2 fi$ is a dependent function in $\prod_{i\in I}X(i,\pi_1 fi)$.

    Now we go from right to left.
    An element of the right hand set is a pair of dependent functions, $\bar{j} \colon (i \in I) \to J(i)$ and $g \colon
    (i \in I) \to X(i, \bar{j}i)$.
    We map this pair to the following element of the left hand set, a dependent function $(i\in I)\to\sum_{j\in J(i)}X(i,j)$:
    \[
    i \mapsto (\bar{j}i, gi).
    \]

    Finally, we verify that the maps are mutually inverse.
    An element $(\bar{j}, g)$ of the right hand set is sent by one map and then the other to the pair
    \[
    (i \mapsto \pi_1(\bar{j}i, gi), i \mapsto \pi_2(\bar{j}i, gi))=(i \mapsto \bar{j}i, i \mapsto gi)=(\bar{j},g)
    \]
    Meanwhile, an element $f$ of the left hand set is sent by one map and then the other to the dependent function
    \[
    i \mapsto (\pi_1 fi, \pi_2 fi).
    \]
    As $fi$ is a pair whose components are $\pi_1 fi$ and $\pi_2 fi$, the dependent function above is precisely $f$.
\end{proof}

When $J(i)=J$ does not depend on $i\in I$, we can simplify the formula in \eqref{eqn.set_completely_distributive}.

\begin{corollary} \label{cor.push_prod_sum_set_indep}
    For any sets $I, J,$ and $(X(i, j))_{i \in I, j \in J}$, we have a natural isomorphism\index{isomorphism!natural}
    \begin{equation} \label{eqn.push_prod_sum_set_indep}
        \prod_{i\in I}\sum_{j\in J}X(i,j)\cong\sum_{\bar{j}\colon I\to J}\prod_{i\in I}X(i,\bar{j}i),
    \end{equation}
    where $\bar{j}$ ranges over all (standard, non-dependent) functions $I\to J$.
\end{corollary}
\begin{proof}
    Take $J(i)\coloneqq J$ for all $i \in I$ in \eqref{eqn.set_completely_distributive}.
    Then the set $\prod_{i\in I}J(i)$ becomes $\prod_{i\in I}J$ (which we may denote in exponential form by $J^I$); its elements, dependent functions $\bar{j}\colon(i\in I)\to J(i)=J$, become standard functions $\bar{j}\colon I\to J$.
\end{proof}

It turns out that being able to push $\prod$ past $\sum$ as in \eqref{eqn.set_completely_distributive} is not a property that is unique to sets.
In general, we refer to a category having this property as follows.

\begin{definition}[Completely distributive category]\index{coproduct}\index{product}
    A category $\Cat{C}$ with all small products and coproducts is \emph{completely distributive}\footnote{While our terminology generalizes that of a completely distributive lattice, which has the additional requirement that the category be a poset, it is unfortunately not standard: a completely distributive category refers to a different concept in some categorical literature. We will not use this other concept, so there is no ambiguity.} if products distribute over coproducts as in \eqref{eqn.set_completely_distributive}; that is, for any set $I$, sets $(J(i))_{i\in I}$, and objects $(X(i,j))_{i\in I,j\in J(i)}$ from $\Cat{C}$, we have a natural isomorphism\index{isomorphism!natural}
    \begin{equation}\label{eqn.cat_completely_distributive}
        \prod_{i\in I}\sum_{j\in J(i)}X(i,j)
        \iso
        \sum_{\bar{j}\in \prod_{i\in I}J(i)}\;\prod_{i\in I}X(i,\bar{j}i).
    \end{equation}
\end{definition}

The term ``completely distributive'' comes from lattice theory. As such it is consistent with two different extensions to categories that may not be posets. We use it to mean that the category has all sums and products and that products distribute over sums. Other authors use it to mean that the category has all colimits and limits and a sort of distributivity between them.
\index{completely distributive category}

So \cref{prop.push_prod_sum_set} states that $\smset$ is completely distributive.
Once we define the category of polynomial functors, we will see that it, too, is completely distributive.

\index{completely distributive category!$\poly$ as}
\index{completely distributive category!$\smset$ as}


\cref{cor.push_prod_sum_set_indep} generalizes to all completely distributive categories as well; we state this formally below.

\begin{corollary} \label{cor.push_prod_sum_obj_indep}
    Let $\Cat{C}$ be a completely distributive category.
    For any sets $I$ and $J$ and objects $(X(i, j))_{i \in I, j \in J}$ in $\Cat{C}$, we have a natural isomorphism\index{isomorphism!natural}
    \begin{equation} \label{eqn.push_prod_sum_obj_indep}
        \prod_{i\in I}\sum_{j\in J}X(i,j)\iso\sum_{\bar{j}\colon I\to J}\prod_{i\in I}X(i,\bar{j}i).
    \end{equation}
\end{corollary}
\begin{proof}
    Again, take $J(i)\coloneqq J$ for all $i \in I$ in \eqref{eqn.cat_completely_distributive}.
\end{proof}

\index{completely distributive category!distributive law in|see{distributive law}}
\index{distributive law}

\begin{exercise}
    Let $\Cat{C}$ be a completely distributive category.
    How is the usual distributive law
    \[
    X\times(Y+Z)\iso X\times Y+X\times Z
    \]
    for $X,Y,Z\in\cat{C}$ a special case of \eqref{eqn.cat_completely_distributive}?
    \begin{solution}
        We wish to show that $X\times (Y+Z)\iso X\times Y+X\times Z$ using \eqref{eqn.cat_completely_distributive}.
        On the left hand side, we are taking a $2$-fold product: a single object times a $2$-fold sum.
        So we should let $I\coloneqq\2$ and let $J(1)\coloneqq\1$, with $X(1,1)\coloneqq X$; and $J(2)\coloneqq\2$, with $X(2,1)\coloneqq Y$ and $X(2,2)\coloneqq Z$.
        Then
        \[
        X\times(Y+Z)\iso \prod_{i\in\2}\sum_{j\in J(i)}X(i,j) \iso \sum_{\bar{j}\in\prod_{i\in\2}J(i)}\;\prod_{i\in \2}X(i,\bar{j}(i)) \iso \sum_{\bar{j}\in\prod_{i\in\2}J(i)}X(1,\bar{j}(1))\times X(2,\bar{j}(2)),
        \]
        where the middle isomorphism follows from \eqref{eqn.cat_completely_distributive}.
        The set $\prod_{i\in\2}J(i)$ contains two functions: $(1\mapsto1,2\mapsto1)$ and $(1\mapsto1,2\mapsto2)$.
        So we can rewrite the right hand side as
        \[
        X(1,1)\times X(2,1)+X(1,1)\times X(2,2)\iso X\times Y+X\times Z.
        \]
    \end{solution}
\end{exercise}

Throughout this book, such as in the exercise below, you will see expressions consisting of alternating products and sums.
Using \eqref{eqn.cat_completely_distributive}, you can always rewrite such an expression as a sum of products, in which every $\sum$ appears before every $\prod$.\footnote{When an expression is written so that every $\sum$ appears before every $\prod$, it is said to be in \emph{disjunctive normal form}.}
This is analogous to how products of sums in high school algebra can always be expanded into sums of products via the distributive property.

\index{disjunctive normal form}

\begin{exercise} \label{exc.push_prod_sum_set}
    Let $I, (J(i))_{i\in I},$ and $(K(i,j))_{(i,j)\in IJ}$ be sets, and for each $(i,j,k)\in IJK$, let $X(i,j,k)$ be an object in a completely distributive category.
    \begin{enumerate}
        \item Rewrite
        \[
        \sum_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k)
        \]
        so that every $\sum$ appears before every $\prod$.
        \item Rewrite
        \[
        \prod_{i\in I}\sum_{j\in J(i)}\prod_{k\in K(i,j)}X(i,j,k)
        \]
        so that every $\sum$ appears before every $\prod$.
        \item Rewrite
        \[
        \prod_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k)
        \]
        so that every $\sum$ appears before every $\prod$.\qedhere
    \end{enumerate}
    \begin{solution}
        \begin{enumerate}
            \item By applying \eqref{eqn.cat_completely_distributive}, we can rewrite
            \[
            \sum_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k)
            \]
            as
            \[
            \sum_{i\in I}\sum_{\bar{k}\in \prod_{j\in J}K(i,j)}\prod_{j\in J(i)}X(i,j,\bar{k}j).
            \]
            \item By applying \eqref{eqn.cat_completely_distributive}, we can rewrite
            \[
            \prod_{i\in I}\sum_{j\in J(i)}\prod_{k\in K(i,j)}X(i,j,k)
            \]
            as
            \[
            \sum_{\bar{j}\in \prod_{i\in I}J(i)}\;\prod_{i\in I}X(i,\bar{j}i)\prod_{k\in K(i,\bar{j}i)}X(i,\bar{j}i,k).
            \]
            \item By applying \eqref{eqn.cat_completely_distributive}, we can rewrite
            \[
            \prod_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k)
            \]
            once as
            \[
            \prod_{i\in I}\sum_{\bar{k}\in\prod_{j\in J}K(i,j)}\prod_{j\in J(i)}X(i,j,\bar{k}j)
            \]
            and then again as
            \[
            \sum_{\bar{\bar{k}}\in\prod_{i\in I}\prod_{j\in J}K(i,j)}\prod_{i\in I}\prod_{j\in J(i)}X(i,j,\bar{\bar{k}}(i,j)).
            \]
        \end{enumerate}
    \end{solution}
\end{exercise}

Now that we understand sums and products of sets, we are ready to explore sums and products of set-valued functors.\index{functor!set-valued}

%-------- Subsection --------%
\section{Sums and products of functors $\smset\to\smset$} \label{sec.poly.rep-sets.sum-prod-func}

Recall that our goal is to define polynomial functors such as $\yon^\2+\2\yon+\1$ and the maps between them.
We have defined representable functors such as $\yon^\2$, $\yon$, and $\1$; we just need to interpret sums of functors $\smset\to\smset$.
But we might as well introduce products of functors at the same time, because they will very much come in handy.
Both these concepts generalize to limits and colimits in $\smset^\smset$.

\begin{proposition} \label{prop.presheaf_lim_ptwise}\index{functor!limit of functors}\index{functor!colimit of functors}\index{limit!of functors}\index{colimit!of functors}
    The category $\smset^\smset$ has all small limits and colimits, and they are computed pointwise.
    In particular, on objects, given a small category $\cat{J}$ and a functor $F\colon \cat{J}\to\smset^\smset$, for all $X\in\smset$, the limit and colimit of $F$ satisfy isomorphisms
    \[
    \left(\lim_{j\in\cat{J}} F(j)\right)(X) \iso \lim_{j\in\cat{J}} \left(F(j)(X)\right) \qqand \left(\colim_{j\in\cat{J}} F(j)\right)(X) \iso \colim_{j\in\cat{J}} \left(F(j)(X)\right)
    \]
    natural in $X$.
\end{proposition}
\begin{proof}
    This is a special case of a more general fact when $\smset^\smset$ is replaced by an arbitrary functor category $\Cat{D}^\Cat{C}$, where $\Cat{D}$ is a category that (like $\smset$) has all small limits and colimits; see \cite[pages 22--23, displays (24) and (25)]{macLane1992sheaves}.
\end{proof}

\index{functor!category of functors}\index{limit!of functors $\smset\to\smset$}\index{colimit!of functors $\smset\to\smset$}

Focusing on the case of coproducts and products, the following corollary is immediate.
\index{coproduct}

\begin{corollary}[Sums and products of functors $\smset\to\smset$] \label{cor.sum_prod_set_endofuncs}
    Given functors $F,G\colon\smset\to\smset$, their categorical coproduct or sum in $\smset^\smset$, denoted $F+G$, is the functor $\smset\to\smset$ defined for $X,Y\in\smset$ and $f\colon X\to Y$ by
    \[
    (F+G)(X)\coloneqq F(X)+G(X) \qqand (F+G)(f)\coloneqq F(f)+G(f);
    \]
    while their categorical product in $\smset^\smset$, denoted $F\times G$ or $FG$, is the functor $\smset\to\smset$ defined for $X,Y\in\smset$ and $f\colon X\to Y$ by
    \[
    (F\times G)(X)\coloneqq F(X)\times G(X) \qqand (F\times G)(f)\coloneqq F(f)\times G(f).
    \]

    More generally, given functors $(F_i)_{i\in I}$ indexed over a set $I$, their categorical coproduct or sum and categorical product in $\smset^\smset$, respectively denoted
    \[
    \sum_{i\in I}F_i\colon\smset\to\smset
    \qqand
    \prod_{i\in I}F_i\colon\smset\to\smset,
    \]
    are respectively defined for $X\in\smset$ by
    \[
    \left(\sum_{i\in I}F_i\right)(X)\coloneqq\sum_{i\in I} F_i(X)
    \qqand
    \left(\prod_{i\in I}F_i\right)(X)\coloneqq\prod_{i\in I} F_i(X).
    \]
    and for functions $f\colon X\to Y$ by
    \[
    \left(\sum_{i\in I}F_i\right)(f)\coloneqq\sum_{i\in I} F_i(f)
    \qqand
    \left(\prod_{i\in I}F_i\right)(f)\coloneqq\prod_{i\in I} F_i(f).
    \]
\end{corollary}

% TODO: sum and products of nat trans & functoriality?

We also note the special case of initial and terminal objects.
Given a set $I\in\smset$, we will also use $I$ to denote the constant functor $\smset\to\smset$ that sends every set to $I$.

\index{functor!constant}

\begin{corollary}[Initial and terminal functors $\smset\to\smset$]
    The constant functor $\0\colon\smset\to\smset$ is initial in $\smset^\smset$, while the constant functor $\1\colon\smset\to\smset$ is terminal in $\smset^\smset$.
\end{corollary}
\begin{proof}
    As the set $\0$ is initial in $\smset$ (for every set $X$ there is a unique map $\0\to X$), \cref{prop.presheaf_lim_ptwise} implies that the constant functor $\0$ is initial in $\smset^\smset$.
    Similarly, as the set $\1$ is terminal in $\smset$ (for every set $X$ there is a unique map $X\to\1$), \cref{prop.presheaf_lim_ptwise} implies that the constant functor $\1$ is terminal in $\smset^\smset$.
\end{proof}

\index{functor!initial}\index{functor!terminal}

Finally, we note that $\smset^\smset$ inherits the distributivity of $\smset$.

\begin{proposition}\label{prop.set_endofunc_distrib}\index{completely distributive category!$\smset^\smset$ as}
    The category $\smset^\smset$ is completely distributive.
\end{proposition}
\begin{proof}
    This follows directly from the fact that $\smset$ itself is completely distributive (\cref{prop.push_prod_sum_set}) and the fact that sums and products in $\smset^\smset$ are computed pointwise (\cref{cor.sum_prod_set_endofuncs}).
\end{proof}

The following exercises justify some notational shortcuts we will use when denoting polynomial functors.
First, for any set $A$ and functor $F\colon\smset\to\smset$, we may write an $A$-indexed sum of copies of $F$ as $AF$, the product of $F$ and the constant functor $A$; for instance, $\yon+\yon\iso\2\yon$.

\begin{exercise} \label{exc.repeated_sum_is_product}
    Show that for a set $A\in\smset$ and a functor $F\colon\smset\to\smset$, an $A$-indexed sum of copies of $F$ is isomorphic to the product of the constant functor $A$ and $F$:
    \[
    \sum_{a \in A}F\iso AF.
    \]
    (This is analogous to the fact that adding up $n$ copies of number is equal to multiplying that same number by $n$.)
    \begin{solution}
        It suffices to show that for all $X\in\smset$, there is an isomorphism
        \[
        \sum_{a\in A}F(X)\iso(AF)(X)
        \]
        natural in $X$.
        The left hand side is the set $\{(a,s)\mid a\in A\text{ and }s\in F(X)\} \iso A \times F(X)$, while the right hand side is also naturally isomorphic to the set $A(X)\times F(X)\iso A\times F(X)$.
        Alternatively, since $\smset^\smset$ is completely distributive by \cref{prop.set_endofunc_distrib}, the result also follows from \eqref{eqn.cat_completely_distributive}, with $I\coloneqq\2, J(1)\coloneqq A, J(2)\coloneqq\1, X(1,a)\coloneqq\1$ (the constant functor) for $a\in A$, and $X(2,1)\coloneqq F$:
        \[
        AF \iso
        \left(\sum_{a\in A}\1\right)F \iso
        \prod_{i\in\2}\sum_{j\in J(i)}X(i,j)
        \iso
        \sum_{\ol{j}\in\prod_{i\in\2}J(i)}\prod_{i\in\2}X(i,\ol{j}(i))
        \iso
        \sum_{a\in A}\1\times F
        \iso
        \sum_{a\in A}F.
        \]
        Here we used the fact that $A\iso\sum_{a\in A}\1$ from \cref{exc.on_sums_prods_sets} \cref{exc.on_sums_prods_sets.sum} (there we proved the statement for sets, but the same statement for the corresponding constant set-valued functors follows immediately).
    \end{solution}
\end{exercise}\index{completely distributive category}

Similarly, we may wish to write an $A$-indexed product of copies of $F$ in exponential form as $F^A$.
But since we have already introduced exponential notation for representable functors, this yields two possible interpretations for the functor $\smset\to\smset$ denoted by $\yon^A$: as the functor represented by $A$, or as the $A$-indexed product of copies of the identity functor $\yon\colon\smset\to\smset$.
In fact, the following exercise shows that there is no ambiguity, as the two interpretations are isomorphic.

\begin{exercise}
    \begin{enumerate}
        \item Show that for a set $I\in\smset$, an $I$-indexed product of copies of the identity functor $\yon\colon\smset\to\smset$ is isomorphic to the functor $\yon^I\colon\smset\to\smset$ represented by $I$:
        \[
        \prod_{i\in I}\yon\iso\yon^I.
        \]
        (This is analogous to the fact that multiplying $n$ copies of a number together is equal to raising that same number to the power of $n$.)
        \item Show that the $I$-indexed product of copies of a representable functor $\yon^A\colon\smset\to\smset$ for some $A\in\smset$ is isomorphic to the functor $\yon^{IA}\colon\smset\to\smset$ represented by the product set $IA$:
        \[
        \prod_{i\in I}\yon^A\iso\yon^{IA}.
        \]
        (Hint: You may use the fact that following natural isomorphism holds between sets of functions:\index{isomorphism!natural}
        \[
        \{ f \colon I\times A\to X \} \iso \{ g \colon I\to X^A \}.
        \]
        The process of converting a function $f$ in the left hand set to the corresponding function $i\mapsto(a\mapsto f(i,a))$ in the right is known as \emph{currying}.) \qedhere
    \end{enumerate}
    \begin{solution}
        \begin{enumerate}
            \item It suffices to show that for all $X\in\smset$, there is an isomorphism
            \[
            \prod_{i\in I} \yon(X) \iso \yon^I(X).
            \]
            natural in $X$.
            We have that $\yon(X)\iso X$ and that $\yon^I(X)\iso X^I$.
            So both sides are naturally isomorphic to the set of functions $I\to X$.

            \item It suffices to show that for all $X\in\smset$, there is an isomorphism
            \[
            \prod_{i\in I} \yon^A(X) \iso \yon^{IA}(X).
            \]
            natural in $X$.
            We have that $\yon^A(X)\iso X^A$, so $\prod_{i\in I}\yon^A(X)\iso(X^A)^I$, and that $\yon^{IA}(X)\iso X^{IA}$.
            By currying, both sides are naturally isomorphic to the set of functions $I\times A\to X$.
        \end{enumerate}
    \end{solution}
\end{exercise}

Henceforth, given $A\in\smset$ and a functor $F\colon\smset\to\smset$, we define
\[
F^A\coloneqq\prod_{a\in A}F.
\]
The exercise above shows that this notation does not conflict with the way we write representable functors as powers of the identity functor $\yon$.
The exercise also shows how a power of a representable functor can be simplified to a single representable functor.

With these ingredients, we are finally ready to define what a polynomial functor is.
We will begin with this definition in the next chapter.

%-------- Section --------%
\section{Summary and further reading}

In this chapter, we reviewed the definition of a \emph{representable functor} $\yon^S\colon\smset\to\smset$ for $S\in\smset$ sending $X\mapsto X^S$.
We then stated and proved the \emph{Yoneda lemma}, a foundational result characterizing maps out of these representables: for an arbitrary functor $F\colon\smset\to\smset$, natural transformations $\yon^S\to F$ are in natural correspondence with elements of $F(S)$.

\index{functor!representable}\index{Yoneda lemma}\index{category!discrete}\index{coproduct}\index{product}

We then reviewed other categorical constructions in $\smset$, many of which carry over to the polynomial functors we introduce in the next chapter.
For a set $I$, we can view it as a discrete category and consider a functor $X\colon I\to\smset$ as an \emph{$I$-indexed family of sets} comprised of a set $X_i$ for each $i\in I$.
An $I$-indexed family of sets $X$ has a \emph{sum} (or \emph{coproduct}), the set of pairs $(i,x)$ with $i\in I$ and $x\in X_i$; and a \emph{product}, the set of \emph{dependent functions} $f\colon(i\in I)\to X_i$.
Such a dependent function sends each $i\in I$ to an element of $X_i$.
These constructions satisfy the universal properties of coproducts and products; moreover, products distribute over coproducts, making $\smset$ a completely distributive category.
All these constructions and properties are inherited by $\smset^\smset$, whose limits (including products) and colimits (including coproducts) are computed pointwise: on one object at a time according to limits and colimits in $\smset$.

\index{dependent function}

For other introductions to the Yoneda lemma, the category of sets, or both, take your pick of \cite{Pierce:1991,Borceux:1994a,MacLane:1998a,Leinster:2014a,Riehl:2017a,fong2019seven,cheng_2022}.

%-------- Section --------%
\section{Exercise solutions}
\Closesolutionfile{solutions}
{\footnotesize
    \input{solution-file1}}

\Opensolutionfile{solutions}[solution-file2]










%------------ Chapter ------------%
\chapter{Polynomial functors} \label{ch.poly.obj}

In this chapter, we will formally introduce our objects of study: polynomial functors.
In addition to the set-theoretic perspective, we will present several more concrete ways to think about polynomials to aid intuition that we will use throughout the rest of this book.
We keep the mathematical content of this chapter fairly light, preferring to solidify our conceptual understanding of polynomials, before advancing to deeper categorical content.

%-------- Section --------%
\section{Introducing polynomial functors} \label{sec.poly.obj.intro}

\begin{definition}[Polynomial functor]
    A \emph{polynomial functor} (or simply \emph{polynomial}) is a functor $p\colon\smset\to\smset$ such that there exists a set $I$, an $I$-indexed family of sets $(p[i])_{i\in I}$, and an isomorphism
    \[
    p\iso\sum_{i\in I}\yon^{p[i]}
    \]
    to the corresponding $I$-indexed sum of representables.
\end{definition}

So, up to isomorphism, a polynomial functor is just a sum of representables.

\index{polynomial functor!as sum of representables}

\begin{remark}
    Given sets $I, A \in \smset$, it follows from \cref{exc.repeated_sum_is_product} that we have an isomorphism of polynomials
    \[
    \sum_{i \in I} \yon^A \iso I\yon^A.
    \]
    So when we write down a polynomial, we will often combine identical representable summands $\yon^A$ by writing them in the form $I\yon^A$.
    In particular, the constant functor $\1$ is a representable functor ($\1 \iso \yon^\0$), so every constant functor $I$ is a polynomial functor: $I \iso \sum_{i \in I} \1$.
\end{remark}

\begin{exercise}
    Consider the polynomial $q\coloneqq\yon^\8+\4\yon$.
    \begin{enumerate}
        \item Does the polynomial $q$ have a representable summand $\yon^\2$?
        \item Does the polynomial $q$ have a representable summand $\yon$?
        \item Does the polynomial $q$ have a representable summand $\4\yon$?
        \qedhere
    \end{enumerate}
    \begin{solution}
    \begin{enumerate}
        \item No, $q$ does not have $\yon^\2$ as a representable summand.
        \item Yes, $q$ does have $\yon$ as a representable summand.
        \item No, $q$ does not have $\4\yon$ as a representable summand, because $\4\yon$ is not a representable functor!
        But to make amends, we could say that $\4\yon$ is a \emph{summand}; this means that there is some $q'$ such that $q\iso q'+\4\yon$, namely $q'\coloneqq\yon^8$. So $\3\yon$ is also a summand, but $\yon^\2$ and $\5\yon$ are not.
    \end{enumerate}
    \end{solution}
\end{exercise}

\index{polynomial functor!summand of}

\begin{example}\label{ex.verbose_poly_eval}
    Consider the polynomial $p\coloneqq\yon^\2+\2\yon+\1$.
    It denotes a functor $\smset\to\smset$; where does this functor send the set $X\coloneqq\{a,b\}$?
    To be precise, we will rather verbosely say that $I\coloneqq\4$ and
    \[
    p[1]\coloneqq\2,\quad
    p[2]\coloneqq\1,\quad
    p[3]\coloneqq\1,\qqand
    p[4]\coloneqq\0\
    \]
    so that $p\cong\sum_{i\in I}\yon^{p[i]}$. Now we have
    \begin{align*}
        p(X) &\iso
        \sum_{i\in\4}\{a,b\}^{p[i]} \\ &=
        \{a,b\}^\2 + \{a,b\}^\1 + \{a,b\}^\1 + \{a,b\}^\0 \\ &\iso
        \{(1,(a,a)),(1,(a,b)),(1,(b,a)),(1,(b,b)),(2,(a)),(2,(b)),(3,(a)),(3,(b)),(4,())\}.
    \end{align*}
    Above, we denote each function $f\colon p[i]\to\{a,b\}$ in the set $\{a,b\}^{p[i]}$ by the $n$-tuple $(f(1),\ldots,f(n))$ whenever $p[i]\coloneqq\ord{n}$.
    For ease of reading, we may drop the parentheses around these $n$-tuples to obtain the equivalent set
    \[
    p(X) \iso
    \{(1,a,a),(1,a,b),(1,b,a),(1,b,b),(2,a),(2,b),(3,a),(3,b),(4)\}.
    \]
    As we might expect, the set $p(X)$ contains $2^2+2+2+1=9$ elements, equal to the value obtained when we plug $|X|=2$ into the original polynomial $p$ when we interpret its coefficients and exponents as numbers instead of sets.
\end{example}

In general, a polynomial $p\coloneqq\sum_{i\in I}\yon^{p[i]}$ applied to a set $X$ expands to
\[
\sum_{i\in I}X^{p[i]}
\]
and can be thought of as the set of all pairs comprised of an element of $I$ and a function $p[i]\to X$ or, equivalently, a $p[i]$-tuple of elements of $X$.

\index{polynomial functor!action on sets}

\begin{exercise}
    In the verbose style of \cref{ex.verbose_poly_eval}, write out all the elements of $p(X)$ for $p$ and $X$ as follows (if there are infinitely many, denote the set $p(X)$ some other way):
    \begin{enumerate}
        \item $p\coloneqq\yon^\3$ and $X\coloneqq\{4,9\}.$
        \item $p\coloneqq\3\yon^\2+\1$ and $X\coloneqq\{a\}$.
        \item $p\coloneqq\0$ and $X\coloneqq\nn$.
        \item $p\coloneqq\4$ and $X\coloneqq\nn$.
        \item $p\coloneqq\yon$ and $X\coloneqq\nn$.
        \qedhere
    \end{enumerate}
    \begin{solution}
        \begin{enumerate}
            \item Let $I\coloneqq\1$ and $p[1]\coloneqq\3$ so that $p\coloneqq\yon^\3\iso\sum_{i\in I}\yon^{p[i]}$.
            Then
            \[
            p(X)\iso\{(1, 4, 4, 4), (1, 4, 4, 9), (1, 4, 9, 4), (1, 4, 9, 9), (1, 9, 4, 4), (1, 9, 4, 9), (1, 9, 9, 4), (1, 9, 9, 9)\}.
            \]

            \item Let $I\coloneqq\4$, $p[1]\coloneqq p[2]\coloneqq p[3]\coloneqq\2$, and $p[4]\coloneqq\1$, so that $p\coloneqq\3\yon^\2+\1\iso\sum_{i\in I}\yon^{p[i]}$.
            Then $p(X)\iso\{(1,a,a),(2,a,a),(3,a,a),(4)\}$.

            \item Let $I\coloneqq\0$ so that $p\coloneqq\0\iso\sum_{i\in I}\yon^{p[i]}$.
            Then $p(X)\iso\0$.
            Alternatively, note that $\0$ is the constant functor that sends every set to $\0$.

            \item Let $I\coloneqq\4$ and $p[i]\coloneqq\0$ for every $i\in I$ so that $p\coloneqq\4\iso\sum_{i\in I}\yon^{p[i]}$.
            Then $p(X)\iso\{(1), (2), (3), (4)\}\iso\4$.
            Alternatively, note that $\4$ is the constant functor that sends every set to $\4$.

            \item Let $I \coloneqq \1$ and $p[1] \coloneqq \1$ so that $p\coloneqq\yon\iso\sum_{i\in I}\yon^{p[i]}$.
            So $p(X)\iso\{(1,n)\mid n\in\nn\}\iso\nn$.
            Alternatively, note that $\yon$ is the identity functor, so it sends $\nn$ to itself.
        \end{enumerate}
    \end{solution}
\end{exercise}

The following proposition shows how the polynomial functor $p$ itself determines the set $I$ over which we sum up representables to obtain $p$.

\index{polynomial functor!$p(1)$ as summands of}

\begin{proposition}\label{prop.apply1}
    Let $p\coloneqq\sum_{i\in I}\yon^{p[i]}$ be an arbitrary polynomial functor. Then $I\cong p(\1)$, so there is an isomorphism of functors
    \begin{equation}\label{eqn.sum_p1}
        p\iso\sum_{i\in p(\1)}\yon^{p[i]}.
    \end{equation}
\end{proposition}
\begin{proof}
    We need to show that $I\iso p(\1)$; the latter claim follows directly.
    In \cref{exc.on_sums_prods_sets} \cref{exc.on_sums_prods_sets.sum}, we showed that $I\iso\sum_{i\in I}\1$, so it suffices to show that $(\yon^{p[i]})(\1)\iso\1$ for all $i \in I$.
    Indeed, $\1^{p[i]}\iso \1$ because there is a unique function $p[i]\to \1$ for each $p[i]$.
\end{proof}
We can draw an analogy between \cref{prop.apply1} and evaluating $p(1)$ for a polynomial $p$ from high school algebra, which yields the sum of the coefficients of $p$.
The notation in \eqref{eqn.sum_p1} will be how we denote arbitrary polynomials from now on, and we will use the following terms to denote the sets $p(\1)$ and $p[i]$ for $i\in p(\1)$ on which a polynomial $p$ depends.

\begin{definition}[Position and direction]
    Given a polynomial functor
    \[
    p\iso\sum_{i\in p(\1)}\yon^{p[i]},
    \]
    we call an element $i\in p(\1)$ a \emph{position of $p$} or a \emph{$p$-position}, and we call an element $a\in p[i]$ a \emph{direction of $p$ at $i$} or a \emph{$p[i]$-direction}.
    We call $p(\1)$ the \emph{position-set of $p$} and $p[i]$ the \emph{direction-set of $p$ at $i$}.
\end{definition}

\index{polynomial functor!positions and directions}
\index{positions!of polynomial|seealso{polynomial functor!positions and directions}}
\index{directions!of polynomial|seealso{polynomial functor!positions and directions}}

Note that the position-set $p(\1)$ along with the $p(\1)$-indexed family of direction-sets $p[-]\colon p(\1)\to\smset$ uniquely characterize a polynomial $p$ up to isomorphism.
Throughout this book, we will often specify a polynomial by giving its positions and its directions at each position.

\begin{exercise}\label{exc.apply0}
    We saw in \cref{prop.apply1} how to interpret the position-set $p(\1)$ of a polynomial $p$, e.g.\ $p\coloneqq\yon^\3+\3\yon^\2+\4$, as the sum of the coefficients of $p$: here $p(\1)\iso\1+\3+\4\iso\8$.
    How might you interpret $p(\0)$?
    \begin{solution}
        We consider $p(\0)$ for arbitrary polynomials $p$.
        A representable functor $\yon^S$ for $S\in\smset$ sends $\0\mapsto\0$ if $S\neq\0$ (as there are then no functions $S\to\0$), but sends $\0\mapsto\1$ if $S=\0$ (as there is a unique function $\0\to\0$).
        So
        \[
        p(\0)\iso\sum_{i\in p(\1)}\left(\yon^{p[i]}\right)(\0)\iso\sum_{\substack{i\in p(\1),\\ p[i]\neq\0}}\0+\sum_{\substack{i\in p(\1),\\ p[i]=\0}}\1\iso\{i\in p(\1)\mid p[i]=\0\}.
        \]
        That is, $p(\0)$ is the set of \emph{constant} positions of $p$, the positions of $p$ that have no directions.
        For example, if $p\coloneqq\yon^\3+\3\yon^\2+\4$, then $p(\0)=\4$.
        In the language of high school algebra, we might call $p(\0)$ the \emph{constant term} of $p$.
    \end{solution}
\end{exercise}

As a functor $\smset\to\smset$, a polynomial should act on functions as well as on sets.
Below, we explain how.

\index{polynomial functor!action on functions}

\begin{proposition} \label{prop.poly_on_functions}
    Let $p$ be an arbitrary polynomial functor, which our notation lets us write as $p\iso\sum_{i\in p(\1)} \yon^{p[i]}$, and let $f\colon X\to Y$ be an arbitrary function.
    Then $p(f)\colon p(X)\to p(Y)$ sends each $(i, g)\in p(X)$, with $i\in p(\1)$ and $g\colon p[i]\to X$, to $(i, g\then f)$ in $p(Y)$.
\end{proposition}
\begin{proof}
    For each $i\in p(\1)$, by \cref{def.representable}, the functor $\yon^{p[i]}$ sends $f$ to the function $X^{p[i]}\to Y^{p[i]}$ mapping each $g\colon p[i]\to X$ to $g\then f\colon p[i]\to Y$.
    So the sum of these functors over $i\in p(\1)$ sends each $(i, g)\in p(X)$ to $(i, g\then f)\in p(Y)$.
\end{proof}

\begin{example}
    Suppose $p\coloneqq\yon^\2+\2\yon+1$. Let $X\coloneqq\{a_1,a_2,b_1\}$ and $Y\coloneqq\{a,b,c\}$, and let $f\colon X\to Y$ be the function sending $a_1,a_2\mapsto a$ and $b_1\mapsto b$. The induced function $p(f)\colon p(X)\to p(Y)$, according to \cref{prop.poly_on_functions}, is shown below:
    \[
    \begin{tikzcd}[row sep=2pt, column sep=3pt, shorten <=-5pt, shorten >=-5pt, dashed]
        \LMO{(1,a_1,a_1)}\ar[rrrr, blue, bend left=25pt]&\LMO{(1,a_1,a_2)}\ar[rrr, blue, bend left=25pt]&\LMO{(1,a_1,b_1)}\ar[rrr, blue, bend left=25pt]&[30pt]&
        \LMO{(1,a,a)}&\LMO{(1,a,b)}&\LMO{(1,a,c)}&&
        \\
        \LMO{(1,a_2,a_1)}\ar[rrrru, blue, bend left=10pt]&\LMO{(1,a_2,a_2)}\ar[rrru, blue, bend left=10pt]&\LMO{(1,a_2,b_1)}\ar[rrru, blue, bend left=10pt]&&
        \LMO{(1,b,a)}&\LMO{(1,b,b)}&\LMO{(1,b,c)}&&
        \\
        \LMO{(1,b_1,a_1)}\ar[rrrru, blue, bend left=10pt]&\LMO{(1,b_1,a_2)}\ar[rrru, blue, bend left=10pt]&\LMO{(1,b_1,b_1)}\ar[rrru, blue, bend left=10pt]&&
        \LMO{(1,c,a)}&\LMO{(1,c,b)}&\LMO{(1,c,c)}&&
        \\
        \LMO{(2,a_1)}\ar[rrrr, blue, bend left=25pt]&\LMO{(2,a_2)}\ar[rrr, blue, bend left=25pt]&\LMO{(2,b_1)}\ar[rrr, blue, bend left=25pt]&&
        \LMO{(2,a)}&\LMO{(2,b)}&\LMO{(2,c)}&&
        \\
        \LMO{(3,a_1)}\ar[rrrr, blue, bend left=25pt]&\LMO{(3,a_2)}\ar[rrr, blue, bend left=25pt]&\LMO{(3,b_1)}\ar[rrr, blue, bend left=25pt]&&
        \LMO{(3,a)}&\LMO{(3,b)}&\LMO{(3,c)}&&
        \\&
        \LMO{(4)}\ar[rrrr, blue]&&&&\LMO{(4)}
    \end{tikzcd}
    \]
\end{example}

\begin{exercise}
    Let $p\coloneqq\yon^\2+\yon$. Choose a function $f\colon\1\to\2$ and write out the induced function $p(f)\colon p(\1)\to p(\2)$.
    \begin{solution}
        We have
        \[
        p(\1) \iso \{(1, 1, 1), (2, 1)\} \qqand p(\2) \iso \{(1, 1, 1), (1, 1, 2), (1, 2, 1), (1, 2, 2), (2, 1), (2, 2)\}.
        \]
        Say we choose the function $f\colon\1\to\2$ that sends $1 \mapsto 1$.
        Then $p(f)$ would send $(1, 1, 1) \mapsto (1, 1, 1)$ and $(2, 1) \mapsto (2, 1)$.
        If we had instead picked $1 \mapsto 2$ as our function $f$, then $p(f)$ would send $(1, 1, 1) \mapsto (1, 2, 2)$ and $(2, 1) \mapsto (2, 2)$.
    \end{solution}
\end{exercise}

%-------- Section --------%
\section{Special classes of polynomial functors} \label{sec.poly.obj.spec}

Here we describe several special classes of polynomials.
We have already defined two such special classes: \emph{representables} and \emph{constants}.
A \emph{representable polynomial} (or simply a \emph{representable}) is a representable functor, i.e.\ a polynomial functor isomorphic to $\yon^A$ for some set $A$.
Meanwhile, a \emph{constant polynomial} (or simply a \emph{constant}) is a constant functor, i.e.\ a polynomial functor isomorphic to $I$, interpreted as a functor, for some set $I$.

\index{polynomial functor!constant|see{constant polynomial}}\index{polynomial functor!representable|see{representable polynomial}}\index{polynomial functor!linear|see{linear polynomial}}
\index{set!as constant polynomial}
\index{constant polynomial}\index{linear polynomial}\index{representable polynomial}

\begin{exercise}
\begin{enumerate}
  \item Characterize when a polynomial $p$ is \textit{representable} in terms of its positions and/or its directions.
  \item Characterize when a polynomial $p$ is \textit{constant} in terms of its positions and/or its directions. \qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
  \item A polynomial $p$ is representable when $p\iso\yon^A$ for some set $A$, and $\yon^A$ has exactly $1$ position.
  Conversely, if a polynomial $p$ has exactly $1$ position, then $p(\1)\iso\1$, so we may write $p$ (up to isomorphism) as $p\iso\sum_{1\in\1}\yon^{p[1]}\iso\yon^{p[1]}$, which is representable.
  So a polynomial $p$ is representable if and only if it has exactly $1$ position.

  \item A polynomial $p$ is constant when $p\iso I$ for some set $I$, and $I$ has no directions at any of its positions.
  Conversely, if every direction-set of a polynomial $p$ is empty, then
  \[
    p\iso\sum_{i\in p(\1)}\yon^{p[i]}\iso\sum_{i\in p(\1)}\yon^\0\iso\sum_{i\in p(\1)}\1\iso p(\1),
  \]
  i.e.\ the set $p(\1)$ viewed as a constant functor.
  So a polynomial $p$ is constant if and only if it has exactly $0$ directions at each position.
\end{enumerate}
\end{solution}
\end{exercise}

Like constants, the other two special classes of polynomials we define here will share their names with their algebraic analogues.
Throughout, let $p\iso\sum_{i\in p(\1)}\yon^{p[i]}$ be a polynomial functor.

\begin{definition}[Linear, affine]
We say that $p$ is \emph{linear}\footnote{Unlike linear polynomials from high school algebra (which are really \emph{affine linear functions} rather than necessarily \emph{linear functions}), our linear polynomial functors have no (nonzero) constant terms: they always send $\0$ to $\0$. A polynomial is \emph{affine} if it is of the form $A\yon+B$, though we will not use this concept much in the book.}
if $p\iso I\yon$ for some set $I$.
\end{definition}
\index{linear polynomial}
\index{polynomial functor!affine|see{affine polynomial}}\index{affine polynomial}

\begin{definition}[Monomial]
We say that $p$ is a \emph{monomial} if $p\iso I\yon^A$ for sets $I$ and $A$.
\end{definition}
\index{monomial}

\begin{example}\index{constant polynomial}\index{linear polynomial}\index{representable polynomial}
  Every constant polynomial $I\iso I\yon^\0$ is a monomial, as is every linear polynomial $I\yon\iso I\yon^\1$ and every representable $\yon^A \iso \1\yon^A$.
  On the other hand, there are monomials that are neither constant, linear, nor representable, such as $\2\yon^\2$ or $\nn\yon^\rr$.
  Moreover, there are polynomials that are not monomials, such as $\yon^\4+\3$ or $\sum_{n\in\nn}\yon^{\ord{n}}$.

  There is only one polynomial that is both constant and linear, namely $\0\iso\0\yon$.
  Similarly, there is only one polynomial (up to isomorphism) that is both constant and representable, namely $\1\iso\yon^\0$.
  Finally, there is only one polynomial (up to isomorphism) that is both linear and representable, namely the identity functor $\1\yon\iso\yon\iso\yon^\1$.

  In general, every set $S$ has a corresponding constant polynomial $S$, linear polynomial $S\yon$, and representable polynomial $\yon^S$; and as long as $|S|\geq2$, these are all distinct.
\end{example}

\begin{exercise}
  \begin{enumerate}
    \item Characterize when a polynomial $p$ is \textit{linear} in terms of its positions and/or its directions.
    \item Characterize when a polynomial $p$ is a \textit{monomial} in terms of its positions and/or its directions. \qedhere
  \end{enumerate}
  \begin{solution}
    \begin{enumerate}
      \item A polynomial $p$ is linear when $p\iso I\yon$ for some set $I$, and $I\yon$ has exactly $1$ direction at each position. (Note that this is even true when $p\iso\0\yon\iso\0$, for then it is true vacuously.)
      Conversely, if a polynomial $p$ has exactly $1$ direction at each position, then $p[i]\iso\1$ for all $i\in p(\1)$, so
      \[
        p
          \iso
        \sum_{i\in p(\1)}\yon^{p[i]}
          \iso
        \sum_{i\in p(\1)}\yon^\1
          \iso
        \sum_{i\in p(\1)}\yon
          \iso
        p(\1)\yon,
      \]
      which is linear.
      So a polynomial $p$ is linear if and only if it has exactly $1$ direction at each position.

      \item A polynomial $p$ is a monomial when $p\iso I\yon^A$ for sets $I$ and $A$, implying that there is an isomorphism of direction-sets $p[i]\iso A\iso p[j]$ for all $p$-positions $i$ and $j$ (i.e.\ all the direction-sets of $p$ have the same cardinality).
      Conversely, if all the direction-sets of a polynomial $p$ are isomorphic to each other, then they are all isomorphic to some set $A$, so we have
      \[
      p
      \iso
      \sum_{i\in p(\1)}\yon^{p[i]}
      \iso
      \sum_{i\in p(\1)}\yon^A
      \iso
      p(\1)\yon^A,
      \]
      which is a monomial.
      So a polynomial $p$ is a monomial if and only if all of its direction-sets have the same cardinality.
    \end{enumerate}
  \end{solution}
\end{exercise}

Later on in \cref{sec.poly.bonus.adj}, we will see how all four of these special classes of polynomials arise from various adjunctions.

%-------- Section --------%
\section{Interpreting positions and directions}

\index{polynomial functor!positions and directions|(}
Let us make an informal digression on how we will think about positions and directions of polynomials in this book.
While this section has little mathematical content, the intuition we build here will guide us as we delve into the deeper theory of polynomials and their applications to modeling interaction.

The main idea is that a \emph{position} is some status that may be held, while the \emph{directions} at each position are the options available when holding that status.
While these positions and directions may be imagined abstractly, here we give some concrete examples.

\begin{example}[Directions as menu options] \label{ex.reps-as-menus}
    Consider a representable and thus polynomial functor $\yon^A$ for a set $A$.
    It has $1$ position and the elements of $A$ as its directions.
    We may think of $A$ as a menu of options to choose from.

    The menu may consist of dinner options available at a wedding; then the corresponding representable functor could be
    \[
        \yon^{\{\const{chicken},\,\const{beef},\,\const{vegetarian}\}};
    \]
    or it may be the menu of a text editor, in which case the representable could be
    \[
        \yon^{\{\const{cut},\,\const{copy},\, \const{paste}\}}.
    \]
    In both these cases, there are exactly $3$ directions, so there is an isomorphism of representable functors
    \[
        \yon^{\{\const{chicken},\,\const{beef},\,\const{vegetarian}\}} \iso \yon^{\{\const{cut},\,\const{copy},\, \const{paste}\}}.
    \]

    Similarly, we may interpret the representable $\yon^\2$ as a $2$-option menu.
    Such menus are ubiquitous in life: yes or no, true or false, heads or tails, 0 or 1.
    A $1$-option menu, represented by $\yon^\1\iso\yon$, is also familiar as an unavoidable choice, the only option: ``sorry, ya just gotta go through it.''
    Having no options, represented by $\yon^\0\iso\1$, is when you actually don't get through it: an impossible decision, a ``dead end.''

    In contrast, we may interpret the representable $\yon^{[0,1]}$ as a menu with an infinite range of options: a slider with one end labeled $0$ and the other labeled $1$, able to take on any value in between.
\end{example}\index{decision!impossible}\index{isomorphism!of representable functors}

For consistency, we will favor the term ``direction'' over ``option'' when referring to the elements of $A$ for a summand $\yon^A$ of a polynomial.
Nevertheless, when we think of a polynomial's directions, we will often think of them as options to choose from.

\cref{ex.reps-as-menus} shows how we may interpret the directions of a single representable summand as options in a menu.
By having multiple representable summands---one for each position---a polynomial may capture more general scenarios with a range of possible menus.

\begin{example}[Modeling with a polynomial] \label{ex.coin-jar}
    Consider a coin jar with a slot that may be open or closed.
    When the slot is open, the jar may accept a penny, a nickel, a dime, or a quarter---there are $4$ options to choose from.
    When the slot is closed, the jar may not accept any coins at all---there are $0$ options.
    We may model this scenario with the polynomial
    \[
        \yon^{\{\const{penny},\,\const{nickel},\,\const{dime},\,\const{quarter}\}}+\yon^\0 \iso \yon^\4+\1.
    \]
    This polynomial has $2$ positions, corresponding to the two statuses the slot could take: open or closed.
    To delineate these positions, we could take advantage of the fact that every singleton set is isomorphic to $\1$ and that $\1\yon^A\iso\yon^A$ to rewrite the above polynomial as
    \[
        \{\const{open}\}\yon^{\{\const{penny},\,\const{nickel},\,\const{dime},\,\const{quarter}\}}+\{\const{closed}\}\yon^\0 \iso \yon^\4+\1.
    \]
\end{example}

\begin{exercise}
    Give another example of a real-world scenario that may be modeled by a polynomial with more than $1$ position.
\begin{solution}
    The stopwatch app on my phone has three positions: a $\const{zero}$ position, from which I may tap a single $\const{start}$ button; a $\const{running}$ position, from which I may tap either a $\const{lap}$ button or a $\const{stop}$ button; and a $\const{stopped}$ position, from which I may tap either a $\const{start}$ button or a $\const{reset}$ button.
    Thinking of the buttons available to press as the directions at each position, the corresponding polynomial is
    \[
        \{\const{zero}\}\yon^{\{\const{start}\}}+\{\const{running}\}\yon^{\{\const{lap},\,\const{stop}\}}+\{\const{stopped}\}\yon^{\{\const{start},\, \const{reset}\}}\iso\yon+\2\yon^\2.
    \]
\end{solution}
\end{exercise}

\index{polynomial functor!positions and directions|)}


%-------- Section --------%
\section{Corolla forests}

We would like to have graphical depictions of our polynomials to make them easy to visualize.
These will take the form of special graphs known as \emph{corolla forests}.
We build up to defining them as follows.

\index{corolla forest|see{polynomial functor!associated corolla forest}}\index{polynomial functor!associated corolla forest}\index{corolla forest|(}


Our first definition will be familiar to students of graph theory, although we will add some technical details suited to our purposes.
\begin{definition}[Rooted tree]\index{tree!rooted}\index{tree!corolla and}
    A \emph{rooted tree} is a directed acyclic graph with a distinguished vertex called the \emph{root} such that there exists a unique directed path from the root to each vertex.
    \index{rooted tree|see{tree, rooted}}

    We allow infinitely and even uncountably many vertices and infinitely and even uncountably many edges incident to each vertex; on the other hand, each pair of vertices is connected by a (necessarily unique) path of finitely many adjacent edges.
\end{definition}
Since all our trees will be rooted, we may refer to them simply as \emph{trees}---roots are implied.
We will draw our trees with roots at the bottom and other vertices ``growing'' upward.

The following terminology will be handy when working with our trees; these terms should be familiar, or at the very least they should match your intuition.
\begin{definition}[Rooted path; height]
    A \emph{rooted path} is a (directed) path in a rooted tree from its root to any vertex.

    \index{rooted tree!rooted path in}
    \index{rooted tree!height of vertex}

    Given a vertex of a rooted tree, its \emph{height} is the length of (i.e.\ number of edges in) the rooted path to that vertex.
\end{definition}
In any rooted tree, the root has height $0$, the length of the empty rooted path to the root itself; every neighbor of the root has height $1$; every neighbor of a vertex of height $1$ either is the root or has height $2$; and so forth.

Now we can define a special kind of tree that we will use to depict representable functors.
\begin{definition}[Corolla]
    A \emph{corolla} is a rooted tree in which every vertex aside from the root has height $1$. We call these vertices the \emph{leaves} of the corolla.

    The \emph{corolla associated to a representable functor $\yon^A$} for $A\in\smset$ is the corolla whose leaves are in bijection with $A$.
\end{definition}

\index{corolla}

\begin{example}
    Here are the corollas associated to various representables:
    \begin{equation*}
    \begin{tikzpicture}[trees, sibling distance=2mm]
        \node["$\yon^{\1}\iso\yon$" below] (1) {$\bullet$}
        child;
    \end{tikzpicture}
    \qquad
    \begin{tikzpicture}[trees, sibling distance=2mm]
        \node["$\yon^{\5}$" below] (1) {$\bullet$}
        child foreach \i in {1,...,5}
        ;
    \end{tikzpicture}
    \qquad
    \begin{tikzpicture}[trees, sibling distance=1mm]
        \node["$\yon^{\1\0}$" below] (1) {$\bullet$}
        child foreach \i in {1,...,10}
        ;
    \end{tikzpicture}
    \qquad
    \begin{tikzpicture}[trees, sibling distance=0.5mm]
        \node["$\yon^{\2\0}$" below] (1) {$\bullet$}
        child foreach \i in {1,...,20}
        ;
    \end{tikzpicture}
    \qquad
    \begin{tikzpicture}[trees, sibling distance=0.25mm]
        \node["$\yon^{\4\0}$" below] (1) {$\bullet$}
        child foreach \i in {1,...,40}
        ;
    \end{tikzpicture}
    \qquad
    \begin{tikzpicture}[trees, sibling distance=0.0625mm]
        \node["$\yon^{[0,1]}$" below] (1) {$\bullet$}
        child foreach \i in {1,...,160}
        ;
    \end{tikzpicture}
\end{equation*}
\end{example}

In the example above, the roots are indicated by dots ($\bullet$), and the leaves are indicated by arrows ($\uparrow$).
Because the direction-set of the representable is in bijection with the leaves of the associated corolla, we can think of each leaf as a direction, so it makes sense to draw the leaves as arrows pointing in different directions.
Thinking of directions as menu options like in the previous section, we may view these corollas as mini-decision trees, indicating all the possible options we could select.
\index{corolla forest!leaf of}\index{corolla forest!corolla of}
\index{decision tree}

\begin{example}
    The corolla associated to $\yon^\0\iso\1$ has \textit{no} leaves: it is the rooted tree consisting of one vertex---the root---and no edges.
    \begin{equation*}
        \begin{tikzpicture}[trees, sibling distance=2mm]
            \node["$\yon^\0\iso\1$" below] (1) {$\bullet$};
        \end{tikzpicture}
    \end{equation*}
    By definition, the root itself is \textit{not} a leaf, so the corolla above does in fact have $0$ leaves.
    With no arrows pointing out, it is the corolla associated to a representable with no directions.
\end{example}

As each representable functor has an associated corolla, each polynomial functor will have an associated disjoint union of corollas that we call a \emph{corolla forest}.

\index{corolla!representable as}

\begin{definition}[Corolla forest]
    A \emph{corolla forest} is a disjoint union of corollas.

    The \emph{corolla forest associated to a polynomial functor} $p\iso\sum_{i\in p(\1)}\yon^{p[i]}$ is the disjoint union of the corollas associated to each representable summand $\yon^{p[i]}$ of $p$.
    When we draw the corolla forest associated to $p$, we may say that we are \emph{drawing $p$ as a (corolla) forest}.
    We call the corollas in this forest corresponding to $p$-positions \emph{$p$-corollas} and the leaves corresponding to $p[i]$-directions \emph{$p[i]$-leaves}.
\end{definition}

\begin{example} \label{ex.corolla-forest}
    We may draw $p\coloneqq\yon^\2+\2\yon+\1$ as a forest like so:
    \begin{equation} \label{pic.forest-example}
    \begin{tikzpicture}[trees]
        \node (1) {$\bullet$}
        child {}
        child {};
        \node[right=.5 of 1] (2) {$\bullet$}
        child {};
        \node[right=.5 of 2] (3) {$\bullet$}
        child {};
        \node[right=.5 of 3] (4) {$\bullet$};
    \end{tikzpicture}
    \end{equation}
    Each of the $4$ corollas in \eqref{pic.forest-example} corresponds to one of the $4$ representable summands of $p$.
    The $4$ roots in \eqref{pic.forest-example} correspond to the $4$ positions of $p$, and the leaves connected to each root correspond to the directions at each position.
    Note that $p$ has $1$ position with $2$ directions, $2$ positions with $1$ direction each, and $1$ position with $0$ directions.
    Hence \eqref{pic.forest-example} is the disjoint union of $1$ corolla with $2$ leaves, $2$ corollas with $1$ leaf each, and $1$ corolla with $0$ leaves.

    Since $p(\1)\iso\4$, we could label the positions of $p$ with the elements of $\4=\{1,2,3,4\}$ so that
    \[
        p[1] = \2, \qquad p[2] = \1, \qquad p[3] = \1, \qquad p[4] = \0.
    \]
    Then we could give these same labels to the roots in \eqref{pic.forest-example}:
    \[
    \begin{tikzpicture}[trees]
        \node["$1$" below] (1) {$\bullet$}
        child {}
        child {};
        \node["$2$" below, right=.5 of 1] (2) {$\bullet$}
        child {};
        \node["$3$" below, right=.5 of 2] (3) {$\bullet$}
        child {};
        \node["$4$" below, right=.5 of 3] (4) {$\bullet$};
    \end{tikzpicture}
    \]
    Similarly, we could label the directions and their corresponding leaves, but we will reserve leaf labels for another purpose.
\end{example}

\begin{exercise}
    Consider the polynomial $p\coloneqq\2\yon^\3+\2\yon+\1$.
    \begin{enumerate}
        \item Draw $p$ as a corolla forest.
        \item How many roots does this forest have?
        \item How many positions of $p$ do these roots represent?
        \item For each $p$-corolla, say how many leaves it has.
        \item For each $p$-position, say how many directions it has. \qedhere
    \end{enumerate}
    \begin{solution}
        \begin{enumerate}
            \item Here is the corolla forest associated to $p\coloneqq\2\yon^\3+\2\yon+\1$ (note that the order in which the corollas are drawn does not matter):
            \[
            \begin{tikzpicture}[trees, sibling distance=3mm]
                \node (1) {$\bullet$}
                child {}
                child {}
                child {};
                \node[right=.7 of 1] (2) {$\bullet$}
                child {}
                child {}
                child {};
                \node[right=.5 of 2] (3) {$\bullet$}
                child {};
                \node[right=.3 of 3] (4) {$\bullet$}
                child {};
                \node[right=.3 of 4] (5) {$\bullet$};
            \end{tikzpicture}
            \]
            \item The forest has $5$ roots.
            \item The roots represent the $5$ positions, one position per root.
            \item \label{sol.forest.leaves} The first and second corollas have $3$ leaves each, the third and fourth corollas have $1$ leaf each, and the fifth corolla has $0$ leaves.
            \item The directions at each position correspond to the leaves in each corolla, so just copy the answer from \cref{sol.forest.leaves}, replacing ``corolla'' with ``position'' and ``leaf'' with ``direction'': the first and second positions have $3$ directions each, the third and fourth positions have $1$ direction each, and the fifth position has $0$ directions.
        \end{enumerate}
    \end{solution}
\end{exercise}

The position-set or any of the direction-sets of a polynomial may be infinite.
This makes their associated corolla forests impossible to draw precisely, but they may be approximated.
We sketch the polynomial $\yon^\3+\nn\yon^{[0,1]}$ as a forest below.
\[%\label{eqn.represented_interval}
\begin{tikzpicture}[trees, sibling distance=0.0625mm]
\node (1) {$\bullet$}
child[sibling distance=3mm] foreach \i in {1,2,3}
;
\node[right=1 of 1] (2) {$\bullet$}
child foreach \i in {1,...,160}
;
\node[right=1 of 2] (3) {$\bullet$}
child foreach \i in {1,...,160}
;
\node[right=1 of 3] (4) {$\bullet$}
child foreach \i in {1,...,160}
;
\node[right=.7 of 4] (5) {$\cdots$};
\end{tikzpicture}
\]

\begin{exercise}
If you were a suitor choosing the corolla forest you love, aesthetically speaking, which would strike your interest? Answer by selecting the associated polynomial:
\begin{enumerate}
    \item $\yon^\2+\yon+\1$
    \item $\yon^\3+\3\yon^\2+\3\yon+\1$
    \item $\yon^\2$
    \item $\yon+\1$
    \item $(\nn\yon)^\nn$
    \item $S\yon^S$ for some set $S$
    \item $\yon^{\1\0\0}+\yon^\2+\3\yon$
    \item $\yon + \2\yon^\4 + \3\yon^\9 + \4\yon^{\1\6} + \cdots$
    \item Your polynomial's name $p$ here.
\end{enumerate}
Any reason for your choice? Draw a sketch of your forest.
\begin{solution}
    Aesthetically speaking, here is a polynomial that may be drawn as a beautiful corolla forest:
    \[
        p\coloneqq\yon^\0+\yon^\1+\yon^\2+\yon^\3+\cdots
    \]
    It is reminiscent (and formally related) to the notion of lists: if $A$ is any set, then $p(A)\iso A^\0+A^\1+A^\2+\cdots$ is the set $\lst(A)$ of lists (i.e.\ finite ordered sequences) with entries in $A$.
    Here is a picture of the lovely forest associated to $p$:
    \[
    \begin{tikzpicture}[trees, sibling distance=3mm]
        \node (1) {$\bullet$};
        \node[right=.3 of 1] (2) {$\bullet$}
        child {};
        \node[right=.4 of 2] (3) {$\bullet$}
        child {}
        child {};
        \node[right=.6 of 3] (4) {$\bullet$}
        child {}
        child {}
        child {};
        \node[right=.6 of 4] {$\cdots$};
    \end{tikzpicture}
    \]
\end{solution}
\end{exercise}

Corolla forests help us visualize the positions and directions of polynomials, and they will especially come in handy in the next chapter, when we describe the morphisms between our polynomials and how they interact with positions and directions.
They may also depict the elements of a polynomial functor applied to a given set, as follows.
We have seen that for a polynomial $p$ and a set $X$,
\[
    p(X) \iso \sum_{i\in p(\1)}X^{p[i]} \iso \{(i,f)\mid i\in p(\1), f\colon p[i]\to X\}.
\]
So an element of $p(X)$ is a $p$-position $i$ along with a function $f$ that maps each direction at $i$ to an element of $X$.
Equivalently, it is a $p$-corolla along with a function that maps each of its leaves to an element of $X$.
Then to draw an element $(i,f)\in p(X)$, we simply need to draw the $p$-corolla corresponding to $i$ and label its leaves with elements of $X$ according to $f$.

\begin{example} \label{ex.corolla-apply-poly}
    In \cref{ex.corolla-forest}, we drew $p\coloneqq\yon^\2+\2\yon+\1$ as a corolla forest like so:
    \[
    \begin{tikzpicture}[trees]
        \node["$1$" below] (1) {$\bullet$}
        child {}
        child {};
        \node["$2$" below, right=.5 of 1] (2) {$\bullet$}
        child {};
        \node["$3$" below, right=.5 of 2] (3) {$\bullet$}
        child {};
        \node["$4$" below, right=.5 of 3] (4) {$\bullet$};
    \end{tikzpicture}
    \]
    Previously, in \cref{ex.verbose_poly_eval}, we wrote out all $9$ elements of $p$ applied to the set $X\coloneqq\{a,b\}$ as tuples.
    We could draw them out instead---an element of $p(X)$ may be depicted as one of the four corollas above with each of its leaves labeled with an element of $X$:
    \[
    \begin{tikzpicture}[trees, sibling distance=5mm]
        \node["$1$" below] (1) {$\bullet$}
            child {node {$a$}}
            child {node {$a$}};
        \node["$1$" below, right=1.2 of 1] (2) {$\bullet$}
            child {node {$a$}}
            child {node {$b$}};
        \node["$1$" below, right=1.2 of 2] (3) {$\bullet$}
            child {node {$b$}}
            child {node {$a$}};
        \node["$1$" below, right=1.2 of 3] (4) {$\bullet$}
            child {node {$b$}}
            child {node {$b$}};
        \node["$2$" below, right=1.1 of 4] (5) {$\bullet$}
            child {node {$a$}};
        \node["$2$" below, right=of 5] (6) {$\bullet$}
            child {node {$b$}};
        \node["$3$" below, right=of 6] (7) {$\bullet$}
            child {node {$a$}};
        \node["$3$" below, right=of 7] (8) {$\bullet$}
            child {node {$b$}};
        \node["$4$" below, right=0.9 of 8] (9) {$\bullet$};
    \end{tikzpicture}
    \]
\end{example}

Throughout this book, we will generally use corolla forests to depict polynomials with relatively small numbers of positions or directions, where drawing out entire corolla forests is manageable.
Later, we will study how building larger rooted trees out of these corollas corresponds to conducting various categorical operations on our polynomials.

\index{corolla forest|)}

%% TODO: how much later?

%-------- Section --------%
\section{Polyboxes}

Before we conclude this chapter, we introduce one more tool for visualizing polynomials whose full power will not be evident until later.

\index{polybox|(}

Throughout this book, we may depict a polynomial $p$ as a pair of boxes stacked on top of each other, like so:
\[
  \begin{tikzpicture}[polybox, tos]
    \node[poly, "$p$" below] (p) {};
    \node[left=0pt of p_pos] {$p(\1)$};
    \node[left=0pt of p_dir] {$p[-]$};
  \end{tikzpicture}
\]
We call this picture the \emph{polyboxes for $p$}.
Think of these boxes as cells in a spreadsheet.
The bottom cell, or the \emph{position box}, is restricted to values in the set $p(\1)$ (as indicated by the label to its left)---it must be filled with a $p$-position, say $i\in p(\1)$:
\[
  \begin{tikzpicture}[polybox, tos]
    \node[poly, "$p$" below] (p) {\at$i$};
    \node[left=0pt of p_pos] {$p(\1)$};
    \node[left=0pt of p_dir] {$p[-]$};
  \end{tikzpicture}
\]
The top cell, or the \emph{direction box}, cannot be filled until the position box below it is.
Once the position box contains a $p$-position $i$, the direction box must be filled with a $p[i]$-direction, say $a\in p[i]$:
\[
  \begin{tikzpicture}[polybox, tos]
    \node[poly, "$p$" below] (p) {$a$\at$i$};
    \node[left=0pt of p_pos] {$p(\1)$};
    \node[left=0pt of p_dir] {$p[-]$};
  \end{tikzpicture}
\]
The $p[-]$ label to the left of the direction box reminds us that the $a$ within it is an element of $p[i]$, where $i$ is the entry in the position box.
Once we are accustomed to polyboxes, we will often drop these reminder labels, so that
\[
  \begin{tikzpicture}[polybox, tos]
    \node[poly, "$p$" below] (p) {$a$\at$i$};
  \end{tikzpicture}
\]
serves as a graphical shorthand for the statement ``consider a polynomial functor $p$ with position $i\in p(\1)$ and direction $a\in p[i]$.''

\index{polybox!spreadsheet}\index{polybox!position box}\index{polybox!direction box}

Viewing polynomials as these restricted two-cell spreadsheets reinforces the idea that directions are like menu options: imagine a dropdown menu for the direction box above a filled position box that lists all the directions to choose from at the given position.
Polyboxes also help us conceptualize the possible pairs of positions and directions of a polynomial whose corolla forest is impractical to draw, as suggested by the following example.

\begin{example}
  Consider the polynomial
  \[
    p\coloneqq\sum_{r\in\rr}\yon^{[-|r|,|r|]},
  \]
  whose positions are the real numbers and whose directions at position $r$ are the real numbers with magnitude at most $|r|$.
  There is no clear way to draw $p$ as a corolla forest, but we could draw its polyboxes
  \[
  \begin{tikzpicture}[polybox, tos]
    \node[poly, "$p$" below] (p) {$s$\at$r$};
    \node[left=0pt of p_pos] {$p(\1)$};
    \node[left=0pt of p_dir] {$p[-]$};
  \end{tikzpicture}
  \]
  with the condition that $r$ and $s$ are real numbers satisfying $|s|\leq|r|$.
\end{example}

We may also use polyboxes to highlight our special classes of polynomials.
When a position box may only be filled with one possible entry, we shade it in like so:
\[
\begin{tikzpicture}[polybox, tos]
  \node[poly, "$p\iso\yon^A$" below, pure] (p) {};
  \node[left=0pt of p_pos] {$p(\1)\iso\1$};
  \node[left=0pt of p_dir] {$A$};
\end{tikzpicture}
\]
The idea is that if there is only one entry that could fill a given box, then it should come pre-filled---no further choice needs to be made to fill it.
Here $p(\1)\iso\1$, so $p$ is \textit{representable}; indeed, $p\iso\yon^A$, where $A$ is the set of possible entries for the unfilled direction box.

Similarly, the polyboxes for a \textit{linear} polynomial $I\yon$, whose direction-set at each position is a singleton, can be drawn like so:
\[
\begin{tikzpicture}[polybox, tos]
  \node[poly, "$I\yon$" below, linear] (p) {};
  \node[left=0pt of p_pos] {$I$};
\end{tikzpicture}
\]
No matter what fills the position box, there is exactly one entry that could fill the direction box, so it comes pre-filled.
The identity polynomial functor $\yon$, which is both representable and linear, therefore has the following polyboxes:
\[
\begin{tikzpicture}[polybox, tos]
  \node[poly, "$\yon$" below, identity] (p) {};
\end{tikzpicture}
\]
It has exactly one position and exactly one direction, so both its boxes come pre-filled.

\index{polybox!for linear polynomials}\index{polybox!for representable polynomials}

Finally, a \textit{constant} polynomial $I$ for some set $I$ has empty direction-sets.
We indicate this by coloring its direction box red:
\[
\begin{tikzpicture}[polybox, tos]
  \node[poly, "$I$" below, constant] (p) {};
  \node[left=0pt of p_pos] {$I$};
\end{tikzpicture}
\]
Because every direction-set is empty, there is nothing that may be written in the direction box.
The red suggests a kind of error---the direction box cannot be filled.
The polynomial functor $\1$, which is both representable and constant, therefore has the following polyboxes:
\[
\begin{tikzpicture}[polybox, tos]
  \node[poly, "$\1$" below, terminal] (p) {};
\end{tikzpicture}
\]

\index{polybox!for constant polynomials}\index{constant polynomial}


In the next chapter, we will introduce the morphisms between polynomial functors and see how their behavior may be depicted using polyboxes.

\index{polybox|)}


%-------- Section --------%
\section{Summary and further reading}

In this chapter, we introduced the main objects of study in this book: \emph{polynomial functors}, which are sums of representable functors $\smset\to\smset$.
We write a polynomial $p$ as $\sum_{i\in p(\1)}\yon^{p[i]}$, calling the elements of $p(\1)$ the \emph{positions} of $p$ and the elements of $p[i]$ the \emph{directions} of $p$ at position $i$.
As a polynomial is determined up to isomorphism by its position-set and direction-sets, we can think of the data of a polynomial as an indexed family of sets $(p[i])_{i\in p(\1)}$.

We highlighted four special classes of polynomials (here $I$ and $A$ are sets):
\begin{itemize}
  \item constants $I$, whose direction-sets are all empty;
  \item linear polynomials $I\yon$, whose direction-sets are all singletons;
  \item representables $\yon^A$, whose position-sets are singletons;
  \item monomials $I\yon^A$, whose direction-sets all have the same cardinality.
\end{itemize}

\index{constant polynomial}\index{linear polynomial}\index{representable polynomial}\index{polynomial functor!monomial|see{monomial}}\index{monomial}

Throughout this book, we will use polynomials to model decision-making agents that hold positions and take directions from those positions.
We can draw a polynomial $p$ graphically as a \emph{corolla forest}, with a \emph{corolla} (a rooted tree whose non-root vertices are all leaves) for every $p$-position $i$ that has a leaf for every $p[i]$-direction.
We can also depict a polynomial as a \emph{polybox picture}, resembling two stacked cells in a spreadsheet, to be filled in with an element of $i$ below and an element of $p[i]$ above.

\index{corolla forest}\index{polybox}

There are many fine sources on polynomial functors. Some of the computer science literature is more relaxed about what a polynomial is. For example, the ``coalgebra community'' often defines a polynomial to include finite power sets (see e.g.\ \cite{jacobs2017introduction}). Other computer science communities use the same definition of polynomial, but refer to it as a \emph{container} and use different words for its positions (they call them ``shapes'') and directions (they call them, rather unfortunately, ``positions''). See e.g.\ \cite{abbot2003categoriesthesis,abbott2005containers}.\index{coalgebra!community}

\index{polynomial functor!term usage by ``coalgebra community''}

But the notion of polynomial functors seems to have originated from Andr\'{e} Joyal. A good introduction to polynomial functors---including an extensive bibliography of references---can be found in \cite{kock2012polynomial} and more extensive notes in \cite{kock2016}; in particular the related work section on page~3 provides a nice survey of the field. A reader may also be interested in the Workshops on Polynomial Functors organized by the Topos Institute: \url{https://topos.site/p-func-workshop/}.

\index{Joyal, Andr\'{e}}\index{Kock, Joachim}

%-------- Section --------%
\section{Exercise solutions}
\Closesolutionfile{solutions}
{\footnotesize
    \input{solution-file2}}

\Opensolutionfile{solutions}[solution-file3]

%------------ Chapter ------------%
\chapter{The category of polynomial functors} \label{ch.poly.cat}
\index{category!of polynomial functors|(}

% TODO: Should we name it "Morphisms between polynomial functors" or maybe "Dependent lenses" instead?? If so, split off symmetric monoidal structures except little bit about coproducts

In this chapter, we will define $\poly$, our main category of interest, so that we have a firm foundation from which to speak about interactive systems.
The objects of $\poly$ are the polynomial functors that we defined in the previous chapter.
Here we will examine the morphisms of $\poly$: natural transformations between polynomial functors.
Along the way, we will present some of $\poly$'s most versatile categorical properties.

\index{natural transformation!between polynomials|see{lens}}


%-------- Section --------%
\section{Dependent lenses between polynomial functors}
\label{sec.poly.cat.morph}


Before we define the category $\poly$ of polynomial functors, we note that polynomial functors live inside a category already: the category $\smset^\smset$ of functors $\smset\to\smset$, whose morphisms are natural transformations.
This leads to a natural definition of morphisms between polynomial functors, from which we can derive a category of polynomial functors for free.
We call such a morphism a \emph{dependent lens}, or a \emph{lens} for short.
If you are familiar with lenses from functional programming, we'll see in \cref{subsec.poly.cat.morph.bimorphic-lens} how our notion of a dependent lens is related.

\index{functional programming!lenses in}
\index{polynomial functor!morphism of|see{lens}}
\index{dependent lens|see{lens}}
\index{lens|(}

\begin{definition}[Dependent lens, $\poly$] \label{def.poly_cat}
Given polynomial functors $p$ and $q$, a \emph{dependent lens} (or simply \emph{lens}) \emph{from $p$ to $q$} is a natural transformation $p\to q$.
Then $\poly$ is the category whose objects are polynomial functors and whose morphisms are dependent lenses.
\end{definition}

In other words, $\poly$ is the full subcategory of $\smset^\smset$ spanned by the polynomial functors: we take the category $\smset^\smset$, throw out all the objects that are not (isomorphic to) polynomials, but keep all the same morphisms between the objects that remain.

\index{polynomial functor!category of polynomials}
\index{polynomial functor!full subcategory of $\smset^\smset$}

Unraveling the familiar definition of a natural transformation, a dependent lens between polynomial functors $p \to q$ thus consists of a function $p(X) \to q(X)$ for every set $X$ such that naturality squares commute.
That is a lot of data to keep track of!
Fortunately, there is a much simpler way to think about these lenses, which we will discover using the Yoneda lemma.

\index{Yoneda lemma}\index{isomorphism!natural}

\begin{exercise} \label{exc.poly_morph_yoneda}
Given a set $S$ and a polynomial $q$, show that a lens $\yon^S \to q$ can be naturally identified with an element of the set $q(S)$.
That is, show that there is an isomorphism
\[
    \poly(\yon^S, q) \iso q(S).
\]
natural in both $S$ and $q$.
Hint: Use the Yoneda lemma (\cref{lemma.yoneda}).
\begin{solution}
We know that $\poly$ is the full subcategory of $\smset^\smset$ spanned by polynomial functors, including $\yon^S$ and $q$.
So $\poly(\yon^S, q)=\smset^\smset(\yon^S, q)$.
Hence the natural isomorphism $\poly(\yon^S, q)\iso q(S)$ follows directly from the Yoneda lemma (\cref{lemma.yoneda}) with $F\coloneqq q$.
\end{solution}
\end{exercise}

The above exercise gives us an alternative characterization for lenses out of representable functors.
But before we can characterize lenses out of polynomial functors in general, we need to describe how coproducts work in $\poly$.
Fortunately, since polynomial functors are defined as coproducts of representables, coproducts in $\poly$ are easy to understand.

\index{coproduct}\index{polynomial functor!coproduct of polynomials|see{polynomial functors, sum of polynomials}}
\index{polynomial functor!product of polynomials|see{polynomial functors, product of polynomials}}

\begin{proposition} \label{prop.poly_coprods}
  The category $\poly$ has all small coproducts, coinciding with coproducts in $\smset^\smset$ given by the operation $\sum_{i \in I}$ for each set $I$.
\end{proposition}
\begin{proof}
  By \cref{cor.sum_prod_set_endofuncs}, the category $\smset^\smset$ has all small coproducts given by $\sum_{i\in I}$.
  The full subcategory inclusion $\poly\to \smset^\smset$ reflects these coproducts, and by definition $\poly$ is closed under the operation $\sum_{i \in I}$.
\end{proof}

Explicitly, given an $I$-indexed family of polynomials $(p_i)_{i \in I}$, its coproduct is
\begin{equation} \label{eqn.poly_coprod}\index{coproduct!of indexed family}
  \sum_{i \in I} p_i \iso \sum_{i \in I} \sum_{j \in p_i(\1)} \yon^{p_i[j]} \iso \sum_{(i,j) \in \sum_{i \in I} p_i(\1)} \yon^{p_i[j]}
\end{equation}
by \cref{cor.sum_prod_set_endofuncs}.
This coincides with our notion of polynomial addition from high school algebra: just add all the terms together, combining like terms to simplify.
Binary coproducts are given by binary sums of functors, appropriately denoted by $+$, while the initial object of $\poly$ is the constant polynomial $\0$.

In particular, \eqref{eqn.poly_coprod} implies that for any polynomials $p$ and $q$, their coproduct $p+q$ is given as follows.
The position-set of $p+q$ is the coproduct of sets $p(\1) + q(\1)$.
At position $(1,i) \in p(\1) + q(\1)$ with $i \in p(\1)$, the directions of $p+q$ are just the $p[i]$-directions; at position $(2,j) \in p(\1) + q(\1)$ with $j \in q(\1)$, the directions of $p+q$ are just the $q[j]$-directions.

Crucially, we have the following corollary.

\begin{corollary} \label{cor.poly-coprod-repr}
  In the category $\poly$, every polynomial $p$ is the coproduct of its representable summands $(\yon^{p[i]})_{i\in p(\1)}$.
\end{corollary}

In other words, writing $p$ as the sum $\sum_{i\in p(\1)}\yon^{p[i]}$ is not just a coproduct in $\smset^\smset$; it is also a coproduct in $\poly$ itself.\index{coproduct!in $\poly$}

We are now ready to give our alternative characterization of dependent lenses.
Recall that a polynomial $p\iso\sum_{i\in p(\1)}\yon^{p[i]}$ can be uniquely identified with an indexed family $p[-]\colon p(\1)\to\smset$, a functor from the set $p(\1)$ viewed as a discrete category.\index{category!discrete}

\begin{proposition}\label{prop.lens-prod-sum}
Given polynomials $p$ and $q$, there is an isomorphism
\begin{equation}\label{eqn.main_formula}
\poly(p,q)\cong\prod_{i\in p(\1)}\sum_{j\in q(\1)}{p[i]}^{q[j]}
\end{equation}
natural in $p$ and $q$.
In particular, a lens $f\colon p\to q$ can be identified with a pair $(f_\1,f^\sharp)$
\begin{equation}\label{eqn.colax_poly_map}
\begin{tikzcd}[column sep=small]
	p(\1)\ar[dr, "p{[-]}"']\ar[rr, "f_\1"]&~&
	q(\1)\ar[dl, "q{[-]}"]\\&
	\smset\ar[u, phantom, near end, "\overset{f^\sharp}{\Leftarrow}"]
\end{tikzcd}
\end{equation}
where $f_\1\colon p(\1)\to q(\1)$ is a function (equivalently, a functor between discrete categories) and $f^\sharp \colon q[f_\1(-)] \to p[-]$ is a natural transformation: a function $f^\sharp_i\colon q[f_\1i]\to p[i]$ for each $i\in p(\1)$.
\end{proposition}
\begin{proof}
We have $p\iso\sum_{i\in p(\1)}\yon^{p[i]}$.
Then by \cref{cor.poly-coprod-repr} and the universal property of the coproduct, we have a natural isomorphism\index{coproduct!universal property of}\index{isomorphism!natural}
\[
    \poly\left(\sum_{i\in p(\1)}\yon^{p[i]}, q\right) \iso \prod_{i\in p(\1)}\poly(\yon^{p[i]},q).
\]
Applying \cref{exc.poly_morph_yoneda} (i.e.\ the Yoneda lemma) and the fact that $q\iso\sum_{j\in q(\1)}\yon^{q[j]}$ yields the natural isomorphism
\[
  \prod_{i\in p(\1)}\poly(\yon^{p[i]},q) \iso \prod_{i\in p(\1)}q(p[i]) \iso \prod_{i\in p(\1)}\sum_{j\in q(\1)}p[i]^{q[j]},
\]
so \eqref{eqn.main_formula} follows.

\index{lens!positions and directions}\index{Yoneda lemma}

The right hand side of \eqref{eqn.main_formula} is the set of dependent functions $f\colon(i\in p(\1))\to\sum_{j\in q(\1)}p[i]^{q[j]}$.
Each such dependent function is uniquely determined by its two projections $\pi_1f\colon(i\in p(\1))\to q(\1)$ and $\pi_2f\colon(i\in p(\1))\to p[i]^{q[\pi_1fi]}$.
These can be identified respectively with a (non-dependent) function $f_\1\coloneqq\pi_1f$ with signature $p(\1)\to q(\1)$ and a natural transformation $f^\sharp\colon q[f_\1(-)]\to p[-]$ whose $i$-component for $i\in p(\1)$ is $f^\sharp_i\coloneqq\pi_2 fi\in p[i]^{q[f_\1i]}$.
\end{proof}

We have now greatly simplified our characterization of a dependent lens $f\colon p\to q$: rather than infinitely many functions satisfying infinitely many naturality conditions, $f$ may simply be specified by a function $f_\1\colon p(\1)\to q(\1)$ and, for each $i\in p(\1)$, a function $f^\sharp_i\colon q[f_\1i]\to p[i]$, without any additional restrictions.
This characterization can be expressed entirely in the language of positions and directions: $f_\1$ is a function from $p$-positions to $q$-positions, while $f^\sharp_i$ for a $p$-position $i$ is a function from $q[f_\1i]$-directions to $p[i]$-directions.
This leads to the following definition.

\begin{definition}[On-positions function, on-directions map and function]
  Given a lens $f\colon p\to q$, let $(f_\1, f^\sharp)$ denote the pair identified with $f$ via \cref{prop.lens-prod-sum}.
  Then we call the function $f_\1\colon p(\1)\to q(\1)$ the \emph{(forward) on-positions function of $f$}, while we call the natural transformation $f^\sharp\colon q[f_\1(-)]\to p[-]$ the \emph{(backward) on-directions map of $f$}.
  For $i\in p(\1)$, we call the $i$-component $f^\sharp_i\colon q[f_\1i]\to p[i]$ of $f^\sharp$ the \emph{(backward) on-directions function of $f$ at $i$}.
\end{definition}

The above definition highlights the bidirectional nature of a lens $f\colon p\to q$: it consists of a function going \textit{forward} on positions, following the direction of $f$ from $p$ to $q$, as well as functions going \textit{backward} on directions, opposing the direction of $f$ from $q$ to $p$.
This forward-backward interaction is what drives the applications of $\poly$ we will study.

\index{lens!bidirectionality of}

We prefer to call a morphism between polynomial functors a ``lens'' rather than a ``natural transformation'' because we wish to emphasize this concrete on-positions and on-directions perspective.
Whenever we do need to view a morphism in $\poly$ as a natural transformation, we will refer to them as such.

In the next several sections, we will give some examples of lenses and intuition for thinking about them in terms of interaction protocols, corolla forests, and polyboxes.


%-------- Section --------%
\section{Dependent lenses as interaction protocols}

Here is our first example of a dependent lens and a real-world interaction it might model.

\begin{example}[Modeling an interaction protocol with a lens]
  Recall our coin jar polynomial from \cref{ex.coin-jar}:
  \[
    q\coloneqq\{\const{open}\}\yon^{\{\const{penny},\,\const{nickel},\,\const{dime},\,\const{quarter}\}}+\{\const{closed}\}\yon^\0.
  \]
  It has $2$ positions: its $\const{open}$ position has $4$ directions representing the $4$ denominations of coins it may take, while its $\const{closed}$ position has $0$ directions to indicate that it cannot take anything.

  \index{lens!as interaction protocol|(}

  Now imagine that we model the owner of this coin jar with the following polynomial:
  \begin{align*}
    p\coloneqq\:
    &\{\const{needy}\}\yon^{\{\const{save},\, \const{spend}\}}
      \\
    +\:
    &\{\const{greedy}\}\yon^{\{\const{accept},\, \const{reject},\,\const{ask for more}\}}
      \\
    +\:
    &\{\const{content}\}\yon^{\{\const{count},\,\const{rest}\}}.
  \end{align*}
  Each of its $3$ positions represents a possible mood of the owner, and the directions at each position represent the options available to an owner in the corresponding mood.
  We will construct a lens $f\colon p\to q$ to model the interaction between the owner and their coin jar.

  Say that a $\const{needy}$ or $\const{greedy}$ owner will keep their coin jar $\const{open}$, while a $\const{content}$ owner will keep their coin jar $\const{closed}$.
  We can express this with an on-positions function $f_\1$ from the set of $p$-positions (on the left) to the set of $q$-positions (on the right), as follows (the dashed arrows indicate the function assigments):
  \begin{center}
  \scalebox{.7}{
  \begin{tikzpicture}
    % q-positions (right)
      % from top to bottom: open, closed
    \node (open) {\const{open}};
    \node[below=.5 of open] (closed) {\const{closed}};

    \node[draw, ellipse, inner sep=0pt, fit=(open)(closed),
      label={[anchor=south,below]270:$q(\1)$}] (qpos) {};

    % p-positions (left)
      % from top to bottom: needy, greedy, content
      % greedy aligned with middle of q-positions
    \node[left=3 of qpos] (greedy) {\const{greedy}};
    \node[above=.5 of greedy] (needy) {\const{needy}};
    \node[below=.5 of greedy] (content) {\const{content}};

    \node[draw, ellipse, inner sep=0pt, fit=(greedy)(needy)(content),
      label={[anchor=south,below]270:$p(\1)$}] (ppos) {};

    \draw[mapsto] (needy) -- (open);
    \draw[mapsto] (greedy) -- (open);
    \draw[mapsto] (content) -- (closed);
  \end{tikzpicture}
  }
  \end{center}

  From there, say that a $\const{needy}$ owner whose coin jar receives a $\const{nickel}$ or higher will choose to $\const{save}$ it, but one whose coin jar receives a $\const{penny}$ will choose to $\const{spend}$ it.
  Meanwhile, a $\const{greedy}$ owner whose coin jar receives a $\const{penny}$ or $\const{nickel}$ will ask for more, but one whose coin jar receives a $\const{dime}$ or $\const{quarter}$ will accept it.
  We can express this behavior with an on-directions map $f^\sharp\colon q[f_\1(-)]\to p[-]$.
  Its $\const{needy}$-component is the on-directions function $f^\sharp_{\const{needy}}\colon q[f_\1(\const{needy})]\to p[\const{needy}]$ drawn as follows:
  \begin{center}
  \scalebox{.7}{
  \begin{tikzpicture}
    % p-directions (left)
      % from top to bottom: save, spend
    \node (save) {\const{save}};
    \node[below=.5 of save] (spend) {\const{spend}};

    \node[draw, ellipse, inner sep=0pt, fit=(save)(spend),
      label={[anchor=south,below]270:$p[\const{needy}]$}] (pdir) {};

    % q-directions (right)
      % from top to bottom: penny, nickel, dime, quarter
      % nickel aligned with save
    \node[right=3 of save] (nickel) {\const{nickel}};
    \node[above=.5 of nickel] (penny) {\const{penny}};
    \node[below=.5 of nickel] (dime) {\const{dime}};
    \node[below=.5 of dime] (quarter) {\const{quarter}};

    \node[draw, ellipse, inner sep=0pt,
      fit=(penny)(nickel)(dime)(quarter),
      label={[anchor=south,below]270:$q[\const{open}]$}] (qdir) {};

    \draw[mapsto] (penny) -- (spend);
    \draw[mapsto] (nickel) -- (save);
    \draw[mapsto] (dime) -- (save);
    \draw[mapsto] (quarter) -- (save);
  \end{tikzpicture}
  }
  \end{center}
  Notice that, by keeping the positions and directions of $p$ on the left and those of $q$ on the right, the on-positions function is drawn from left to right, while the on-directions functions must be drawn right to left.
  The $\const{greedy}$-component of $f^\sharp$ is the on-directions function $f^\sharp_{\const{greedy}}\colon q[f_\1(\const{greedy})]\to p[\const{greedy}]$ drawn like so:
  \begin{center}
  \scalebox{.7}{
    \begin{tikzpicture}
      % q-directions (right)
        % from top to bottom: penny, nickel, dime, quarter
      \node (nickel) {\const{nickel}};
      \node[above=.5 of nickel] (penny) {\const{penny}};
      \node[below=.5 of nickel] (dime) {\const{dime}};
      \node[below=.5 of dime] (quarter) {\const{quarter}};

      \node[draw, ellipse, inner sep=0pt,
        fit=(penny)(nickel)(dime)(quarter),
        label={[anchor=south,below]270:$q[\const{open}]$}] (qdir) {};

      % p-directions (left)
        % from top to bottom: accept, reject, ask for more
        % reject aligned with middle of q-directions
      \node[left=3 of qdir] (reject) {\const{reject}};
      \node[above=.5 of reject] (accept) {\const{accept}};
      \node[below=.5 of reject] (ask) {\const{ask for more}};

      \node[draw, ellipse, inner sep=0pt, fit=(accept)(reject)(ask),
        label={[anchor=south,below]270:$p[\const{needy}]$}] (pdir) {};

      \draw[mapsto] (penny) -- (ask);
      \draw[mapsto] (nickel) -- (ask);
      \draw[mapsto] (dime) -- (accept);
      \draw[mapsto] (quarter) -- (accept);
    \end{tikzpicture}
  }
  \end{center}
  Finally, since $q[f_\1(\const{content})]=q[\const{closed}]=\0$, the $\const{content}$-component of $f^\sharp$ is the vacuously-defined on-directions function $f^\sharp_{\const{content}}\colon \0\to p[\const{content}]$.
  Together, the on-positions function $f_\1$ and the on-directions map $f^\sharp$ defined above completely characterize a lens $f\colon p\to q$ depicting the interaction between the coin jar and its owner.
\end{example}

\index{lens!bidirectionality of}

More generally, a lens depicts what we call an \emph{interaction protocol}, a kind of dialogue between two agents regarding their positions and directions.
Say that one agent is represented by a polynomial $p$ and another by a polynomial $q$.
Then a lens $f\colon p\to q$ is an interaction protocol that prescribes how the positions of $p$ influence the positions of $q$ and how the directions of $q$ influence the directions of $p$.
Each $p$-position $i\in p(\1)$ is passed forward via the on-positions function of $f$ to a $q$-position $f_\1i\in q(\1)$.
Then each $q[f_\1i]$-direction $b$ is passed backward via the on-directions function of $f$ at $i$ to a $p[i]$-direction $f^\sharp_ib$.

To visualize these lenses, we may use either our corolla forests or our polyboxes.

\index{lens!as interaction protocol|)}

%-------- Section --------%
\section{Corolla forest pictures of dependent lenses}

The corolla forest associated to a polynomial concretely depicts its positions and directions, making it easy to extend our corolla forest pictures to depict the dependencies between the positions and directions of two polynomials that a lens between them prescribes.

\index{lens!corolla forest depiction}

\begin{example}\label{ex.practice_with_poly_morphisms}
Let $p\coloneqq \yon^\3+\2\yon$ and $q\coloneqq\yon^\4+\yon^\2+\2$.
We draw them as corolla forests with their positions labeled:
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, blue!50!black, "$p$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$}
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$}
      child {};
    \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$}
      child {};
  \end{tikzpicture}
  };
%
	\node (p2) [draw, red!75!black, right=2 of p1, "$q$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$}
      child {}
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$}
      child {}
      child {};
    \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$}
    ;
    \node[right=.5 of 3,"\tiny 4" below] (4) {$\bullet$}
    ;
  \end{tikzpicture}
  };
\end{tikzpicture}
\]
To give a lens $p\to q$, we must send each $p$-position $i\in p(\1)$ to a $q$-position $j\in q(\1)$, then send each direction in $q[j]$ back to one in $p[i]$.
We can draw such a lens as follows.
\[
\begin{tikzpicture}
	\node (p1) {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "{\color{blue!50!black}\tiny 1}" below] (1) {$\bullet$}
      child[blue!50!black] {coordinate (11)}
      child[blue!50!black] {coordinate (12)}
      child[blue!50!black] {coordinate (13)};
    \node[right=1.5 of 1, red!75!black, "{\color{red!75!black}\tiny 1}" below] (2) {$\bullet$}
      child[red!75!black] {coordinate (21)}
      child[red!75!black] {coordinate (22)}
      child[red!75!black] {coordinate (23)}
      child[red!75!black] {coordinate (24)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (13);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (13);
      \draw[postaction={decorate}] (24) to (13);
    \end{scope}
  \end{tikzpicture}
	};
%
	\node (p2) [right=1 of p1] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "{\color{blue!50!black}\tiny 2}" below] (1) {$\bullet$}
      child[blue!50!black] {coordinate (11)};
    \node[right=of 1, red!75!black, "{\color{red!75!black}\tiny 1}" below] (2) {$\bullet$}
      child[red!75!black] {coordinate (21)}
      child[red!75!black] {coordinate (22)}
      child[red!75!black] {coordinate (23)}
      child[red!75!black] {coordinate (24)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (11);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (11);
      \draw[postaction={decorate}] (24) to (11);
    \end{scope}
  \end{tikzpicture}
	};
%
	\node (p3) [below right=-1.05cm and 1 of p2] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "{\color{blue!50!black}\tiny 3}" below] (1) {$\bullet$}
      child[blue!50!black] {};
    \node[right=of 1, red!75!black, "{\color{red!75!black}\tiny 4}" below] (2) {$\bullet$}
		;
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
  \end{tikzpicture}
	};
\end{tikzpicture}
\]
This represents one possible lens $f\colon p\to q$.
The horizontal solid arrows pointing rightward in the picture above tell us that the on-positions function $f_\1\colon p(\1)\to q(\1)$ is given by
\[
  f_\1(1) \coloneqq 1, \qquad f_\1(2) \coloneqq 1, \qqand f_\1(3) \coloneqq 4.
\]
Then the curved dashed arrows pointing leftward in the picture above describe the on-directions map $f^\sharp\colon q[f_\1(-)]\to p[-]$.
On the left, the arrows depict one possible on-directions function $f^\sharp_1\colon q[1]\to p[1]$ from the $4$ directions in $q[1]$ to the $3$ directions in $p[1]$.
In the middle, the arrows depict the only possible on-directions function $f^\sharp_2\colon q[1]\to p[2]$ because $|p[2]|=1$.
Finally, on the right, there are no curved arrows, depicting the only possible on-directions function $f^\sharp_3\colon q[4]\to p[3]$ because $|q[4]|=0$.
\end{example}

\begin{exercise}\label{exc.practice_poly_maps}
\begin{enumerate}
	\item Draw the corolla forests associated to $p\coloneqq\yon^\3+\yon+\1$, $q\coloneqq \yon^\2+\yon^\2+\2$, and $r\coloneqq\yon^\3$.
	\item Pick an example of a dependent lens $p\to q$ and draw it as we did in \cref{ex.practice_with_poly_morphisms}.
	\item Explain the behavior of your lens as an interaction protocol in terms of positions and directions.
	\item Explain in those terms why there can't be any lenses $p\to r$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item Here are the corolla forests associated to $p\coloneqq\yon^\3+\yon+\1$, $q\coloneqq \yon^\2+\yon^\2+\2$, and $r\coloneqq\yon^\3$ (with each root labeled for convenience).
    \[
    \begin{tikzpicture}[rounded corners]
    	\node (p) [draw, blue!50!black, "$p$" above] {
    	\begin{tikzpicture}[trees, sibling distance=2.5mm]
            \node["\tiny 1" below] (1) {$\bullet$}
              child {}
              child {}
              child {};
            \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$}
              child {};
            \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$};
        \end{tikzpicture}
        };
    %
    	\node (q) [draw, red!75!black, right=2 of p, "$q$" above] {
    	\begin{tikzpicture}[trees, sibling distance=2.5mm]
            \node["\tiny 1" below] (1) {$\bullet$}
              child {}
              child {};
            \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$}
              child {}
              child {};
            \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$};
            \node[right=.5 of 3,"\tiny 4" below] (4) {$\bullet$};
        \end{tikzpicture}
        };
    %
    	\node (r) [draw, green!50!black, right=2 of q, "$r$" above] {
    	\begin{tikzpicture}[trees, sibling distance=2.5mm]
        \node["\tiny 1" below] (1) {$\bullet$}
          child {}
          child {}
          child {};
        \end{tikzpicture}
      };
    \end{tikzpicture}
    \]
	\item Here is one possible lens $p\to q$ (you may have drawn others).

	\[
    \begin{tikzpicture}
    	\node (p1) {
        	\begin{tikzpicture}[trees, sibling distance=2.5mm]
                \node[blue!50!black, "\tiny 1" below] (1) {$\bullet$}
                  child[blue!50!black] {coordinate (11)}
                  child[blue!50!black] {coordinate (12)}
                  child[blue!50!black] {coordinate (13)};
                \node[right=1.5 of 1, red!75!black, "\tiny 2" below] (2) {$\bullet$}
                  child[red!75!black] {coordinate (21)}
                  child[red!75!black] {coordinate (22)};
                \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
                \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
                  \draw[postaction={decorate}] (21) to (13);
                  \draw[postaction={decorate}] (22) to (11);
                \end{scope}
            \end{tikzpicture}
    	};
        %
    	\node (p2) [right=1 of p1, yshift=-2mm] {
        	\begin{tikzpicture}[trees, sibling distance=2.5mm]
                \node[blue!50!black, "\tiny 2" below] (1) {$\bullet$}
                  child[blue!50!black] {coordinate (11)};
                \node[right=of 1, red!75!black, "\tiny 4" below] (2) {$\bullet$};
                \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
            \end{tikzpicture}
    	};
        %
    	\node (p3) [right=1 of p2, yshift=-2mm] {
        	\begin{tikzpicture}[trees, sibling distance=2.5mm]
                \node[blue!50!black, "\tiny 3" below] (1) {$\bullet$};
                \node[right=of 1, red!75!black, "\tiny 3" below] (2) {$\bullet$};
                \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
            \end{tikzpicture}
    	};
    \end{tikzpicture}
    \]
    \item As depicted, our lens assigns to the first position of $p$ the second position of $q$, whose first and second directions are passed back to the third and first directions, respectively, of the first position of $p$.
    Then the second position of $p$ is assigned the fourth position of $q$, which has no directions; effectively, the choice of direction of the second position of $p$ has been canceled.
    Finally, the third position of $p$ is assigned the third position of $q$; here neither position has any directions.
    \item There cannot be a lens $p \to r$ for the following reason: if we send the third position of $p$, which has no directions, to the sole position of $r$, which has $3$ directions, then there is no way to pass a choice of one of those $3$ directions back to any of the options on the third menu of $p$, as there are no such options.
\end{enumerate}
\end{solution}
\end{exercise}

%-------- Section --------%
\section{Polybox pictures of dependent lenses}

\index{lens!polybox depiction|(}

Another way to visualize a dependent lens $f\colon p\to q$ is to draw the polyboxes for $p$ and $q$.
\begin{equation} \label{eqn.polybox_lens}
  \begin{tikzpicture}
    \node (f) ["$f\colon p\to q$" above] {
      \begin{tikzpicture}[polybox, tos]
        \node[poly, dom, "$p$" below] (p) {};
        \node[left=0pt of p_pos] {$p(\1)$};
        \node[left=0pt of p_dir] {$p[-]$};

        \node[poly, cod, right=of p, "$q$" below] (q) {};
        \node[right=0pt of q_pos] {$q(\1)$};
        \node[right=0pt of q_dir] {$q[-]$};

        \draw (p_pos) -- node[below] {} (q_pos);
        \draw (q_dir) -- node[above] {} (p_dir);
      \end{tikzpicture}
    };
  \end{tikzpicture}
\end{equation}
Thinking of the polyboxes as cells in a spreadsheet, the lens prescribes how the values of the cells are computed.
The boxes colored blue accept user input, while the other boxes are computed from that input according to the spreadsheet's rules.

\index{lens!spreadsheet depiction}

The arrows track the flow of information, starting from the lower left.
When the user fills the blue position box of $p$ with some $i\in p(\1)$, the arrow pointing right indicates that the lens fills the position box of $q$ with some $j\in q(\1)$ based on $i$.
The corresponding assignment $i\mapsto j$ is the on-position function $f_\1$ of the lens.

Then when the user fills the blue direction box of $q$ with some $b\in q[j]$, the arrow pointing left indicates that the lens fills the direction box of $p$ with some $a \in p[i]$ based on $i$ and $b$.
Fixing $i\in p(\1)$, the assignment $b\mapsto a$ is an on-directions function $f^\sharp_i$ of the lens.

Once all the boxes are filled, we obtain the following:
\[ \label{eqn.polybox_lens_filled}
\begin{tikzpicture}[polybox, mapstos]
  \node[poly, dom, "$p$" left] (p) {$a\vphantom{b}$\at$i\vphantom{j}$};
  \node[poly, cod, "$q$" right, right=of p] (q) {$b$\at$j$};
  \draw (p_pos) -- node[below] {$f_\1$} (q_pos);
  \draw (q_dir) -- node[above] {$f^\sharp$} (p_dir);
\end{tikzpicture}
\]
Here $j\coloneqq f_\1i$ and $a\coloneqq f^\sharp_ib$.
So a lens is any protocol that will fill the remaining boxes once the user fills the blue boxes, following the directions of the arrows drawn.
Be careful: although the arrow $f^\sharp$ is drawn from the codomain's direction box, it also takes into account what is entered into the domain's position box previously.
After all, the on-directions map of a lens is dependent on both a position of the domain and a direction of the codomain.

Here is an example of a lens depicted with polyboxes that would be difficult to draw with corolla forests.

\begin{example}[Modeling with a lens in polyboxes] \label{ex.lend-return}
  Caroline asks each of her parents for $20$ dollars. Each parent gives Caroline a positive amount of money not exceeding $20$ dollars. Caroline spends some of the money she receives before returning the remainder to each parent proportionally according to the amount she received from each.

  To model this interaction as a lens $f\colon p\to q$, we first define the polynomials $p$ and $q$.
  Let
  \[
    p\coloneqq\sum_{(i,j)\in(0,20]\times(0,20]}\yon^{[0,i]\times[0,j]}
  \]
  be the polynomial that models the parents: its position $(i,j)\in(0,20]\times(0,20]$ consists of the quantities of money that each parent gives to Caroline, and its direction $(i',j')\in[0,i]\times[0,j]$ at that position consists of the quantities of money that Caroline returns to each parent.
  Then let
  \[
    q\coloneqq\sum_{k\in(0,\infty)}\yon^{[0,k]}
  \]
  be the polynomial that models Caroline: its position $k\in(0,\infty)$ is the total money that Caroline receives (perhaps Caroline is prepared to receive more money than she is asking for, even if her parents are not prepared to give it), while its direction $r\in[0,k]$ is the money Caroline has remaining after spending some of it.
  Then we draw the lens $f$ that models their interaction in polyboxes as
  \[
  \begin{tikzpicture}[polybox, mapstos]
    \node[poly, dom, "$p$" left] (p) {$\left(\dfrac{ir}{i+j}, \dfrac{jr}{i+j}\right)$\at$(i,j)$};
    \node[poly, cod, "$q$" right, right=of p] (q) {$r\vphantom{\dfrac{jr}{i+j}}$\at$i+j$};
    \draw (p_pos) -- node[below] {$f_\1$} (q_pos);
    \draw (q_dir) -- node[above] {$f^\sharp$} (p_dir);
  \end{tikzpicture}
  \]
  We interpret this as saying that the on-positions function $f_\1$ from $p(\1)=(0,20]\times(0,20]$ to $q(\1)=(0,\infty)$ is defined on $(i,j)\in(0,20]\times(0,20]$ to be
  \[
    f_\1(i,j)\coloneqq i+j,
  \]
  while the on-directions function $f^\sharp_{(i,j)}$ from $q[i+j]=[0,i+j]$ to $p[(i,j)]=[0,i]\times[0,j]$ is defined on $r\in[0,i+j]$ to be
  \[
    f^\sharp_{(i,j)}(r) \coloneqq \left(\dfrac{ir}{i+j}, \dfrac{jr}{i+j}\right);
  \]
  the polybox picture expresses this more compactly.
  Notice that the position box of $q$ depends only on the position box of $p$, while the direction box of $p$ depends on the position box of $p$ as well as the direction box of $q$.
\end{example}

The above example illustrates how we can use polyboxes to specify a particular lens---or, equivalently, how we can use polyboxes to \emph{define} a lens, the same way we might define a function by writing it as a formula in a dummy variable.
Later on we will see how polyboxes help depict how lenses compose.
\index{lens!polybox depiction|)}

%-------- Section --------%
\section{Computations with dependent lenses}

Our concrete characterization of dependent lenses allows us to enumerate them.

\begin{example}[Enumerating lenses]
  Let $p\coloneqq \yon^\3+\2\yon$ and $q\coloneqq\yon^\4+\yon^\2+\2$, as in \cref{ex.practice_with_poly_morphisms}.
  Again, we draw them as corolla forests with their positions labeled:
  \[
  \begin{tikzpicture}[rounded corners]
    \node (p1) [draw, blue!50!black, "$p$" above] {
      \begin{tikzpicture}[trees, sibling distance=2.5mm]
        \node["\tiny 1" below] (1) {$\bullet$}
        child {}
        child {}
        child {};
        \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$}
        child {};
        \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$}
        child {};
      \end{tikzpicture}
    };
    %
    \node (p2) [draw, red!75!black, right=2 of p1, "$q$" above] {
      \begin{tikzpicture}[trees, sibling distance=2.5mm]
        \node["\tiny 1" below] (1) {$\bullet$}
        child {}
        child {}
        child {}
        child {};
        \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$}
        child {}
        child {};
        \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$}
        ;
        \node[right=.5 of 3,"\tiny 4" below] (4) {$\bullet$}
        ;
      \end{tikzpicture}
    };
  \end{tikzpicture}
  \]

  \index{lens!enumeration}

  How many lenses are there from $p$ to $q$? The first $p$-position must be sent to any $q$-position: 1, 2, 3, or 4. Sending it to 1 would require choosing an on-directions function $q[1]\to p[1]$, or $\4\to\3$; there are $3^4$ of these.
  Similarly, there are $3^2$ possible on-directions functions $q[2]\to p[1]$, as well as $3^0$ on-directions functions $q[3]\to p[1]$ and $3^0$ on-directions functions $q[4]\to p[1]$.
  Hence there are a total of $3^4+3^2+3^0+3^0=92$ ways to choose $f_\1(1)$ and $f^\sharp_1$.

  The second $p$-position must also be sent to 1, 2, 3, or 4 before selecting $f^\sharp_2$; there are $1^4+1^2+1^0+1^0=4$ ways to do this.
  Identically, there are $4$ ways to choose $f_\1(3)$ and $f^\sharp_3$.

  In total, there are $92 \cdot 4 \cdot 4=1472$ lenses $p\to q$.
  This coincides with what we obtain by taking the cardinality of both sides of \eqref{eqn.main_formula} and plugging in our values for $p$ and $q$:
  \begin{align*}
    |\poly(p, q)| &= \prod_{i \in p(\1)} |q(p[i])| \\
    &= \prod_{i\in\3}\left(|p[i]|^4 + |p[i]|^2 + 2\right) \\
    &= (3^4 + 3^2 + 2)(1^4 + 1^2 + 2)^2 \\
    &= 92 \cdot 4^2 = 1472.
  \end{align*}
\end{example}

\index{Yoneda lemma}
\begin{exercise}
For any polynomial $p$ and set $A$, e.g.\ $A\coloneqq\2$, the Yoneda lemma gives an isomorphism $\poly(\yon^A,p)\iso p(A)$, so the number of lenses $\yon^A\to p$ should be equal to the cardinality of $p(A)$.
\begin{enumerate}
	\item Choose a polynomial $p$ with finitely many positions and directions and draw both $\yon^\2$ and $p$ as corolla forests.
	\item Count all the lenses $\yon^\2\to p$. How many are there?
	\item Compute the cardinality of $p(\2)$. Is this the same as the previous answer?
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We let $p\coloneqq\yon^\3+\1$ (you could have selected others) and draw both $\yon^\2$ and $p$ as corolla forests, labeling each root for convenience.
    \[
    \begin{tikzpicture}[rounded corners]
    	\node (y2) [draw, blue!50!black, "$\yon^\2$" above] {
    	\begin{tikzpicture}[trees, sibling distance=2.5mm]
            \node["\tiny 1" below] (1) {$\bullet$}
              child {}
              child {};
        \end{tikzpicture}
        };
    %
    	\node (p) [draw, red!75!black, right=2 of y2, "$p$" above] {
    	\begin{tikzpicture}[trees, sibling distance=2.5mm]
            \node["\tiny 1" below] (1) {$\bullet$}
              child {}
              child {}
              child {};
            \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$};
        \end{tikzpicture}
        };
    \end{tikzpicture}
    \]

    \item When constructing a lens $\yon^2\to p$, the unique position of $\yon^\2$ can be sent to either $p$-position.
    If it is sent to the first $p$-position, then each of the $3$ directions in $p[1]$ must be sent to one of the $2$ directions of $\yon^\2$, for a total of $2^3 = 8$ lenses.
    Otherwise, the unique position of $\yon^\2$ is sent to the second $p$-position, at which there are no directions; so there is exactly $1$ lens like this.
    Hence there are $8+1=9$ lenses $\yon^\2\to p$.

    \item The cardinality of $p(\2)$ is $|\2^\3+\1|=9$, which agrees with previous answer, as predicted by the Yoneda lemma.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
For each of the following polynomials $p,q$, compute the number of lenses $p\to q$.
\begin{enumerate}
	\item $p\coloneqq\yon^\3$,\quad $q\coloneqq\yon^\4$.
	\item $p\coloneqq\yon^\3+\1$,\quad $q\coloneqq\yon^\4$.
	\item $p\coloneqq\yon^\3+\1$,\quad $q\coloneqq\yon^\4+\1$.
	\item $p\coloneqq\4\yon^\3+\3\yon^\2+\yon$,\quad $q\coloneqq\yon$.
	\item $p\coloneqq\4\yon^\3$,\quad $q\coloneqq\3\yon$.
\qedhere
\end{enumerate}
\begin{solution}
By \eqref{eqn.main_formula}, we have for all $p,q\in\poly$ that
\[
    |\poly(p,q)| = \prod_{i\in p(\1)}|q(p[i])|.
\]
\begin{enumerate}
	\item If $p\coloneqq\yon^\3$ and $q\coloneqq\yon^\4$, then
	\[
	    |\poly(p,q)| = \prod_{i\in\1}|p[i]|^4 = 3^4 = 81.
	\]
	\item If $p\coloneqq\yon^\3+\1$ and $q\coloneqq\yon^\4$, then
	\[
	    |\poly(p,q)| = \prod_{i\in\2}|p[i]|^4 = 3^4\cdot0^4 = 0.
	\]
	\item If $p\coloneqq\yon^\3+\1$ and $q\coloneqq\yon^\4+\1$, then
	\[
	    |\poly(p,q)| = \prod_{i\in\2}\left(|p[i]|^4+1\right) = (3^4 + 1)(0^4 + 1) = 82.
	\]
	\item If $p\coloneqq\4\yon^\3+\3\yon^\2+\yon$ and $q\coloneqq\yon$, then
	\[
	    |\poly(p,q)| = \prod_{i\in\8}|p[i]| = 3^4\cdot2^3\cdot1 = 648.
	\]
	\item If $p\coloneqq\4\yon^\3$ and $q\coloneqq\3\yon$, then
	\[
	    |\poly(p,q)| = \prod_{i\in\4}3|p[i]| = (3\cdot3)^4 = 6561.
	\]
\qedhere
\end{enumerate}
\end{solution}
\end{exercise}

The following exercises provide alternative formulas for the set of lenses between two polynomials.

\begin{exercise}\label{exc.practice_sum_prod}
\begin{enumerate}
\item Show that the following are isomorphic:
\begin{equation}\label{eqn.poly_p_q}
  \poly(p,q)
  \iso
  \prod_{i\in p(\1)}\sum_{j\in q(\1)}\prod_{b\in q[j]}\sum_{a\in p[i]}\1.
\end{equation}
\item \label{exc.practice_sum_prod.useful} Show that the following are isomorphic:
	\begin{equation}\label{eqn.useful_misc472}
	\poly(p,q)\iso\sum_{f_\1\colon p(\1)\to q(\1)}\,\prod_{j\in q(\1)}\smset\Bigg(q[j],\prod_{\substack{i \in p(\1), \\ f_\1i = j}}p[i]\Bigg).
	\end{equation}
\item Using the language of positions and directions, describe how an element of the right hand side of \eqref{eqn.useful_misc472} corresponds to a lens $p\to q$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{longenum}
\item By \eqref{eqn.main_formula}, it suffices to show that for all $i\in p(\1)$ and $j\in q(\1)$, we have
\[
    p[i]^{q[j]} \iso \prod_{b\in q[j]}\sum_{a\in p[i]}\1.
\]
Indeed, by \eqref{eqn.push_prod_sum_set_indep} and \cref{exc.on_sums_prods_sets}, we have
\begin{align*}
    \prod_{b \in q[j]} \sum_{a \in p[i]} \1 &\iso \sum_{\bar{a} \colon q[j] \to p[i]} \, \prod_{b \in q[j]} \1 \tag*{\eqref{eqn.push_prod_sum_set_indep}} \\
    &\iso \sum_{\bar{a} \colon q[j] \to p[i]} \1 \tag{\cref{exc.on_sums_prods_sets} \cref{exc.on_sums_prods_sets.prod}} \\
    &\iso \smset(q[j], p[i]) \tag{\cref{exc.on_sums_prods_sets} \cref{exc.on_sums_prods_sets.sum}} \\
    &\iso p[i]^{q[j]}.
\end{align*}

\item By \eqref{eqn.main_formula} and \eqref{eqn.push_prod_sum_set_indep}, we have
\begin{align*}
    \poly(p, q) &\iso \prod_{i \in p(\1)} \sum_{j \in q(\1)} p[i]^{q[j]} \tag*{\eqref{eqn.main_formula}} \\
    &\iso \sum_{f_\1 \colon p(\1) \to q(\1)} \; \prod_{i \in p(\1)} p[i]^{q[f_\1i]} \tag*{\eqref{eqn.push_prod_sum_set_indep}} \\
    &\iso \sum_{f_\1 \colon p(\1) \to q(\1)} \; \prod_{j \in q(\1)} \; \prod_{\substack{i \in p(\1), \\ f_\1i = j}} p[i]^{q[j]} \tag{$\ast$} \\
    &\iso \sum_{f_\1\colon p(\1)\to q(\1)} \; \prod_{j\in q(\1)}\smset\Bigg(q[j],\prod_{\substack{i \in p(\1), \\ f_\1i = j}}p[i]\Bigg) \tag{Universal property of products}
\end{align*}
where ($\ast$) follows from the fact that for any function $f_\1\colon p(\1)\to q(\1)$, its domain $p(\1)$ can be written as the disjoint union of preimages $f_\1\inv(j) = \{i \in p(1) \mid f_\1i = j\}$ for each $j \in q(\1)$.

\item To explain how an element of the set
\[
	D_{p,q} \coloneqq \sum_{f_\1\colon p(\1)\to q(\1)}\prod_{j\in q(\1)}\smset\Bigg(q[j],\prod_{\substack{i \in p(\1), \\ f_\1i = j}}p[i]\Bigg)
\]
corresponds to a lens $p\to q$, we first give the instructions for choosing an element of $D_{p,q}$ as a nested list.
\begin{quote}
To choose an element of $D_{p,q}$:
\begin{longenum}
    \item choose a function $f_\1 \colon p(\1) \to q(\1)$;
    \item for each element $j \in q(\1)$:
    \begin{longenum}
        \item for each element of $q[j]$:
        \begin{longenum}
            \item for each element $i \in p(\1)$ satisfying $f_\1i = j$:
            \begin{longenum}
                \item choose an element of $p[i]$.
            \end{longenum}
        \end{longenum}
    \end{longenum}
\end{longenum}
\end{quote}
So $f_\1$ sends each $p$-position to a $q$-position, as an on-positions function should.
Then for each $q$-position $j$, each $q[j]$-direction $b$, and each $p$-position $i$ that $f_\1$ sends to $j$, we choose an element of $p[i]$ that $f^\sharp_i\colon q[j]\to p[i]$ assigns to $b$.
As every $p$-position $i$ is sent to some $q$-position $j$ by $f_\1$, this completely characterizes $f^\sharp_i$ for every $p$-position $i$.
\end{longenum}
\end{solution}
\end{exercise}

In \eqref{eqn.poly_coprod}, we gave an explicit formula for coproducts in $\poly$ inherited from coproducts in $\smset^\smset$, as justified by \cref{prop.poly_coprods}.
We can now use \eqref{eqn.main_formula} to directly verify that the expression on the right hand side of \eqref{eqn.poly_coprod} satisfies the universal property for the coproduct of polynomials $(p_i)_{i\in I}$ in $\poly$.

\index{polynomial functor!sum of polynomials}

\begin{exercise}%\label{exc.poly_coprod}
Use \eqref{eqn.main_formula} to verify that
\[
    \poly\left(\sum_{(i,j)\in\sum_{i\in I}p_i(\1)}\yon^{p_i[j]}, q\right) \iso \prod_{i \in I}\poly(p_i, q)
\]
for all polynomials $(p_i)_{i\in I}$ and $q$.
\begin{solution}
We have
\begin{align*}
    \poly\left(\sum_{(i,j) \in \sum_{i \in I} p_i(\1)} \yon^{p_i[j]}, q\right) &\iso \prod_{(i,j) \in \sum_{i \in I} p_i(\1)} q(p_i[j])
    \tag*{\eqref{eqn.main_formula}} \\
    &\iso \prod_{i \in I} \prod_{j \in p_i(\1)} q(p_i[j]) \\
    &\iso \prod_{i \in I} \poly(p_i, q).
    \tag*{\eqref{eqn.main_formula}}
\end{align*}
\end{solution}
\end{exercise}

For the remaining exercises in this section, we introduce the concept of the \emph{derivative} $\dot{p}$ of a polynomial $p$.

\begin{example}[Derivatives]\label{ex.derivatives}
The \emph{derivative} of a polynomial $p$, denoted $\dot{p}$, is defined as
\[
  \dot{p}\coloneqq\sum_{i\in p(\1)}\sum_{a\in p[i]}\yon^{p[i]-\{a\}}.
\]
For example, if $p\coloneq\yon^{\{U,V,W\}}+\{A\}\yon^{\{X\}}+\{B\}\yon^{\{X\}}$ then
\[\dot{p}=\{U\}\yon^{\{V,W\}}+\{V\}\yon^{\{U,W\}}+\{W\}\yon^{\{U,V\}}+\{(A,X)\}\yon^\0+\{(B,X)\}\yon^\0.\]
Up to isomorphism $p\cong\yon^\3+\2\yon$ and $\dot{p}\cong\3\yon^\2+\2$.
Indeed, this coincides with the familiar notion of derivatives of polynomials from calculus.

Since
\[
  \dot{p}\yon\iso\sum_{i\in p(\1)}\sum_{a\in p[i]}\yon^{p[i]-\{a\}}\yon\iso\sum_{i\in p(\1)}\sum_{a\in p[i]}\yon^{p[i]-\{a\}+\1}\iso\sum_{i\in p(\1)}\sum_{a\in p[i]}\yon^{p[i]},
\]
there exists a canonical lens $\dot{p}\yon\to p$; you will define this lens in \cref{exc.deriv-ops}.
The lens arises in computer science in the context of ``plugging in to one-hole contexts''; we will not explore that here, but see \cite{mcbride2001derivative} and \cite{abbott2003derivatives} for details.%The Derivative of a Regular Type is its Type of One-Hole Contexts. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.22.8611&rep=rep1&type=pdf.
%https://www.cs.nott.ac.uk/~psztxa/publ/tlca03.pdf

A lens $f'\colon p\to \dot{q}$ is similar to a lens $f\colon p\to q$, except that each $p$-position explicitly selects a direction of $q$ to remain unassigned.
More precisely, while the on-positions function of $f$ sends each $p$-position to a $q$-position, the on-positions function of $f'$ sends each $p$-position $i$ to some $(j,a)\in \sum_{j\in q(\1)}q[j]$, picking out not only a $q$-position $j$, but also a $q[j]$-direction $a$.
Then the on-directions function of $f'$ at $i$ sends every $q[j]$-direction \emph{other than $a$} back to a $p[i]$-direction.
\index{polynomial functor!derivative of|see{derivative}}\index{derivative}
\end{example}

\begin{exercise} \label{exc.deriv-ops}
The derivative is not very well-behaved categorically, but it is nevertheless intriguing.
Take $p,q\in\poly$.
\begin{enumerate}
	\item Give an explicit construction for the canonical lens $\dot{p}\yon\to p$ from \cref{ex.derivatives}.
	\item Is there always a lens $p\to \dot{p}$?
  If so, prove it; if not, give a counterexample.
	\item Is there always a lens $\dot{p}\to p$?
  If so, prove it; if not, give a counterexample.
	\item Given a lens $p\to q$, is there always a lens $\dot{p}\to\dot{q}$?
  If so, prove it; if not, give a counterexample.
	\item We will define the binary operations $\otimes$ and $\ihom{-,-}$ on $\poly$ later on in \eqref{eqn.parallel_def} and \eqref{eqn.par_hom}; and in \cref{exc.dir_hom_p_yon_dir_p}, you will be able to use \cref{exc.par_hom_sum} to deduce that
	\begin{equation} \label{eqn.dir_hom_p_yon_dir_p}
	    \ihom{p, \yon} \otimes p \iso \sum_{f \in \prod_{i \in p(\1)} p[i]} \; \sum_{i \in p(\1)} \yon^{p(\1) \times p[i]},
	\end{equation}
	Construct a canonical lens $\ihom{p,\yon}\otimes p\to \dot{p}$.
	\item In \cref{ex.derivatives}, we described a lens $p\to\dot{q}$ in terms of ``unassigned'' directions. Describe a lens $p\yon\to q$ in terms of ``unassigned'' directions as well.
	\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
  \item We construct a lens $\dot{p}\yon\to p$, or a lens
  \[
    \sum_{i\in p(\1)}\sum_{a\in p[i]}\yon^{p[i]}\to\sum_{i\in p(\1)}\yon^{p[i]},
  \]
  as follows.
  It sends each position $(i,a)\in\sum_{i\in p(\1)}p[i]$ of $\dot{p}\yon$ to its first projection $i\in p(\1)$, and it is the identity on directions.

	\item There is not always a lens $p\to \dot{p}$: if $p\coloneqq\1$, then $\dot{p}\coloneqq\0$, and there is no lens $\1\to\0$.

	\item There is not always a lens $\dot{p}\to p$: take $p\coloneqq\yon$, so $\dot{p}\coloneqq\1$.
	A lens $\1\to\yon$ must have an on-directions function $\1\to\0$, but there is no such function.

	\item We show that even when there is a lens $p \to q$, there is not necessarily a lens $\dot{p}\to\dot{q}$.
	Take $p \coloneqq \yon$ and $q \coloneqq \1$.
	Then there is a lens $p \to q$ that sends the unique position of $\yon$ to the unique position of $\1$ and is the empty function on directions.
	But $\dot{p} = \1$ and $\dot{q} = \0$, and there is no lens $\1 \to \0$.

	\item We construct a lens $g\colon\ihom{p,\yon}\otimes p\to \dot{p}$, where $\ihom{p,\yon} \otimes p$ is given by \eqref{eqn.dir_hom_p_yon_dir_p}, as follows.
	The on-positions function $g_\1$ takes $f\in\prod_{i\in p(\1)}p[i]$ and $i\in p(\1)$ and sends the pair of them to the $\dot{p}$-position corresponding to $i\in p(\1)$ and $fi\in p[i]$.
	Then $\dot{p}[(i, fi)]=p[i]-\{fi\}$ and $(\ihom{p,\yon}\otimes p)[(f, i)]\iso p(\1)\times p[i]$, so the on-directions function $g^\sharp_{(f,i)}$ can send each $a\in p[i]-\{fi\}$ to $(i,a)\in p(\1)\times p[i]$.

	\item We describe a lens $p\yon\to q$ in terms of ``unassigned'' directions.
	Observe that $p\yon$ has the same positions as $p$ but has one more direction than $p$ does at each position.
	Given a position $i\in p(\1)$, we denote this extra $(p\yon)[i]$-direction by $\ast_i$, identifying $(p\yon)[i]$ with $p[i]+\{\ast_i\}$.
	So a lens $f\colon p\yon\to q$ sends each $p$-position $i$ to a $q$-position $j$, but every $q[j]$-direction could be sent back to either an original $p[i]$-direction or the extra $(p\yon)[i]$-direction $\ast_i$.
	So a lens $p\yon\to q$ is like a lens $p\to q$, except that some of the directions of $q$ may remain ``unassigned'' to any direction of $p$, which we signify by assigning them to $\ast_i$ instead.
  In other words, a lens $p\yon\to q$ could be interpreted as a lens $p\to q$ whose on-directions functions may only be partially defined.
\end{enumerate}
\end{solution}
\index{derivative!not well-behaved in $\poly$}
\end{exercise}

We will not use derivatives very much in the rest of this text, except as shorthand to denote the set of all directions of a polynomial: given a polynomial $p$, its directions comprise the set $\dot{p}(\1)$.

\begin{exercise} \label{exc.deriv-directions}
  Show that $\dot{p}(\1)$ is isomorphic to the set of all directions of $p$ (i.e.\ the sum of all direction-sets of $p$).
  \begin{solution}
    We have
    \[
      \dot{p}(\1)\iso\sum_{i\in p(\1)}\sum_{a\in p[i]}\1^{p[i]-\{a\}} \iso \sum_{i \in p(\1)} p[i],
    \]
    which is precisely the set of all directions of $p$.
  \end{solution}
  \index{derivative! $\dot{p}(1)$ as directions of $p$}
\end{exercise}

%-------- Section --------%
\section{Dependent lenses between special polynomials}

In \cref{sec.poly.obj.spec}, we considered four special classes of polynomials (here $I$ and $A$ are sets): constant polynomials $I$, linear polynomials $I\yon$, representable polynomials $\yon^A$, and monomials $I\yon^A$.
Of special note are the constant and linear polynomial $\0$, the constant and representable polynomial $\1$, and the linear and representable polynomial $\yon$.
We now consider lenses with these special polynomials as domains or codomains, highlighting some important examples using polyboxes and leaving most of the rest as exercises.
Let $p$ be a polynomial throughout.

\begin{example}[Lenses from linear polynomials] \label{ex.lens-from-lin}\index{lens!from linear polynomial}
  A lens $f\colon I\yon\to p$ can be drawn in polyboxes as follows:
  \[
    \begin{tikzpicture}[polybox, tos]
      \node[poly, linear dom] (p) {};
      \node[left=0pt of p_pos] {$I$};

      \node[poly, cod, right=of p] (q) {};
      \node[right=0pt of q_pos] {$p(\1)$};
      \node[right=0pt of q_dir] {$p[-]$};

      \draw (p_pos) -- node[below] {$f_\1$} (q_pos);
      \draw (q_dir) -- node[above] {$!$} (p_dir);
    \end{tikzpicture}
  \]
  Recall that we shade in the direction box of a linear polynomial to indicate that it can only be filled with one entry.
  Hence the on-directions map of $f$ is uniquely determined, so $f$ is completely characterized by its on-positions function $f_\1$.
  We conclude that lenses $I\yon\to p$ can be identified with functions $I\to p(\1)$.
\end{example}

\begin{exercise}[Lenses from $\0$ and $\yon$] \label{exc.lens-from-0-or-yon}
\begin{enumerate}
  \item Use \cref{ex.lens-from-lin} to verify that $\0$ is the initial object of $\poly$.
  \item Use \cref{ex.lens-from-lin} to show that lenses $\yon\to p$ can be identified with $p$-positions. \qedhere
\end{enumerate}
\begin{solution}
  \begin{enumerate}
    \item Since $\0\iso\0\yon$ is a linear polynomial, \cref{ex.lens-from-lin} tells us that lenses $\0\to p$ can be identified with functions $\0\to p(\1)$.
    There is exactly one function $\0\to p(\1)$, so there is exactly one lens $\0\to p$.
    \item Since $\yon\iso\1\yon$ is a linear polynomial, \cref{ex.lens-from-lin} tells us that lenses $\yon\to p$ can be identified with functions $\1\to p(\1)$, which in turn can be identified with elements of $p(\1)$.
  \end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}[Lenses to linear polynomials]\index{lens!to linear polynomial}
  Characterize lenses $p\to I\yon$ in terms of $I$ and the positions and directions of $p$.
\begin{solution}
  A lens $f\colon p\to I\yon$ consists of an on-positions function $f_\1\colon p(\1)\to I$ and, for each $j\in p(\1)$, an on-directions function $f^\sharp_j\colon\1\to p[j]$.
  Equivalently, this is a function $p(\1)\to I$ and a choice of direction at every position of $p$, i.e.\ a dependent function $(j\in p(\1))\to p[j]$.
\end{solution}
\end{exercise}

\begin{example}[Lenses to constants] \label{ex.lens-to-const}\index{lens!to constant polynomial}
  A lens $f\colon p\to I$ can be drawn in polyboxes as follows:
  \[
  \begin{tikzpicture}[polybox, tos]
    \node[poly, dom] (p) {};
    \node[left=0pt of p_pos] {$p(\1)$};
    \node[left=0pt of p_dir] {$p[-]$};

    \node[poly, constant, right=of p] (q) {};
    \node[right=0pt of q_pos] {$I$};

    \draw (p_pos) -- node[below] {$f_\1$} (q_pos);
    \draw[dashed] (q_dir) -- node[above] {$!$} (p_dir);
  \end{tikzpicture}
  \]
  Recall that we color the direction box of a constant red to indicate that it cannot be filled with any entry.
  Hence the on-directions map of $f$ is again uniquely determined, so $f$ is completely characterized by its on-positions function $f_\1$.
  (While the on-directions map of $f$ does exist---it is vacuous---it can never produce an element to fill the direction box of $p$, so we draw it with a dashed line.)
  We conclude that lenses $p\to I$ can be identified with functions $p(\1)\to I$.
\end{example}

\begin{exercise}[Lenses to $\1$ and $\0$]
  \begin{enumerate}
    \item Use \cref{ex.lens-to-const} to show that $\1$ is the terminal object of $\poly$.
    \item Show that there is exactly one lens whose codomain is $\0$.
    What is its domain? \qedhere
  \end{enumerate}
  \begin{solution}
    \begin{enumerate}
      \item Since $\1$ is a constant, \cref{ex.lens-to-const} tells us that lenses $p\to\1$ can be identified with functions $p(\1)\to\1$.
      There is exactly one function $p(\1)\to\1$, so there is exactly one lens $p\to\1$.
      \item Since $\0$ is a constant, \cref{ex.lens-to-const} tells us that lenses $p\to\0$ can be identified with functions $p(\1)\to\0$, which only exist when $p(\1)=\0$.
      The only polynomial with an empty position-set is $\0$ itself, and there is a unique function from the set $\0$ to itself, so there is a unique lens from the constant polynomial $\0$ to itself as well.
      If $p$ is not the constant $\0$, then there are no lenses $p\to\0$.
    \end{enumerate}
  \end{solution}
\end{exercise}

\begin{exercise}[Lenses from constants such as $\1$]\index{lens!from constant polynomial}
  \begin{enumerate}
    \item Characterize lenses $I\to p$ in terms of $I$ and the positions and directions of $p$.
    You may find it helpful to refer to $p(\0)$; see \cref{exc.apply0}.
    \item Use the previous part to characterize lenses $\1\to p$. \qedhere
  \end{enumerate}
  \begin{solution}
  \begin{enumerate}
    \item A lens $f\colon I\to p$ consists of an on-positions function $f_\1\colon I\to p(\1)$ and, for each $i\in I$, an on-directions function $f^\sharp_i\colon p[i]\to\0$.
    There is exactly one such on-directions function when $p[i]=\0$ and no such on-directions function otherwise.
    It follows that a lens $f\colon I\to p$ can be identified with a function $f_\1\colon I\to p(\1)$ whose image is contained in the set of $p$-positions with no directions.
    By \cref{exc.apply0}, this set of $p$-positions can be identified with the set $p(\0)$ (the \emph{constant} term of $p$); so a lens $I\to p$ is equivalent to a function $I\to p(\0)$.
    \item From the previous part, a lens $\1\to p$ may be identified with a function $\1\to p(\0)$ and thus an element of $p(\0)$.
  \end{enumerate}
  \end{solution}
\end{exercise}

\index{Yoneda lemma}

We already know from the Yoneda lemma (see \cref{exc.poly_morph_yoneda}) that lenses $\yon^A\to p$ correspond to elements of $p(A)$, so we understand lenses from representables. Thus we turn our attention to lenses $p\to\yon^A$.\index{lens!from representable polynomial}

\begin{example}[Lenses to representables]\index{lens!to representable polynomial}
  A lens $f\colon p\to\yon^A$ can be drawn in polyboxes as follows:
  \[
  \begin{tikzpicture}
    \node (f) {
      \begin{tikzpicture}[polybox, tos]
        \node[poly, dom] (p) {};
        \node[left=0pt of p_pos] {$p(\1)$};
        \node[left=0pt of p_dir] {$p[-]$};

        \node[poly, pure cod, right=of p] (q) {};
        \node[right=0pt of q_dir] {$A$};

        \draw (p_pos) -- node[below] {$!$} (q_pos);
        \draw (q_dir) -- node[above] {$f^\sharp$} (p_dir);
      \end{tikzpicture}
    };
  \end{tikzpicture}
  \]
  Recall that we shade in the position box of a representable to indicate that it can only be filled with one entry.
  Hence the on-positions function of $f$ is uniquely determined, so $f$ is completely characterized by its on-directions map $f^\sharp$, which takes a $p$-position $i$ and a direction $a\in A$ and sends them to a direction $b\in p[i]$.
  We conclude that lenses $p\to\yon^A$ can be identified with dependent functions $((i,a)\in p(\1)\times A)\to p[i]$.
\end{example}

\begin{example}[Lenses to $\yon$]
  As a special case of the previous example, a lens $\gamma\colon p\to\yon$ can be drawn in polyboxes as follows:
  \[
  \begin{tikzpicture}
    \node (f) {
      \begin{tikzpicture}[polybox, tos]
        \node[poly, dom] (p) {};
        \node[left=0pt of p_pos] {$p(\1)$};
        \node[left=0pt of p_dir] {$p[-]$};

        \node[poly, identity, right=of p] (q) {};

        \draw (p_pos) -- node[below] {$!$} (q_pos);
        \draw (q_dir) -- node[above] {$\gamma^\sharp$} (p_dir);
      \end{tikzpicture}
    };
  \end{tikzpicture}
  \]\index{dependent function!lens to $\yon$ as}
  Such lenses can be identified with dependent functions $(i\in p(\1))\to p[i]$, which, abusing notation, we also denote by $\gamma$.
  For each $p$-position $i$ in the blue position box, $\gamma$ picks out a $p[i]$-direction to fill the unshaded direction box.
  (Remember that the arrow labeled $\gamma^\sharp$ depends not only on the direction box to its right, but also on the position box of $p$.)
  So it makes sense to abbreviate the polybox picture of $\gamma$ like so:
  \begin{equation}\label{eqn.map_to_0ary_composite}
    \begin{tikzpicture}[polybox, tos]
      \node[poly, dom, "$p$" left] (p) {};
      \draw (p_pos) to[climb'] node[right] {$\gamma$} (p_dir);
    \end{tikzpicture}
  \end{equation}
\end{example}

\index{dependent function!lens to $\yon$ as}

The correspondence between lenses $p\to\yon$ and dependent functions $(i\in p(\1))\to p[i]$ exhibited in the previous example also follows directly from \eqref{eqn.main_formula}: taking $q\coloneqq\yon$, we have
\begin{equation} \label{eqn.gamma_prod}
\poly(p,\yon)\iso\prod_{i\in p(\1)}\sum_{j\in\1}p[i]^\1\iso\prod_{i\in p(\1)}p[i],
\end{equation}
where the right hand side is precisely the set of dependent functions $(i\in p(\1))\to p[i]$.
By \cref{exc.product_as_sections}, such functions may be identified with the \emph{sections} of the projection function from $\dot{p}(\1)\iso\sum_{i\in p(\1)}p[i]$, the set of all directions of $p$ (see \cref{exc.deriv-directions}), to $p(\1)$, the set of all positions of $p$, sending each $(i,a)\in\dot{p}(\1)$ to $i\in p(\1)$.
The fact that this projection determines $p$ (up to isomorphism) motivates the following definition.

\begin{definition}[Section; bundle] \label{def.sec-bun}\index{section|(}\index{lens!to $\yon$ as section}\index{bundle}\index{section|(}
  For $p\in\poly$, a \emph{section} of $p$ is a lens $p\to\yon$.
  We denote the set of all sections of $p$ by $\Gamma(p)$; that is,
  \begin{equation} \label{eqn.gamma_def}
    \Gamma(p)\coloneqq\poly(p,\yon).
  \end{equation}
  The \emph{bundle} of $p$, denoted $\pi_p$, is the projection function
  \[
    \dot{p}(\1)\iso\sum_{i\in p(\1)}p[i]\to p(\1)
  \]
  sending $(i,a)\mapsto i$.
\end{definition}

With this terminology, we can say that $p$ is determined (up to isomorphism) by its bundle, and that the sections of $p$ can be identified with the sections of its bundle.

To visualize the bundle of $p$, simply draw it as a corolla forest: a \emph{bundle} of arrows.
The bundle projects each leaf down to its root.
To visualize a section of $p$, picture its corollas piled atop each other; a section $\gamma\colon p\to\yon$ is then a \emph{cross-section} of this pile of $p$-corollas, picking out an arrow from each one---a direction at each position.

Alternatively, you could think of the arrow curving back to the polyboxes for $p$ in our picture  \eqref{eqn.map_to_0ary_composite} of a section $\gamma\colon p\to\yon$ as \emph{sectioning} off the polyboxes for $p$ from any polyboxes that may otherwise appear to its right.
We clarify this intuition by returning to a previous example of a polynomial and considering its sections.

\begin{example}[Modeling with sections] \label{ex.spend-section}
  Recall from \cref{ex.lend-return} the polynomial
  \[
    q\coloneqq\sum_{k\in(0,\infty)}\yon^{[0,k]}
  \]
  whose positions $k\in(0,\infty)$ are the possible quantities of money that Caroline receives from her parents and whose directions $r\in[0,k]$ are the possible quantities of money that Caroline has remaining after spending some of it.

  One component that was missing from our model was how Caroline spends her money.
  A section for $q$ fills in this gap by closing the loop from the money Caroline receives to the money she has remaining.
  Explicitly, a section $\gamma\colon q\to\yon$ corresponds to a dependent function $(k\in(0,\infty))\to[0,k]$ that uses the amount of money that Caroline receives to determine the amount of money that she will have remaining.

  For instance, if Caroline always spends half the money she receives, then the polyboxes for the section $\gamma\colon q\to\yon$ that models this behavior can be drawn as follows:
  \[
  \begin{tikzpicture}[polybox,mapstos]
    \node[poly, dom, "$q$" left] (p) {$k/2$\at$k$};
    \draw (p_pos) to[climb'] node[right] {$\gamma$} (p_dir);
  \end{tikzpicture}
  \]
  Without $\gamma$, we do not know how much money Caroline will decide to spend; having $\gamma$ makes her decision deterministic and sections this decision off from unknown variables.
  Of course, the position box of $q$, the amount of money Caroline receives, is still open to outside influence, as determined by a lens to $q$ such as the one from \cref{ex.lend-return}.
\end{example}

The definition of $\Gamma$ given in \eqref{eqn.gamma_def} makes $\Gamma$ a functor $\poly\to\smset\op$ satisfying the following.

\begin{proposition}\label{prop.gamma_pres_coproduct}
  The sections functor $\Gamma\colon\poly\to\smset\op$ sends $(0,+)$ to $(1,\times)$:
  \[
  \Gamma(\0)\iso\1
  \qqand
  \Gamma(p+q)\iso\Gamma(p)\times\Gamma(q).
  \]
\end{proposition}

\index{section|)}

\begin{exercise}\index{colimit!in $\poly$}
  Prove \cref{prop.gamma_pres_coproduct}.
  \begin{solution}
    The functor $\Gamma$ is defined as the hom-functor $\poly(-,\yon)\colon\poly\to\smset\op$, which exhibits the universal property of colimits by sending colimits in $\poly$ to limits in $\smset$.
    Hence \cref{prop.gamma_pres_coproduct} follows from \cref{prop.poly_coprods}.
    More explicitly, $\Gamma(\0)=\poly(\0,\yon)\iso\1$ since $\0$ is initial in $\poly$, and $\Gamma(p+q)=\poly(p+q,\yon)\iso\poly(p+q,\yon)=\Gamma(p)\times\Gamma(q)$ since $+$ gives coproducts in $\poly$.
  \end{solution}
\end{exercise}

\index{functor!of sections, $\Gamma$}
\index{section|)}
%\begin{remark}
%  The sections functor $\Gamma\colon\poly\to\smset\op$ is also normal lax monoidal in the sense that there are canonical functions
%  \[
%  \1\cong\Gamma(\yon)
%  \qqand
%  \Gamma(p)\times\Gamma(q)\to\Gamma(p\otimes q)
%  \]
%  satisfying certain well-known laws. But we won't need this, so we omit its proof.
%\end{remark}

We conclude this section by discussing lenses between monomials, which arise in functional programming.

\begin{example}[Lenses between monomials are bimorphic lenses] \label{subsec.poly.cat.morph.bimorphic-lens}
  Lenses whose domains and codomains are both monomials are especially simple to write down, because they can be characterized as a pair of (standard, not dependent) functions that are independent of each other, as follows.

  \index{lens!bimorphic|see{lens, between monomials}}\index{lens!between monomials}

  Given $I,J,A,B\in\smset$, a lens $f\colon I\yon^A\to J\yon^B$ is determined by an on-positions function $f_\1\colon I\to J$ and an on-directions map: for each $i\in I$, an on-directions function $f^\sharp_i\colon B\to A$.
  But the data of such an on-directions map may be repackaged as a single function $f^\sharp\colon I\times B\to A$.
  We can do this because every position in $I$ has the same direction-set $A$, and every position in $J$ has the same direction-set $B$.

  In functional programming, such a pair of functions is called a \emph{bimorphic lens}, or a \emph{lens} for short.
  In categorical terms, we may say that the monomials in $\poly$ span a full subcategory of $\poly$ equivalent to \emph{the category of bimorphic lenses}, defined in \cite{hedges2018limits} (here the category is named after its morphisms rather than its objects).
  When such a lens arises in functional programming, the two functions that comprise it are given special names:
  \begin{equation}\label{eqn.bimorphic_lens}
    \begin{aligned}
      \lensget\coloneqq f_\1 &\colon J\to I\\
      \lensput\coloneqq f^\sharp &\colon I\times B\to A
    \end{aligned}
  \end{equation}
  Each position $i\in I$ \emph{gets} a position $f_\1i\in J$ and \emph{puts} each direction $b\in B$ back to a direction $f^\sharp(i,b)\in A$.

  So a natural transformation between two monomial functors is a bimorphic lens.
  Then a natural transformation between two polynomial functors is a more general kind of lens: a \emph{dependent} lens, where the direction-sets depend on the positions.
  Favoring the dependent version, we call these natural transformations \emph{lenses}.
\end{example}

\begin{example}[Very well-behaved lenses] \label{ex.lens_get_put}\index{lens!very well-behaved}
  Consider the monomial $S\yon^S$.
  Its position-set is $S$, and its direction-set at each position $s\in S$ is again just $S$.
  We could think of each direction as pointing to the `next' position to move to.
  We will start to formalize this idea in \cref{ex.do_nothing} and continue this work throughout the following chapters.

  Then here is one way we can think of a lens $f\colon S\yon^S\to T\yon^T$.
  Say that Otto takes positions in $S$, while Tim takes positions in $T$.
  Tim will act as Otto's proxy as follows.
  Tim will model Otto's position via the on-positions function $S\to T$ of $f$: if Otto is at position $s\in S$, then Tim will be at position $f_\1 s\in T$.
  On the other hand, Otto will take his directions from Tim via the on-directions map $S\times T\to S$ of $f$: if Tim follows the direction $t'\in T$, then Otto will head from his current position $s\in S$ in the direction $f^\sharp(s,t')\in S$.
  We interpret these directions as new positions for Otto and Tim to move to.
  So as Otto moves through the positions in $S$, he is both modeled and directed by Tim moving through the positions in $T$.

  With this setup, there are three conditions that we might expect the lens $f\colon S\yon^S\to T\yon^T$ to satisfy:
  \begin{enumerate}
    \item With Otto at $s\in S$, if Tim stays put at $f_\1s$ (i.e.\ the direction he selects at $f_\1s$ is still $f_\1s$), then Otto should stay put at $s$ (i.e.\ the direction he selects at $s$ is still $s$):
    \[
      f^\sharp(s,f_\1s)=s.
    \]

    \item Once Tim moves to $t\in T$ and Otto moves from $s\in S$ accordingly, Tim's new position should model Otto's new position:
    \[
      f_\1(f^\sharp(s,t))=t.
    \]

    \item If Tim moves to $t$, then to $t'$, Otto should end up at the same position as where he would have ended up if Tim had moved directly to $t'$ in the first place:
    \[
      f^\sharp(f^\sharp(s,t),t')=f^\sharp(s,t')
    \]
  \end{enumerate}
  Such a lens is known to functional programmers as a \emph{very well-behaved lens}; the three conditions above are its \emph{lens laws}.
  We will see these conditions emerge from more general theory in \cref{ex.very_well_behaved_lenses}.\index{lens!laws}
\end{example}

%-------- Section --------%
\section{Translating between natural transformations and lenses} \label{subsec.poly.cat.morph.translate}

\index{lens!as natural transformation|(}

We now know that we can specify a morphism $p\to q$ in $\poly$ in one of two ways:
\begin{itemize}
    \item in the language of functors, by specifying a natural transformation $p \to q$, i.e.\ for each $X\in\smset$, a function $p(X)\to q(X)$ such that naturality squares commute; or
    \item in the language of positions and directions, by specifying a lens $p\to q$, i.e.\ a function $f_\1 \colon p(\1) \to q(\1)$ and, for each $i \in p(\1)$, a function $f^\sharp_i \colon q[f_\1i] \to p[i]$.
\end{itemize}
But how are these two formulations related?
Given the data of a lens and that of a natural transformation between polynomials, how can we tell if they correspond to the same morphism?
We want to be able to translate between these two languages.

Our Rosetta Stone turns out to be the proof of the Yoneda lemma.
The lemma itself is the crux of the proof of \cref{prop.lens-prod-sum}, which states that these two formulations of morphisms between polynomials are equivalent; so unraveling these proofs reveals the translation we seek.

\index{Yoneda lemma}

\begin{proposition} \label{prop.morph_arena_to_func}
Given $p,q\in\poly$, let $f_\1\colon p(\1)\to q(\1)$ be a function between their position-sets (like an on-positions function) and $f^\sharp\colon q[f_\1(-)]\to p[-]$ be a natural transformation whose components are functions between their direction-sets (like an on-directions map).
Then the isomorphism in \eqref{eqn.main_formula} identifies $(f_\1,f^\sharp)$ with the natural transformation $f\colon p\to q$ whose $X$-component $f_X\colon p(X)\to q(X)$ for $X\in\smset$ sends each
\[
    (i,g)\in\sum_{i\in p(\1)} X^{p[i]}\iso p(X)
\]
with $i\in p(\1)$ and $g\colon p[i]\to X$ to
\[
    (f_\1i,f^\sharp_i\then g)\in\sum_{j\in q(\1)}X^{q[j]}\iso q(X).
\]
\end{proposition}
\begin{proof}\index{Yoneda lemma}
As an element of the product over $I$ on the right hand side of \eqref{eqn.main_formula}, the pair $(f_\1,f^\sharp)$ is equivalently an $I$-indexed family of pairs $((f_\1i,f^\sharp_i))_{i\in I}$, where each pair $(f_\1i,f^\sharp_i)$ is an element of
\[
    \sum_{j\in q(\1)}p[i]^{q[j]}\iso q(p[i]).
\]
By the Yoneda lemma (\cref{lemma.yoneda}), we have an isomorphism $q(p[i])\iso\poly(\yon^{p[i]}, q)$; and by the proof of the Yoneda lemma, this isomorphism sends $(f_\1i, f^\sharp_i)$ to the natural transformation $f^i\colon\yon^{p[i]} \to q$ whose $X$-component is the function $f^i_X\colon X^{p[i]}\to q(X)$ given by sending $g\colon p[i]\to X$ to
\begin{align*}
    q(g)(f_\1i, f^\sharp_i) &= \left(\sum_{j \in q(\1)} g^{q[j]}\right)(f_\1i, f^\sharp_i) \tag{\cref{cor.sum_prod_set_endofuncs}} \\
    &= \left(f_\1i, g^{q[f_\1i]}(f^\sharp_i)\right) \tag{\cref{def.sum-prod-func} and \cref{exc.sum-prod-func}} \\
    &= (f_\1i, f^\sharp_i\then g) \tag{\cref{def.representable}}.
\end{align*}
Then the $p(\1)$-indexed family of natural transformations $(f^i)_{i\in p(\1)}$ is an element of
\[
  \prod_{i\in p(\1)}\poly(\yon^{p[i]}, q)\iso\poly\left(\sum_{i\in p(\1)}\yon^{p[i]},q\right),
\]
where the isomorphism follows from the universal property of coproducts, as in the proof of \cref{prop.lens-prod-sum}.
Unwinding this isomorphism, we find that $(f^i)_{i\in I}$ corresponds to the natural transformation $f$ from $\sum_{i\in p(\1)}\yon^{p[i]}\iso p$ to $q$ that we desire.
\end{proof}

\begin{example} \label{ex.morph-corolla-with-labels}
Let us return once more to the polynomials $p \coloneqq \yon^\3 + \2\yon$ and $q \coloneqq \yon^\4 + \yon^\2 + \2$ from \cref{ex.practice_with_poly_morphisms} and the lens $f \colon p \to q$ depicted below:
\[
\begin{tikzpicture}
	\node (p1) {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 1" below] (1) {$\bullet$}
      child[blue!50!black] {coordinate (11)}
      child[blue!50!black] {coordinate (12)}
      child[blue!50!black] {coordinate (13)};
    \node[right=1.5 of 1, red!75!black, "\tiny 1" below] (2) {$\bullet$}
      child[red!75!black] {coordinate (21)}
      child[red!75!black] {coordinate (22)}
      child[red!75!black] {coordinate (23)}
      child[red!75!black] {coordinate (24)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (13);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (13);
      \draw[postaction={decorate}] (24) to (13);
    \end{scope}
  \end{tikzpicture}
	};
%
	\node (p2) [right=1 of p1] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 2" below] (1) {$\bullet$}
      child[blue!50!black] {coordinate (11)};
    \node[right=of 1, red!75!black, "\tiny 1" below] (2) {$\bullet$}
      child[red!75!black] {coordinate (21)}
      child[red!75!black] {coordinate (22)}
      child[red!75!black] {coordinate (23)}
      child[red!75!black] {coordinate (24)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (11);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (11);
      \draw[postaction={decorate}] (24) to (11);
    \end{scope}
  \end{tikzpicture}
	};
%
	\node (p3) [below right=-1.05cm and 1 of p2] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 3" below] (1) {$\bullet$}
      child[blue!50!black] {};
    \node[right=of 1, red!75!black, "\tiny 4" below] (2) {$\bullet$}
		;
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
  \end{tikzpicture}
	};
\end{tikzpicture}
\]
Fix a set $X \coloneqq \{a,b,c,d,e\}$.
When viewed as a natural transformation, $f$ has as its $X$-component a function $f_X \colon p(X) \to q(X)$.
In other words, for each element of $p(X)$, the lens $f$ should tell us how to obtain an element of $q(X)$.

We saw in \cref{ex.corolla-apply-poly} that each $(i,g)\in p(X)$ may be drawn as a $p$-corolla (corresponding to $i$) whose leaves are labeled with elements of $X$ (according to $g$).
For example, here we draw $(1,g)\in p(X)$, where $g\colon p[1]\to X$ is given by $1\mapsto c, 2 \mapsto e,$ and $3 \mapsto a$:
\begin{equation} \label{diag.corolla-apply-elt}
\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 1" below] (1) {$\bullet$}
      child[blue!50!black] {node {$c$}}
      child[blue!50!black] {node {$e$}}
      child[blue!50!black] {node {$a$}};
\end{tikzpicture}
\end{equation}
Similarly, each element of $q(X)$ can be drawn as a $q$-corolla whose leaves are labeled with elements of $X$.
So what element of $q(X)$ is $f_X(1, g)$?

\cref{prop.morph_arena_to_func} tells us that $f_X(1, g)=(f_\1(1), f^\sharp_1\then g)$, so we need only focus on the behavior of $f$ at $p$-position $1$:
\[
\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 1" below] (1) {$\bullet$}
      child[blue!50!black] {coordinate (11)}
      child[blue!50!black] {coordinate (12)}
      child[blue!50!black] {coordinate (13)};
    \node[right=1.5 of 1, red!75!black, "\tiny 1" below] (2) {$\bullet$}
      child[red!75!black] {coordinate (21)}
      child[red!75!black] {coordinate (22)}
      child[red!75!black] {coordinate (23)}
      child[red!75!black] {coordinate (24)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (13);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (13);
      \draw[postaction={decorate}] (24) to (13);
    \end{scope}
\end{tikzpicture}
\]
To draw $(f_\1(1), f^\sharp_1\then g)$, we first draw the $q$-corolla corresponding to $f_\1(1)$, the corolla on the right hand side above.
Then we label each leaf of that corolla by following the arrow from that leaf to a $p[1]$-leaf, and use the label there from \eqref{diag.corolla-apply-elt} (as prescribed by $g$).
So $f_X(1, g)$ looks like
\[
\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[red!75!black, "\tiny 1" below] (1) {$\bullet$}
      child[red!75!black] {node {$a$}}
      child[red!75!black] {node {$c$}}
      child[red!75!black] {node {$a$}}
      child[red!75!black] {node {$a$}};
\end{tikzpicture}
\]

\end{example}

\cref{prop.morph_arena_to_func} lets us translate from lenses to natural transformations.
The following corollary tells us how to go in the other direction.
In particular, it justifies the notation $f_\1$ for the on-positions function of $f$: it is the $\1$-component of $f$ as a natural transformation.

\begin{corollary} \label{cor.morph_func_to_arena}
Let $p$ and $q$ be polynomial functors, and let $f \colon p \to q$ be a natural transformation between them.
Then the isomorphism in \eqref{eqn.main_formula} sends $f$ to the lens whose on-positions function $f_\1\colon p(\1)\to q(\1)$ is the $\1$-component of $f$ and whose on-directions map $f^\sharp\colon q[f_\1(-)]\to p[-]$ satisfies, for all $i\in p(\1)$,
\[
    (f_\1i, f^\sharp_i) = f_{p[i]}(i, \id_{p[i]}).
\]
\end{corollary}
\begin{proof}
By \cref{prop.morph_arena_to_func}, the $\1$-component of $f$ is a function $p(\1)\to q(\1)$ sending every $i \in p(\1)$ to $f_\1i \in q(\1)$, so the on-positions function $f_\1$ is indeed equal to the $\1$-component of $f$.
Moreover, for each $i\in p(\1)$, the $p[i]$-component $f_{p[i]} \colon p(p[i]) \to q(p[i])$ of $f$ sends $(i,\id_{p[i]})\in p(p[i])$ to $(f_\1i, f^\sharp_i \then \id_{p[i]}) = (f_\1i, f^\sharp_i)$.
\end{proof}

\index{lens!as natural transformation|)}


%-------- Section --------%
\section{Identity lenses and lens composition} \label{subsec.poly.cat.morph.id-comp}


Thus far, we have seen how the category $\poly$ of polynomial functors and natural transformations can be identified with the category of indexed families of sets and lenses.
But in order to actually discuss the latter category, we need to be able to give identity lenses and describe how lenses compose.
We can do so by translating between lenses and natural transformations.

For instance, given a polynomial $p$, its identity lens should correspond to the identity natural transformation of $p$ as a functor.

\index{lens!identity}

\begin{exercise}[Identity lenses] \label{exc.arena_morph_id}
For $p\in\poly$, let $\id_p \colon p \to p$ be its identity natural transformation, whose $X$-component $(\id_p)_X\colon p(X)\to p(X)$ for $X\in\smset$ is the identity function on $p(X)$; that is, $(\id_p)_X=\id_{p(X)}$.

Use \cref{cor.morph_func_to_arena} to show that the on-positions function $(\id_p)_\1\colon p(\1)\to p(\1)$ and the on-directions functions $(\id_p)^\sharp_i\colon p[(\id_p)_\1i]\to p[i]$ for $i\in p(\1)$ of $\id_p$ are all identity functions.
\begin{solution}
Fix $i\in p(\1)$.
Since the $p[i]$-component $(\id_p)_{p[i]}$ of $\id_p$ is the identity function on $p(p[i])$, by \cref{cor.morph_func_to_arena},
\[
    ((\id_p)_\1i, (\id_p)^\sharp_i) = (\id_p)_{p[i]}(i,\id_{p[i]}) = (i,\id_{p[i]}).
\]
Hence the on-positions function $(\id_p)_\1\colon p(\1)\to p(\1)$ maps every $i\in p(\1)$ to itself, so it is an identity function; and each on-directions function $(\id_p)^\sharp_i\colon p[i]\to p[i]$ is equal to $\id_{p[i]}$.
\end{solution}
\end{exercise}

Similarly, we may infer how two lenses compose by translating them to natural transformations, composing those, then translating back to lenses.

\index{lens!composition of}

\begin{exercise}[Composing lenses] \label{exc.arena_morph_comp}
For $p,q,r\in\poly$, let $f\colon p\to q$ and $g\colon q\to r$ be natural transformations, and let $h\coloneqq f\then g$ be their composite, whose $X$-component $h_X\colon p(X)\to r(X)$ for $X\in\smset$ is the composite of the $X$-components of $f$ and $g$; that is, $h_X=f_X\then g_X$.

Use \cref{prop.morph_arena_to_func} and \cref{cor.morph_func_to_arena} to show that the on-positions function $h_\1\colon p(\1)\to r(\1)$ of $h$ is given by $h_\1=f_\1\then g_\1$, while the on-directions function $h^\sharp_i$ of $h$ for $i\in p(\1)$ is given by $h^\sharp_i=g^\sharp_{f_\1i}\then f^\sharp_i$.
\begin{solution}
Fix $i\in p(\1)$.
By \cref{prop.morph_arena_to_func} and \cref{cor.morph_func_to_arena},
\begin{align*}
    (h_\1i, h^\sharp_i) &= h_{p[i]}(i, \id_{p[i]}) \tag{\cref{cor.morph_func_to_arena}} \\
    &= g_{p[i]}(f_{p[i]}(i, \id_{p[i]})) \tag{$h = f \then g$} \\
    &= g_{p[i]}(f_\1i, f^\sharp_i) \tag{\cref{cor.morph_func_to_arena}} \\
    &= (g_\1f_\1i, g^\sharp_{f_\1i} \then f^\sharp_i). \tag{\cref{prop.morph_arena_to_func}}
\end{align*}
\end{solution}
\end{exercise}

% TODO: turn following to prop?
% The following corollary about interpreting commutative diagrams in $\poly$ is immediate from the preceding exercise.

The following proposition, a restatement of the previous exercise, allows us to interpret commutative diagrams of polynomials in $\poly$ in terms of commutative diagrams of their position- and direction-sets in $\smset$.

\begin{proposition} \label{prop.comm_poly}
Given $p,q,r\in\poly$ and lenses $f\colon p\to q, g\colon q\to r,$ and $h\colon p\to r$, the diagram
\[
\begin{tikzcd}
    p \ar[r, "f"] \ar[dr, "h"'] & q \ar[d, "g"] \\
    & r
\end{tikzcd}
\]
commutes in $\poly$ if and only if the forward on-positions diagram
\[
\begin{tikzcd}
    p(\1) \ar[r, "f_\1"] \ar[dr, "h_\1"'] & q(\1) \ar[d, "g_\1"] \\
    & r(\1)
\end{tikzcd}
\]
commutes in $\smset$ and, for each $i \in p(\1)$, the backward on-directions diagram
\[
\begin{tikzcd}
    p[i] & q[f_\1i] \ar[l, "f^\sharp_i"'] \\
    & r[h_\1i] \ar[u, "g^\sharp_{f_\1i}"'] \ar[ul, "h^\sharp_i"]
\end{tikzcd}
\]
commutes in $\smset$.
\end{proposition}

We can use this fact to determine whether a given diagram in $\poly$ commutes, as in the following exercise.

\begin{exercise}
Using \cref{prop.comm_poly}, verify explicitly that, for $p, q \in \poly$, the polynomial $p+q$ given by the binary sum of $p$ and $q$ satisfies the universal property of the coproduct of $p$ and $q$.
That is, provide lenses $\iota \colon p \to p + q$ and $\kappa \colon q \to p + q$, then show that for any other polynomial $r$ equipped with lenses $f \colon p \to r$ and $g \colon q \to r$, there exists a unique lens $h\colon p+q\to r$ (shown dashed) making the following diagram commute:
\begin{equation} \label{eqn.coprod_univ_prop}
\begin{tikzcd}
	p \ar[r, "\iota"] \ar[dr, "f"'] &
	p + q \ar[d, "h", dashed] &
	q \ar[l, "\kappa"'] \ar[dl, "g"] \\
	& r
\end{tikzcd}
\end{equation}
\begin{solution}
We provide lenses $\iota\colon p\to p+q$ and $\kappa\colon q\to p+q$ as follows.
On positions, they are the canonical inclusions $\iota_\1\colon p(\1)\to p(\1)+q(\1)$ and $\kappa_\1\colon q(\1)\to p(\1)+q(\1)$; on directions, they are identities.
To show that $p+q$ equipped with $\iota$ and $\kappa$ satisfies the universal property of the coproduct, we apply \cref{prop.comm_poly}.
In order for \eqref{eqn.coprod_univ_prop} to commute, it must commute on positions---that is, the following diagram of sets must commute:
\begin{equation} \label{eqn.coprod_univ_prop_pos}
\begin{tikzcd}
	p(\1) \ar[r, "\iota_\1"] \ar[dr, "f_\1"'] &
	p(\1) + q(\1) \ar[d, "h_\1", dashed] &
	q(\1) \ar[l, "\kappa_\1"'] \ar[dl, "g_\1"] \\
	& r(\1)
\end{tikzcd}
\end{equation}
But since $p(1)+q(\1)$ along with the inclusions $\iota_\1$ and $\kappa_\1$ form the coproduct of $p(\1)$ and $q(\1)$ in $\smset$, there exists a unique $h_\1$ for which \eqref{eqn.coprod_univ_prop_pos} commutes.
Hence $h$ is uniquely characterized on positions.
In particular, it must send each $(1,i) \in p(\1)+q(\1)$ with $i \in p(\1)$ to $f_\1i$ and each $(2,j) \in p(\1)+q(\1)$ with $j \in q(\1)$ to $g_\1j$.

Moreover, if \eqref{eqn.coprod_univ_prop} is to commute on directions, then for every $i\in p(\1)$ and $j \in q(\1)$, the following diagrams of sets must commute:
\begin{equation} \label{eqn.coprod_univ_prop_dir}
\begin{tikzcd}[sep=large]
	p[i] & (p+q)[(1,i)] \ar[l, "\iota^\sharp_i"'] & (p+q)[(2,j)] \ar[r, "\kappa^\sharp_j"] & q[j] \\
	& r[f_\1i] \ar[ul, "f^\sharp_i"] \ar[u, "h^\sharp_{(1,i)}"', dashed] & r[g_\1j] \ar[u, "h^\sharp_{(2,j)}", dashed] \ar[ur, "g^\sharp_j"']
\end{tikzcd}
\end{equation}
But $(p+q)[(1,i)] \iso p[i]$ and $\iota^\sharp_i$ is the identity, so we must have $h^\sharp_{(1,i)} = f^\sharp_i$.
Similarly, $(p+q)[(2,j)] \iso q[j]$ and $\kappa^\sharp_j$ is the identity, so we must have $h^\sharp_{(2,j)} = g^\sharp_j$.
Hence $h$ is also uniquely characterized on directions, so it is unique overall.
Moreover, we have shown that we can define $h$ on positions so that \eqref{eqn.coprod_univ_prop_pos} commutes, and that we can define $h$ on directions such that the diagrams in \eqref{eqn.coprod_univ_prop_dir} commute.
As the commutativity of the diagrams in \eqref{eqn.coprod_univ_prop_pos} and \eqref{eqn.coprod_univ_prop_dir} together imply the commutativity of \eqref{eqn.coprod_univ_prop}, it follows that there exists $h$ for which \eqref{eqn.coprod_univ_prop} commutes.
\end{solution}
\end{exercise}

Now that we know how lens composition works in $\poly$, we have a better handle on how it behaves categorically.
For instance, we can verify functoriality in $\poly$, as in the following exercise.

\begin{exercise}[A functor $\Cat{Top}\to\poly$] \label{exc.top_poly_func}\index{topological space}
This exercise is for those who know what topological spaces and continuous maps are. It will not be used again in this book.
\begin{enumerate}
	\item Given a topological space $X$, define a polynomial $p_X$ whose positions are the points in $X$ and whose directions at $x\in X$ are the open neighborhoods of $x$.
  That is,
  \[
    p_X\coloneqq\sum_{x \in X}\yon^{\{ U\ss X \mid x\in U, \, U\text{ open} \}}.
  \]
  Given a continuous map $f\colon X\to Y$, define a lens $p_X\to p_Y$ either by writing down its formula or drawing it in polyboxes.
	\item Show that the assignment above defines a functor $\Cat{Top}\to\poly$.
	\item Is this functor full? Is it faithful?
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
	\item \label{exc.top_poly_func.morphs} Given a continuous map $f \colon X \to Y$, we define a lens $p_f \colon p_X \to p_Y$ as follows.
	The on-positions function is just $f$; then for each $p_X$-position $x\in X$, the on-directions function $(p_f)^\sharp_x\colon p_Y[f(x)]\to p_X[x]$ sends each open neighborhood $U$ of $f(x)$ to $f\inv(U)$, which we know is an open neighborhood of $x$ because $f$ is continuous.
  The polybox picture for $p_f$ is as follows:
  \[
  \begin{tikzpicture}[polybox, mapstos]
    \node[poly, dom, "$p$" below] (p) {$f\inv(U)$\at$x\vphantom{f}$};

    \node[poly, cod, right=of p, "$q$" below] (q) {$U\vphantom{f\inv}$\at$f(x)$};

    \draw (p_pos) -- node[below] {$f$} (q_pos);
    \draw (q_dir) -- node[above] {$f\inv$} (p_dir);
  \end{tikzpicture}
  \]

	\item To show that $p_X$ is functorial in $X$, it suffices to show that sending continuous maps $f\colon X\to Y$ to their induced lenses $p_f\colon p_X\to p_Y$ preserves identities and composition.
	First, we show for $X\in\Cat{Top}$ that the lens $p_{\id_X}$ is an identity.
	By \cref{exc.top_poly_func.morphs}, the on-positions function of $p_{\id_X}$ is $\id_X$, and for each $x\in X$ the on-directions function $(p_f)^\sharp_x\colon p_X[x]\to p_X[x]$ sends $U\in p_X[x]$ to $(\id_X)\inv(U) = U$.
	Hence $p_{\id_X}$ is the identity on both positions and directions; it follows from \cref{exc.arena_morph_id} that $p_{\id_X}$ is the identity lens.

	We now show for $X,Y,Z\in\Cat{Top}$ and continuous maps $f\colon X\to Y$ and $g\colon Y\to Z$ that $p_f\then p_g = p_{f\then g}$.
	By \cref{exc.top_poly_func.morphs} and \cref{exc.arena_morph_comp}, the on-positions functions of both $p_f\then p_g$ and $p_{f\then g}$ are equal to $f\then g$, so it suffices to show for each $x\in X$ that
	\[
	    (p_{f \then g})^\sharp_x = (p_g)^\sharp_{f(x)} \then (p_f)^\sharp_x.
	\]
	By \cref{exc.top_poly_func.morphs}, the left hand side sends each $U\in p_Z[g(f(x))]$ to $(f \then g)\inv(U)$, while the right hand side sends $U$ to $f\inv(g\inv(U))$; by elementary set theory, these sets are equal.

	\item The functor is not full.
	Consider the spaces $X\coloneqq\2$ with the indiscrete topology (i.e.\ the only open sets are the empty set and $X$) and $Y\coloneqq\2$ with the discrete topology (i.e.\ all subsets are open).
	Then $p_X\iso\2\yon$ (each point in $X$ has exactly one open neighborhood: the entire space $X$) and $p_Y\iso\2\yon^\2$ (each point in $Y$ has exactly two open neighborhoods: a singleton set and $Y$ itself), so our functor induces a map from the set of continuous functions $X\to Y$ to the set of lenses $\2\yon\to\2\yon^\2$.
	We claim this map is not surjective: in particular, consider the lens $h\colon\2\yon\to\2\yon^\2$ that is the identity on positions (and uniquely defined on directions).
	Then a continuous function $f\colon X\to Y$ that our functor sends to $h$ must also be the identity on the underlying sets of $X$ and $Y$.
	But such a function cannot be continuous: a singleton subset of $Y$ is open, but its preimage under $f$ is a singleton subset of $X$ and therefore not open.
	So our functor sends no continuous function $X\to Y$ to $h$ and therefore is not full.
	The functor is, however, faithful: given spaces $X$ and $Y$ and continuous function $f\colon X\to Y$, we can uniquely recover $f$ from $p_f$ by taking its on-positions function $(p_f)_\1=f$.
\end{enumerate}
\end{solution}
\end{exercise}

%-------- Section --------%
\section{Polybox pictures of lens composition}

\index{polybox!for lens composition|(}

Given lenses $f\colon p\to q$ and $g\colon q\to r$, we can piece their polyboxes together to form polyboxes for their composite, $f\then g\colon p\to r$:
\[
\begin{tikzpicture}[polybox, tos]
  \node[poly, dom, "$p$" below] (p) {};

  \node[poly, right=of p, "$q$" below] (q) {};

  \node[poly, cod, right=of q, "$r$" below] (r) {};

  \draw (p_pos) -- node[below] {$f_\1$} (q_pos);
  \draw (q_dir) -- node[above] {$f^\sharp$} (p_dir);

  \draw (q_pos) -- node[below] {$g_\1$} (r_pos);
  \draw (r_dir) -- node[above] {$g^\sharp$} (q_dir);
\end{tikzpicture}
\]
The position box for $q$, which would be blue as part of the polyboxes for $g\colon q\to r$ alone, is instead filled in via $f_\1$; similarly, the direction box for $q$, which would be blue as part of the polyboxes for just $f\colon p\to q$, is filled in via $g^\sharp$.
This forms a spreadsheet-filling protocol that acts as the polyboxes for $f\then g$.

\index{lens!spreadsheet depiction}

As we follow the arrows from left to right and up and left again, take care to note that the arrow $g^\sharp$ depends not only on the direction box of $r$, but also the position box of $q$ that came before it.
Similarly, $f^\sharp$ depends on both the position box of $p$ and the direction box of $q$.
On the other hand, the arrow $g_\1$ depends only on the position box of $q$, and not the position box of $p$ that came before it: $g_\1$ is the on-positions function for a lens $q\to r$ and therefore depends only on its domain $q$. (Of course, changing the position box of $p$ may change the position box of $q$ via $f_\1$, thus indirectly affecting what $g_\1$ enters in the position box for $r$; we mean that if the position box of $p$ changes but the position box of $q$ does not, $g_\1$ will not change the position box of $r$.)
Similarly, $g^\sharp$ does not depend on the position box of $p$, and $f^\sharp$ does not depend on either box of $r$.
The key is to let each arrow depend on exactly the boxes that come before it in the domain and codomain of the lens that the arrow is a part of.

If we have another lens $h\colon p\to r$, we can interpret the equation $f\then g = h$ by filling in their polyboxes and comparing them:
\[
\begin{tikzpicture}
  \node (1) {
    \begin{tikzpicture}[polybox, mapstos]
      \node[poly, dom, "$p$" below] (p) {$f^\sharp_ig^\sharp_{f_\1i}c$\at$i\vphantom{f_\1}$};

      \node[poly, right=of p, "$q$" below] (q) {$g^\sharp_{f_\1i}c$\at$f_\1i$};

      \node[poly, cod, right=of q, "$r$" below] (r) {$c\vphantom{f^\sharp_ig^\sharp_{f_\1i}c}$\at$g_\1f_\1i$};

      \draw (p_pos) -- node[below] {$f_\1$} (q_pos);
      \draw (q_dir) -- node[above] {$f^\sharp$} (p_dir);

      \draw (q_pos) -- node[below] {$g_\1$} (r_pos);
      \draw (r_dir) -- node[above] {$g^\sharp$} (q_dir);
    \end{tikzpicture}
  };
  \node[right=1.8 of 1] (2) {
    \begin{tikzpicture}[polybox, mapstos]
      \node[poly, dom, "$p$" below] (p) {$h^\sharp_ic$\at$i\vphantom{h_\1}$};
      \node[poly, cod, "$r$" below, right=of p] (q) {$\vphantom{h^\sharp_i}c$\at$h_\1i$};
      \draw (p_pos) -- node[below] {$h_\1$} (q_pos);
      \draw (q_dir) -- node[above] {$h^\sharp$} (p_dir);
    \end{tikzpicture}
  };
  \node at ($(1.east)!.5!(2.west)$) {=};
\end{tikzpicture}
\]
Here we have filled the blue boxes on either side with the same entries.
Then if we match up the uncolored boxes in the domain and codomain on either side, we can read off the equations
\[
  g_\1f_\1i = h_\1i \qqand f^\sharp_ig^\sharp_{f_\1i}c = h^\sharp_ic
\]
for every $p$-position $i$ and $r[h_\1i]$-direction $c$, which agrees with \cref{exc.arena_morph_comp} and \cref{prop.comm_poly}.
Throughout this book, we will often read off equalities of positions and directions from polybox pictures of lens equations in this way.

Note that there is redundancy in the above polybox picture: we have filled in all the boxes for clarity, but their entries are determined by the entries in the blue boxes and the labels on the arrows.
So we may omit the entries in the uncolored boxes without losing information, leaving the reader to fill in the blanks:
\[
\begin{tikzpicture}
  \node (1) {
    \begin{tikzpicture}[polybox, mapstos]
      \node[poly, dom, "$p$" below] (p) {$\phantom{c}$\at$i$};

      \node[poly, right=of p, "$q$" below] (q) {$\phantom{c}$\at$\phantom{i}$};

      \node[poly, cod, right=of q, "$r$" below] (r) {$c$\at$\phantom{i}$};

      \draw (p_pos) -- node[below] {$f_\1$} (q_pos);
      \draw (q_dir) -- node[above] {$f^\sharp$} (p_dir);

      \draw (q_pos) -- node[below] {$g_\1$} (r_pos);
      \draw (r_dir) -- node[above] {$g^\sharp$} (q_dir);
    \end{tikzpicture}
  };
  \node[right=1.8 of 1] (2) {
    \begin{tikzpicture}[polybox, mapstos]
      \node[poly, dom, "$p$" below] (p) {$\phantom{c}$\at$i$};
      \node[poly, cod, "$r$" below, right=of p] (q) {$c$\at$\phantom{i}$};
      \draw (p_pos) -- node[below] {$h_\1$} (q_pos);
      \draw (q_dir) -- node[above] {$h^\sharp$} (p_dir);
    \end{tikzpicture}
  };
  \node at ($(1.east)!.5!(2.west)$) {=};
\end{tikzpicture}
\]

\begin{remark}
  The reader may be concerned that when working with polyboxes, we refer to ``spreadsheets'' and ``protocols'' without being rigorous about what they are or what it means to set them equal.
  We choose to elide this issue to highlight the graphical intuition rather than grinding through the details.
  This is not to say our work with polyboxes will lack rigor moving forward---if you are particularly worried, you should think of polyboxes as an alternate way to present information about indexed families of sets, dependent functions, and sum and product sets that can be systematically translated---via elementary steps, though perhaps with some laborious bookkeeping---into the more standard $\in$ and $\sum$ and $\prod$ notation we have been using thus far.

  For example, given lenses $f\colon p\to q$ and $g\colon q\to r$, the polyboxes on the left hand side of the equation above should be interpreted as the element of the set
  \[
    \prod_{i\in p(\1)}\sum_{k\in r(\1)}p[i]^{r[k]}\iso\poly(p,r)
  \]
  corresponding to the lens $p\to r$ whose on-positions function $p(\1)\to r(\1)$ is the composite of the on-positions functions $f_\1$ and $g_\1$ and whose on-directions function $r[g_\1f_\1i]\to p[i]$ at $i\in p(\1)$ is equal to the composite of the on-directions functions $g^\sharp_{f_\1i}$ and $f^\sharp_i$.
  In other words, the polyboxes represent the composite lens $f\then g$.
  But the polyboxes show how lenses pass positions and directions back and forth far more legibly than the last two sentences can.
  Throughout the rest of this book, we will see how this polybox notation provides immediate, reader-friendly computations and justifications; but all these results can be translated back into more grounded mathematical language as desired.
\end{remark}

\begin{example}[Modeling with a composite lens in polyboxes]
  By composing the lens $f\colon p\to q$ from \cref{ex.lend-return} that models the exchange of money between Caroline (modeled by $q$) and her parents (modeled by $p$) with the lens $\gamma\colon q\to\yon$ from \cref{ex.spend-section} that models how Caroline spends her money, we obtain a lens $f\then\gamma\colon p\to\yon$ that models how Caroline's parents spend their money through Caroline.
  The polybox picture of the composite lens $f\then\gamma$ is given by merging the polybox pictures of $f$ and $\gamma$:
  \[
  \begin{tikzpicture}
    \node (1) {
      \begin{tikzpicture}[polybox, mapstos]
        \node[poly, dom, "$p$" below] (p) {$\left(\dfrac{i}{i+j}\cdot\dfrac{i+j}2,\dfrac{j}{i+j}\cdot\dfrac{i+j}2\right)$\at$(i,j)$};

        \node[poly, right=of p, "$q$" below] (q) {$\vphantom{\left(\dfrac{j}{i+j}\dfrac{i+j}2\right)}\dfrac{i+j}2$\at$i+j$};

        \draw (p_pos) -- node[below] {$f_\1$} (q_pos);
        \draw (q_dir) -- node[above] {$f^\sharp$} (p_dir);

        \draw (q_pos) to[climb'] node[right] {$\gamma$} (q_dir);
      \end{tikzpicture}
    };
    \node[right=1.8 of 1] (2) {
      \begin{tikzpicture}[polybox, mapstos]
        \node[poly, dom, "$p$" below] (p) {$(i/2,j/2)$\at$(i,j)$};

        \draw (p_pos) to[climb'] node[right] {$f\then\gamma$} (p_dir);
      \end{tikzpicture}
    };
    \node at ($(1.east)!.5!(2.west)$) {=};
  \end{tikzpicture}
  \]
  Here $(i,j)\in p(\1)=(0,20]\times(0,20]$.
  The right hand side summarizes what happens to the parents: if the first parent gives away $i$ dollars and the second parent gives away $j$ dollars, eventually the first parent will receive $i/2$ dollars and the second parent will receive $j/2$ dollars.
  The factored left hand side describes how this happens: the parents give $i$ and $j$ dollars respectively to Caroline, who takes the $i+j$ dollars total and spends half of it.
  She then returns the remaining half to her parents, splitting the money proportionately according to the amount each parent contributed.
\end{example}


\index{polybox!for lens composition|)}
\index{lens|)}


%-------- Section --------%
\section{Symmetric monoidal products of polynomial functors} \label{sec.poly.cat.monoidal}

\index{monoidal structure}\index{polynomial functor!monoidal structure on|see{monoidal structure}}

One of the reasons $\poly$ is so versatile is that there is an abundance of monoidal structures on it.
Monoidal structures are the key ingredient to many applications of categories to real-world settings, and $\poly$ is no different in that regard.
As a bonus, if you know how to add and multiply polynomials from high school algebra, then you already know how to compute two of the monoidal products on $\poly$.

We have already seen one of these monoidal structures on $\poly$: the cocartesian monoidal structure, which gives $\poly$ its finite coproducts.
In fact, we know from \cref{prop.poly_coprods} that $\poly$ has all coproducts: they are given by an operation that looks just like addition.
It turns out $\poly$ has all products as well, giving it a cartesian monoidal structure that looks just like multiplication.

\index{monoidal structure!cartesian (products)}\index{monoidal structure!cocartesian (sums)}\index{coproduct}

\begin{proposition}\label{prop.poly_prods}\index{polynomial functor!product of polynomials}
  The category $\poly$ has arbitrary products, coinciding with products in $\smset^\smset$ given by the operation $\prod_{i \in I}$.
\end{proposition}
\begin{proof}
  Unsurprisingly, the proof is very similar to that of \cref{prop.poly_coprods}.

  By \cref{cor.sum_prod_set_endofuncs}, the category $\smset^\smset$ has arbitrary products given by $\prod_{i \in I}$.
  The full subcategory inclusion $\poly \to \smset^\smset$ reflects these products.
  It remains to show that $\poly$ is closed under the operation $\prod_{i \in I}$.\index{polynomial functor!sum of polynomials}

  By \cref{prop.set_endofunc_distrib}, $\smset^\smset$ is completely distributive.
  Hence, given polynomials $(p_i)_{i \in I}$, we can use \eqref{eqn.cat_completely_distributive} to write their product in $\smset^\smset$ as
  \begin{equation} \label{eqn.poly_prod}
    \prod_{i \in I} p_i \iso \prod_{i \in I} \sum_{j \in p_i(\1)} \yon^{p_i[j]} \iso \sum_{\bar{j} \in \prod_{i \in I} p_i(\1)} \prod_{i \in I} \yon^{p_i[\bar{j}i]} \iso \sum_{\bar{j} \in \prod_{i \in I} p_i(\1)} \yon^{\sum_{i \in I} p_i[\bar{j}i]},
  \end{equation}
  which, as a coproduct of representables, is in $\poly$.
  % We will see that $\1$ is a terminal object and that the product of $p$ and $q$ in $\poly$ is the usual product of $p$ and $q$ as polynomials. That is, if $p\coloneqq\sum_{i\in p(\1)}\yon^{p[i]}$ and $q\coloneqq\sum_{j\in q(\1)}\yon^{q[j]}$ are in standard notation, then
  % \begin{equation}\label{eqn.poly_times}
    % p\times q\cong\sum_{i\in p(\1)}\sum_{j\in q(\1)}\yon^{p[i]+q[j]}.
    % \end{equation}
  % We leave the proof as an exercise; see \cref{exc.poly_times}.
\end{proof}

\index{polynomial functor!product of polynomials}

\begin{corollary} \label{prop.poly_completely_distributive}\index{completely distributive category}
  The category $\poly$ is completely distributive.
\end{corollary}
\begin{proof}
  This is a direct consequence of the fact that $\poly$ has arbitrary (co)products coinciding with (co)products in $\smset^\smset$ (\cref{prop.poly_coprods,prop.poly_prods}) and the fact that $\smset^\smset$ itself is completely distributive (\cref{prop.set_endofunc_distrib}).
\end{proof}

The result above will allow us to apply \eqref{eqn.cat_completely_distributive}, or sometimes specifically \eqref{eqn.push_prod_sum_obj_indep}, to push $\prod$'s past $\sum$'s of polynomials whenever we so desire.

\begin{exercise}%\label{exc.poly_prod}
    Use \eqref{eqn.main_formula}
    to verify that
    \[
    \poly\left(q, \prod_{i \in I} p_i\right) \iso \prod_{i \in I} \poly(q, p_i)
    \]
    for all polynomials $(p_i)_{i \in I}$ and $q$, as one would expect from the universal property of products.
    \qedhere
  \begin{solution}
    Given $q \in \poly$ and $p_i \in \poly$ for each $i \in I$ for some set $I$, we use \eqref{eqn.main_formula} to verify that
      \begin{align*}
        \poly\left(q, \prod_{i \in I} p_i\right) &\iso \prod_{k \in q(\1)} \left(\prod_{i \in I} p_i\right)(q[k])
        \tag*{\eqref{eqn.main_formula}} \\
        &\iso \prod_{k \in q(\1)} \prod_{i \in I} p_i(q[k]) \\
        &\iso \prod_{i \in I} \prod_{k \in q(\1)} p_i(q[k]) \\
        &\iso \prod_{i \in I} \poly(q, p_i).
        \tag*{\eqref{eqn.main_formula}}
      \end{align*}
      % \end{enumerate}
  \end{solution}
\end{exercise}

\begin{exercise}
  Let $p_1\coloneqq\yon+\1, p_2\coloneqq\yon+\2,$ and $p_3\coloneqq\yon^\2$.
  What is $\prod_{i\in\3}p_i$ according to \eqref{eqn.poly_prod}? Is the answer what you would expect?
  \begin{solution}
    Given $p_1\coloneqq\yon+\1,p_2\coloneqq\yon+\2,$ and $p_3\coloneqq\yon^\2$, we compute $\prod_{i\in\3}p_i$ via \eqref{eqn.poly_prod} as follows:
    \begin{align*}
      \prod_{i\in\3} p_i
      &\iso
      \sum_{\bar{j} \in \prod_{i\in\3} p_i(\1)} \yon^{\sum_{i\in\3} p_i[\bar{j}(i)]}
      \tag*{\eqref{eqn.poly_prod}} \\
      &\iso
      \sum_{\bar{j} \colon (i\in\3) \to p_i(\1)} \yon^{p_1[\bar{j}(1)] + p_2[\bar{j}(2)] + p_3[\bar{j}(3)]} \\
      &\iso
      \yon^{p_1[1] + p_2[1] + p_3[1]}
      + \yon^{p_1[1] + p_2[2] + p_3[1]}
      + \yon^{p_1[1] + p_2[3] + p_3[1]} \\
      &+ \yon^{p_1[2] + p_2[1] + p_3[1]}
      + \yon^{p_1[2] + p_2[2] + p_3[1]}
      + \yon^{p_1[2] + p_2[3] + p_3[1]} \\
      &\iso
      \yon^{\1 + \1 + \2}
      + \yon^{\1 + \0 + \2}
      + \yon^{\1 + \0 + \2} \\
      &+ \yon^{\0 + \1 + \2}
      + \yon^{\0 + \0 + \2}
      + \yon^{\0 + \0 + \2} \\
      % &\iso
      % \yon^\4 + \yon^\3 + \yon^\3 + \yon^\3 + \yon^\2 + \yon^\2 \\
      &\iso
      \yon^\4 + \3\yon^\3 + \2\yon^\2,
    \end{align*}
    as we might expect from standard polynomial multiplication.
  \end{solution}
\end{exercise}

It follows from \eqref{eqn.poly_prod} that the terminal object of $\poly$ is $\1$, and that binary products are given by
\begin{equation}\label{eqn.poly_times}
  p \times q \iso \sum_{i \in p(\1)} \sum_{j \in q(\1)} \yon^{p[i] + q[j]}.
\end{equation}

We will sometimes write $pq$ rather than $p\times q$:
\[
  pq\coloneqq p\times q.
\]

\index{corolla forest!for product of polynomials}
\begin{example}
  We can draw the product of two polynomials in terms of their associated forests. Let $p\coloneqq\yon^\3+\yon$ and $q\coloneqq\yon^\4+\yon^\2+\1$.
  \[
  \begin{tikzpicture}[rounded corners]
    \node (p1) [draw, blue!50!black, "$p$" above] {
      \begin{tikzpicture}[trees, sibling distance=2.5mm]
        \node["\tiny 1" below] (1) {$\bullet$}
        child {}
        child {}
        child {};
        \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$}
        child {};
      \end{tikzpicture}
    };
    %
    \node (p2) [draw, red!75!black, right=2 of p1, "$q$" above] {
      \begin{tikzpicture}[trees, sibling distance=2.5mm]
        \node["\tiny 1" below] (1) {$\bullet$}
        child {}
        child {}
        child {}
        child {};
        \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$}
        child {}
        child {};
        \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$}
        ;
      \end{tikzpicture}
    };
  \end{tikzpicture}
  \]
  Then $pq\cong\yon^\7+\2\yon^\5+\2\yon^\3+\yon$.
  We take all pairs of positions, and for each pair we take the disjoint union of the directions.
  \[
  \begin{tikzpicture}[rounded corners]
    \node (p1) [draw, "$pq$" above] {
      \begin{tikzpicture}[trees, sibling distance=2.5mm]
        \node["\tiny {(1,1)}" below] (11) {$\bullet$}
        child[blue!50!black] {}
        child[blue!50!black] {}
        child[blue!50!black] {}
        child[red!75!black] {}
        child[red!75!black] {}
        child[red!75!black] {}
        child[red!75!black] {};
        \node[right=1.5 of 11, "\tiny {(1,2)}" below] (12) {$\bullet$}
        child[blue!50!black] {}
        child[blue!50!black] {}
        child[blue!50!black] {}
        child[red!75!black] {}
        child[red!75!black] {};
        \node[right=1.5 of 12, "\tiny {(1,3)}" below] (13) {$\bullet$}
        child[blue!50!black] {}
        child[blue!50!black] {}
        child[blue!50!black] {};
        \node[right=1.5 of 13, "\tiny {(2,1)}" below] (21) {$\bullet$}
        child[blue!50!black] {}
        child[red!75!black] {}
        child[red!75!black] {}
        child[red!75!black] {}
        child[red!75!black] {};
        \node[right=1.5 of 21, "\tiny {(2,2)}" below] (22) {$\bullet$}
        child[blue!50!black] {}
        child[red!75!black] {}
        child[red!75!black] {};
        \node[right=1.5 of 22, "\tiny {(2,3)}" below] (23) {$\bullet$}
        child[blue!50!black] {};
      \end{tikzpicture}
    };
  \end{tikzpicture}
  \]
\end{example}

In practice, we can multiply polynomial functors the same way we would multiply two polynomials in high school algebra.

\begin{exercise} \label{exc.general_poly_times}
  \begin{enumerate}
    \item \label{exc.general_poly_times.monomial} Show that for sets $A_1, B_1, A_2, B_2$, we have
    \[
    B_1\yon^{A_1} \times B_2\yon^{A_2} \iso B_1 B_2\yon^{A_1 + A_2}.
    \]
    \item \label{exc.general_poly_times.polynomial} Show that for sets $(A_i)_{i \in I},(A_j)_{j \in J},(B_i)_{i \in I},$ and $(B_j)_{j \in J}$, we have
    \[
    \left(\sum_{i \in I} B_i\yon^{A_i}\right) \times \left(\sum_{j \in J} B_j\yon^{A_j}\right) \iso \sum_{i \in I} \sum_{j \in J} B_i B_j \yon^{A_i + A_j}.
    \]
  \end{enumerate}
  \begin{solution}
    \begin{enumerate}
      \item We compute the product using \eqref{eqn.poly_times}:
      \begin{align*}
        B_1\yon^{A_1} \times B_2\yon^{A_2} &\iso \left(\sum_{i \in B_1} \yon^{A_1}\right) \times \left(\sum_{j \in B_2} \yon^{A_2}\right) \\
        &\iso \sum_{i \in B_1} \sum_{j \in B_2} \yon^{A_1 + A_2} \\
        &\iso B_1 B_2\yon^{A_1 + A_2}.
      \end{align*}
      \item We expand the product by applying \eqref{eqn.set_completely_distributive}, with $I_1 \coloneqq I$ and $I_2 \coloneqq J$:
      \begin{align*}
        \left(\sum_{i \in I} B_i\yon^{A_i}\right) \times \left(\sum_{j \in J} B_j\yon^{A_j}\right) &\iso \prod_{k \in \2} \sum_{i \in I_k} B_i\yon^{A_i} \\
        &\iso \sum_{\bar{i} \in \prod_{k \in \2} I_k} \prod_{k \in \2} B_{\bar{i}(k)}\yon^{A_{\bar{i}(k)}} \\
        &\iso \sum_{(i,j) \in IJ} B_i\yon^{A_i} \times B_j\yon^{A_j} \\
        &\iso \sum_{i \in I} \sum_{j \in J} B_i B_j \yon^{A_i + A_j}
      \end{align*}
      where the last isomorphism follows from \cref{exc.general_poly_times.monomial}.
    \end{enumerate}
  \end{solution}
\end{exercise}

As lenses, the canonical projections $\pi \colon pq \to p$ and $\varphi \colon pq \to q$ behave quite naturally: on positions, they are the projections from $(pq)(\1) \iso p(\1) \times q(\1)$ to $p(\1)$ and $q(\1)$, respectively; on directions, they are the inclusions $p[i] \to p[i] + q[j]$ and $q[j] \to p[i] + q[j]$ for each position $(i, j)$ of $pq$.

\begin{exercise} \label{exc.poly_prod}
  Verify that, for $p, q \in \poly$, the polynomial $pq$ given by \eqref{eqn.poly_times} along with the lenses $\pi \colon pq \to p$ and $\varphi \colon pq \to q$ described above satisfy the universal property of the product of $p$ and $q$.
  \begin{solution}
    We wish to show that, for $p, q \in \poly$, the polynomial $pq$ along with the lenses $\pi \colon pq \to p$ and $\varphi \colon pq \to q$ as described in the text satisfy the universal property of the product.
    That is, we must show that for any $r \in \poly$ and lenses $f \colon r \to p$ and $g \colon r \to q$, there exists a unique lens $h \colon r \to pq$ for which the following diagram commutes:
    \begin{equation} \label{eqn.prod_univ_prop}
      \begin{tikzcd}
        r \ar[d, "f"'] \ar[r, "g"] \ar[dr, "h", dashed] & q \\
        p & pq. \ar[l, "\pi"] \ar[u, "\varphi"']
      \end{tikzcd}
    \end{equation}
    We apply \cref{prop.comm_poly}.
    In order for \eqref{eqn.prod_univ_prop} to commute, it must commute on positions---that is, the following diagram of sets must commute:
    \begin{equation} \label{eqn.prod_univ_prop_pos}
      \begin{tikzcd}
        r(\1) \ar[d, "f_\1"'] \ar[r, "g_\1"] \ar[dr, "h_\1", dashed] & q(\1) \\
        p(\1) & (pq)(\1). \ar[l, "\pi_\1"] \ar[u, "\varphi_\1"']
      \end{tikzcd}
    \end{equation}
    But since $(pq)(\1) \iso p(1) \times q(\1)$ along with the projections $\pi_\1$ and $\varphi_\1$ form the product of $p(\1)$ and $q(\1)$ in $\smset$, there exists a unique $h_\1$ for which \eqref{eqn.prod_univ_prop_pos} commutes.
    Hence $h$ is uniquely characterized on positions.
    In particular, it must send each $k \in r(\1)$ to the pair $(f_\1k, g_\1k) \in (pq)(\1)$.

    Moreover, if \eqref{eqn.coprod_univ_prop} is to commute on directions, then for every $k \in r(\1)$, the following diagram of sets must commute:
    \begin{equation} \label{eqn.prod_univ_prop_dir}
      \begin{tikzcd}[sep=large]
        r[k] & q[g_\1k] \ar[l, "g^\sharp_k"'] \ar[d, "\varphi^\sharp_{(f_\1k, g_\1k)}"] \\
        p[f_\1k] \ar[u, "f^\sharp_k"] \ar[r, "\pi^\sharp_{(f_\1k, g_\1k)}"'] & (pq)[(f_\1k, g_\1k)]. \ar[ul, "h^\sharp_k"', dashed]
      \end{tikzcd}
    \end{equation}
    As $(pq)[(f_\1k, g_\1k)] \iso p[f_\1k] + q[g_\1k]$ along with the inclusions $\pi^\sharp_{(f_\1k, g_\1k)}$ and $\varphi^\sharp_{(f_\1k, g_\1k)}$ form the coproduct of $p[f_\1k]$ and $q[g_\1k]$ in $\smset$, there exists a unique $h^\sharp_k$ for which \eqref{eqn.prod_univ_prop_dir} commutes.
    Hence $h$ is also uniquely characterized on directions, so it is unique overall.
    Moreover, we have shown that we can define $h$ on positions so that \eqref{eqn.prod_univ_prop_pos} commutes, and that we can define $h$ on directions such that \eqref{eqn.prod_univ_prop_dir} commutes.
    As the commutativity of \eqref{eqn.prod_univ_prop_pos} and \eqref{eqn.prod_univ_prop_dir} together imply the commutativity of \eqref{eqn.prod_univ_prop}, it follows that there exists $h$ for which \eqref{eqn.prod_univ_prop} commutes.
  \end{solution}
\end{exercise}

Much of \cref{part.comon} will focus on the remarkable features of another monoidal structure, an asymmetric one, whose definition we will postpone---we will save its surprises for when we can better savor them.
But here we introduce a third symmetric monoidal structure, given by an operation you were not allowed to do to polynomials back in high school.
% TODO: explain coprods, prods (both univ prop and monoidal), and parallel products in terms of interaction

\index{monoidal structure!parallel|see{Parallel product}}
\index{Dirichlet product|see{parallel product}}
\index{parallel product|(}

\begin{definition}[Parallel product of polynomials] \label{def.parallel}
Let $p$ and $q$ be polynomials. Their \emph{parallel product} (also called \emph{Dirichlet product}), denoted $p\otimes q$, is given by the formula
\begin{equation}\label{eqn.parallel_def}
p\otimes q\coloneqq\sum_{i\in p(\1)}\sum_{j\in q(\1)}\yon^{p[i]\times q[j]}.
\end{equation}
\end{definition}

One should compare this with the formula for the product of polynomials shown in \eqref{eqn.poly_times}. The difference is that the parallel product multiplies exponents where the categorical product adds them.

\begin{exercise} \label{exc.general_poly_parallel_times}
  \begin{enumerate}
    \item \label{exc.general_poly_parallel_times.monomial} Show that for sets $A_1, B_1, A_2, B_2$, we have
    \[
    B_1\yon^{A_1} \otimes B_2\yon^{A_2} \iso B_1 B_2\yon^{A_1 A_2}.
    \]
    \item \label{exc.general_poly_parallel_times.polynomial} Show that for sets $(A_i)_{i \in I},(A_j)_{j \in J},(B_i)_{i \in I},$ and $(B_j)_{j \in J}$, we have
    \[
    \left(\sum_{i \in I} B_i\yon^{A_i}\right) \otimes \left(\sum_{j \in J} B_j\yon^{A_j}\right) \iso \sum_{i \in I} \sum_{j \in J} B_i B_j \yon^{A_i A_j}.
    \]
  \end{enumerate}
  \begin{solution}
    \begin{enumerate}
      \item We compute the parallel product using \eqref{eqn.parallel_def}:
      \begin{align*}
        B_1\yon^{A_1} \otimes B_2\yon^{A_2} &\iso \left(\sum_{i \in B_1} \yon^{A_1}\right) \otimes \left(\sum_{j \in B_2} \yon^{A_2}\right) \\
        &\iso \sum_{i \in B_1} \sum_{j \in B_2} \yon^{A_1 \times A_2} \\
        &\iso B_1 B_2\yon^{A_1 A_2}.
      \end{align*}

      \item We expand the parallel product as follows:
      \begin{align*}
        \left(\sum_{i \in I} B_i\yon^{A_i}\right) \otimes \left(\sum_{j \in J} B_j\yon^{A_j}\right) &\iso \left(\sum_{i \in I} \sum_{i' \in B_i} \yon^{A_i}\right) \otimes \left(\sum_{j \in J} \sum_{j' \in B_j} \yon^{A_j}\right) \\
        &\iso \sum_{i \in I} \sum_{i' \in B_i} \sum_{j \in J} \sum_{j' \in B_j} \yon^{A_i \times A_j} \\
        &\iso \sum_{i \in I} \sum_{j \in J} \sum_{i' \in B_i} \sum_{j' \in B_j} \yon^{A_i A_j} \\
        &\iso \sum_{i \in I} \sum_{j \in J} B_i B_j \yon^{A_i A_j}.
      \end{align*}
    \end{enumerate}
  \end{solution}
\end{exercise}

\begin{exercise} \label{exc.some_parallel_prods}
  \begin{enumerate}
    \item \label{exc.some_parallel_prods.const} If $p\coloneqq A$ and $q\coloneqq B$ are constant polynomials, what is $p\otimes q$?
    \item \label{exc.some_parallel_prods.lin} If $p\coloneqq A\yon$ and $q\coloneqq B\yon$ are linear polynomials, what is $p\otimes q$?
    \item \label{exc.some_parallel_prods.pos_prod} For arbitrary $p,q\in\poly$, show that the sets $(p\otimes q)(\1)$ and $p(\1)\times q(\1)$ are isomorphic.
    \qedhere
  \end{enumerate}
  \begin{solution}
    \begin{enumerate}
      \item By \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial}, we have $A\otimes B \iso A\yon^\0\otimes B\yon^\0 \iso AB\yon^{\0} \iso AB$.
      \item By \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial}, we have $A\yon\otimes B\yon \iso A\yon^\1\otimes B\yon^\1 \iso AB\yon^\1 \iso AB\yon$.
      \item By \eqref{eqn.parallel_def},
      \[
      (p \otimes q)(\1) \iso \sum_{i \in p(\1)} \sum_{j \in q(\1)} \1^{p[i] \times q[j]} \iso p(\1) \times q(\1).
      \]
    \end{enumerate}
  \end{solution}
\end{exercise}

\begin{exercise}
  Consider the polynomials $p\coloneqq \2\yon^\2+\3\yon$ and $q\coloneqq\yon^\4+\3\yon^\3$.
  \begin{enumerate}
    \item What is $p\times q$?
    \item What is $p\otimes q$?
    \item Expand the following expression in the variable $y$ according to the ordinary laws of arithmetic.
    \[
    (2\cdot2^y+3\cdot 1^y) \cdot
    (1\cdot4^y+3\cdot 3^y)
    \]
    The factors of the above product are called \emph{Dirichlet series}.\index{Dirichlet series}
    \item Describe the connection between the last two parts. (This is why the parallel product $\otimes$ is also known as the \emph{Dirichlet product}.) \qedhere
  \end{enumerate}
  \begin{solution}
    \begin{enumerate}
      \item We compute $p \times q$ using \cref{exc.general_poly_times} \cref{exc.general_poly_times.polynomial}:
      \begin{align*}
        p \times q &\iso \2\yon^{\2 + \4} + (\2 \times \3)\yon^{\2 + \3} + \3\yon^{\1 + \4} + (\3 \times \3)\yon^{\1 + \3} \\
        &\iso \2\yon^\6 + \6\yon^\5 + \3\yon^\5 + \9\yon^\4 \\
        &\iso \2\yon^\6 + \9\yon^\5 + \9\yon^\4.
      \end{align*}

      \item We compute $p \otimes q$ using \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.polynomial}:
      \begin{align*}
        p \otimes q &\iso \2\yon^{\2 \times \4} + (\2 \times \3)\yon^{\2 \times \3} + \3\yon^\4 + (\3 \times \3)\yon^\3 \\
        &\iso \2\yon^\8 + \6\yon^\6 + \3\yon^\4 + \9\yon^\3.
      \end{align*}

      \item We evaluate $(2\cdot2^y+3\cdot1^y+1) \cdot
      (1\cdot4^y+3\cdot3^y+2)$ using ordinary laws of arithmetic:
      \begin{align*}
        (2\cdot2^y+3\cdot 1^y) \cdot (1\cdot4^y+3\cdot 3^y) &= 2\cdot1\cdot2^y\cdot4^y + 2\cdot3\cdot2^y\cdot3^y + 3\cdot1\cdot1^y\cdot4^y + 3\cdot3\cdot1^y\cdot3^y \\
        &= 2\cdot8^y + 6\cdot6^y + 3\cdot4^y + 9\cdot3^y.
      \end{align*}

      \item We describe the connection between the last two parts as follows.
      Given a polynomial $p$, we let $d(p)$ denote the Dirichlet series $\sum_{i \in p(\1)} |p[i]|^y$.
      Then by \eqref{eqn.parallel_def},
      \begin{align*}
        d(p \otimes q) &= \sum_{i \in p(\1)} \sum_{j \in q(\1)} |p[i] \times q[j]|^y \\
        &= \sum_{i \in p(\1)} |p[i]|^y \sum_{j \in q(\1)} |q[j]|^y \\
        &= d(p) \cdot d(q).
      \end{align*}
      The last two parts are simply an example of this identity for a specific choice of $p$ and $q$.
    \end{enumerate}
  \end{solution}
\end{exercise}

\begin{example}
We can draw the parallel product of two polynomials in terms of their associated forests. Let $p\coloneqq\yon^\3+\yon$ and $q\coloneqq\yon^\4+\yon^\2+\1$.
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, blue!50!black, "$p$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$}
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$}
      child {};
  \end{tikzpicture}
  };
%
	\node (p2) [draw, red!75!black, right=2 of p1, "$q$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$}
      child {}
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$}
      child {}
      child {};
    \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$}
    ;
  \end{tikzpicture}
  };
\end{tikzpicture}
\]
Then $p\otimes q\cong\yon^{\1\2}+\yon^\6+\yon^\4+\yon^\2+\2$.
We take all pairs of positions, and for each pair we take the product of the directions.
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, "$p \otimes q$" above] {
	\begin{tikzpicture}[trees, sibling distance=2mm]
    \node["\tiny {(1,1)}" below] (11) {$\bullet$}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
    ;
    \node[right=2 of 11, "\tiny {(1,2)}" below] (12) {$\bullet$}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
    ;
    \node[right=1.5 of 12, "\tiny {(1,3)}" below] (13) {$\bullet$}
    ;
   \node[right=1.5 of 13, "\tiny {(2,1)}" below] (21) {$\bullet$}
      child {}
      child {}
      child {}
      child {}
 		;
		\node[right=1.5 of 21, "\tiny {(2,2)}" below] (22) {$\bullet$}
      child {}
      child {}
 		;
    \node[right=1.5 of 22, "\tiny {(2,3)}" below] (23) {$\bullet$}
 		;
	\end{tikzpicture}
	};
\end{tikzpicture}
\]
\end{example}

\index{monoidal structure!corolla forest depiction}

\begin{exercise}
  Let $p\coloneqq\yon^\2+\yon$ and $q\coloneqq\2\yon^\4$.
  \begin{enumerate}
    \item Draw $p$ and $q$ as corolla forests.
    \item Draw $pq=p\times q$ as a corolla forest.
    \item Draw $p\otimes q$ as a corolla forest.
    \qedhere
  \end{enumerate}
  \begin{solution}
    \begin{enumerate}
      \item Here are $p$ and $q$ drawn as corolla forests:
      \[
      \begin{tikzpicture}[rounded corners]
        \node (p1) [draw, blue!50!black, "$p$" above] {
          \begin{tikzpicture}[trees, sibling distance=2.5mm]
            \node["\tiny 1" below] (1) {$\bullet$}
            child {}
            child {};
            \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$}
            child {};
          \end{tikzpicture}
        };
        %
        \node (p2) [draw, red!75!black, right=2 of p1, "$q$" above] {
          \begin{tikzpicture}[trees, sibling distance=2.5mm]
            \node["\tiny 1" below] (1) {$\bullet$}
            child {}
            child {}
            child {}
            child {};
            \node[right=1 of 1,"\tiny 2" below] (2) {$\bullet$}
            child {}
            child {}
            child {}
            child {};
          \end{tikzpicture}
        };
      \end{tikzpicture}
      \]

      \item Here is $pq$ drawn as a corolla forest:
      \[
      \begin{tikzpicture}[rounded corners]
        \node (p1) [draw, "$pq$" above] {
          \begin{tikzpicture}[trees, sibling distance=2.5mm]
            \node["\tiny {(1,1)}" below] (11) {$\bullet$}
            child[blue!50!black] {}
            child[blue!50!black] {}
            child[red!75!black] {}
            child[red!75!black] {}
            child[red!75!black] {}
            child[red!75!black] {};
            \node[right=1.5 of 11, "\tiny {(1,2)}" below] (12) {$\bullet$}
            child[blue!50!black] {}
            child[blue!50!black] {}
            child[red!75!black] {}
            child[red!75!black] {}
            child[red!75!black] {}
            child[red!75!black] {};
            \node[right=1.5 of 12, "\tiny {(2,1)}" below] (21) {$\bullet$}
            child[blue!50!black] {}
            child[red!75!black] {}
            child[red!75!black] {}
            child[red!75!black] {}
            child[red!75!black] {};
            \node[right=1.5 of 21, "\tiny {(2,2)}" below] (22) {$\bullet$}
            child[blue!50!black] {}
            child[red!75!black] {}
            child[red!75!black] {}
            child[red!75!black] {}
            child[red!75!black] {};
          \end{tikzpicture}
        };
      \end{tikzpicture}
      \]
      \item Here is $p \otimes q$ drawn as a corolla forest:
      \[
      \begin{tikzpicture}[rounded corners]
        \node (p1) [draw, "$pq$" above] {
          \begin{tikzpicture}[trees, sibling distance=2mm]
            \node["\tiny {(1,1)}" below] (11) {$\bullet$}
            child {}
            child {}
            child {}
            child {}
            child {}
            child {}
            child {}
            child {}
            ;
            \node[right=2 of 11, "\tiny {(1,2)}" below] (12) {$\bullet$}
            child {}
            child {}
            child {}
            child {}
            child {}
            child {}
            child {}
            child {}
            ;
            \node[right=1.5 of 12, "\tiny {(2,1)}" below] (21) {$\bullet$}
            child {}
            child {}
            child {}
            child {}
            ;
            \node[right=1.5 of 21, "\tiny {(2,2)}" below] (22) {$\bullet$}
            child {}
            child {}
            child {}
            child {}
            ;
          \end{tikzpicture}
        };
      \end{tikzpicture}
      \]
    \end{enumerate}
  \end{solution}
\end{exercise}




\begin{exercise}\label{exc.prepare_poly_smc}
Let $p,q,r\in\poly$ be any polynomials.
\begin{enumerate}
  \item Show that there is an isomorphism $p\otimes\yon\iso p$.
  \item Show that there is an isomorphism $(p\otimes q)\otimes r\iso p\otimes (q\otimes r)$.
  \item Show that there is an isomorphism $p\otimes q \iso q\otimes p$.
 \qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
  \item We show that $p\otimes\yon\iso p$:
  \begin{align*}
      p \otimes y &\iso \sum_{i \in p(\1)} \sum_{j \in \1} \yon^{p[i] \times \1} \tag*{\eqref{eqn.parallel_def}} \\
      &\iso \sum_{i \in p(\1)} \yon^{p[i]} \iso p.
  \end{align*}

  \item We show that $(p\otimes q)\otimes r\cong p\otimes (q\otimes r)$:
  \begin{align*}
      (p \otimes q) \otimes r &\iso \left(\sum_{i \in p(\1)} \sum_{j \in q(\1)} \yon^{p[i] \times q[j]}\right) \otimes r \tag*{\eqref{eqn.parallel_def}} \\
      &\iso \sum_{i \in p(\1)} \sum_{j \in q(\1)} \left(\sum_{k \in r(\1)} \yon^{(p[i] \times q[j]) \times r[k]}\right) \tag*{\eqref{eqn.parallel_def}} \\
      &\iso \sum_{i \in p(\1)} \left(\sum_{j \in q(\1)} \sum_{k \in r(\1)} \yon^{p[i] \times (q[j] \times r[k])}\right) \tag{Associativity of $\sum$ and $\times$} \\
      &\iso p \otimes \left(\sum_{j \in q(\1)} \sum_{k \in r(\1)} \yon^{q[j] \times r[k]}\right) \tag*{\eqref{eqn.parallel_def}} \\
      &\iso p \otimes (q \otimes r). \tag*{\eqref{eqn.parallel_def}} \\
  \end{align*}

  \item We show that $(p\otimes q)\cong(q\otimes p)$:
  \begin{align*}
      p \otimes q &\iso \sum_{i \in p(\1)} \sum_{j \in q(\1)} \yon^{p[i] \times q[j]} \tag*{\eqref{eqn.parallel_def}} \\
      &\iso \sum_{j \in q(\1)} \sum_{i \in p(\1)} \yon^{q[j] \times p[i]} \tag{Commutativity of $\sum$ and $\times$} \\
      &\iso q \otimes p. \tag*{\eqref{eqn.parallel_def}} \\
  \end{align*}
\end{enumerate}
\end{solution}
\end{exercise}

In \cref{exc.prepare_poly_smc}, we have gone most of the way to proving that $(\poly,\yon,\otimes)$ is a symmetric monoidal category.
We sketch the rest of the proof as follows.

\begin{proposition}\label{prop.parallel_monoidal}
The category $\poly$ has a symmetric monoidal structure $(\yon,\otimes)$ where $\otimes$ is the parallel product from \cref{def.parallel}.
\end{proposition}
\begin{proof}[Sketch of proof]
Given two lenses $f\colon p\to p'$ and $g\colon q\to q'$, we need to define a lens $(f\otimes g)\colon (p\otimes q)\to (p'\otimes q')$. This is easiest to define using polyboxes, keeping in mind that the positions and directions of a parallel product are pairs of positions and directions of its constituent factors:
\[
\begin{tikzpicture}[polybox, mapstos]
  \node[poly, dom, "$p\otimes q$" left] (p) {$(f^\sharp_i a,g^\sharp_j b)$\at$(i,j)\vphantom{(f_\1i,g_\1j)}$};
  \node[poly, cod, "$p'\otimes q'$" right, right=of p] (q) {$(a,b)\vphantom{(f^\sharp_i a,g^\sharp_j b)}$\at$(f_\1i,g_\1j)$};
  \draw (p_pos) -- node[below] {$(f\otimes g)_\1$} (q_pos);
  \draw (q_dir) -- node[above] {$(f\otimes g)^\sharp$} (p_dir);
\end{tikzpicture}
\]
Here $i\in p(\1), j\in q(\1), a\in p'[f_\1 i],$ and $b\in q'[g_\1 j]$.

Then \cref{exc.prepare_poly_smc} gives us the unitors, associator, and braiding.
We have not proven the functoriality of $\otimes$, the naturality of the isomorphisms from \cref{exc.prepare_poly_smc}, or all the coherences between these isomorphisms, but we ask the reader to take them on trust or to check them for themselves.
Alternatively, we may invoke the Day convolution to obtain the monoidal structure $(\yon, \otimes)$ directly: see \cref{prop.day}.
\end{proof}\index{Day convolution}

\begin{exercise}
  \begin{enumerate}
    \item What is $(\3\yon^\5+\6\yon^\2)\otimes\4$? Hint: $\4=\4\yon^\0$.
    \item Is the class of constant polynomials a \emph{$\otimes$-ideal}; that is, is the parallel product of a polynomial and a constant polynomial always a constant? \qedhere
  \end{enumerate}
  \begin{solution}
    \begin{enumerate}
      \item We compute $(\3\yon^\5+\6\yon^\2)\otimes\4$ using \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.polynomial} and the fact that $\4=\4\yon^\0$:
      \begin{align*}
        (\3\yon^\5+\6\yon^\2)\otimes\4\yon^\0 &\iso (\3\times\4)\yon^{\5\times\0} + (\6\times\4)\yon^{\2\times\0} \\
        &\iso \1\2\yon^\0 + \2\4\yon^\0 \\
        &\iso \3\6.
      \end{align*}
      \item Given a polynomial $p$ and a set $J$ viewed as a constant polynomial, we have
      \begin{align*}
        p\otimes J &\iso \left(\sum_{i\in p(\1)}\yon^{p[i]}\right)\otimes\left(\sum_{j\in J}\yon^\0\right) \\
        &\iso \sum_{i\in p(\1)}\sum_{j\in J}\yon^{p[i]\times\0} \\
        &\iso p(\1)J\yon^\0 \\
        &\iso p(\1)J,
      \end{align*}
      itself a constant polynomial; so the class of constant polynomials is indeed a $\otimes$-ideal.
    \end{enumerate}
  \end{solution}
\end{exercise}

\index{parallel product!of special polynomials}

\begin{exercise}\label{exc.dir_closed_classes}\index{affine polynomial}\index{linear polynomial}\index{monomial}
Which of the following special classes of polynomials are closed under $\otimes$? Note also whether they contain $\yon$.
\begin{enumerate}
	\item The class $\{A\yon^\0\mid A\in\smset\}$ of constant polynomials.
	\item The class $\{A\yon\mid A\in\smset\}$ of linear polynomials.
	\item The class $\{A\yon+B\mid A,B\in\smset\}$ of affine polynomials.
	\item The class $\{A\yon^\2+B\yon+C\mid A,B,C\in\smset\}$ of quadratic polynomials.
	\item The class $\{A\yon^B\mid A,B\in\smset\}$ of monomials.
	\item The class $\{S\yon^S\mid S\in\smset\}$.
	\item The class $\{p\in\poly\mid p(\1)\text{ is finite}\}$. \qedhere
\end{enumerate}
\begin{solution}
For each of the following classes of polynomials, we determine whether they are closed under $\otimes$ and whether they contain $\yon$.
\begin{enumerate}
	\item The set $\{A\yon^\0\mid A\in\smset\}$ of constant polynomials is closed under $\otimes$ by the solution to \cref{exc.some_parallel_prods} \cref{exc.some_parallel_prods.const}.
	But the set does not contain $\yon$, as $\yon$ is not a constant polynomial.
	\item The set $\{A\yon\mid A\in\smset\}$ of linear polynomials is closed under $\otimes$ by the solution to \cref{exc.some_parallel_prods} \cref{exc.some_parallel_prods.lin} and does contain $\yon$, as $\yon \iso \1\yon$.
	\item The set $\{A\yon+B\mid A,B\in\smset\}$ of affine polynomials is closed under $\otimes$, for \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.polynomial} yields
	\[
	    (A\yon + B) \otimes (A'\yon + B') \iso AA'\yon + AB' + BA' + BB'.
	\]
	The set contains $\yon$, as $\yon \iso \1\yon + \0 $.
	\item The set $\{A\yon^\2+B\yon+C\mid A,B,C\in\smset\}$ of quadratic polynomials is not closed under $\otimes$, for even though $\yon^\2 \iso \1\yon^\2 + \0\yon + \0$ is a quadratic polynomial, \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial} implies that
	\[
	    \yon^\2 \otimes \yon^\2 \iso \yon^\4,
	\]
	which is not quadratic.
	The set contains $\yon$, as $\yon \iso \0\yon^\2 + \1\yon + \0$.
	\item The set $\{A\yon^B\mid A,B\in\smset\}$ of monomials is closed under $\otimes$ by \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial} and does contain $\yon$, as $\yon \iso \1\yon^\1$.
	\item The set $\{S\yon^S\mid S\in\smset\}$ is closed under $\otimes$, for \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial} returns
	\[
	    S\yon^S \otimes T\yon^T \iso ST\yon^{ST}.
	\]
	The set contains $\yon$, as $\yon \iso \1\yon^\1$.
	\item The set $\{p\in\poly\mid p(\1)\text{ is finite}\}$ is closed under $\otimes$ by the solution to \cref{exc.some_parallel_prods} \cref{exc.some_parallel_prods.pos_prod} and the fact that the product of two finite sets is itself finite.
	The set contains $\yon$, as $\yon(\1) \iso \1$ is finite.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
What is the smallest class of polynomials that is closed under $\otimes$ and contains $\yon$?
\begin{solution}
The smallest class of polynomials that is closed under $\otimes$ and contains $\yon$ is just $\{\yon\}$.
This is because by \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial}, we have $\yon \otimes \yon \iso \yon$.
\end{solution}
\end{exercise}

\index{distributive law!for parallel product}

\begin{exercise}
Show that for any $p_1,p_2,q\in\poly$ there is an isomorphism
\[
(p_1+p_2)\otimes q\iso (p_1\otimes q)+(p_2\otimes q).
\]
\begin{solution}
We show that $(p_1 + p_2) \otimes q \iso (p_1 \otimes q) + (p_2 \otimes q)$ using \eqref{eqn.parallel_def}:
\begin{align*}
    (p_1 + p_2) \otimes q &\iso \sum_{k \in \2} \sum_{i \in p_k(\1)} \sum_{j \in q(\1)} \yon^{p_k[i] \times q[j]} \\
    &\iso \sum_{i \in p_1(\1)} \sum_{j \in q(\1)} \yon^{p_1[i] \times q[j]} + \sum_{i \in p_2(\1)} \sum_{j \in q(\1)} \yon^{p_2[i] \times q[j]} \\
    &\iso (p_1 \otimes q) + (p_2 \otimes q).
\end{align*}
\end{solution}
\end{exercise}

\begin{remark}
  Monoids in $\poly$ with respect to the parallel product $\otimes$ are particularly interesting---they have a kind of collective semantics, letting agents aggregate their contributions and distribute returns on those contributions in a coherent way.
  We leave discussion of them to future work, so as not to distract us from our main story.

  % TODO: cite collectives paper
\end{remark}

\index{monoid!for parallel product}

There is a more general way to obtain monoidal structures on $\poly$ like $\times$ and $\otimes$ using a construction known as the \emph{Day convolution}, defined by a special kind of colimit known as a \emph{coend}. If you have not seen the Day convolution or coends before, do not fret: we will not use them elsewhere in the book, and rest assured that the fact about coends known as the \emph{co-Yoneda lemma} employed in the following proof is a standard and purely formal result.

\index{Day convolution}\index{distributive law!for Day monoidal structures}
\index{monoidal structure!from monoidal structure on $\smset$}

\begin{proposition} \label{prop.day}
For any monoidal structure $(I,\star)$ on $\smset$, there is a corresponding monoidal structure $(\yon^I, \odot)$ on $\poly$, where $\odot$ is the Day convolution.
Moreover, $\odot$ distributes over coproducts.\index{coproduct!distributing over}

In the case of $(\0,+)$ and $(\1,\times)$, this procedure returns the $(\1,\times)$ and $(\yon,\otimes)$ monoidal structures respectively.
\end{proposition}
\begin{proof}
Any monoidal structure $(I,\star)$ on $\smset$ induces a monoidal structure on $\smset^\smset$ with the Day convolution $\odot$ as the tensor product and $\yon^I$ as the unit.
To prove that this monoidal structure restricts to $\poly$, it suffices to show that $\poly$ is closed under the Day convolution.

Given polynomials $p$ and $q$, their Day convolution in $\smset^\smset$ is given by the coend
\begin{equation} \label{eqn.day_conv.coend}
    p \odot q \iso \int^{(A,B)\in\smset^\2} \yon^{A \star B} \times p(A) \times q(B).
\end{equation}
We can rewrite the product $p(A) \times q(B)$ as
\[
    p(A) \times q(B) \iso \left(\sum_{i \in p(\1)} A^{p[i]}\right) \times \left(\sum_{j \in q(\1)} B^{q[i]}\right) \iso \sum_{(i,j) \in p(\1) \times q(\1)} A^{p[i]} \times B^{q[i]}
\]
So because products distribute over coproducts in $\smset^\smset$ and coends always commute with coproducts (as they are both colimits), we can rewrite \eqref{eqn.day_conv.coend} as
\begin{align*}
    p \odot q &\iso \sum_{(i,j) \in p(\1) \times q(\1)} \int^{(A,B)\in\smset^\2} \yon^{A \star B} \times A^{p[i]} \times B^{q[i]} \\
    &\iso \sum_{(i,j) \in p(\1) \times q(\1)} \int^{(A,B)\in\smset^\2} \yon^{A \star B} \times \smset^\2((p[i],q[j]),(A,B))
\end{align*}
which, by the co-Yoneda lemma, can be rewritten as
\begin{equation} \label{eqn.day_conv.poly}
    p \odot q \iso \sum_{(i,j) \in p(\1) \times q(\1)} \yon^{p[i] \star q[j]},
\end{equation}
which is in $\poly$.
That the Day convolution distributes over coproducts also follows from the fact that products distribute over coproducts in $\smset^\smset$ and that coends commute with coproducts; or, alternatively, directly from \eqref{eqn.day_conv.poly}.

We observe that \eqref{eqn.day_conv.poly} gives $(\yon^I, \odot) = (\1, \times)$ when $(I, \star) \coloneqq (\0, +)$ and $(\yon^I, \odot) = (\yon, \otimes)$ when $(I, \star) \coloneqq (\1, \times)$.
\end{proof}\index{Day convolution}

\begin{exercise}
There is a monoidal structure on $\smset$ whose unit is $\0$ and whose product is given by $(A, B)\mapsto A+AB+B$.
\begin{enumerate}
	\item Verify that the operation $(A, B)\mapsto A+AB+B$ on $\smset$ is associative.
	\item Verify that $\0$ is the unit for the above operation.
	\item Let $(\1,\odot)$ denote the corresponding monoidal structure on $\poly$ obtained via \cref{prop.day}. Compute the monoidal product $(\yon^\3+\yon)\odot(\2\yon^\2+\2)$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item To show that the operation $(A,B)\mapsto A+AB+B$ on $\smset$ is associative, observe that
    \begin{align*}
        (A + AB + B) + (A + AB + B)C + C &\iso A + AB + B + AC + ABC + BC + C \\
        &\iso A + AB + ABC + AC + B + BC + C \\
        &\iso A + A(B + BC + C) + (B + BC + C).
    \end{align*}
    \item To show that $\0$ is the unit for this operation, observe that
    \[
        (A, \0) \mapsto A + A\0 + \0 \iso A
    \]
    and
    \[
        (\0, B) \mapsto \0 + \0B + B \iso B.
    \]
    \item Taking $A \star B \coloneqq A + AB + B$ in \cref{prop.day} to obtain a monoidal product $\odot$ on $\poly$, we can use \eqref{eqn.day_conv.poly} to compute that
    \begin{align*}
        (\yon^\3+\yon)\odot(\2\yon^\2+\2) &\iso (\yon^\3+\yon^\1)\odot(\yon^\2+\yon^\2+\yon^\0+\yon^\0) \\
        &\iso \yon^{\3\star\2} + \yon^{\3\star\2} + \yon^{\3\star\0} + \yon^{\3\star\0} + \yon^{\1\star\2} + \yon^{\1\star\2} + \yon^{\1\star\0} + \yon^{\1\star\0} \\
        &\iso \2\yon^{\1\1} + \2\yon^\3 + \2\yon^\5 + \2\yon^\1.
    \end{align*}
\end{enumerate}
\end{solution}
\end{exercise}

\index{parallel product|)}

%-------- Section --------%z1
\section{Summary and further reading}

In this chapter, we introduced the category $\poly$, whose objects are polynomial functors and whose morphisms are the natural transformations between them.
We call these natural transformations \emph{dependent lenses}, or \emph{lenses} for short.
We also proved our first categorical property of $\poly$: that it has all small coproducts.
%In the next chapter, we will review the coproduct construction before turning to other operations on our polynomials.

The main result of this chapter was a concrete characterization of our dependent lenses between polynomial functors.
A dependent lens $f\colon p\to q$ is characterized by its
\begin{itemize}
  \item \emph{on-positions function}, a function $f_\1\colon p(\1)\to q(\1)$ sending $p$-positions forward to $q$-positions; and its
  \item \emph{on-directions functions}, one for each $p$-position $i$ denoted $f^\sharp_i\colon q[f_\1 i]\to p[i]$ sending $q[f_\1 i]$-directions backward to $p[i]$-directions.
\end{itemize}
This forward-backward relationship is what makes dependent lenses so well-suited for modeling \emph{interaction protocols}.
Given two agents with positions and directions, a dependent lens between them defines an interaction protocol that describes how the position of the first agent determines the position of the second agent and how the direction of the second agent determines the direction of the first.
This perspective is exhibited by our corolla and polybox pictures for lenses.
We studied examples of lenses between special polynomials: in particular, lenses between monomials are known as \emph{bimorphic lenses} in functional programming literature.

\index{lens!between monomials}

We then unwound our interpretation of natural transformations between polynomials as dependent lenses with on-positions and on-directions functions to describe what happens to these functions when lenses compose.
This gave us an accessible way to interpret commutative diagrams in $\poly$ that is particularly convenient to express using polyboxes.

Finally, we considered various categorical structures on $\poly$, e.g.\ that it has all products and coproducts, and that these distribute: $\prod\sum\to\sum\prod$.
\begin{align*}
	\sum_{a\in A}p_a&\coloneqq\sum_{(a,i)\in\sum_{a\in A}p_a(1)}\yon^{p_a[i]}&
	\prod_{a\in A}p_a&\coloneqq\sum_{i\in\prod_{a\in A}p_a(1)}\yon^{\sum_{a\in A}p_a[i a]}
\\
	p_1+p_2&\coloneqq\sum_{(a,i)\in\{(1,i_1)\mid i_1\in p_1(\1)\}+\{(2,i_2)\mid i_2\in p_2(\1)\}}\yon^{p_a[i]}&
	p_1\times p_2&\coloneqq\sum_{(i_1,i_2)\in p_1(1)\times p_2(1)}\yon^{p_1[i_1]+p_2[i_2]}
\end{align*}
We also discussed how one can take any monoidal product $\star$ from $\smset$ and lift it to a monoidal product $\cdot$ on $\poly$:
\[
	p_1\odot p_2\coloneqq\sum_{i_1,i_2)\in p_1(1)\times p_2(1)}\yon^{p_1[i_1]\cdot p_2[i_2]}
\]
A special case of this is the product structure $\times$ on $\poly$, which emerges from the coproduct structure $+$ on $\smset$. The other case of interest is the parallel (or Dirichlet) product structure $\otimes$ on $\poly$, which emerges from the product structure $\times$ on $\smset$:
\[
	p_1\otimes p_2\coloneqq\sum_{i_1,i_2)\in p_1(1)\times p_2(1)}\yon^{p_1[i_1]\times p_2[i_2]}
\]

Variants of lenses are studied in compositional game theory \cite{hedges2016compositionality,hedges2017coherence,hedges2018limits,hedges2018morphisms}, in categorical database theory \cite{johnson2012lenses}, in functional programming and programming language theory \cite{bohannon2006relational,oconnor2011functor,abou2016reflections}, and in more generalized categorical settings \cite{gibbons2012relating,spivak2019generalized}.

\index{compositional game theory}
\index{database}
\index{functional programming!lenses in}

%-------- Section --------%
\section{Exercise solutions}
\Closesolutionfile{solutions}
{\footnotesize
\input{solution-file3}}

\Opensolutionfile{solutions}[solution-file4]
\index{category!of polynomial functors|)}

%------------ Chapter ------------%
\chapter{Dynamical systems as dependent lenses} \label{ch.poly.dyn_sys}

One of the main goals of this book is to use dependent lenses in $\poly$ to model dynamical systems and automata.
In this chapter, we will begin to see how to do this through an array of examples.

%-------- Section --------%
\section{Moore machines}\label{sec.poly.dyn_sys.moore}

\index{Moore machine|(}

We start with our simplest example of a dynamical system: a deterministic state machine with a fixed range of states, inputs, and outputs.
At any point in time, this machine will inhabit one of its possible states and return output according to that current state. It can also update its current state according to the input it receives.

\begin{definition}[Moore machine]\label{def.moore_machine}
  A \emph{Moore machine} consists of the following data: three sets,
  \begin{itemize}
    \item a set $S$, called the \emph{state-set}, whose elements are \emph{states};
    \item a set $I$, called the \emph{position-set} (or \emph{output-set}), whose elements are \emph{positions} (or \emph{outputs});
    \item a set $A$, called the \emph{direction-set} (or \emph{input-set}), whose elements are \emph{directions} (or \emph{inputs});
  \end{itemize}
  and two functions,
  \begin{itemize}
    \item $\text{return}\colon S\to I$;
    \item $\text{update}\colon S\times A\to S$.
  \end{itemize}
  To emphasize the role that the three sets play, we can specify that this is an $(A,I)$\emph{-Moore machine with states} $S$.
\end{definition}

\index{Moore machine!states of}
\index{Moore machine!update}\index{Moore machine!return}

The input/output terminology is standard, while the position/direction terminology is our own: we will soon see how the positions and directions of a Moore machine relate to that of a polynomial.

We should interpret an $(A,I)$-Moore machine as follows.
At any time, the machine inhabits one of the states in its state-set $S$.
Say its current state is $s\in S$.
We can ask the machine to perform one of the following two tasks.
\begin{itemize}
  \item We can ask the machine to \emph{return its position}: it should then produce the position $\text{return}(s) \in I$.
  \item We can feed the machine one of its \emph{directions} $a\in A$ and ask it to \emph{update its state}: it should then replace its current state with the new state $\text{update}(s,a)\in S$.
  Note that the new state depends not only on the direction the machine receives but also on the state the machine inhabits when it receives that direction.
\end{itemize}

We may visualize a Moore machine with a \emph{transition diagram} as follows.

\index{Moore machine!transition diagram}
\index{interface!monomial|(}

\begin{example}[A Moore machine's transition diagram]\label{ex.Moore_three}
Given $A\coloneqq\{{\color{orange}\text{orange}},{\color{dgreen}\text{green}}\}$ and $I\coloneqq\{0,1\}$, we can draw a transition diagram for an $(A,I)$-Moore machine with $S\coloneqq\3$ states as follows:
\begin{equation} \label{eqn.trans_diag}
\begin{tikzpicture}
	\node[draw] {
  \begin{tikzcd}[column sep=small]
  	\LMO{0}\ar[rr, dgreen, thick, bend left]\ar[loop left, thick, orange]&&
  	\LMO{1}\ar[ll, thick, orange, bend left]\ar[dl, bend left, thick, dgreen]\\&
  	\LMO{1} \ar[ul, thick, orange, bend left] \ar[loop left, thick, dgreen]
  \end{tikzcd}
  };
\end{tikzpicture}
\end{equation}
Each state is labeled by the position it returns according to the machine's return function.
Additionally, each state has two outgoing arrows, one ${\color{orange}\text{orange}}$ and one ${\color{dgreen}\text{green}}$, corresponding to the two possible directions.
The targets of the arrows indicate the updated state according to the machine's update function.

Say the machine starts at the bottom state.
By feeding it a sequence of directions---say $({\color{orange}\text{orange}}, {\color{orange}\text{orange}}, {\color{dgreen}\text{green}}, {\color{orange}\text{orange}},\ldots)$---we can send the machine through its states via its update function and return the position at each state:
\begin{enumerate}
    \item Starting at the bottom state, the machine returns the position $1$.
    \item Following the {\color{orange}orange} arrow from the bottom state, the machine updates its state to the left state.
    \item At the left state, the machine returns the position $0$.
    \item Following the {\color{orange}orange} arrow from the left state, the machine updates its state to---once again---the left state.
    \item At the left state, the machine returns the position $0$.
    \item Following the {\color{dgreen}green} arrow from the left state, the machine updates its state to the right state.
    \item At the right state, the machine returns the position $1$.
    \item Following the {\color{orange}orange} arrow from the right state, the machine updates its state to the left state.
    \item At the left state, the machine returns the position $0$.

    \ldots
\end{enumerate}
In summary, starting from the bottom state, this Moore machine sends the sequence $({\color{orange}\text{orange}}, {\color{orange}\text{orange}}, {\color{dgreen}\text{green}}, {\color{orange}\text{orange}},\ldots)$ of directions in $A$ to the sequence $(1,0,0,1,0,\ldots)$ of positions in $I$.
\end{example}

In general, given an initial state $s_0\in S$, an $(A,I)$-Moore machine with states $S$ sends every sequence $(a_1,a_2,a_3,\ldots)$ of directions in $A$ to a sequence $(i_0,i_1,i_2,i_3,\ldots)$ of positions in $I$, defined inductively as follows, via an intermediary sequence $(s_0,s_1,s_2,s_3,\ldots)$ of states in $S$:
\[
    b_k\coloneqq \text{return}(s_k) \qqand s_{k+1}\coloneqq \text{update}(s_k, a_{k+1})
\]
for all $k\in\nn$.
We will see that $\poly$ gives us a more concise way to express this in \cref{ex.input_output}.

Comparing \cref{def.moore_machine} with \cref{subsec.poly.cat.morph.bimorphic-lens}, we find that an $(A,I)$-Moore machine with states $S$ is precisely a lens between monomials $\varphi\colon S\yon^S\to I\yon^A$ with on-positions function $\varphi_\1\coloneqq\text{return}\colon S\to I$ and on-directions map $\varphi^\sharp\coloneqq S\times A\to S$.
The positions and directions of the Moore machine are the positions and directions of the codomain of the corresponding lens, while the domain of the lens has the states of the Moore machine as both its positions and its directions.
So we can repackage \cref{def.moore_machine} as follows.

\begin{definition}[Moore machine, version 2]\label{def.moore_machine2}
  For $S,I,A\in\smset$, an $(A,I)$-\emph{Moore machine with states} $S$ is a lens \[\varphi\colon S\yon^S\to I\yon^A\] in $\poly$.
  We call
  \begin{itemize}
    \item the domain monomial $S\yon^S$ the machine's \emph{state system}: its position-set (equivalently, its direction-set) is the machine's \emph{state-set}, and its positions (equivalently, its directions) are the machine's \emph{states};
    \item the codomain monomial $I\yon^A$ the machine's \emph{interface}: its position-set and direction-set are the machine's \emph{position-set} and \emph{direction-set}, and its positions and directions are the machine's \emph{positions} and \emph{directions};
    \item the on-positions function $\varphi_\1\colon S\to I$ the machine's \emph{return function};
    \item the on-directions map $\varphi^\sharp\colon S\times A\to S$ the machine's \emph{update function}.
  \end{itemize}
\end{definition}
\index{state system}\index{interface}\index{Moore machine}

We call the codomain of a Moore machine its \emph{interface} because it encodes how an outsider interacts with the machine: an outsider observes the positions of the interface that the machine returns and feeds the directions of the interface to the machine to update it.
Rather than directly observing and altering the machine's states, an outsider must interact with the machine via its interface.

\index{Moore machine!interface of|seealso{interface}}

\begin{exercise}
In this exercise, we will write the Moore machine from \cref{ex.Moore_three} as a lens $\varphi$ between monomials.
\begin{enumerate}
    \item What is the machine's state system, the domain of $\varphi$?
    \item What is the machine's interface, the codomain of $\varphi$?
\end{enumerate}
Call the left state $L$, the right state $R$, and the bottom state $B$.
\begin{enumerate}[resume]
    \item What is the machine's return function, the on-positions function of $\varphi$?
    \item What is the machine's update function, the on-directions map of $\varphi$?
    \item Draw the first two steps listed in \cref{ex.Moore_three} of the machine's operation (starting at the bottom state and receiving the direction ${\color{orange}\text{orange}}$) using polyboxes.
    \qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item As $S=\3$, the state system is $S\yon^S=\3\yon^\3$.
    \item As $I=\{0,1\}$ and $A=\{{\color{orange}\text{orange}},{\color{dgreen}\text{green}}\}$, the interface is $I\yon^A=\{0,1\}\yon^{\{{\color{orange}\text{orange}},\,{\color{dgreen}\text{green}}\}}$.
    \item The return function $S\to I$ sends $L\mapsto 0, R\mapsto 1,$ and $B\mapsto 1$.
    \item The update function $S\times A\to S$ sends
    \begin{align*}
        (L, {\color{orange}\text{orange}})\mapsto L&,\quad(L, {\color{dgreen}\text{green}})\mapsto R, \\
        (R, {\color{orange}\text{orange}})\mapsto L&,\quad(R, {\color{dgreen}\text{green}})\mapsto B, \\
        (B, {\color{orange}\text{orange}})\mapsto L&,\quad(B, {\color{dgreen}\text{green}})\mapsto B.
    \end{align*}
    \item In the first two steps, the machine sends its bottom state $B$ to its position $1$ via its return function, then sends its bottom state $B$ and its direction ${\color{orange}\text{orange}}$ to its left state $L$ via its update function.
    We can interpret this in terms of the lens $\varphi$ and depict the steps in polyboxes as
    \[
    \begin{tikzpicture}[polybox, mapstos]
      \node[poly, dom] (p) {$L$\at$B$};
        \node[left=0pt of p_pos] {$S$};
        \node[left=0pt of p_dir] {$S$};

      \node[poly, cod, right=of p] (q) {${\color{orange}\text{orange}}$\at$1$};
        \node[right=0pt of q_pos] {$I$};
        \node[right=0pt of q_dir] {$A$};

      \draw (p_pos) -- node[below] {$f$} (q_pos);
      \draw (q_dir) -- node[above] {$f\inv$} (p_dir);
    \end{tikzpicture}
    \]
\end{enumerate}
\end{solution}
\end{exercise}

Here are some more examples of Moore machines.

\begin{example}[Counter]\label{ex.counting_trajectory}
There is a $(\1,\nn)$-Moore machine with states $\nn$ that, with initial state $0\in\nn$, returns the sequence of natural numbers $(0,1,2,3,\ldots)$.
The machine is given by the lens $\nn\yon^\nn\to\nn\yon$ whose on-positions function is the identity $\nn\to\nn$ and whose on-directions map $\nn\times\1\iso\nn\to\nn$ sends $n\mapsto n+1$.
Here it is in polyboxes (recall that the shaded direction box indicates that the direction-set is a singleton, i.e.\ there is no choice to be made in filling it in):
\[
\begin{tikzpicture}[polybox, mapstos]
  \node[poly, dom] (s) {$n+1$\at$n$};
    \node[left=0pt of s_pos] {$\nn$};
    \node[left=0pt of s_dir] {$\nn$};

 	\node[poly, cod, linear, right=of s] (p) {$\vphantom{1}$\at$n$};
    \node[right=0pt of p_pos] {$\nn$};

 	\draw (s_pos) to[first] (p_pos);
 	\draw (p_dir) to[last] (s_dir);
\end{tikzpicture}
\]
The picture tells us that if the current state (the left position box) is $n\in\nn$, the next state (the left direction box) is $n+1\in\nn$.
Since the machine just returns its current state as a position, the sequence of positions returned will always be an increasing sequence of consecutive natural numbers starting at the initial state.
\end{example}

\begin{example}[Moving in the plane]\label{ex.R2_moore}
  Let us construct a Moore machine with positions in $\rr^2$, which we may think of as locations in the coordinate plane, and directions in $[0,\infty)\times[0,2\pi)$, which we may think of as commands to move a certain distance $r\in[0,\infty)$ at a certain angle $\theta\in[0,2\pi)$.
  We will let the machine's state-set be $\rr^\2$ as well, so the machine is a lens
  \[
    \rr^\2\yon^{\rr^\2}\to\rr^\2\yon^{[0,1]\times[0,2\pi)}.
  \]
  We can define such a lens using polyboxes:
  \[
  \begin{tikzpicture}[polybox, mapstos]
    \node[poly, dom] (s) {$(x+r\cos\theta,y+r\sin\theta)$\at$(x,y)$};
    \node[left=0pt of s_pos] {$\rr^\2$};
    \node[left=0pt of s_dir] {$\rr^\2$};

    \node[poly, cod, right=of s] (p) {$(r,\theta)\vphantom{y}$\at$(x,y)$};
    \node[right=0pt of p_pos] {$\rr^\2$};
    \node[right=0pt of p_dir] {$[0,\infty)\times[0,2\pi)$};

    \draw (s_pos) to[first] (p_pos);
    \draw (p_dir) to[last] (s_dir);
  \end{tikzpicture}
  \]
\end{example}

\begin{exercise}
Explain in words what the Moore machine in \cref{ex.R2_moore} does.
\begin{solution}
At any time, the Moore machine in \cref{ex.R2_moore} is located at a point on the coordinate plane, say $(x, y) \in \rr^\2$.
This location is its current state.
When we ask the machine to return its position, it will tell us those coordinates, since the return function is the identity.
Then if we give the machine a direction $(r, \theta)$ for some distance $r \in [0,1]$ and angle $\theta \in [0, 2\pi)$, the machine will move by that distance, at that angle counterclockwise from the positive $x$-axis, from $(x, y)$ to
\[
    (x+r\cos\theta, y+r\sin\theta) = (x,y) + r(\cos\theta, \sin\theta)
\]
(here we treat $\rr^\2$ as a vector space, so that $r(\cos\theta, \sin\theta)$ is a vector of length $r$ at the angle $\theta$).
\end{solution}
\end{exercise}

\index{Moore machine!memoryless}
\index{Moore machine!induced by function}

\begin{example}[Functions as memoryless Moore machines]\label{ex.funs_to_moore}
Given a function $f\colon A\to I$, there is a corresponding $(A,I)$-Moore machine with states $I$ that takes in an element of $A$ and returns the element of $I$ obtained by applying $f$.

It is given by the lens $I\yon^I\to I\yon^A$ defined as follows:
\[
\begin{tikzpicture}[polybox, mapstos]
  \node[poly, dom] (s) {$f(a)$\at$i$};
  \node[left=0pt of s_pos] {$I$};
  \node[left=0pt of s_dir] {$I$};

  \node[poly, cod, right=of s] (p) {$\vphantom{f}a$\at$i$};
  \node[right=0pt of p_pos] {$I$};
  \node[right=0pt of p_dir] {$A$};

  \draw (s_pos) to[first] (p_pos);
  \draw (p_dir) to[last] (s_dir);
\end{tikzpicture}
\]
That is, this lens is the identity on positions, returning the state directly as its position, and on directions it is the function $I\times A\To{\pi_2}A\To{f} I$, which ignores the current state and applies $f$ to the direction received to compute the new state.

If the machine starts in state $i_0$ and is given a sequence of directions $(a_1,a_2,\ldots)$ from $A$, the machine will return the positions $(i_0,f(a_1),f(a_2),\ldots)$. We say this machine is \emph{memoryless}, because at no point does the state of the machine actually depend on any previous states; instead, its state depends only on the last direction it received.
\end{example}

\begin{exercise}\label{exc.funs_to_moore}
Suppose we have a function $f\colon A\times I\to I$.
\begin{enumerate}
	\item Find a corresponding $(A,I)$-Moore machine $I\yon^I\to I\yon^A$.
  You may draw it out in polyboxes.
	\item Would you say the machine is memoryless?
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We seek an $(A,I)$-Moore machine $I\yon^I\to I\yon^A$ corresponding to the function $f\colon A\times I\to I$.
    We know that an $(A,I)$-Moore machine $I\yon^I \to I\yon^A$ consists of a return function $I \to I$ and an update function $I \times A \to I$.
    So we can simply let the return function be the identity on $I$ and the update function be $I \times A \iso A \times I \To{f} B$, i.e.\ the function $f$ with its inputs swapped.
    In polyboxes, the machine looks like
    \[
    \begin{tikzpicture}[polybox, mapstos]
      \node[poly, dom] (s) {$f(a,i)$\at$i$};
      \node[left=0pt of s_pos] {$I$};
      \node[left=0pt of s_dir] {$I$};

      \node[poly, cod, right=of s] (p) {$\vphantom{f}a$\at$i$};
      \node[right=0pt of p_pos] {$I$};
      \node[right=0pt of p_dir] {$A$};

      \draw (s_pos) to[first] (p_pos);
      \draw (p_dir) to[last] (s_dir);
    \end{tikzpicture}
    \]

    \item Generally, such a machine is not memoryless.
    Unlike in \cref{ex.funs_to_moore}, the update function $I \times A \iso A \times I \To{f} I$ does appear to depend on its first input, namely the previous state, which $f$ takes as its second input.
    We can see this from out polybox picture above: the left direction box, which contains the new state $f(a,i)$, depends on the current state $i\in I$ in the left position box.

    However, if $f$ factors through the projection $\pi_1 \colon A \times I \to A$, i.e.\ if $f$ can be written as a composite $A \times I \To{\pi_1} A \To{f'} B$ for some $f' \colon A \to B$, then the resulting machine \emph{is} memoryless: it is the memoryless Moore machine from \cref{ex.funs_to_moore} corresponding to $f'$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Find $A,I\in\smset$ such that the following can be identified with a lens $S\yon^S\to I\yon^A$, and explain in words what the corresponding $(A,I)$-Moore machine does (there may be multiple possible solutions):
\begin{enumerate}
	\item a \emph{discrete dynamical system}, i.e.\ a set of states $S$ and a transition function $S\to S$ that describes how to transfer from state to state.
	\item a \emph{magma}, i.e.\ a set $S$ and a function $S\times S\to S$.
	\item a set $S$ and a subset $S'\ss S$.\qedhere
\end{enumerate}
\begin{solution}
For each of the following constructs, we find $A,I\in\smset$ such that the construct can be identified with a lens $\varphi\colon S\yon^S\to I\yon^A$, i.e.\ a return function $\varphi_\1\colon S\to B$ and an update function $\varphi^\sharp\colon S\times A\to S$.
\begin{enumerate}
  \item Given a discrete dynamical system with states $S$ and transition funtion $n\colon S\to S$, we can set $A\coloneqq I\coloneqq\1$.
  Then $\varphi_\1\colon S\to\1$ is unique, while $\varphi^\sharp\colon S\times\1\to S$ is given by $S\times\1\iso S\To{n}S$.
  The corresponding Moore machine can only be fed one direction (you could think of that direction as a button that simply says ``advance to the next state'') and can only return one position (which tells us no information).
  So it is just a set of states and a deterministic way to move from state to state.

  We could have also set $A\coloneqq\0$ and $I\coloneqq S$, so that $\varphi_\1\coloneqq n$ and $\varphi^\sharp\colon S\times\0\to S$ is unique, but this formulation is somewhat less satisfying: this is a Moore machine that never moves between its states, effectively functioning as a lookup table between whatever state the machine happens to be in and its position, which also happens to refer to some state.

  \item Given a magma consisting of a set $S$ and a function $m\colon S\times S\to S$, we can set $A \coloneqq S$ and $I\coloneqq\1$.
  Then $\varphi_\1\colon S\to\1$ is unique, while $\varphi^\sharp\colon S\times S\to S$ is equal to $m$.
  The corresponding Moore machine always returns the same position.
  It uses the binary operation $m$ to combine the current state with a given direction---which also refers to a state---to obtain the new state.

  Alternatively, we could have set the update function to be $m$ with its inputs swapped.
  The difference here is that the new state is given by applying $m$ with the direction on the left and the current state on the right, rather than the other way around.
  If $m$ is noncommutative, this would yield a different Moore machine.

  We could have also set $A\coloneqq\0$ and $I\coloneqq S^S$, so that $\varphi^\sharp\colon S\times\0\to S$ is unique, while currying $m$ gives $\varphi_\1$, so that $\varphi_\1 s$ is the function $S \to S$ given by $s' \mapsto m(s, s')$.
  Alternatively, $\varphi_\1 s$ could be the function $s' \mapsto m(s', s)$.
  Either way, this is again a Moore machine that never moves between its states, functioning as a lookup table between the machine's current state and the function $m$ partially applied to that state on one side or the other.

  \item Given a set $S$ and a subset $S' \ss S$, we can set $A\coloneqq\0$ and $I\coloneqq\2$.
  Then $\varphi^\sharp\colon S\times\0\to S$ is unique, while we define $\varphi_\1\colon S\to\2$ by
  \[
      \varphi_\1 s =
      \begin{cases}
          1 & \text{if } s \in S' \\
          2 & \text{if } s \notin S'
      \end{cases}
  \]
  so that $S'$ can be recovered from $\varphi_\1$ as its fiber over $1$.
  The corresponing Moore machine never moves between its states, but returns one of two positions indicating whether or not the current state is in the subset $S'$.
\end{enumerate}
\end{solution}
\end{exercise}

The previous examples of Moore machines mostly had identities as return functions.
In the following exercises, we will build examples of Moore machines that do not return their entire states as positions.

\begin{exercise}[Robot with health]
Think of the Moore machine in \cref{ex.R2_moore} as a robot and modify it as follows.

Add to its state a ``health meter,'' which takes a real value between 0 and 1 representing the robot's health.
Have the robot lose half its health each time it moves to a location whose $x$-coordinate is negative.
Do not return the robot's health; instead, use its health $h$ as a multiplier, allowing it to move a distance of $hr$ given an input of $r$.
\begin{solution}
The original Moore machine had states $\rr^\2$, so to add a health meter that takes values in $[0,1]$, we take the cartesian product to obtain a new set of states $\rr^\2\times[0,1]$.
The position-set and direction-set are unchanged, so the Moore machine is a lens
\[
    \rr^\2\times[0,1]\yon^{\rr^\2\times[0,1]} \to \rr^\2\yon^{[0,\infty)\times[0,2\pi)}.
\]\index{product}
Its return function $\rr^\2\times[0,1]\to\rr^\2$ is the canonical projection, as the machine returns only its location in $\rr^\2$ and not its health; while its update function
\[
    \rr^\2 \times [0,1] \times [0,\infty) \times [0,2\pi) \to \rr^\2 \times [0,1]
\]
sends $(x, y, h, r, \theta)$ to
\[
    (x + hr\cos\theta, y + hr\sin\theta, h'),
\]
where $h' = h/2$ if the machine's new $x$-coordinate $x + hr\cos\theta < 0$ and $h' = h$ otherwise.
As polyboxes, the lens is
\[
\begin{tikzpicture}[polybox, mapstos]
  \node[poly, dom] (s) {$(x + hr\cos\theta, y + hr\sin\theta, h')$\at$(x,y,h)$};
  \node[left=0pt of s_pos] {$\rr^\2\times[0,1]$};
  \node[left=0pt of s_dir] {$\rr^\2\times[0,1]$};

  \node[poly, cod, right=of s] (p) {$(r,\theta)\vphantom{hy}$\at$(x,y)$};
  \node[right=0pt of p_pos] {$\rr^\2$};
  \node[right=0pt of p_dir] {$[0,\infty)\times[0,2\pi)$};

  \draw (s_pos) to[first] (p_pos);
  \draw (p_dir) to[last] (s_dir);
\end{tikzpicture}
\]
\end{solution}
\end{exercise}

\index{Moore machine!tape of Turing machine}

\begin{exercise}[Tape of a Turing machine]
A Turing machine has a tape consisting of a cell for each integer.
Each cell bears a value $v\in V\coloneqq\{0,1,-\}$, and one of the cells $c\in\zz$ is distinguished as the ``current'' cell.
So the set of states of the tape is $V^\zz\times\zz$.

The Turing machine interacts with the tape by asking for the value of the current cell, an element of $V$; and by changing the value of the current cell before moving left (i.e.\ replacing the current cell $c\in\zz$ with the new cell $c-1$) or right (i.e.\ replacing $c$ with $c+1$).
Hence the tape's position-set is $V$ and its direction-set is $V\times\{\const{left},\,\const{right}\}$.

\begin{enumerate}
	\item If we model the tape as a Moore machine $t\colon S\yon^S\to I\yon^A$, what are $S,I,$ and $A$?
	\item Write down the specific $t$ that makes it act like a tape as specified above.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item The tape has states $S\coloneqq V^\zz \times \zz$, positions $I\coloneqq V$, and directions $A\coloneqq V\times\{\const{left},\,\const{right}\}$; as a Moore machine, it is a lens
    \[
        t \colon (V^\zz \times \zz)\yon^{V^\zz \times \zz} \to V\yon^{V \times \{\const{left},\,\const{right}\}}.
    \]
    \item The return function of $t$ should give the value in the current cell of the tape.
    So $t_\1\colon V^\zz\times\zz\to V$ is the evaluation map: it sends $(f,c)$ with $f\colon\zz\to V$ and $c\in\zz$ to $f(c)\in V$.
    Then on a given direction $(v,d)\in V\times\{\const{left},\,\const{right}\}$, the update function of $t$ writes $v$ in the tape's current cell before shifting the current cell number up or down by one according to whether $d$ is $\const{right}$ or $\const{left}$.
    More precisely,
    \[
        t^\sharp \colon (V^\zz \times \zz) \times (V \times \{\const{left},\,\const{right}\}) \to V^\zz \times \zz
    \]
    sends current tape $f\colon\zz\to V$, current cell number $c\in\zz$, new value $v \in V$, and $d \in \{\const{left},\,\const{right}\}$ to the new tape $f' \colon \zz \to V$ defined by
    \[
        f'(n)\coloneqq
        \begin{cases}
            v & \text{if } n = c \\
            f(n) & \text{if } n \neq c
        \end{cases}
    \]
    and the new cell number $c-1$ if $d=\const{left}$ and $c+1$ if $d=\const{right}$.
\end{enumerate}
\end{solution}
\end{exercise}

\index{Moore machine!file reader}

\begin{exercise}[File-reader]\label{exc.file_reader}
Say that a \emph{file} of length $n$ is a function $f\colon\ord{n}\to\Set{ascii}$, where $\Set{ascii}\coloneqq\2\5\6$.
We refer to elements of $\ord{n}=\{1,\ldots,n\}$ as \emph{entries} in the file and, for each entry $i\in\ord{n}$, the value $f(i)\in\Set{ascii}$ as the \emph{character} at entry $i$.

Given a file $f$, design a file-reading Moore machine whose position-set is $\Set{ascii} + \{\const{done}\}$
and whose direction-set is
\[
\{(s,t)\mid 1\leq s\leq t\leq n\}+\{\const{continue}\}.
\]
Given a direction $(s,t)$, the file-reader should go to entry $s$ in the file and return the character at that entry.
If the given direction is instead $\const{continue}$, the file-reader should move to the next entry (i.e.\ from $s$ to $s+1$) and read that character---unless the new entry would be greater than $t$, in which case the file-reader should return $\const{done}$ until it receives another $(s,t)$ pair.
\begin{solution}
There are many options for the machine's state-set; we choose to use pairs of entries $(i,t)\in\ord{n}^\2$, where $i$ is the entry where the file-reader is currently located and $t$ is the entry where the file-reader should stop.
We will also include a $\const{stopped}$ state for when the file-reader has already stopped.
So our Moore machine is a lens
\[
    (\ord{n}^\2+\{\const{stopped}\})\yon^{\ord{n}^\2+\{\const{stopped}\}} \to (\Set{ascii} + \{\const{done}\})\yon^{\{(s,t)\mid 1\leq s\leq t\leq n\}+\{\const{continue}\}}.
\]
If the file-reader's current state is $\const{stopped}$, then the file-reader should return the position ``done.''
Otherwise, the file-reader should return the character at the entry where the file-reader is currently located.
So its return function $\ord{n}^\2+\{\const{stopped}\} \to \Set{ascii} + \{\const{done}\}$ sends $(i, t)$ to $f(i)$ and $\const{stopped}$ to $\const{done}$.
Meanwhile, the update function
\[
    (\ord{n}^\2+\{\const{stopped}\}) \times (\{(s,t)\mid 1\leq s\leq t\leq n\}+\{\const{continue}\}) \to \ord{n}^\2+\{\const{stopped}\}
\]
behaves as follows on the current state and given direction: regardless of the current state, if the given direction is a pair $(s,t)$, the new state will also be $(s,t)$.
On the other hand, if the given direction is $\const{continue}$ and the current state is a pair $(i,t)$, the new state should be the pair $(i+1,t)$ if $i+1\leq t$ and $\const{done}$ otherwise.
Finally, if the given direction is $\const{continue}$ and the current state is $\const{stopped}$, the new state should still be $\const{stopped}$.
\end{solution}
\end{exercise}

While \cref{exc.file_reader} gives us a functioning file-reader, it is rather awkward that we are still able to give the direction $\const{continue}$ even when the position is $\const{done}$, or provide a new range of entries before the file-reader has finished reading from the previous range.
In \cref{sec.poly.dyn_sys.depend_sys}, we will introduce a generalization of Moore machines to handle cases like these, where the array of directions the machine can receive changes depending on its current position.
In particular, we will be able to let the file-reader ``close its port,'' so that it cannot receive signals while it is busy reading, but open its port once it is \const{done}; see \cref{ex.generalized_file_reader}.

\subsection{Deterministic state automata}

The diagram in \cref{ex.Moore_three} may look familiar to those who have studied automata theory; in fact, a deterministic state automaton can be expressed as a Moore machine with a distinguished initial state.

\index{deterministic state automaton}\index{Moore machine!deterministic state automaton|see{deterministic state automaton}}

\begin{definition}[Deterministic state automaton, language]\label{def.dfa}
A \emph{deterministic state automaton} consists of
\begin{itemize}
	\item a set $S$ of \emph{states};
	\item a set $A$ of \emph{symbols};
	\item an \emph{update function} $u\colon S\times A\to S$;
	\item an \emph{initial state} $s_0\in S$;
	\item a subset $F\ss S$ of \emph{accept states}.
\end{itemize}
Let\index{list}
\[
  \lst(A)=\sum_{n\in\nn}A^\ord{n}
\]
denote the set of finite sequences $(a_1,\ldots,a_n)$ of symbols in $A$; we call such a sequence a \emph{word}.
We say that the automaton \emph{accepts} the word $(a_1,\ldots,a_n)$ if starting at the initial state and following the symbols in the word leads us to an accept state---or, more formally, if the sequence $(s_0,s_1,\ldots,s_n)$ defined inductively by
\[
  s_{k+1}\coloneqq u(s_k,a_{k+1})
\]
is such that $s_n$ is an accept state: $s_n\in F$.

We call a subset of $\lst(A)$ a \emph{language}, and we say that the set of all words in $\lst(A)$ that the automaton accepts is the language \emph{recognized} by the automaton.
\end{definition}

\index{deterministic state automaton!language of}

\begin{remark}
When we study a deterministic state automaton, we are usually interested in which words the automaton accepts and, more generally, what language the automaton recognizes.
While intuitive, the condition we provided for when an automaton accepts a word can be cumbersome to work with.
In \cref{ex.dsa_lang_recog}, we will give a more compact way of describing whether an automaton accepts a word and specifying the language the automaton recognizes.
Better yet, we will find that this alternative formulation arises naturally from the theory of $\poly$.
\end{remark}

\begin{proposition} \label{prop.dsa}
A deterministic state automaton with a set of states $S$ and a set of symbols $A$ can be identified with a pair of lenses
\[
  \yon\to S\yon^S\to \2\yon^A.
\]
\end{proposition}
\begin{proof}
By \cref{exc.lens-from-0-or-yon}, a lens $\yon\to S\yon^S$ can be identified with an initial state $s_0\in S$.
Then a lens $S\yon^S\to\2\yon^A$ consists of a return function $f\colon S\to\2$, which can be identified with a subset of accept states $F \ss S$, together with an update function $u \colon S\times A\to S$.
\end{proof}

In other words, we can think of a deterministic state automaton as a Moore machine with position set $\2$ along with a distinguished initial state; the Moore machine has the same states and update function as the automaton and the automaton's symbols as its directions.

Now imagine if we wanted to construct a version of this automaton that stops reading symbols (i.e.\ directions) whenever the machine enters an accept state (i.e.\ returns one position instead of the other).
To do this would require a machine whose set of possible directions is dependent on its current position.
Instead of an update function $u\colon S\times A\to S$, we would need an update function that takes a direction $a\in A$ if the state $s\in S$ is \emph{not} an accept state (say, if $f(s)=1$) but takes a direction in $\0$ (i.e.\ no direction) if the state $s$ \emph{is} an accept state (if $f(s)=2$).
So there would be one update function $u_s\colon A\to S$ if $f(s)=1$ and a different update function $u_s\colon\0\to S$ if $f(s)=2$.
But these are exactly the on-directions functions of a lens $S\yon^S\to\yon^A+\1$!
Indeed, replacing our interface monomial with a general polynomial is exactly how we will obtain our generalized dependent Moore machines.

\index{interface!monomial|)}
\index{Moore machine|)}

%-------- Section --------%
\section{Dependent dynamical systems}\label{sec.poly.dyn_sys.depend_sys}

\index{Moore machine!dependent|see{dynamical system, dependent}}
\index{dynamical system|(}\index{interface!polynomial}

Each of our Moore machines above has a monomial $I\yon^A$ as an interface.
Every representable summand of such an interface has the same representing set $A$, so the set of directions that can be fed into the machine is always $A$.
But by replacing $I\yon^A$ with an arbitrary polynomial $p$, which may have a different direction-set at each position, we can model a broader class of machines.

\begin{definition}[Dependent dynamical system]\label{def.gen_moore}
  A \emph{dependent dynamical system} (or a \emph{dependent Moore machine}, or simply a \emph{dynamical system}) is a lens \[\varphi\colon S\yon^S\to p\] for some $S\in\smset$ and $p\in\poly$.
  We call
  \begin{itemize}
    \item the domain monomial $S\yon^S$ the machine's \emph{state system}---its position-set (equivalently, its direction-set) is the machine's \emph{state-set}, and its positions (equivalently, its directions) are the machine's \emph{states};
    \item the codomain polynomial $p$ the machine's \emph{interface}---its position-set and direction-sets are the machine's \emph{position-set} and \emph{direction-sets}, and its positions and directions are the machine's \emph{positions} and \emph{directions};
    \item the on-positions function $\varphi_\1\colon S\to p(\1)$ the machine's \emph{return function};
    \item the on-directions map $\varphi^\sharp\colon p[\varphi_\1(-)]\to S$ the machine's \emph{update map}, and the on-directions function $\varphi^\sharp_s\colon p[\varphi_\1s]\to S$ at $s\in S$ the machine's \emph{update function} at $s$.
  \end{itemize}
\end{definition}
\index{state system}

\begin{example}[Dynamical systems as polyboxes]
  We can express a dynamical system $\varphi\colon S\yon^S\to p$ in polyboxes as
  \[
    \begin{tikzpicture}[polybox, mapstos]
      \node[poly, dom] (S) {$t$\at$s\vphantom{i}$};
        \node[left=0pt of S_pos] {$S$};
        \node[left=0pt of S_dir] {$S$};

      \node[poly, cod, right=of S, "$p$" right] (p) {$a\vphantom{t}$\at$i$};

      \draw (S_pos) -- node[below] {return} (p_pos);
      \draw (p_dir) -- node[above] {update} (S_dir);
    \end{tikzpicture}
  \]
  We can visualize $\varphi$ as a channel between the internal state system on the left and the external interface on the right.
  The state system enters its current state $s\in S$ into the left position box, and the return function converts this state to a position $i\in p(\1)$ of the interface.
  At $i$, the interface has a direction-set $p[i]$; an interacting agent selects one of these directions $a\in p[i]$ to enter into the right direction box.
  Finally, the update map uses the current state $s$ and the position $i$ to fill the left direction box with the new state $t\in S$.
  Then the process repeats with $t$ in place of $s$.
\end{example}
\index{interface}

\begin{remark}
It may seem limiting that the set of possible directions a dependent dynamical system can receive should depend on the current \emph{position} rather than the current \emph{state}; but this makes sense philosophically if we accept that the system's interface should capture \emph{everything} about how it interacts with the outside world.
In particular, the system's position should capture everything an external observer could possibly perceive about the system, while the direction-set should capture all the ways in which an external agent can choose to interact with the system.
But if the set of directions available to an external agent \emph{changes}, the external agent should be able to detect this fact---the system's position must have changed as well!
On the other hand, if the internal state changes, but the external position remains the same, the agent wouldn't see any difference---they wouldn't know to interact with the system any differently, so the directions available to them would have to stay the same, too.
\end{remark}

Here are some examples of dependent dynamical systems.
We begin by finishing the example at the end of the last section.

\index{dependent Moore machine|see{dynamical system}}
\index{deterministic state automaton!halting}

\begin{example}[Halting deterministic state automata]\label{ex.regular_lang_stop}
Recall deterministic state automata from \cref{def.dfa}.
Say we want such an automaton to halt after reaching an accept state and read no more symbols.
Then rather than a lens $S\yon^S\to \2\yon^A$, we could use a lens
\[
  \varphi\colon S\yon^S\to \yon^A+\1\iso\{\const{reject}\}\yon^A+\{\const{accept}\}.
\]
To give such a lens, we first need to provide a return function $\varphi_\1\colon S\to\{\const{reject},\,\const{accept}\}$.
We let $\varphi$ send the accept states to $\const{accept}$ and every other state to $\const{reject}$.

If we reach an accept state, we want the machine to halt.
So at the position $\const{accept}$, corresponding to the summand $\1$, there are no directions available.
This makes the update function $\varphi^\sharp_s$ vacuous when $\varphi_\1s=\const{accept}$.

On the other hand, when $\varphi_\1s=\const{reject}$, the update functions $\varphi^\sharp_s\colon A\to S$ specify how the machine updates its state for each direction in $A$ if the current state is $s$.
This corresponds to the automaton's update function.

When equipped with an initial state $s_0\in S$ specified by a lens $\yon\to S\yon^S$, we call these dependent dynamical systems \emph{halting deterministic state automata}.
Given a word $(a_1,\ldots,a_n)\in\lst(A)$, we say that the automaton \emph{accepts} this word if starting at the initial state and following the elements in the sequence leads us to an accept state, \emph{without reaching an accept state any earlier}---or, more formally, if the sequence $(s_0,s_1,\ldots,s_n)$ defined inductively by
\[
  s_{k+1}\coloneqq \varphi^\sharp_{s_k}a_{k+1}
\]
is such that $s_n$ is the sequence's first accept state:
\[
  \varphi_\1s_k=\begin{cases}
    \const{reject} & \text{if } k<n \\
    \const{accept} & \text{if } k=n
  \end{cases}
\]
We call the set of all words accepted by the automaton the language \emph{recognized} by the automaton.
\end{example}

\index{deterministic state automaton!language of}

\begin{remark}
Again, the conditions for when such an automaton accepts a word are rather awkward to formally state.
We will see in \cref{ex.halt_dsa_accept} an alternative way of saying whether a word is accepted by a halting deterministic state automaton.
\end{remark}

\begin{exercise}\label{exc.halt_dsa}
Consider the halting deterministic state automaton shown below:
\begin{equation} \label{eqn.halt_dsa}
\begin{tikzcd}[column sep=small]
	\bul[blue]\ar[rr, bend left, orange]\ar[loop left, dgreen]&&
	\bul[dyellow]\ar[dl, bend left, orange]\ar[ll, dgreen, bend left]\\&
	\bul[red]
\end{tikzcd}
\end{equation}
% \begin{equation} \label{eqn.halt_dsa}
% \begin{tikzpicture}
% \node[draw] {
% \begin{tikzcd}[column sep=small]
% 	\LMO{}\ar[rr, bend left, orange]\ar[loop left, dgreen]&&
% 	\LMO{}\ar[dl, bend left, orange]\ar[ll, dgreen, bend left]\\&
% 	\LMO{}
% \end{tikzcd}
% };
% \end{tikzpicture}
% \end{equation}
Let the left state $\bul[blue]$ be $1$, the right state $\bul[dyellow]$ be $2$, and the bottom state $\bul[red]$ be $3$.
We designate $\bul[blue]$, state $1$, as the initial state.
We can also call the orange arrows ``{\color{orange}orange}'' and the green arrows ``{\color{dgreen}green}.''
Answer the following questions, in keeping with the notation from \cref{ex.regular_lang_stop}.

\begin{enumerate}
	\item What is $S$?
	\item What is $A$?
	\item Based on the labeled transition diagram, which states are accept states, and which are not?
	\item Specify the corresponding lens $S\yon^S\to\yon^A+\1$.
	\item Name a word that is accepted by this automaton.
	\item Name a word that is not accepted by this automaton.
	Why not?
	Can you find another word that is not accepted by this automaton for a different reason?
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item The set of states is $S\coloneqq\{1,2,3\}=\3$ (or equivalently $S\coloneqq\{\bul[blue],\bul[dyellow],\bul[red]\}\iso\3$).
    \item The set of input symbols is $A\coloneqq\{{\color{orange}\text{orange}},{\color{dgreen}\text{green}}\}$.
    \item The automaton should halt at the accept states, so the accept states are exactly the states that have no arrows coming out of them---in this case, only state 3.
    States 1 and 2 are not accept states.
    \item Let the corresponding lens be $\varphi\colon S\yon^S\to\yon^A+\1$, or $\varphi\colon \3\yon^\3\to\yon^{\{{\color{orange}\text{orange}},\,{\color{dgreen}\text{green}}\}}+\1$.
    According to the previous part, $\varphi$ has a return function $\varphi_\1\colon S\to\2$ sending states 1 and 2, as non-accept states, to 1; and sending state 3, as an accept state, to 2.
    Then $\varphi^\sharp_3$ is vacuous, while the other two update functions are given by the the targets of the arrows in \eqref{eqn.halt_dsa} as follows:
    \begin{align*}
        \varphi^\sharp_1({\color{orange}\text{orange}})\coloneqq2&,\varphi^\sharp_1({\color{dgreen}\text{green}})\coloneqq1;\\
        \varphi^\sharp_2({\color{orange}\text{orange}})\coloneqq3&,\varphi^\sharp_2({\color{dgreen}\text{green}})\coloneqq1,
    \end{align*}
    while
    \item Some examples of words accepted by this automaton include the word $({\color{orange}\text{orange}},{\color{orange}\text{orange}}),$ the word $({\color{orange}\text{orange}},{\color{dgreen}\text{green}},{\color{orange}\text{orange}},{\color{orange}\text{orange}}),$ and the word $({\color{dgreen}\text{green}},{\color{orange}\text{orange}},{\color{dgreen}\text{green}},{\color{dgreen}\text{green}},{\color{dgreen}\text{green}},{\color{orange}\text{orange}},{\color{orange}\text{orange}})$.
    \item Some words are not accepted by the automaton because they lead you to a non-accept state (1 or 2); others are not accepted by the automaton because they lead you to an accept state (3) too early.
    Some examples of the former possibility include the words $({\color{dgreen}\text{green}},{\color{dgreen}\text{green}})$ and $({\color{orange}\text{orange}},{\color{dgreen}\text{green}},{\color{orange}\text{orange}},{\color{dgreen}\text{green}})$, while some examples of the latter possibility include the words $({\color{dgreen}\text{green}},{\color{orange}\text{orange}},{\color{orange}\text{orange}},{\color{dgreen}\text{green}})$ and $({\color{orange}\text{orange}},{\color{orange}\text{orange}},{\color{orange}\text{orange}},{\color{orange}\text{orange}},{\color{orange}\text{orange}},{\color{orange}\text{orange}},{\color{orange}\text{orange}})$.
\end{enumerate}
\end{solution}
\end{exercise}

For further examples, every graph gives rise to a dynamical system; but to ensure that we are discussing the same concept, let us fix the definition of a graph.

\begin{definition}[Graph] \label{def.graph}\index{graph}
A \emph{graph} $G \coloneqq (E \tto V)$ consists of
\begin{itemize}
  \item a set $E$ of \emph{edges};
  \item a set $V$ of \emph{vertices};
  \item a \emph{source function} $s\colon E\to V$ that assigns each edge a source vertex;
  \item a \emph{target function} $t\colon E\to V$ that assigns each edge a target vertex.
\end{itemize}
\end{definition}

So when we say ``graph,'' we mean a \emph{directed} graph, and we allow multiple edges between the same pair of vertices as well as self-loops.

\begin{example}[Graphs as dynamical systems] \label{ex.graph_dyn}\index{graph}
Given a graph $G\coloneqq(E\tto V)$ with source and target functions $s,t\colon E\to V$, there is an associated polynomial
\[
    g\coloneqq\sum_{v \in V} \yon^{s\inv(v)}.
\]
Its positions are the vertices of the graph, and its directions at $v\in V$ are the edges coming out of $v$.
We call this the \emph{emanation polynomial} of $G$.

\index{graph}

The graph itself induces a dynamical system $\varphi\colon V\yon^V\to g$, where $\varphi_\1 = \id_V$ and $\varphi^\sharp_v e = t(e)$.
So its states as well as its positions are the vertices of the graph, and a direction at a vertex $v\in V$ is an edge $e\in E$ coming out of $v$ that takes us from $v=s(e)$ along the edge $e$ to its target vertex $\varphi^\sharp_v e=t(e)$.
\end{example}

\index{graph!as dynamical system}

\begin{exercise}
Pick your favorite graph $G$, and consider the associated dynamical system as in \cref{ex.graph_dyn}.
Draw its labeled transition diagram as in \eqref{eqn.trans_diag} or \eqref{eqn.halt_dsa}.
\begin{solution}
No matter what graph you chose, \cref{ex.graph_dyn} tells us that if you were to draw the labeled transition diagram of its associated dynamical system, you would just end up with a picture of your graph!
The vertices of your graph are the states, and the edges of your graph are the possible transitions between them.
\end{solution}
\end{exercise}

% \begin{example}[Inputting an initial state]
% Suppose you have a closed system $f^\sharp\colon S\yon^S\to\yon$. The modeler can choose an initial state $\yon\to S\yon^S$, but what if we want some other system to choose the initial state? We haven't gotten to wiring diagrams yet, but the idea is to create a system that starts as not-closed---accepting as input a state $s\in S$---and then dives into its closed loop with that initial state.

% Let $S'\coloneqq S+\1$, so that the initial state $\yon\to S'\yon^{S'}$ now is canonical: it's the new $\1$. We also have a canonical inclusion $S\To{i}S'$. We will give a lens
% \[
% S'\yon^{S'}\to\yon+\yon^S
% \]
% that starts out with its outer box in the mode $\yon^S$ of accepting an $S$-input, and then moves to the mode $\yon$ so that it is a closed system forever after.

% To give a lens $S'\yon^{S'}\to\yon+\yon^S$, it is sufficient to give two morphisms: $S\yon^{S'}\to\yon$ and $\yon^{S'}\to\yon^S$. The first is equivalent to a function $S\to S'$ and we take the map $S\To{f^\sharp}S\To{i}S'$; this means that whenever we want to update the state from a state in $S$ we'll just do whatever our original closed system did. The second is also equivalent to a function $S\to S'$ and we use $i$; this means that whatever state is input at the beginning will be what we take as our first noncanonical state.
% \end{example}

\begin{example}\label{ex.generalized_file_reader}
In \cref{exc.file_reader}, we built a file-reader as a Moore machine, where a file is a function $f\colon\ord{n}\to\Set{ascii}$ from entries to characters.
Now we turn that file-reader into a dependent dynamical system $\varphi\colon S\yon^S\to p$ with only one direction while reading.

We let $S \coloneqq \{(s,t)\mid 1\leq s\leq t\leq n\}$, so that each state consists of a current entry $s$ and a terminal entry $t$.
Meanwhile, our interface $p$ will have two labeled copies of $\Set{ascii}$ as positions:
\[
    p(\1)\coloneqq\{\const{ready},\const{busy}\}\times\Set{ascii}.
\]
So each $p$-position is a pair $(m,c)$, where $c\in\Set{ascii}$ and $m$ is one of two modes: $\const{ready}$ or $\const{busy}$.
Then we define the direction-sets of $p$ for each $c\in\Set{ascii}$ as follows:
\[
    p[(\const{ready}, c)]\coloneqq S \qqand p[(\const{busy}, c)]\coloneqq\{\const{advance}\}\iso\1.
\]
That way, our file-reader can receive as its direction any pair of entries in $S$ when it is $\const{ready}$ but can only be told to $\const{advance}$ when it is $\const{busy}$.

We want our file-reader to be $\const{ready}$ if its current entry is the terminal entry; otherwise, it will be $\const{busy}$.
In either case, it will return the character at the current entry.
So we define the return function $\varphi_\1$ such that, for all $(s,t)\in S$,
\begin{align*}
  \varphi_\1(s, t) =
  \begin{cases}
    (\text{\const{ready}}, f(s)) &\mbox{if $s = t$}\\
    (\text{\const{busy}}, f(s)) &\mbox{otherwise}
  \end{cases}
\end{align*}

While the file-reader is $\const{ready}$, we want to set its new current and terminal entries to equal the given direction.
So for each $(s,s)\in S$, define the update function $\varphi^\sharp_{(s,s)}\colon S\to S$ to be the identity on $S$.

On the other hand, while the file-reader is $\const{busy}$, we want it to step forward through the file each time it receives an input.
So for each $(s,t)\in S$ for which $s<t$, we let the update function $\varphi^\sharp_{(s,t)}\colon\1\to S$ specify the element $(s+1,t)\in S$, thus shifting its current entry up by $1$.
\end{example}

\index{file reader|see{Moore machine, file reader}}
\index{Moore machine!file reader}
\index{file searcher|see{Moore machine, file searcher}}
\index{Moore machine!file searcher}

\begin{exercise} \label{exc.file_searcher}
Say instead of a file-reader, we wanted a file-searcher, which acts just like the file-reader from \cref{ex.generalized_file_reader} except that it only returns $c\in\Set{ascii}$ in its position when $c$ is a specific character; say $c=100$.
Otherwise, it returns the placeholder character $\_$.
Give the lens for this file-searcher by explicitly defining its return (on-positions) and update (on-directions) functions.
Hint: You should be able to use the same state system.
\begin{solution}
We give a file-searcher $\psi\colon S\yon^S\to q$ according to the specification as follows.
Its possible positions should form the set
\[
    q(\1)\coloneqq\{\const{ready}, \const{busy}\}\times\{100,\_\}.
\]
The direction-sets of $q$ can be defined in the same way we defined the direction-sets of $p$: for each $c\in\{100,\_\}$, we have
\[
    q[(\const{ready}, c)]\coloneqq S \qqand q[(\const{busy}, c)]\coloneqq\1.
\]
Then we set the return function $\psi_\1$ to behave like $\varphi_\1$, but with characters not equal to $100$ replaced with $\_$: so for all $(s,t)\in S$,
\begin{align*}
  \psi_\1(s,t) =
  \begin{cases}
    (\const{ready}, 100) &\mbox{if $s=t$ and $f(s)=100$}\\
    (\const{ready}, \_) &\mbox{if $s=t$ and $f(s)\neq100$}\\
    (\const{busy}, 100) &\mbox{if $s\neq t$ and $f(s)=100$}\\
    (\const{busy}, \_) &\mbox{otherwise}\\
  \end{cases}
\end{align*}
Then the update functions of $\psi$ behave just like those of $\varphi$.
For each $(s,t)\in S$ for which $s=t$, we define the update function $\psi^\sharp_{(s,t)}\colon S\to S$ to be the identity on $S$.
On the other hand, for each $(s,t)\in S$ for which $s\neq t$, we let the update function $\psi^\sharp_{(s,t)}\colon \1\to S$ specify the element $(s+1, t)\in S$, thus shifting its current entry up by $1$.
\end{solution}
\end{exercise}

In the previous exercise, we manually constructed a file-searcher that acted very much like a file-reader.
In \cref{exc.file_searcher_wrap}, we will see a simpler way to construct a file-searcher by leveraging the file-reader we have already defined.
Moreover, this construction will highlight precisely how our file-searcher is related to our file-reader.
This will be possible using \emph{wrapper interfaces}, which we will introduce in \cref{subsec.poly.dyn_sys.new.wrap}.

\index{interface!wrapper}
\index{robot|(}

\begin{example}\label{ex.grid_robot}
Choose $n\in\nn$, a \emph{grid size}, and for each $i\in\ord{n}$, let $D_i$ be the set
\[
	D_i\coloneqq
	\begin{cases}
		\{0,+1\}&\tn{ if }i=1\\
		\{-1,0,+1\}&\tn{ if } 1<i<n\\
		\{-1,0\}&\tn{ if }i=n
	\end{cases}
\]
We can think of $D_i$ as the set of ways a robot could move from location $i$.
If $1<i<n$, a robot may shift its location by $-1$ (move left/down), $0$ (remain still), or $+1$ (move right/up).
But a robot already at $i=1$ cannot shift its location by $-1$; likewise, a robot already at $i=n$ cannot shift its location by $+1$.

Then we can model a robot told to move in an $\ord{n}\times\ord{n}$ grid as a dependent dynamical system
\[
    \varphi\colon (\ord{n}\times\ord{n})\yon^{\ord{n}\times\ord{n}}\to\sum_{(i,j)\in\ord{n}\times\ord{n}}\yon^{D_i\times D_j}.
\]
The robot's state is a location $(i,j)\in\ord{n}\times\ord{n}$ in the grid.
We let $\varphi_\1\coloneqq\id_{\ord{n}\times\ord{n}}$ so that the dynamical system returns its state as its position: the robot expresses its position by moving to that location in the grid.

For each $(i,j)\in\ord{n}\times\ord{n}$, we let $\varphi^\sharp_{(i,j)}$ send each pair $(d,e)\in D_i\times D_j$ to the grid location $(i+d,j+e)\in(n,n)$.
Concretely, this says that if a robot located at $(i,j)$ receives the pair $(d,e)$ as its direction, its new position will be $(i+d,j+e)$.
As polyboxes, the dynamical system is given by
  \[
\begin{tikzpicture}[polybox, mapstos]
  \node[poly, dom] (s) {$(i+d,j+e)$\at$(i,j)$};
  \node[left=0pt of s_pos] {$\ord{n}\times\ord{n}$};
  \node[left=0pt of s_dir] {$\ord{n}\times\ord{n}$};

  \node[poly, cod, right=of s] (p) {$(d,e)\vphantom{j}$\at$(i,j)$};
  \node[right=0pt of p_pos] {$\ord{n}\times\ord{n}$};
  \node[right=0pt of p_dir] {$D_i\times D_j$};

  \draw (s_pos) to[first] (p_pos);
  \draw (p_dir) to[last] (s_dir);
\end{tikzpicture}
\]

Our definition of $D_i$ for each $i\in\ord{n}$ guarantees that this position is still inside the grid; with this setup, the robot has fewer ways to move on the sides or corners of the grid than anywhere else:
\[
\begin{tikzpicture}[scale=.5]
  \draw[step=1cm,gray,very thin] (-3,-3) grid (4,4);
	\draw[->, red ] (-2.5,3.5) -- (-1.5, 3.5);
	\draw[->, red ] (-2.5,3.5) -- (-1.5, 2.5);
	\draw[->, red ] (-2.5,3.5) -- (-2.5, 2.5);
	\draw[->, blue] (1.5, 0.5) -- (0.5, 0.5);
	\draw[->, blue] (1.5, 0.5) -- (2.5, 0.5);
	\draw[->, blue] (1.5, 0.5) -- (1.5, 1.5);
	\draw[->, blue] (1.5, 0.5) -- (1.5,-0.5);
	\draw[->, blue] (1.5, 0.5) -- (0.5, 1.5);
	\draw[->, blue] (1.5, 0.5) -- (0.5,-0.5);
	\draw[->, blue] (1.5, 0.5) -- (2.5, 1.5);
	\draw[->, blue] (1.5, 0.5) -- (2.5,-0.5);
\end{tikzpicture}
\]
In this picture, $\ord{n}\coloneqq\7$, and the $4$ directions at position $(1,7)$ and $9$ directions at position $(5,4)$ are shown (recall that remaining still is an option in either case).

Note that in this example, the positions are literally the positions in the grid where the robot could be, and the directions at each position are literally the directions in which the robot can move!
\end{example}

\begin{exercise} \label{exc.grid_reward}
Modify the dynamical system from \cref{ex.grid_robot} as follows.
\begin{enumerate}
	\item Replace the interface with a new polynomial $p$ so that at each grid location, the robot can receive not only the direction it should move in but also a ``reward value'' $r\in\rr$.
	\item Replace the state-set with a new set $S$ so that an element $s\in S$ may include both the robot's position and a list of all reward values so far.
	\item With your new $p$ and $S$, define a new dynamical system $\varphi'\colon S\yon^S\to p$ that preserves the behavior of the dynamical system from \cref{ex.grid_robot} while updating the robot's reward list without returning it externally.
\qedhere
\end{enumerate}
\begin{solution}
We modify the dynamical system from \cref{ex.grid_robot}.
\begin{enumerate}
    \item Previously, the direction-set at position $(i,j)\in\ord{n}\times\ord{n}$ of our interface was $D_i\times D_j$.
    But now we also want to give the robot a ``reward value'' $r\in\rr$.
    So our new direction-set should be $D_i\times D_j\times\rr$:
    \[
        p\coloneqq\sum_{(i,j)\in\ord{n}\times\ord{n}} \yon^{D_i\times D_j\times\rr}.
    \]
    \item Previously, a state was just a location in the grid: an element of $\ord{n}\times\ord{n}$.
    But now we want to be able to record a list of reward values as well.
    Since each reward value is a real number, we define the state-set to be $S\coloneqq\ord{n}\times\ord{n}\times\lst(\rr)$.\index{list}
    \item The former return function $\varphi_\1$ was the identity on $\ord{n}\times\ord{n}$.
    The new return function $\varphi'_\1$ should still just return the robot's current grid position; but since it is now a function from $S=\ord{n}\times\ord{n}\times\lst(\rr)$, it should instead be the canonical projection $\varphi'_\1\colon \ord{n}\times\ord{n}\times\lst(\rr)\to\ord{n}\times\ord{n}$.

    For each former state $(i,j)\in\ord{n}\times\ord{n}$, the former update function $\varphi^\sharp_{(i,j)}\colon D_i\times D_j\to\ord{n}\times\ord{n}$ sent $(d,e)\mapsto(i+d,j+e)$.
    With an extra component $(r_1,\ldots,r_k)\in\lst(\rr)$ of the state, the new update function $(\varphi')^\sharp_{(i,j,(r_1,\ldots,r_k))}\colon D_i\times D_j\times\rr\to\ord{n}\times\ord{n}\times\lst(\rr)$ sends $(d,e,r)\mapsto(i+d,j+e,(r_1,\ldots,r_k,r))$, also updating the list of rewards.
    As polyboxes, the new dynamical system is given by
    \[
    \begin{tikzpicture}[polybox, mapstos]
      \node[poly, dom] (s) {$(i+d,j+e,(r_1,\ldots,r_k,r))$\at$(i,j,(r_1,\ldots,r_k))$};
      \node[left=0pt of s_pos] {$\ord{n}\times\ord{n}\times\lst(\rr)$};
      \node[left=0pt of s_dir] {$\ord{n}\times\ord{n}\times\lst(\rr)$};

      \node[poly, cod, right=of s] (p) {$(d,e,r)\vphantom{j}$\at$(i,j)$};
      \node[right=0pt of p_pos] {$\ord{n}\times\ord{n}$};
      \node[right=0pt of p_dir] {$D_i\times D_j\times\rr$};

      \draw (s_pos) to[first] (p_pos);
      \draw (p_dir) to[last] (s_dir);
    \end{tikzpicture}
    \]
\end{enumerate}
\end{solution}
\end{exercise}

In the previous exercise, we added a reward system to the robot on the grid by manually redefining the associated lens.
But there is a simpler way to think about the new system: it is the juxtaposition of two systems, a robot system and a reward system, in parallel.
We will see how to express this in terms of lenses in \cref{exc.grid_reward_par}, once we explain how to juxtapose systems like this in general in \cref{subsec.poly.dyn_sys.new.par}.
In fact, we will see in \cref{exc.grid_robot_par} that the robot-on-a-grid system itself can be viewed as the juxtaposition of two systems, and this perspective will provide a structured way to generalize \cref{ex.grid_robot} to more than two dimensions.

\index{robot|)}
\index{dynamical system|)}


%-------- Section --------%
\section{Constructing new dynamical systems from old}\label{sec.poly.dyn_sys.new}

\index{dynamical system!constructing new from old|(}

We have seen how dependent dynamical systems can be modeled as lenses in $\poly$ of the form $S\yon^S\to p$.
But we have yet to take full advantage of the categorical structure that $\poly$ provides.
In particular, based only on what we know of $\poly$ so far from \cref{ch.poly.cat}, we have three rather different ways of obtaining new dynamical systems from old ones:
\begin{enumerate}
    \item Given dynamical systems $S\yon^S\to p$ and $S\yon^S\to q$, we can use the universal property of the \emph{categorical product} to obtain a dynamical system $S\yon^S\to p\times q$; see \cref{subsec.poly.dyn_sys.new.prod}.
    \item Given dynamical systems $\varphi\colon S\yon^S\to p$ and $\psi\colon T\yon^T\to q$, we can take their parallel product to obtain a dynamical system $\varphi\otimes\psi\colon ST\yon^{ST}\to p\otimes q$; see \cref{subsec.poly.dyn_sys.new.par}.
    \item Given a dynamical system $\varphi\colon S\yon^S\to p$ and a lens $f\colon p\to q$, we can compose them to obtain a dynamical system $\varphi\then f\colon S\yon^S\to q$; see \cref{subsec.poly.dyn_sys.new.wrap}.
\end{enumerate}
Each of these operations has a concrete interpretation in terms of the systems' behavior.
In this section, we will review each of them in turn.

%---- Subsection ----%
\subsection{Categorical products: multiple interfaces operating on the same states}\label{subsec.poly.dyn_sys.new.prod}

\index{interface!multiple}

\index{dynamical system!constructing from product}

Let $I$ be a set, and say that we have an $I$-indexed family of dependent dynamical systems $(\varphi_i\colon S\yon^S\to p_i)_{i\in I}$ that all share the same state-set $S$.
Then since $\poly$ has all small products, the universal property of products induces a lens \[\varphi\colon S\yon^S\to\prod_{i\in I}p_i,\] which is itself a dynamical system with state-set $S$.
By \eqref{eqn.poly_prod}, the interface of $\varphi$ (i.e.\ the product that is its codomain) has position-set
\[
  \left(\prod_{i\in I}p_i\right)\!(\1)\iso\prod_{i\in I}\left(p_i(\1)\right)
\]
and, at each position $\bar{j}\colon(i\in I)\to p_i(\1)$, direction-set
\[
  \sum_{i\in I}p_i[\bar{j}i].
\]
We then characterize the dynamics of $\varphi$ in terms of each $\varphi_i$ by leveraging the universal property of products in $\poly$, detailed in the binary case in the solution to \cref{exc.poly_prod}, as follows.\index{dynamics}
The return function
\[
  \varphi_\1\colon S\to\prod_{i\in I}\left(p_i(\1)\right)
\]
sends each state $s\in S$ to the dependent function $(i\in I)\to p_i(\1)$ sending $i\in I$ to the position $(\varphi_i)_\1s$ returned by the corresponding dynamical system $\varphi_i$ at the state $s$.
Then the update function
\[
  \varphi^\sharp_s\colon\sum_{i\in I}p_i[(\varphi_i)_\1s]\to S
\]
at $s\in S$ sends each pair $(i,d)$ in its domain, with $i\in I$ and $d\in p_i[(\varphi_i)_\1s]$, to where the update function of $\varphi_i$ at $s$ sends the direction $d$: namely $(\varphi_i)^\sharp_s d$.
We can write $\varphi$ using polyboxes as follows:
\[
\begin{tikzpicture}[polybox, mapstos]
  \node[poly, dom] (S) {$(\varphi_i)^\sharp_sd$\at$s\vphantom{(\varphi_i)_\1s}$};
  \node[left=0pt of S_pos] {$S$};
  \node[left=0pt of S_dir] {$S$};

  \node[poly, cod, right=of S, "$\prod_{i\in I}p_i$" right] (p) {$(i,d)\vphantom{(\varphi_i)^\sharp_sd}$\at$i\mapsto(\varphi_i)_\1s$};

  \draw (S_pos) to[first] (p_pos);
  \draw (p_dir) to[last] (S_dir);
\end{tikzpicture}
\]

In other words, if there are multiple interfaces that drive the same set of states, we may view them as a single product interface that drives those states.
This single dynamical system returns positions in all of the original systems at once; then it can receive a direction from any one of the original systems' direction-sets and update its state accordingly.
It is as though all of the dynamical systems can detect the current state of the combined system, but only one of them can change the state at a time.
So products give us a universal way to combine multiple polynomial interfaces into one.

\begin{exercise}
Given a set $I$, suppose we have an $(A_i,B_i)$-Moore machine with state-set $S$ for each $i\in I$.
Show that there is an induced $\left(\sum_{i\in I}A_i, \prod_{i\in I}B_i\right)$-Moore machine, again with state-set $S$.
\begin{solution}
We are given a lens $S\yon^S\to B_i\yon^{A_i}$ for each $i\in I$.
The universal property of products in $\poly$ then induces a lens
\[
    S\yon^S\to\prod_{i\in I}B_i\yon^{A_i}.
\]
By \eqref{eqn.poly_prod}, its codomain is the product of monomials
\[
  \prod_{i\in I}B_i\yon^{A_i}\iso\left(\prod_{i\in I}B_i\right)\yon^{\sum_{i\in I}A_i},
\]
which, in particular, is still a monomial.
Hence the induced lens is a $\left(\sum_{i\in I}A_i,\prod_{i\in I}B_i\right)$-Moore machine with state-set $S$.
\end{solution}
\end{exercise}\index{Moore machine}

\begin{example} \label{ex.prod_diagrams}
Consider two four-state dependent dynamical systems $\varphi\colon\4\yon^\4\to\rr\yon^{\{r,b\}}$ and $\psi\colon\4\yon^\4\to \zz_{\geq0}\yon^{\{g,p\}}+\zz_{<0}\yon^{\{g\}}$, drawn below as labeled transition diagrams (we think of $r,b,g,$ and $p$ as red, blue, green, and purple, respectively):
\[
\begin{tikzpicture}
	\node[draw] (1) {
  \begin{tikzcd}[row sep=15pt]
  	\LMO{\pi}\ar[r, bend left=15pt, red]\ar[loop left=15pt, blue]&
  	\LMO{0}\ar[l, bend left=15pt, red]\ar[d, bend left=15pt, blue]\\
  	\LMO[under]{-1.41}\ar[u,bend left=15pt, red]\ar[r, bend right=15pt, blue]&
  	\LMO[under]{2.72}\ar[l, bend right=15pt, red]\ar[loop right=15pt, blue]
  \end{tikzcd}
	};
	\node[draw, right=of 1] {
  \begin{tikzcd}[row sep=15pt]
  	\LMO{-2}\ar[d, green!50!black]&
  	\LMO{4}\ar[l, green!50!black]\ar[d, blue!50!purple]\\
  	\LMO[under]{-8}\ar[loop left, green!50!black]&
  	\LMO[under]{16}\ar[ul, green!50!black]\ar[l, blue!50!purple]
  \end{tikzcd}
  };
 \end{tikzpicture}
\]

The universal property of products provides a unique way to put these systems together to obtain a dynamical system $\4\yon^\4\to\rr\zz_{\geq0}\yon^{\{r,b,g,p\}}+\rr\zz_{<0}\yon^{\{r,b,g\}}$ that looks like this:
\[
\begin{tikzpicture}
	\node[draw] (1) {
  \begin{tikzcd}
  	\LMO{(\pi,-2)}\ar[r, bend left=15pt, red]\ar[loop left, blue]\ar[d, bend left=15pt, green!50!black]&
  	\LMO{(0,4)}\ar[l, bend left=15pt, red]\ar[d, bend left=15pt, blue]\ar[d, bend right=15pt, blue!50!purple]\ar[l, green!50!black]\\
  	\LMO[under]{(-1.41,-8)}\ar[u,bend left=15pt, red]\ar[r, bend right=15pt, blue]\ar[loop left, green!50!black]&
  	\LMO[under]{(2.72,16)}\ar[l, bend right=15pt, red]\ar[l, blue!50!purple]\ar[loop right=15pt, blue]\ar[ul, green!50!black]
  \end{tikzcd}
  };
\end{tikzpicture}
\]
Each state now returns two positions: one according to the return function of $\varphi$, and another according to the return function of $\psi$.
As for the possible directions, we can now choose either a direction of $\varphi$ (either $r$ or $b$), in which case the dynamical system will update its state according to the update map of $\varphi$; or a direction of $\psi$ (either $g$ or sometimes $p$), in which case the dynamical system will update its state according to the update map of $\psi$.
\end{example}

\index{event-based systems}

\begin{exercise}[Toward event-based systems]
Let $\varphi\colon S\yon^S\to p$ be a dynamical system.
We can think of it as requiring a direction at each time step to update its state.

Suppose we want to change $\varphi$ into an \emph{event-based system}: one that does not always receive a direction and changes state only when it does.
That is, we want every position of $p$ to have an extra direction that, when selected, never changes the state.
We want a new system $\varphi'\colon S\yon^S\to p'$ that has this behavior; what should $p'$ and $\varphi'$ be?
\begin{solution}
Given a dynamical system $\varphi\colon S\yon^S\to p$, we seek a new dynamical system $\varphi'\colon S\yon^S\to p'$ with an additional direction that does not change the state.
We can think of this as having two different interfaces acting on the same system: the original interface $p$ of $\varphi$ and a new interface with only one possible direction that does not change the state.
This latter interface also need not distinguish between its positions; it should have a single position that provides no additional information.
So the second interface we want acting on $S\yon^S$ is $\yon$.

If $\yon$ were the only interface acting on the state system, we would have a Moore machine $\epsilon\colon S\yon^S\to\yon$ whose return function is the unique function $S\to\1$ and whose update function should be the identity function on $S$, since the direction never changes the state.
Then $p'$ is the product of the two interfaces $p$ and $\yon$, while $\varphi'\colon S\yon^S\to p'$ is the unique lens induced by $\varphi\colon S\yon^S\to p$ and $\epsilon\colon S\yon^S\to\yon$.
In particular, $p'\iso p\yon\iso\sum_{i\in p(\1)}\yon^{p[i]+\1}$, and if we let $\ast$ denote the additional direction not in $p$ at each position of $p\yon$, we can write $\varphi'$ in polyboxes as
\[
\begin{tikzpicture}[polybox, mapstos]
  \node[poly, dom] (S) {$\varphi^\sharp_sa$\at$s\vphantom{\varphi_\1s}$};
  \node[left=0pt of S_pos] {$S$};
  \node[left=0pt of S_dir] {$S$};

  \node[poly, cod, right=of S, "$p\yon$" right] (p) {$a\vphantom{\varphi^\sharp_sa}$\at$\varphi_\1s$};

  \draw (S_pos) to[first] (p_pos);
  \draw (p_dir) to[last] (S_dir);
\end{tikzpicture}
\]
when the chosen direction is from the original $p$ (i.e.\ $a\in p[\varphi_\1s]$ above), coinciding with the behavior of the original system $\varphi$; or as
\[
\begin{tikzpicture}[polybox, mapstos]
  \node[poly, dom] (S) {$s$\at$s\vphantom{\varphi_\1s}$};
  \node[left=0pt of S_pos] {$S$};
  \node[left=0pt of S_dir] {$S$};

  \node[poly, cod, right=of S, "$p\yon$" right] (p) {$\ast$\at$\varphi_\1s$};

  \draw (S_pos) to[first] (p_pos);
  \draw (p_dir) to[last] (S_dir);
\end{tikzpicture}
\]
when the chosen direction is the additional direction $\ast$ that does not change the state.
It turns out this construction is universal; it is known as \emph{copointing}.
\end{solution}
\end{exercise}

\index{polynomial functor!copointed}

%---- Subsection ----%
\subsection{Parallel products: juxtaposing dynamical systems}\label{subsec.poly.dyn_sys.new.par}

\index{dynamical system!juxtaposing}
\index{parallel product!and dynamical systems|(}

Another way to combine two polynomials---and indeed two lenses---is by taking their parallel product, as in \cref{def.parallel}.
In particular, the parallel product of two state systems is still a state system.
So parallel products give us another way to create new dynamical systems from old ones.
The procedure is straightforward: take the product of the state-sets as the new state-set, the product of the position-sets as the new position-set, and the product of the direction-sets at each of position in a tuple of positions as the new direction-set at that tuple.

For $n\in\nn$, say that we have $n$ dynamical systems: a lens $\varphi_i\colon S_i\yon^{S_i}\to p_i$ for each $i\in\ord{n}$.
Then we can take the parallel product of all of them to obtain a lens
\[
  \varphi\colon\bigotimes_{i\in\ord{n}}\left(S_i\yon^{S_i}\right)\to\bigotimes_{i\in\ord{n}}p_i.
\]
By inductively applying \cref{exc.general_poly_parallel_times}, we find that the domain of $\varphi$ is
\[
  \bigotimes_{i\in\ord{n}}\left(S_i\yon^{S_i}\right)\iso\left(\prod_{i\in\ord{n}}S_i\right)\yon^{\prod_{i\in\ord{n}}S_i},
\]
so $\varphi$ is itself a dependent dynamical system with state-set $\prod_{i\in\ord{n}}S_i$.
Meanwhile, inductively applying \eqref{eqn.parallel_def} yields
\[
  \left(\bigotimes_{i\in\ord{n}}p_i\right)\!(\1)\iso\prod_{i\in\ord{n}}\left(p_i(\1)\right)
\]
as the position-set and, at each position $(j_i)_{i\in\ord{n}}$,
\[
  \prod_{i\in\ord{n}}p_i[j_i]
\]
as the direction-set of the interface of $\varphi$.

We can characterize the dynamics of $\varphi$ in terms of each constituent dynamical system $\varphi_i$ as follows.
By our proof sketch of \cref{prop.parallel_monoidal}, the return function
\[
  \varphi_\1\colon \prod_{i\in\ord{n}}S_i\to\prod_{i\in\ord{n}}\left(p_i(\1)\right)
\]
sends each $n$-tuple of states $(s_i)_{i\in\ord{n}}$ in its domain, with each $s_i\in S_i$, to the $n$-tuple of positions $((\varphi_i)_\1s_i)_{i\in\ord{n}}$ returned by each of the constituent dynamical systems at each state.
Then at the $n$-tuple of states $(s_i)_{i\in\ord{n}}\in\prod_{i\in\ord{n}}S_i$, the update function
\[
  \varphi^\sharp_{(s_i)_{i\in\ord{n}}}\colon\prod_{i\in\ord{n}}p_i[(\varphi_i)_\1s_i]\to\prod_{i\in\ord{n}}S_i
\]
sends each $n$-tuple of directions $(d_i)_{i\in\ord{n}}$ in its domain, with each $d_i\in p_i[(\varphi_i)_\1s_i]$, to the $n$-tuple $((\varphi_i)^\sharp_{s_i}d_i)_{i\in\ord{n}}$ consisting of states to which the update function of each $\varphi_i$ at $s_i$ sends $d_i$.
We can write $\varphi$ using polyboxes as follows:
\[
\begin{tikzpicture}[polybox, mapstos]
  \node[poly, dom] (S) {$((\varphi_i)^\sharp_{s_i}d_i)_{i\in\ord{n}}$\at$(s_i)_{i\in\ord{n}}\vphantom{((\varphi_i)_\1s_i)_{i\in\ord{n}}}$};
  \node[left=0pt of S_pos] {$\prod_{i\in\ord{n}}S_i$};
  \node[left=0pt of S_dir] {$\prod_{i\in\ord{n}}S_i$};

  \node[poly, cod, right=of S, "$\bigotimes_{i\in\ord{n}}p_i$" right] (p) {$(d_i)_{i\in\ord{n}}\vphantom{((\varphi_i)^\sharp_{s_i}d_i)_{i\in\ord{n}}}$\at$((\varphi_i)_\1s_i)_{i\in\ord{n}}$};

  \draw (S_pos) to[first] (p_pos);
  \draw (p_dir) to[last] (S_dir);
\end{tikzpicture}
\]

In other words, multiple dynamical systems running in parallel can be thought of as a single dynamical system.
This system stores the states of all the constituent systems at once and returns positions from all of them together; then it can receive directions from all of the constituent systems' direction-sets at those positions at once and update each constituent state accordingly.
So parallel products give us a way to juxtapose multiple dynamical systems in parallel to form a single system.

\begin{exercise}
Given $n\in\nn$, suppose we have an $(A_i,B_i)$-Moore machine with state-set $S_i$ for every $i \in \ord{n}$.
Show that there is an induced $\left(\prod_{i \in \ord{n}} A_i, \prod_{i \in \ord{n}} B_i\right)$-Moore machine with state-set $\prod_{i \in \ord{n}} S_i$.
\begin{solution}
We are given a lens $S_i\yon^{S_i}\to B_i\yon^{A_i}$ for each $i\in\ord{n}$.
By inductively applying \cref{exc.general_poly_parallel_times}, we find that their parallel product in $\poly$ is a lens
\[
  \left(\prod_{i\in\ord{n}}S_i\right)\yon^{\prod_{i\in\ord{n}}S_i}\iso\bigotimes_{i\in\ord{n}}S_i\yon^{S_i}\to\bigotimes_{i\in\ord{n}}B_i\yon^{A_i}\iso\left(\prod_{i\in\ord{n}}B_i\right)\yon^{\prod_{i\in\ord{n}}A_i},
\]
which is a $\left(\prod_{i \in \ord{n}} A_i, \prod_{i \in \ord{n}} B_i\right)$-Moore machine with state-set $\prod_{i\in\ord{n}}S_i$.
This works because the parallel product of monomials is still a monomial.
\end{solution}
\end{exercise}\index{Moore machine}

\begin{example} \label{ex.par_diagrams}
Consider dependent dynamical systems $\varphi\colon\2\yon^\2\to \rr_{<0}\yon^{\{b,r\}}+\rr_{\geq0}\yon^{\{b\}}$ and $\psi\colon\3\yon^\3\to\zz_{<0}\yon^{\{r\}}+\{0\}\yon^{\{r,y\}}+\zz_{>0}\yon^{\{y\}}$, drawn below as labeled transition diagrams (we think of $b,r,$ and $y$ as blue, red, and yellow, respectively):
\[
\begin{tikzpicture}
	\node[draw] (1) {
  \begin{tikzcd}[row sep=15pt]
  	\LMO{\sqrt{7}}\ar[d, bend left=15pt, blue]\\
  	\LMO[under]{-e}\ar[u, bend left=15pt, red]\ar[loop right=15pt, blue]
  \end{tikzcd}
	};
	\node[draw, right=of 1] {
  \begin{tikzcd}[row sep=15pt]
  	\LMO{-5}\ar[r, bend left=15pt, red]&
  	\LMO{0}\ar[l, bend left=15pt, dyellow]\ar[r, red]&
  	\LMO{8}\ar[loop right=15pt, dyellow]
  \end{tikzcd}
  };
 \end{tikzpicture}
\]
Taking their parallel product, we obtain a dynamical system with state system $\6\yon^\6$ and interface
\begin{align*}
    &\rr_{<0}\zz_{<0}\yon^{\{(b,r),(r,r)\}}+\rr_{<0}\{0\}\yon^{\{(b,r),(b,y),(r,r),(r,y)\}}+\rr_{<0}\zz_{>0}\yon^{\{(b,y),(r,y)\}}\\
    +\:&\rr_{\geq0}\zz_{<0}\yon^{\{(b,r)\}}+\rr_{\geq0}\{0\}\yon^{\{(b,r),(b,y)\}}+\rr_{\geq0}\zz_{>0}\yon^{\{(b,y)\}}
\end{align*}
that looks like this (we use purple to indicate $(b,r)$, red to indicate $(r,r)$, green to indicate $(b,y)$, and orange to indicate $(r,y)$):
\[
\begin{tikzpicture}
	\node[draw] (1) {
  \begin{tikzcd}
    \LMO{(\sqrt{7},-5)}\ar[dr, bend left=15pt, blue!50!purple] &
    \LMO{(\sqrt{7},0)}\ar[dl, green!50!black]\ar[dr, blue!50!purple] &
    \LMO{(\sqrt{7},8)}\ar[d, bend left=15pt, green!50!black] \\
    \LMO[under]{(-e,-5)}\ar[ur, bend left=15pt, red]\ar[r, blue!50!purple] &
    \LMO[under]{(-e,0)}\ar[ul, orange!75!black]\ar[ur, red]\ar[l, bend left=15pt, green!50!black]\ar[r, blue!50!purple] &
    \LMO[under]{(-e,8)}\ar[u, bend left=15pt, orange!75!black]\ar[loop right=15pt, green!50!black]
  \end{tikzcd}
  };
\end{tikzpicture}
\]
Each state---actually a pair of states from the constituent state-sets---returns two positions, one according to the return function of $\varphi$ and another according to the return function of $\psi$.
Then every direction must be a pair of directions from the constituent interfaces at those positions, with the update function updating each state in the pair according to each direction in the pair via the constituent update functions of $\varphi$ and $\psi$.
\end{example}

\begin{exercise} \label{exc.grid_reward_par}
Explain how the dynamical system $\varphi'\colon S'\yon^{S'}\to p'$ you built in \cref{exc.grid_reward} can be expressed as the parallel product of the robot-on-a-grid dynamical system $\varphi\colon S\yon^S\to p$ from \cref{ex.grid_robot} with another dynamical system, $\psi\colon T\yon^T\to q$.
Be sure to specify $T, q,$ and $\psi$.
\begin{solution}
We will show that taking the parallel product of the robot-on-a-grid dynamical system $\varphi\colon S\yon^S\to p$ from \cref{ex.grid_robot} and a reward-tracking dynamical system $\psi\colon T\yon^T\to q$ we will define shortly yields the dynamical system $\varphi'\colon S'\yon^{S'}\to p'$ from \cref{exc.grid_reward}.

The reward-tracking dynamical system should have states in $\lst(\rr)$ to record a list of reward values, unchanging position, and directions in $\rr$ to give new reward values.
So it is the lens $\lst(\rr)\yon^{\lst(\rr)}\to\yon^\rr$ that has a uniquely defined return function, while its update map sends each state $(r_1,\ldots,r_k)\in\lst(\rr)$ and each direction $r\in\rr$ to the new state $(r_1,\ldots,r_k,r)$.

Then the dynamical system from \cref{exc.grid_reward} is the parallel product of the robot-on-a-grid dynamical system from \cref{ex.grid_robot} with the reward-tracking dynamical system $\lst(\rr)\yon^{\lst(\rr)}\to\yon^\rr$, as can be seen in the solution to \cref{exc.grid_reward}.
\end{solution}
\end{exercise}

\begin{exercise} \label{exc.grid_robot_par}
\begin{enumerate}
    \item Explain how the robot-on-a-grid dynamical system $\varphi\colon S\yon^S\to p$ from \cref{ex.grid_robot} can be written as the parallel product of some dynamical system with itself.
    \item Use $k$-fold parallel products to generalize \cref{ex.grid_robot} to robots on $k$-dimensional grids.\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item The robot-on-a-grid dynamical system from \cref{ex.grid_robot} can be written as the parallel product of two robot-on-a-line dynamical systems of the form $\lambda\colon\ord{n}\yon^{\ord{n}}\to\sum_{i\in\ord{n}}\yon^{D_i}$, where $\lambda_\1\coloneqq\id_{\ord{n}}$ and $\lambda^\sharp_i$ for each $i\in\ord{n}$ sends each direction $d\in D_i$ to the position on the line given by $i+d$.
    This yields a robot that can move along a single axis, and the parallel product of this robot with itself yields a robot that can move along two different axes at once, which is precisely our robot-on-a-grid dynamical system.
    \item To create a dynamical system consisting of a robot moving in a $k$-dimensional grid of size $n$ along every dimension, we just take the $k$-fold parallel product of the dynamical system $\lambda\colon\ord{n}\yon^{\ord{n}}\to\sum_{i\in\ord{n}}\yon^{D_i}$ we just defined to obtain a dynamical system \[\lambda^{\otimes k}\colon\ord{n}^\ord{k}\yon^{\ord{n}^\ord{k}}\to\sum_{(i_1,\ldots,i_k)\in\ord{n}^\ord{k}}\yon^{\prod_{j\in\ord{k}}D_{i_j}}.\]
    In fact, we could have used a different $n_j$ for each $j\in\ord{k}$ instead of $n$ to obtain a robot moving in an arbitrary $k$-dimensional grid of size $n_1\times\cdots\times n_k$ as a $k$-fold parallel product.
\end{enumerate}
\end{solution}
\end{exercise}
\index{robot}

Intuitively, the parallel product takes two dynamical systems and puts them in the same room together so that they can be run at the same time.
But it does not allow for any interaction \emph{between} the two systems.
For that, we will need to use what we call a wrapper interface.
We will introduce wrapper interfaces in the next section before describing how they can be used in conjunction with parallel products to model general interaction in \cref{sec.poly.dyn_sys.interact}.

\index{parallel product!and dynamical systems|)}

%---- Subsection ----%
\subsection{Composing lenses: wrapper interfaces}\label{subsec.poly.dyn_sys.new.wrap}

\index{dynamical system!wrapper interface|see{interface, wrapper}}\index{interface!wrapper|(}

Given a dynamical system $\varphi\colon S\yon^S\to p$, say that we want to interact with its state system using a new interface $q$ rather than $p$.
We can do this whenever we have a lens $f\colon p\to q$, which we could compose with our original dynamical system to obtain a new system $S\yon^S\To{\varphi}p\To{f}q$.
We call the lens $f$ the \emph{wrapper} and its codomain $q$ the \emph{wrapper interface}, which we \emph{wrap} around $\varphi$ (or sometimes just $p$, if a dynamical system $\varphi$ has yet to be specified) using $f$.

How does this new composite system $\varphi\then f$ relate to the original dynamical system $\varphi$?
The lens $f$ converts a position $i$ from $p$ to a position $f_\1i$ from $q$; at the same time, it allows the choice of direction from $p[i]$ to depend on a choice of direction from $q[f_\1i]$, converting directions of the wrapper interface $q$ to directions of the original interface $p$.
Precomposing $f$ with a dynamical system yields a new dynamical system that lets an agent interact with the original system using only this new interface wrapped around it.

\begin{example} \label{ex.wrap_diagrams}
Consider a dependent dynamical system $\varphi\colon\6\yon^\6\to p$ with
\[
    p\coloneqq\{1\}\yon^{\{b,y,r\}}+\{2\}\yon^{\{b,r\}}+\{3\}\yon^{\{b\}}+\{4\}\yon^{\{r\}},
\]
drawn below as a labeled transition diagram (we think of $b,y,$ and $r$ as blue, yellow, and red, respectively):
\[
\begin{tikzpicture}
	\node[draw] (1) {
  \begin{tikzcd}
    \LMO{1}\ar[r, blue]\ar[dr, dyellow]\ar[d, red] &
    \LMO{2}\ar[loop above=5pt, blue]\ar[d, bend right=15pt, red] &
    \LMO{3}\ar[l, blue] \\
    \LMO[under]{4}\ar[loop left=15pt, red] &
    \LMO[under]{1}\ar[l, blue]\ar[u, bend right=15pt, dyellow]\ar[r, red] &
    \LMO[under]{4}\ar[u, red]
  \end{tikzcd}
  };
\end{tikzpicture}
\]
We will wrap the interface \[q\coloneqq\{a\}\yon^{\{g,p,o\}}+\{b\}\yon^{\{g,p\}}+\{c\}\] around $\varphi$ using the following lens $f\colon p\to q$ (we think of $g,p,$ and $o$ as green, purple, and orange, respectively):
\[
\begin{tikzpicture}
	\node (p1) {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$}
      child[blue] {coordinate (11)}
      child[dyellow] {coordinate (12)}
      child[red] {coordinate (13)};
    \node[right=1.5 of 1, "\tiny $b$" below] (2) {$\bullet$}
      child[green!50!black] {coordinate (21)}
      child[blue!50!purple] {coordinate (22)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (13);
      \draw[postaction={decorate}] (22) to (12);
    \end{scope}
  \end{tikzpicture}
	};
%
	\node (p2) [below right=-1.05cm and 1 of p1] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 2" below] (1) {$\bullet$}
      child[blue] {coordinate (11)}
      child[red] {coordinate (12)};
    \node[right=of 1, "\tiny $c$" below] (2) {$\bullet$};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
  \end{tikzpicture}
	};
%
	\node (p3) [right=3.5 of p1] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 3" below] (1) {$\bullet$}
      child[blue] {coordinate (11)};
    \node[right=1.5 of 1, "\tiny $b$" below] (2) {$\bullet$}
      child[green!50!black] {coordinate (21)}
      child[blue!50!purple] {coordinate (22)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (11);
      \draw[postaction={decorate}] (22) to (11);
    \end{scope}
  \end{tikzpicture}
	};
%
	\node (p4) [right=1 of p3] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 4" below] (1) {$\bullet$}
      child[red] {coordinate (11)};
    \node[right=1.5 of 1, "\tiny $a$" below] (2) {$\bullet$}
      child[green!50!black] {coordinate (21)}
      child[blue!50!purple] {coordinate (22)}
      child[orange!75!black] {coordinate (23)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (11);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (11);
    \end{scope}
  \end{tikzpicture}
	};
\end{tikzpicture}
\]
Composing $\varphi$ with $f$, we obtain a dynamical system $\6\yon^\6\To{\varphi}p\To{f}q$ that looks like this:
\[
\begin{tikzpicture}
	\node[draw] (1) {
  \begin{tikzcd}
    \LMO{b}\ar[d, green!50!black]\ar[dr, blue!50!purple] &
    \LMO{c} &
    \LMO{b}\ar[l, bend right=15pt, green!50!black]\ar[l, bend left=15pt, blue!50!purple] \\
    \LMO[under]{a}\ar[loop left=15pt, green!50!black]\ar[loop below=5pt, blue!50!purple]\ar[loop right=15pt, orange!75!black] &
    \LMO[under]{b}\ar[r, green!50!black]\ar[u, blue!50!purple] &
    \LMO[under]{a}\ar[u, bend left=20pt, green!50!black]\ar[u, blue!50!purple]\ar[u, bend right=20pt, orange!75!black]
  \end{tikzcd}
  };
\end{tikzpicture}
\]
Each state returns a $q$-position $j$ according to where the on-positions function of $f$ sends the $p$-position $i$ that the state returns.
Then each $q[j]$-direction is sent to a $p[i]$-direction via the on-directions function of $f$ at $i$, and the update function of $\varphi$ uses this $p[i]$-direction to compute the new state.
So $f$ allows us to operate $\varphi$ with the wrapper interface $q$ instead of the original interface $p$.
\end{example}

\begin{exercise} \label{exc.file_searcher_wrap}
In \cref{exc.file_searcher}, we built a file-searcher $\psi\colon S\yon^S\to q$ by taking the file-reader $\varphi\colon S\yon^S\to p$ from \cref{ex.generalized_file_reader} and replacing its interface $p$ with a new interface $q$ while keeping its state system $S\yon^S$ the same.
Express this construction as wrapping $q$ around $\varphi$ by giving a lens $f\colon p\to q$ for which composing $\varphi$ with $f$ yields $\psi$.
\begin{solution}
We give a lens $f\colon p\to q$ for which composing the file-reader $\varphi\colon S\yon^S\to p$ from \cref{ex.generalized_file_reader} with $f$ yields the file-searcher $\psi\colon S\yon^S\to q$ from \cref{exc.file_searcher}.
The file-searcher returns the same position as the file-reader when the second coordinate of that position is $100$, but replaces the second coordinate with a blank $\_$ otherwise.
So the on-positions function of $f$ should send each $(m,c)\in p(\1)$ to
\[
    f_\1(m,c) \coloneqq
        \begin{cases}
            (m,c) & \text{if } c = 100 \\
            (m,\_) & \text{otherwise}.
        \end{cases}
\]
Then the file-searcher acts just like the file-reader does on inputs, so every on-directions function of $f$ should be an identity function.
\end{solution}
\end{exercise}

\begin{example}[Polybox pictures of wrapper interfaces]
  In polyboxes, composing a dynamical system $\varphi\colon S\yon^S\to p$ with a wrapper $f\colon p\to q$ looks like this:
  \begin{equation*}
    \begin{tikzpicture}[polybox, mapstos]
      \node[poly, dom, "$S\yon^S$" left] (S) {$t$\at$s\vphantom{o'}$};

      \node[poly, right=of S, "$p$" below] (p) {$i$\at$o\vphantom{o'}$};

      \node[poly, cod, right=of p, "$q$" right] (q) {$i'$\at$o'$};

      \draw (S_pos) -- node[below] {return} (p_pos);
      \draw (p_dir) -- node[above] {update} (S_dir);
      \draw (p_pos) -- node[below] {$f_\1$} (q_pos);
      \draw (q_dir) -- node[above] {$f^\sharp$} (p_dir);
    \end{tikzpicture}
  \end{equation*}
  The position $o$ displayed by the intermediary interface $p$ is instead exposed as a position $f_\1(o)=o'$ of the wrapper interface $q$ in the rightmost position box.
  Moreover, the direction box of $p$ is no longer blue: an agent who wishes to interact with the middle interface $p$ can only do so via the rightmost interface $q$.
  The on-directions function of the wrapper at $o$ converts a direction $i'\in q[o']$ from the rightmost direction box into a direction $i\in p[o]$.

  Picture the agent standing to the right of all the polyboxes (i.e.\ ``outside'' of the system) with their attention directed leftward (i.e.\ ``inward''), receiving positions from the white position box and feeding directions into the blue direction box.
  To an agent who is unaware of its inner workings, the composite dynamical system $\varphi\then f$ might as well look like this:
  \begin{equation*}
    \begin{tikzpicture}[polybox, mapstos]
      \node[poly, dom, "$S\yon^S$" left] (S) {$t$\at$s\vphantom{o'}$};

      \node[poly, cod, right=of S, "$q$" right] (q) {$i'$\at$o'$};

      \draw (S_pos) -- node[below] {return$'$} (q_pos);
      \draw (q_dir) -- node[above] {update$'$} (S_dir);
    \end{tikzpicture}
  \end{equation*}
\end{example}

In the next section, we describe a special kind of wrapper.

%---- Subsection ----%
\subsection{Sections as wrappers}\label{subsec.poly.dyn_sys.new.sit_encl}

Say we wanted to model a dynamical system $\varphi\colon S\yon^S\to p$ within a closed system, for which an external agent can perceive no change in position and effect no change in direction.
We can think of this as wrapping $\yon$, the interface with one position and one direction, around $\varphi$.
To do so, we must specify a wrapper $\gamma\colon p\to\yon$.
In the language of \cref{def.sec-bun}, this is precisely a \emph{section} of $p$.
As we noted then, this name is appropriate, since $\gamma$ a way of sectioning off the interface $p$ from the outside world.

Recall that a section $\gamma\colon p\to\yon$ can be identified with a dependent function $\gamma\colon(i\in p(\1))\to p[i]$ that sends each $p$-position $i$ to a $p[i]$-direction.
In the language of dynamical systems, a section of the interface $p$ chooses a fixed input at every position of $p$.
A section for your interface dictates what you'll see (the direction you receive) given anything you might do (the position you provide); there is no need for any further outside interference.

\index{dependent function}

\begin{exercise} \label{exc.enclosures_as_functions}
Let $\varphi\colon S\yon^S\to B\yon^A$ be an $(A,B)$-Moore machine.
\begin{enumerate}
	\item Is it true that a section $\gamma\colon B\yon^A\to\yon$ can be identified with a function $A\to B$?
	\item Describe how to interpret a section $\gamma\colon B\yon^A\to\yon$ as a wrapper around an interface $B\yon^A$.
	\item Given a section $\gamma$, describe the dynamics of the composite Moore machine $S\yon^S\To{\varphi}B\yon^A\To{\gamma}\yon$ obtained by wrapping $\yon$ around $\varphi$ using $\gamma$.
\qedhere
\end{enumerate}\index{dynamics}
\begin{solution}
\begin{enumerate}
	\item No, it represents a function $B\to A$!
	A section sends each output $b\in B$ to an input $a\in A$.
	\item As a wrapper around an interface $B\yon^A$, a section $\gamma\colon B\yon^A\to\yon$ corresponds to a function $g\colon B\to A$ that feeds the input $g(b)\in A$ into the system whenever it returns the output $b\in B$.
	\item Composing our original Moore machine $S\yon^S\to B\yon^A$ with a section $\gamma$ yields a Moore machine $S\yon^S\To{\varphi}B\yon^A\To{\gamma}\yon$ that returns unchanging output and receives unchanging input.
	If we identify the Moore machine with its return function $S\to B$ and its update function $S\times A\to S$, and if we identify the section $\gamma$ with a function $g\colon B\to A$, then their composite Moore machine $S\yon^S\to\yon$ can be identified with a function $S\to S$, equal to the composite
	\[
	    S\To{\Delta}S\times S\To{\id_S\times\text{return}}S\times B\To{\id_S\times g}S\times A\To{\text{update}}S,
	\]
	where $\Delta$ is the diagonal map $s\mapsto(s,s)$.
	This composite map $S\to S$ sends every state to the next according to the output the original state returns, the input that the section gives in response to that output, and the update function that sends the original state and the input to the new state.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{example}[The do-nothing section] \label{ex.do_nothing}
There is something rather off-putting about the way we model dynamical systems as lenses $\varphi\colon S\yon^S\to p$.
We know that $\varphi$ tells us how state-positions return output-positions and, given a current state-position, how input-directions update state-directions.
But we rely only on the labels of elements in $S$ to tell us which positions and directions refer to the same states!

Nothing inherent in the language of $\poly$ makes these associations between state-positions and state-directions for us; we have to bank on the position-set and direction-sets of the state system being the same set for the machine to work properly.
Put another way, the monomials $\{4,6\}\yon^{\{4,6\}},\{4,6\}\yon^{\{4,8\}},$ and $\{3,5\}\yon^{\{6,7\}}$ are all isomorphic in $\poly$, but the first can be a state system while the other two cannot!

To address this issue, we need a way to connect the positions of a polynomial to its own directions in the language of $\poly$.
Here's where sections save the day: a lens $S\yon^S\to\yon$ is just a way of assigning to each position in $S$ a direction in $S$.
So we can define $\epsilon\colon S\yon^S\to\yon$ to be the section that sends each position $s\in S$ to the direction $s$ at $s$ corresponding to the same state.
Note that $\epsilon$ can be identified with the identity function on $S$ (see \cref{exc.enclosures_as_functions}).
Now $\poly$ knows which direction is associated with the same state as the position it is at.

In this way, we can generalize our notion of state systems to monomials $S\yon^{S'}$ equipped with a bijection $S\to S'$, which we can then translate to a section $\epsilon\colon S\yon^{S'}\to\yon$.
But for convenience of notation, we will continue to identify the position-set of a state system with each of its direction-sets.

More concretely, the section $\epsilon\colon S\yon^S\to\yon$ acts as a very special (if rather unexciting) dynamical system: it is the \emph{do-nothing section}, with only one possible output and one possible input that always keeps the current state the same.
While the system does, well, nothing, we do know one key fact about it: given any state system, regardless of the state-set, we can always define a do-nothing section on it.\footnote{It may have bothered you that we call $S\yon^S$, which is a single polynomial, a state \emph{system}, when we also use the word ``system'' to refer to dependent dynamical systems, which are lenses $S\yon^S\to p$.
The existence of the do-nothing section explains why our terminology does not clash: every polynomial $S\yon^S$ comes equipped with a dependent dynamical system $\epsilon\colon S\yon^S\to\yon$, so it really is a state \emph{system}.
Later on we'll see other systems that come with $S\yon^S$ for free.}

Yet this isn't the whole story.
The do-nothing section knows, at each position, the direction that keeps the system at the same state; but it doesn't know which of the other directions send the system to which of the other positions' states.
We're still relying on the labels of direction-sets being the same for that: for instance, the polynomials $\{1',2',3'\}\yon^{\{1,2,3\}}$ and $\{1'\}\yon^{\{0,1,4\}}+\{2'\}\yon^{\{2,5,6\}}+\{3'\}\yon^{\{-8,-1,3\}}$ are isomorphic, but even with a do-nothing section matching $1'\mapsto1,2'\mapsto2,3'\mapsto3$ to make the first one into a state system, we don't have a way to tell $\poly$ how to make the second one a state system yet.

From another perspective, $\epsilon\colon S\yon^S\to\yon$ does nothing, while $\varphi\colon S\yon^S\to p$ does ``one thing'': it steps through the system once, generating the current state's output with the return function and taking in input with the update function.
It's all set to take another step, but how does $\poly$ know which state to visit next?
Is there a lens that does ``two things,'' ``$n$ things,'' or ``arbitrarily many things''?
Can we actually $\emph{run}$ a dynamical system in $\poly$?
We'll develop the machinery to answer these questions over the course of the three chapters in \cref{part.comon}, starting in \cref{subsec.comon.comp.def.dyn_sys}.
\end{example}

\begin{example}[Polybox pictures of sections as wrappers]
  In polyboxes, composing a system $S\yon^S\to p$ with a section $\gamma$ of $p$ can be depicted as
  \begin{equation*}
    \begin{tikzpicture}[polybox, tos]
      \node[poly, dom, "$S\yon^S$" left] (S) {};

      \node[poly, right=of S, "$p$" below] (p) {};

      \node[poly, identity, right=of p, "$\yon$" right] (yon) {};

      \draw (S_pos) -- node[below] {return} (p_pos);
      \draw (p_dir) -- node[above] {update} (S_dir);
      \draw (p_pos) -- node[below] {$!$} (yon_pos);
      \draw (yon_dir) -- node[above] {$\gamma^\sharp$} (p_dir);
    \end{tikzpicture}
  \end{equation*}
  or, equivalently, as
  \begin{equation*}
    \begin{tikzpicture}[polybox, tos]
      \node[poly, dom, "$S\yon^S$" left] (S) {};

      \node[poly, right=of S, "$p$" below] (p) {};

      \draw (S_pos) -- node[below] {return} (p_pos);
      \draw (p_dir) -- node[above] {update} (S_dir);
      \draw (p_pos) to[climb'] node[right] {$\gamma$} (p_dir);
    \end{tikzpicture}
  \end{equation*}
  Remember: in a polybox depiction of a dynamical system, the world outside the system exists to the right of all the boxes.
  So the first picture represents $\yon$ as a gray wall, cutting off any interaction between the system to its left and the world to its right.
  Meanwhile, the second picture illustrates how a sectioned-off system independently selects inputs to the intermediary interface $p$ via $\gamma$, according to the outputs of $p$ that the inner (leftward) system $S\yon^S\to p$ returns.
  While the second picture shows us why the closed system neither seeks nor requires external input, the first picture helps remind us that the output of $p$ never reaches the outside world either.
  The composite system is therefore equivalent to the section drawn as follows:
  \begin{equation*}
    \begin{tikzpicture}[polybox, tos]
      \node[poly, dom, "$S\yon^S$" left] (S) {};

      \draw (S_pos) to[climb'] node[right] {$\gamma'$} (S_dir);
    \end{tikzpicture}
  \end{equation*}
  In lens parlance, $\gamma'\colon S\yon^S\to\yon$ is the original system $S\yon^S\to p$ composed with $\gamma\colon p\to\yon$; in the language of dependent functions, $\gamma'\colon S\to S$ is given by
  \[
  \gamma'(s)=\text{update}(s,\gamma(\text{return}(s))) \qquad \text{for all }s\in S,
  \]
  where we interpret $\gamma$ as a dependent function $(i\in p(\1))\to p[i]$.
  We can deduce this equation by matching up the previous two different polybox pictures, knowing that they represent the same lens.
  Placing the boxes side by side and filling them in with dummy variables (making sure to fill the blue boxes on either side with the same entry) makes the equation easier to read off the picture:
  \[
  \begin{tikzpicture}
    \node (1) {
      \begin{tikzpicture}[polybox, mapstos]
        \node[poly, dom, "$S\yon^S$" left] (S) {$t$\at$s$};

        \node[poly, right=of S, "$p$" below] (p) {$o$\at$i$};

        \draw (S_pos) -- node[below] {return} (p_pos);
        \draw (p_dir) -- node[above] {update} (S_dir);
        \draw (p_pos) to[climb'] node[right] {$\gamma$} (p_dir);
      \end{tikzpicture}
    };
    \node[right=1.8 of 1] (2) {
      \begin{tikzpicture}[polybox, mapstos]
        \node[poly, dom, "$S\yon^S$" left] (S) {$t'$\at$s$};

        \draw (S_pos) to[climb'] node[right] {$\gamma'$} (S_dir);
      \end{tikzpicture}
    };
    \node at ($(1.east)!.5!(2.west)$) {=};
  \end{tikzpicture}
  \]
  Matching up the upper boxes of the domain in both pictures, we have that $t=t'$, so
  \[
  \gamma'(s)=t'=t=\text{update}(s,o)=\text{update}(s,\gamma(i))=\text{update}(s,\gamma(\text{return}(s))).
  \]
  Later on we will read more intricate equations off of polyboxes in this manner, although we won't spell it out in so much detail; we encourage you to trace through the arrows on your own.
\end{example}

\begin{example}[The do-nothing section in polyboxes]  \label{ex.do_nothing_polybox}
  In \cref{ex.do_nothing}, we saw that every state system $S\yon^S$ can be equipped with a section $\epsilon\colon S\yon^S\to\yon$ called the do-nothing section, which assigns each state-position to its corresponding state-direction, thus leaving the state unchanged.
  That is, it is the section whose polyboxes can be drawn as follows:
  \begin{equation*}
    \begin{tikzpicture}[polybox, mapstos]
      \node[poly, dom, "$S\yon^S$" left] (S) {$s$\at$s$};

      \draw (S_pos) to[climb'] node[right] {$\epsilon$} (S_dir);
    \end{tikzpicture}
  \end{equation*}
\end{example}

\index{dynamical system!constructing new from old|)}

%-------- Section --------%
\section{General interaction}\label{sec.poly.dyn_sys.interact}

We now have all the pieces we need to model interactions between dependent dynamical systems, which can change their interfaces and interaction patterns, using $\poly$.
\index{interaction|(}
\index{interaction pattern|see{interaction}}

\subsection{Wrapping juxtaposed dynamical systems together}
\index{parallel product!and interaction}

When wrapper interfaces are used in conjunction with parallel products, they may encode multiple interacting dynamical systems as a single system.
Explicitly, given $n\in\nn$ and $n$ dynamical systems, $\varphi_i\colon S_i\yon^{S_i}\to p_i$ for every $i\in\ord{n}$, we can first juxtapose them into a single dynamical system
\[\varphi\colon \left(\prod_{i\in\ord{n}}S_i\right)\yon^{\prod_{i\in\ord{n}}S_i}\to\bigotimes_{i\in\ord{n}}p_i\]
by taking their parallel product.
Then we can wrap an interface $q$ around $\varphi$ using a wrapper $f$, yielding a new dynamical system
\[\left(\prod_{i\in\ord{n}}S_i\right)\yon^{\prod_{i\in\ord{n}}S_i}\To{\varphi}\bigotimes_{i\in\ord{n}}p_i\To{f}q.\]
On positions, $f$ gives a way of combining all the outputs of the constituent interfaces into a single output of the wrapper interface.
On directions, $f$ takes into account the current outputs of each of the constituent interfaces, as well as any new output given to the wrapper interface, then uses all of this to give input to each of the constituent interfaces.
In particular, a judiciously chosen on-directions function could feed output from some interfaces as inputs to others.
When $f$ is a wrapper around a parallel product of interfaces, we call $f$ the \emph{interaction pattern} between those interfaces.

\begin{example}[Repeater]\index{dynamical system!repeater}
Suppose we have a dependent dynamical system $\varphi\colon S\yon^S\to A\yon+\yon$, which takes unchanging input and sometimes returns elements of $A$ as output while other times returning only silence.
What if we wanted to construct a system $\psi$ that operates just like $\varphi$, but \emph{always} returns elements of $A$ as output?
Where $\varphi$ would have returned silence, we want $\psi$ to instead \emph{repeat} the last element of $A$ that it returned. (We allow $\psi$ to repeat an arbitrary element of $A$ if $\varphi$ returns silence before it has returned any elements of $A$ yet.)

Let's think like a programmer.
What we need is a way to store an element of $A$ and then retrieve it.
So whenever $\varphi$ returns an element of $A$ as output, we store it; then when $\varphi$ returns silence, we retrieve the last element of $A$ we stored and return that instead.

What should this storage-retrieval dynamical system look like?
It needs to take elements of $A$ as input, return elements of $A$ as output, and store elements of $A$ as states.
In fact, the identity lens $\iota\colon A\yon^A\to A\yon^A$ works perfectly: it returns the element of $A$ currently stored as output and updates its state to the input it receives.

Now we can juxtapose our original system $\varphi$ with the storage-retrieval system $\iota$ by taking their parallel product, yielding a dynamical system
\[
    \varphi\otimes\iota\colon SA\yon^{SA}\to (A\yon+\yon)\otimes A\yon^A
\]
that runs both systems simultaneously---and independently.
But what we want is for $\varphi$ and $\iota$ to interact with each other, and for the resulting system to only output elements of $A$.
To do so, we need to wrap an interface $A\yon$ around $\varphi\otimes\iota$ by composing it with some lens
\[
    f\colon(A\yon+\yon)\otimes A\yon^A\to A\yon,
\]
the interaction pattern between the interfaces $A\yon+\yon$ and $A\yon^A$, which we must define.

Since $\otimes$ distributes over $+$, it suffices to give lenses
\[
    g\colon A\yon\otimes A\yon^A\to A\yon \qqand h\colon\yon\otimes A\yon^A\to A\yon.
\]
The former corresponds to the case where $\varphi$ outputs an element of $A$, while the latter corresponds to the case where $\varphi$ is silent.

When $\varphi$ outputs an element of $A$, we want to return that output, but we also want to give that output as input to $\iota$ so that it can be stored.
We don't need to do anything with the output of $\iota$; we can simply discard it.
So $g$ should send $(a,a')\mapsto a$ on positions, returning the output of $\varphi$ and discarding the output of $\iota$; and the on-directions function $g^\sharp_{(a,a')}\colon\1\to A$ should specify the direction $a\in A$, feeding the output of $\varphi$ as input to $\iota$.

Meanwhile, when $\varphi$ outputs silence, we want to return the output of $\iota$ instead.
We also need to feed the output of $\iota$ back into $\iota$ as input so that it can continue to be stored.
So $h$ should be the identity on positions as well as the identity on directions.
\end{example}

\begin{example}[Paddling]\label{ex.paddler}
Say we wanted to build a Moore machine with interface $\nn\yon$; we may interpret its natural number output as the machine's current location.
What if we don't want this machine to jump around wildly?
Instead, suppose we want to be very strict about what how far the machine can move and what makes it move.

To accomplish this, we introduce two intermediary systems, which we call the \emph{paddler} and the \emph{tracker}:%
\footnote{Perhaps one could refer to the tracker as the \emph{demiurge}; it is responsible for maintaining the material universe.}
\[
  \text{paddler}\colon S\yon^S\to\2\yon
  \qqand
  \text{tracker}\colon T\yon^T\to\nn\yon^\2
\]
The paddler has interface $\2\yon$ because it is blind (i.e.\ takes no inputs) and can only move (i.e.\ output) its paddle to the left side or the right side: $\2\cong\{\text{left, right}\}$. The tracker has interface $\nn\yon^\2$ because it will announce the location of the machine (as an element $n\in\nn$) and watch what side the paddler is on (as an element of $\2$).
We can wrap an interface $\nn\yon$ around them both using an interaction pattern
\[
    \2\yon\otimes\nn\yon^\2\to\nn\yon
\]
whose on-positions function is the projection $\2\nn\to\nn$, returning the location returned by the tracker, and whose on-directions function is the projection $\2\nn\to\2$, passing the output of the paddler as input to the tracker.

\index{dynamics}

Let's leave the paddler's dynamics alone---how you make that paddler behave is totally up to you---and instead focus on the dynamics of the tracker.
We want it to watch for when the paddle switches from left to right or from right to left; at that moment it should push the paddler forward one unit. Thus the states of the tracker are given by $T\coloneqq\2\nn$, storing what side the paddler is on and the current location.
The on-positions function of the tracker is the projection $\2\nn\to\nn$ that returns the current location; then at each $(d,i)\in\2\nn$, the on-directions function of the tracker $\2\to\2\nn$ sends
\[
  d'\mapsto
	\begin{cases}
		(d',i)&\tn{if }d=d'\\
		(d',i+1)&\tn{if }d\neq d',
	\end{cases}
\]
storing the new direction of the paddler as well as moving the machine forward one unit if the paddle switches while keeping the machine still if the paddle stays still.
\end{example}

\index{dynamical system!paddler}
\begin{exercise}
Change the dynamics and state system of the tracker in \cref{ex.paddler} so that it exhibits the following behavior.

When the paddle switches once and stops, the tracker increases its location by one unit and stops, as before in \cref{ex.paddler}. But when the paddle switches twice in a row, the tracker increases its location by two units on the second switch! So if it is quiet for a while and then switches three times in a row, the tracker will increase its location by one then two then two.
\begin{solution}
We define a new tracker $T'\yon^{T'}\to\nn\yon^\2$ based on the one from \cref{ex.paddler} to watch for when the paddle switches sides once, at which point the tracker should increase its location by one, and watch for when the paddle switches sides twice in a row, at which point the tracker should increase its location by two.
To do this, we need the tracker to remember not just the current side the paddle is on, but the previous side the paddle was on as well.
The tracker should still remember the current location.
Thus the states of the tracker are given by $T\coloneqq\2\times\2\nn$, storing the previous side the paddler was on, the current side the paddler is on, and the current location.
The on-positions function of the tracker is the projection $\2\times\2\nn\to\nn$ that returns the current location; then at each $(d,d',i)\in\2\nn$, the on-directions function of the tracker $\2\to\2\times\2\nn$ sends
\[
  d''\mapsto
	\begin{cases}
		(d',d'',i)&\tn{if }d'=d''\\
		(d',d'',i+1)&\tn{if }d'\neq d''\tn{ and }d=d'\\
		(d',d'',i+2)&\tn{if }d'\neq d''\tn{ and }d\neq d'
	\end{cases}
\]
storing both the last side the paddle was on and the new side the paddle is on as well as moving the machine forward one unit if the paddle switches after not switching and two units if the paddle switches after just switching.
\end{solution}
\end{exercise}

\begin{example}
Suppose you have two systems with the same interface $p\coloneqq q\coloneqq\rr^\2\yon^{\rr^\2-\{(0,0)\}}$.
\[
\begin{tikzpicture}
	\node (m1) {\faMotorcycle};
	\node[above=-.15 of m1] (e1) {\faEye};
	\node[draw, thick, blue!10, fit = (m1) (e1)] {};
	\node[below right=0 and 1 of m1] (m2) {\scalebox{-1}[1]{\faMotorcycle}};
	\node[above=-.15 of m2] (e2) {\faEye};
	\node[draw, thick, blue!10, fit = (m2) (e2)] {};
\end{tikzpicture}
\]
The output of each interface indicates the location of the system, while the range of possible inputs indicate the locations that the system could observe, relative to the location of the system itself.
Taking all pairs of reals except $(0,0)$ corresponds to the fact that the eye cannot see that which is at the same position as the eye.

Let's have the two systems constantly approaching each other with a force equal to the reciprocal of the squared distance between them.
If they finally collide, let's have the whole thing come to a halt.
To do this, we want the wrapper interface to be $\{\text{`go'}\}\yon+\{\text{`stop'}\}$, so that if the system returns `go' it can still advance to the next state, but if it returns `stop' it halts.
The wrapper $\rr^\2\yon^{\rr^\2-\{(0,0)\}}\otimes\rr^\2\yon^{\rr^\2-\{(0,0)\}}\to\{\text{`go'}\}\yon+\{\text{`stop'}\}$ is given on positions by
\[
  \big((x_\1,y_1),(x_2,y_2)\big)\mapsto
	\begin{cases}
		\text{`stop'}&\mbox{ if $x_1=x_2$ and $y_1=y_2$}\\
		\text{`go'}&\mbox{ otherwise}.
	\end{cases}
\]
On directions, we use the function
\[
  \big((x_1,y_1),(x_2,y_2)\big)\mapsto \big((x_2-x_1,y_2-y_1),(x_1-x_2,y_1-y_2)\big),
\]
so that each system is able to see the location of the other system relative to its own, i.e.\ the vector pointing from itself to the other system (unless that vector is zero, in which case the whole thing should have already halted).

\index{dynamics}

We can use these vectors to define the internal dynamics of each system so that they move the way we want them to.
Each system will hold as its internal state its current location and velocity, i.e.\ $S=\rr^\2\times\rr^\2$.
To define a lens $S\yon^S\to\rr^\2\yon^{\rr^\2-\{(0,0)\}}$ we simply return the current location, update the current location by adding the current velocity, and update the current velocity by adding a vector with appropriate magnitude pointing to the other system:
\begin{align*}
	\rr^\2\times\rr^\2&\To{\text{return}}\rr^\2\\
	\big((x,y),(v_x, v_y)\big)&\Mapsto{\text{return}}(x,y)
\end{align*}
\begin{align*}
	\rr^\2\times\rr^\2\times(\rr^\2-\{(0,0)\})&\To{\text{update}}\rr^\2\times\rr^\2\\
	\big((x,y),(v_x,v_y),(a,b)\big)&\Mapsto{\text{update}}\left(x+v_x,y+v_y,v_x+\frac{a}{(a^2+b^2)^{3/2}},v_y+\frac{b}{(a^2+b^2)^{3/2}}\right)
\end{align*}
\end{example}

\begin{exercise}\index{robot}
Suppose $(X,d)$ is a metric space, i.e.\ $X$ is a set of points and $d\colon X\times X\to\rr_{\geq0}$ is a distance function satisfying the usual laws.
Let's have robots interact in this space.

Let $A,A'$ be sets, each thought of as a set of signals, and let $a_0\in A$ and $a_0'\in A'$ be elements, each thought of as a default value. Let $p\coloneqq AX\yon^{A'X}$ and $p'\coloneqq A'X\yon^{AX}$, and imagine there are two robots, one with interface $p$, returning a signal as an element of $A$ and its location as a point in $X$, and one with interface $p'$, returning a signal as an element of $A'$ and also its location as a point in $X$.
\begin{enumerate}
	\item Write down an interaction pattern $p\otimes p'\to\yon$ such that each robot receives the other's location, but that it only receives the other's signal when their locations $x,x'$ are sufficiently close, namely when $d(x,x')<1$.
	Otherwise, it receives the default signal.
	\item Write down an interaction pattern $p\otimes p'\to\yon^{[0,5]}$ where the value $s\in [0,5]$ is a scalar, allowing the signal to travel $s$ times further.
	\item Suppose that each robot has a set $S,S'$ of possible private states in addition to their locations.
	What functions are involved in providing a dynamical system $\varphi\colon SX\yon^{SX}\to AX\yon^{A'X}$, if the location state $x\in X$ is directly returned without modification?
	\item Change the setup in any way so that each robot only extends a port to hear the other's signal when the distance between them is less than $s$. Otherwise, they can only detect the position (element of $X$) that the other currently inhabits.
	(Don't worry too much about timing---one missed signal when the robots first get close or one extra signal when the robots first get far is okay.)
\qedhere
\end{enumerate}
\begin{solution}
Here $(X,d)$ is a metric space, $A,A'$ are sets of signals with default signals $a_0\in A$ and $a'_0\in A'$, and there are two robots, one with interface $p\coloneqq AX\yon^{A'X}$ returning a signal in $A$ and a location in $X$ and another with interface $p'\coloneqq A'X\yon^{AX}$ returning a signal in $A'$ and a location in $X$.
\begin{enumerate}
    \item An interaction pattern $p\otimes p'\to\yon$ consists of a trivial on-positions function $AX\times A'X\to\1$ (indicating that no outputs leave the system) and an on-directions function $AX\times A'X\to A'X\times AX$ indicating what inputs the robots should receive according to the outputs they return.
    To model the fact that the robots receive each others' locations, but only receive each others' signals rather than the default signals when the distance between their locations is less than $1$ according to the distance function $d$, this on-directions function should send
    \[
        ((a,x),(a',x'))\mapsto
          \begin{cases}
          	((a',x'),(a,x))&\tn{ if }d(x,x')<1\\
          	((a'_0,x'),(a_0,x))&\tn{ otherwise}.
          \end{cases}
    \]
    \item An interaction pattern $p\otimes p'\to\yon^{[0,5]}$ that allows the signal to travel $s\in[0,5]$ times further consists of a still trivial on-positions function and an on-directions function $AX\times A'X\times[0,5]\to A'X\times AX$ indicating what inputs the robots should receive according to the external input $s\in[0,5]$ as well as the outputs they return.
    To model the fact that the robots receive each others' locations, but only receive each others' signals rather than the default signals when the distance between their locations is less than $s$ according to the distance function $d$, this on-directions function should send
    \[
        ((a,x),(a',x'),s)\mapsto
          \begin{cases}
          	((a',x'),(a,x))&\tn{ if }d(x,x')<s\\
          	((a'_0,x'),(a_0,x))&\tn{ otherwise}.
          \end{cases}
    \]
    \item To provide a dynamical system $\varphi\colon SX\yon^{SX}\to AX\yon^{A'X}$ under the condition that the on-positions function preserves the second coordinate $x\in X$, we must provide the first projection $SX\to A$ of an on-positions function that turns the robot's private state and current location into the signal it returns, as well as an on-directions function $SX\times A'X\to SX$ that provides a new private state and location for the robot given its old private state, old location, and the signal and location it receives from the other robot.

    \item To have the robots listen for each others' signals only when they are sufficiently close, we must move away from monomial interfaces and Moore machines to leverage dependency.
    There are several ways of doing this; we give just one method below.
    With $D\coloneqq\{\text{`close'},\text{`far'}\}$, let the robots' new interfaces be
    \[
        p\coloneqq \{\text{`close'}\}AX\yon^{DA'X}+\{\text{`far'}\}AX\yon^{DX} \qqand p'\coloneqq \{\text{`close'}\}A'X\yon^{DAX}+\{\text{`far'}\}A'X\yon^{DX},
    \]
    so that they may receive input telling them whether they are close or far, but cannot receive signals in $A$ or $A'$ when they are `far.'

    Then by the distributivity of $\otimes$ over $+$, their new interaction pattern $p\otimes p'\to\yon^{[0,5]}$ can be specified by four lenses, all trivial on positions: the lens
    \[
        \{\text{`close'}\}AX\yon^{DA'X}\otimes\{\text{`close'}\}A'X\yon^{DAX}\to\yon^{[0,5]},
    \]
    given by the on-directions function
    \[
        ((\text{`close'},a,x),(\text{`close'},a',x'),s)\mapsto
          \begin{cases}
          	((\text{`close'},a',x'),(\text{`close'},a,x))&\tn{ if }d(x,x')<s\\
          	((\text{`far'},a'_0,x'),(\text{`far'},a_0,x))&\tn{ otherwise};
          \end{cases}
    \]
    the lens
    \[
        \{\text{`far'}\}AX\yon^X\otimes\{\text{`far'}\}A'X\yon^X\to\yon^{[0,5]},
    \]
    given by the on-directions function
    \[
        ((\text{`far'},a,x),(\text{`far'},a',x'),s)\mapsto
          \begin{cases}
          	((\text{`close'},x'),(\text{`close'},x))&\tn{ if }d(x,x')<s\\
          	((\text{`far'},x'),(\text{`far'},x))&\tn{ otherwise};
          \end{cases}
    \]
    and two other lenses that can be defined arbitrarily, as they should never come up in practice.

    Finally, in order for each robot to properly remember whether the other is close or far, we record an element of $D$ in its state that is returned and updated: one robot is a lens
    \[
        \varphi\colon DSX\yon^{DSX}\to \{\text{`close'}\}AX\yon^{DA'X}+\{\text{`far'}\}AX\yon^{DX}
    \]
    whose on-positions function preserves not just the third coordinate $x\in X$ but also the first coordinate $d\in D$, while the on-directions function also preserves the first coordinate $d\in D$; and the other robot is constructed similarly.
\end{enumerate}
\end{solution}
\end{exercise}

\index{interface!wrapper|)}

\subsection{Sectioning juxtaposed dynamical systems off together}

We saw in \cref{subsec.poly.dyn_sys.new.sit_encl} that a section (i.e.\ lens to $\yon$) for the interface of a dynamical system sections that dynamical system off as a closed system.
So it should not come as a surprise that a section for a parallel product of interfaces yields an interaction pattern between the interfaces that only allows the interfaces to interact with each other, cutting off any other interaction with the outside world.

\index{parallel product!section for}

\index{interaction!picking up the chalk|(}\index{interface!closed}

\begin{example}[Picking up the chalk]\label{ex.pickup_chalk}
Imagine that you see some chalk and you pinch it between your thumb and forefinger.
An amazing thing about reality is that you will then have the chalk, in the sense that you can move it around.
How might we model this in $\poly$?
We will construct a closed dynamical system---one with interface $\yon$---consisting of only you and the chalk.
To do so, we will provide an interface for you, and interface for the chalk, and a section for your juxtaposition.

Say that your hand can be at one of two heights, down or up, and that you can either press (apply pressure between your thumb and forefinger) or not press. Say too that you take in information about the chalk's height. Here are the two sets we'll be using:
\[
	H\coloneqq\{\text{`down', `up'}\}
	\qqand
	P\coloneqq\{\text{`press', `no press'}\}.
\]
Your interface is $HP\yon^H$: returning your own height and pressure, and receiving the chalk's height.

As for the chalk, it is either `in' your possession or `out' of it.
Either way, it also returns its height, which is either `down' or `up' in the air.
The chalk always takes in information about whether pressure is being applied or not.
When it's `out' of your possession, that's the whole story, but when it is `in' your possession, it also receives your hand's height.
All together, here are the two interfaces:
\[
	\const{You}\coloneqq HP\yon^H
	\qqand
	\const{Chalk}\coloneqq \{\text{`out'}\}H\yon^P + \{\text{`in'}\}H\yon^{HP}.
\]

Now we want to give the interaction pattern between you and the chalk.
As we said before, you see the chalk's height.
If your hand is not at the height of the chalk, the chalk receives no pressure.
Otherwise, your hand is at the height of the chalk, so the chalk receives your pressure (or lack thereof).
Furthermore, if the chalk is in your possession, it also receives your hand's height.

To provide a lens $\gamma\colon\const{You}\otimes\const{Chalk}\to\yon$, we use the fact that $\const{Chalk}$ is a sum and that $\otimes$ distributes over $+$.
Thus we need to give two lenses
\[
	\alpha\colon HP\yon^H\otimes H\yon^P\to\yon
	\qqand
	\beta\colon HP\yon^H\otimes H\yon^{HP}\to\yon
\]
The lens $\beta$, corresponding to when the chalk is in your possession, is quite easy to describe; it can be unfolded to a function
$HPH\to HHP$, and we take it to be the obvious map sending your height and pressure to the chalk and the chalk's height to you; see \cref{exc.pickup_chalk}. But $\alpha$ is more semantically interesting: it is given by
\[
  (h_\const{You},p_\const{You},h_\const{Chalk})\mapsto
  \begin{cases}
  	(h_\const{Chalk},\text{`no press'}) & \tn{ if } h_\const{You} \neq h_\const{Chalk} \\
  	(h_\const{Chalk},p_\const{You}) & \tn{ if } h_\const{You} = h_\const{Chalk}.
  \end{cases}
\]

So now we've got you and the chalk in sectioned off together by $\gamma$, so we are ready to add some dynamics.
Your dynamics can be whatever you want, so let's just add some dynamics to the chalk (you'll get to give yourself some dynamics in \cref{exc.pickup_chalk}).
The chalk has only four states $C\coloneqq \{\text{`out'}, \text{`in'}\} \times H \cong\4$: the $H$ coordinate is its current height, and the other coordinate is whether or not it is in your possession.
We will give a dynamical system $C\yon^C\to\const{Chalk}$ with states $C$ and interface $\const{Chalk}$, i.e.\ a lens
\begin{equation}\label{eqn.chalk_dynamics}
	\{\text{`out'}, \text{`in'}\} \times H\yon^{\{\text{`out'}, \text{`in'}\} \times H}\to \{\text{`out'}\}H\yon^P + \{\text{`in'}\}H\yon^{HP}.
\end{equation}
On positions, as you might guess, the chalk returns its height and whether it is in your possession directly.
On directions, if it's not in your possession, it falls down unless you catch it (i.e.\ apply pressure to it so that it enters your possession); if it is in your possession, it takes whatever height you give it.
So we can express the on-directions function of \eqref{eqn.chalk_dynamics} at $(\text{`out'}, h_\const{Chalk})$ as
\begin{align*}
	\text{`no press'} &\mapsto (\text{`out', `down'}) \\
	\text{`press'} &\mapsto (\text{`in'}, h_\const{Chalk})
\end{align*}
and the on-directions function of \eqref{eqn.chalk_dynamics} at $(\text{`in'}, h_\const{Chalk})$ as
\begin{align*}
	(h_\const{You}, \text{`no press'}) &\mapsto (\text{`out'}, h_\const{You}) \\
	(h_\const{You}, \text{`press'}) &\mapsto (\text{`in'}, h_\const{You}).
\end{align*}
Obviously, this is all quite complicated, intricate, and contrived.
Our goal here is to show that you can define interactions in which one system can engage with or disengage from another, where one system controls the behavior of the other when the two are engaged.
\end{example}\index{control}

\begin{exercise}\label{exc.pickup_chalk}
\begin{enumerate}
	\item In \cref{ex.pickup_chalk}, we said that $\beta\colon HP\yon^H\otimes H\yon^{HP}\to\yon$ was easy to describe and given by a function $HPH\to HHP$. Explain what's being said, and provide the function.
	\item Provide dynamics to the $\const{You}$ interface (i.e.\ specify a dynamical system with interface $\const{You}$) so that you repeatedly reach down and grab the chalk, lift it with your hand, and drop it.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item A lens $HP\yon^H \otimes H\yon^{HP} \to \yon$ consists of an on-positions function $HPH \to \1$ and an on-directions function $HPH \times \1 \to HHP$.
    This amounts to a function $HPH \to HHP$.
    We can easily define this function to be the isomorphism that sends $(h,p,h') \in HPH$ to $(h,h',p)$.
    \item To model the way in which you cycle through three possible actions---reaching down and grabbing the chalk, lifting it with your hand, and dropping it---it is simplest to work with a set of $\3$ possible states.
    So we will give your dynamics as a lens $\3\yon^\3 \to HP\yon^H$, where the return function $\3 \to HP$ indicates what happens at each state, sending $1 \mapsto (\text{down, press}), 2 \mapsto (\text{up, press})$, and $3 \mapsto (\text{up, no press})$.
    Then the update function $\3H \to \3$ always goes to the next state, regardless of input: it ignores the $H$ coordinate and sends $1$ to $2$, $2$ to $3$, and $3$ to $1$.
\end{enumerate}
\end{solution}
\end{exercise}

\index{interaction!picking up the chalk|)}
\index{section}

Given $n\in\nn$ and polynomials $p_1,\ldots,p_n$ as interfaces, a section $p_1\otimes\cdots\otimes p_n\to\yon$ sections off these $n$ interfaces together.
The following proposition provides an alternative perspective on such sections.

\begin{proposition}\label{prop.situations2}
Given polynomials $p,q\in\poly$, there is a bijection
\begin{equation} \label{eqn.situations2}
\Gamma(p\otimes q)\cong\smset\big(q(\1),\Gamma(p)\big)\;\times\;\smset\big(p(\1),\Gamma(q)\big).
\end{equation}
\end{proposition}
The idea is that specifying a section for the interfaces $p$ and $q$ together is equivalent to specifying a section for $p$ for every output $q$ might return and specifying a section for $q$ for every output $p$ might return.
\begin{proof}[Proof of \cref{prop.situations2}]
This is a direct calculation:
\begin{align*}
	\Gamma(p\otimes q) &\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}(p[i]\times q[j]) \\
	&\iso
	\left(\prod_{j\in q(\1)}\prod_{i\in p(\1)}p[i]\right)\times
		 \left(\prod_{i\in p(\1)}\prod_{j\in q(\1)}q[j]\right) \\
	&\iso
	\smset(q(\1),\Gamma(p))\times\smset(p(\1),\Gamma(q)).
\end{align*}
\end{proof}

\begin{example}
A section $f\colon B\yon^A\otimes B'\yon^{A'}\to \yon$, corresponds to a map $BB'\to AA'$. In other words, for every pair of outputs $(b,b')\in BB'$, the section $f$ specifies a pair of inputs $(a,a')\in AA'$.

Let's think of elements of $B$ and $B'$ not as outputs, but as locations that two machines may occupy.
\[
\begin{tikzpicture}[oriented WD, bb port length=0]
	\node[bb={1}{0}, fill=blue!10, dotted] (p) {$b$};
	\node[bb={1}{0}, fill=blue!10, dotted, below right=-0.5 and 0.5 of p] (q) {$b'$};
	\node[bb={0}{0}, inner sep=10pt, fit=(p) (q)] {};
	\node at (p_in1) {\faEye};
	\node at (q_in1) {\faEye};
\end{tikzpicture}
\hspace{.5in}
\begin{tikzpicture}[oriented WD, bb port length=0]
	\node[bb={1}{0}, fill=blue!10, dotted] (p) {$b$};
	\node[bb={1}{0}, fill=blue!10, dotted, below left=-0.5 and 0.5 of p] (q) {$b'$};
	\node[bb={0}{0}, inner sep=10pt, fit=(p) (q)] {};
	\node at (p_in1) {\faEye};
	\node at (q_in1) {\faEye};
\end{tikzpicture}
\]
Then given a pair of locations $(b,b')$, the interaction pattern $f$ tells us what the two eyes see, i.e.\ what values of $(a,a')$ they get.
Equivalently, \eqref{eqn.situations2} says that the interaction pattern tells us what values the first eye sees at any location when the second eye's location is fixed at $b'$, as well as what values the second eye sees at any location when the first eye's location is fixed at $b$.

Here we see that \eqref{eqn.situations2} provides two ways to interpret the interaction pattern between two interfaces in a closed system: either as a section around each interface that the other is part of, or as a single section around them both.
\end{example}

\begin{exercise}
Let $p\coloneqq q\coloneqq\nn\yon^\nn$.
We wish to specify a section around their juxtaposition.
\begin{enumerate}
    \item Say we wanted to feed the output of $q$ as input to $p$.
    What function $f\colon q(\1)\to\Gamma(p)$ captures this behavior?
    \item Say we wanted to feed the sum of the outputs of $p$ and $q$ as input to $q$.
    What function $g\colon p(\1)\to\Gamma(q)$ captures this behavior?
    \item What section $\gamma\colon p\otimes q\to\yon$ does the pair of functions $(f,g)$ correspond to via \eqref{eqn.situations2}?
	\item Let dynamical systems $\varphi\colon\nn\yon^\nn\to p$ and $\psi\colon\nn\yon^\nn\to q$ both be the identity on $\nn\yon^\nn$.
	Suppose $\varphi$ starts in the state $0\in\nn$ and $\psi$ starts in the state $1\in\nn$.
	Describe the behavior of the system obtained by sectioning $\varphi$ and $\psi$ off together with $\gamma$, i.e.\ the system $(\varphi\otimes\psi)\then\gamma$.
\qedhere
\end{enumerate}
\begin{solution}
We have $p\coloneqq q\coloneqq\nn\yon^\nn$.
\begin{enumerate}
    \item If we want to feed the output of $q$ as input to $p$, the corresponding function $f\colon q(\1)\to\Gamma(p)$ should send any output $b\in q(\1)$ to the section $\nn\to\nn$ of $p$ that sends any natural number output of $p$ to $b$ itself.
    That is, $f$ is the function $b\mapsto(\_\mapsto b)$.
    \item If we want to feed the sum of the outputs of $p$ and $q$ as input to $q$, the corresponding function $g\colon p(\1)\to\Gamma(q)$ should send any output $a\in q(\1)$ to the section $\nn\to\nn$ of $q$ that sends every natural number output $b$ of $q$ to the sum $a+b$.
    That is, $g$ is the function $a\mapsto(b\mapsto a+b)$.
    \item Together, $f$ and $g$ form a function $\nn\times\nn\to\nn\times\nn$ mapping $(a,b)\mapsto((f(b))(a),(g(a))(b))=(b,a+b)$, which is the section $\gamma\colon p\otimes q\to\yon$ that $(f,g)$ corresponds to via \eqref{eqn.situations2}.
    \item As $\varphi$ and $\psi$ are both the identity, the system $(\varphi\otimes\psi)\then\gamma$ is really just the system $\gamma\colon p\otimes q\to\yon$.
    When it is at state $(a,b)$, its next state will be $(b,a+b)$.
    So if its initial state is $(0,1)$, its following states will be $(1,1),(1,2),(2,3),(3,5),(5,8),(8,13),\ldots$, forming the familiar Fibonacci sequence.
\end{enumerate}
\end{solution}
\end{exercise}

\index{interaction!picking up the chalk}

\begin{exercise}
We will use \eqref{eqn.situations2} to consider the interaction pattern $\gamma$ between \const{You} and \const{Chalk} from \cref{ex.pickup_chalk} as a pair of functions $\const{You}(\1)\to\Gamma(\const{Chalk})$ and $\const{Chalk}(\1)\to\Gamma(\const{You})$.
\begin{enumerate}
	\item How does the chalk's output specify a section for you? That is, write the map $\const{Chalk}(\1)\to\Gamma(\const{You})$.
	\item How does your output specify a section for the chalk? That is, write the map $\const{You}(\1)\to\Gamma(\const{Chalk})$.
\qedhere
\end{enumerate}
\begin{solution}
We wish to write the section $\gamma\colon\const{You}\otimes\const{Chalk}\to\yon$ from \cref{ex.pickup_chalk} as a pair of functions $\const{You}(\1)\to\Gamma(\const{Chalk})$ and $\const{Chalk}(\1)\to\Gamma(\const{You})$ via \eqref{eqn.situations2}.
\begin{enumerate}
    \item Fix an output $(s_\const{Chalk}, h_\text{chalk})\in\const{Chalk}(\1)=\{\text{`out'},\text{`in'}\}H$ of the chalk.
    If $s_\const{Chalk}=\text{`out'}$, then the corresponding section $\const{You}\iso HP\yon^H\to\yon$ given by $f$ via \eqref{eqn.situations2} can be thought of as the function $HP\to H$ sending
    \[
        (h_\const{You},p_\const{You})\mapsto h_\const{Chalk},
    \]
    according to the behavior of $\alpha\colon HP\yon^H\otimes H\yon^P\to\yon$ when we fix $h_\text{chalk}$ to be position in the rightmost $H$ and focus on the result in the exponent $H$ on the left.
    Meanwhile, if $s_\const{Chalk}=\text{`in'}$, then the corresponding section $\const{You}\iso HP\yon^H\to\yon$ can also be thought of as the function $HP\to H$ sending
    \[
        (h_\const{You},p_\const{You})\mapsto h_\const{Chalk},
    \]
    according to the behavior of $\beta\colon HP\yon^H\otimes H\yon^{HP}\to\yon$ when we fix $h_\text{chalk}$ to be position in the rightmost $H$ and focus on the result in the exponent $H$ on the left.
    So overall, the map $\const{Chalk}(\1)\to\Gamma(\const{You})$ sends
    \[
        (\_,h_\text{chalk})\mapsto((\_,\_)\mapsto h_\const{Chalk}).
    \]

    \item Fix an output $(h_\const{You},p_\const{You})\in\const{You}(\1)=HP$ of the chalk.
    Then the corresponding section $\const{Chalk}\iso\{\text{`out'}\}H\yon^P + \{\text{`in'}\}H\yon^{HP}\to\yon$ can be thought of as a pair of functions: one $\{\text{`out'}\}H\to P$ sending
    \[
        (\text{`out'},h_\const{Chalk})\mapsto
            \begin{cases}
  	            \text{`no press'} & \tn{ if } h_\const{You} \neq h_\const{Chalk} \\
  	            p_\const{You} & \tn{ if } h_\const{You} = h_\const{Chalk}.
            \end{cases}
    \]
    according to the behavior of $\alpha\colon HP\yon^H\otimes H\yon^P\to\yon$ when we fix $(h_\const{You},p_\const{You})$ to be position in $HP$ on the left and focus on the result in the exponent $P$ on the right; and another $\{\text{`in'}\}H\to HP$ sending
    \[
        (\text{`in'},h_\const{Chalk})\mapsto(h_\const{You},p_\const{You})
    \]
    according to the behavior of $\beta\colon HP\yon^H\otimes H\yon^{HP}\to\yon$ when we fix $(h_\const{You},p_\const{You})$ to be position in $HP$ on the left and focus on the result in the exponent $HP$ on the right.
    So overall, the map $\const{You}(\1)\to\Gamma(\const{Chalk})$ sends
    \[
        (h_\const{You},p_\const{You})\mapsto\left(
            \begin{aligned}
                (\text{`out'},h_\const{Chalk})&\mapsto
                    \begin{cases}
          	            \text{`no press'} & \tn{ if } h_\const{You} \neq h_\const{Chalk} \\
          	            p_\const{You} & \tn{ if } h_\const{You} = h_\const{Chalk}
                    \end{cases} \\
                (\text{`in'},h_\const{Chalk})&\mapsto(h_\const{You},p_\const{You})
            \end{aligned}
        \right).
    \]
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
\begin{enumerate}
	\item State and prove a generalization of \eqref{eqn.situations2} from \cref{prop.situations2} for $n$-many polynomials $p_1,\ldots,p_n\in\poly$.
	\item Generalize the ``idea'' statement between \cref{prop.situations2} and its proof.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We generalize \eqref{eqn.situations2} for $n$ polynomials as follows.
    Given polynomials $p_1,\ldots,p_n\in\poly$, we claim there is a bijection
    \[
        \Gamma\left(\bigotimes_{i=1}^n p_i \right) \iso \prod_{i=1}^n \smset\left(\prod_{\substack{1 \leq j \leq n, \\ j \neq i}} p_j(\1), \Gamma(p_i)\right).
    \]
    The $n=1$ case is clear, and the $n=2$ case is given by \eqref{eqn.situations2}.
    Then by induction on $n$, we have
    \begin{align*}
        \Gamma\left(\bigotimes_{i=1}^n p_i \right) &\iso \smset\left(p_n(\1), \Gamma\left(\bigotimes_{i=1}^{n-1} p_i \right)\right) \times \smset\left(\prod_{i=1}^{n-1} p_i(\1), \Gamma(p_n)\right) \tag*{\eqref{eqn.situations2}} \\
        &\iso \smset\left(p_n(\1), \prod_{i=1}^{n-1} \smset\left(\prod_{\substack{1 \leq j \leq n-1, \\ j \neq i}} p_j(\1), \Gamma(p_i)\right)\right) \times \smset\left(\prod_{i=1}^{n-1} p_i(\1), \Gamma(p_n)\right) \tag{Inductive hypothesis} \\
        &\iso \prod_{i=1}^{n-1} \smset\left(\prod_{\substack{1 \leq j \leq n, \\ j \neq i}} p_j(\1), \Gamma(p_i)\right) \times \smset\left(\prod_{i=1}^{n-1} p_i(\1), \Gamma(p_n)\right) \tag{Universal properties of products and internal homs},
    \end{align*}
    and the result follows.
    \item The general idea is that specifying a section for interfaces $p_1,\ldots,p_n$ together is equivalent to specifying a section for $p_i$ for every output all the other interfaces might return, for each $i\in\ord{n}$.
\end{enumerate}
\end{solution}
\end{exercise}

\index{section}

\subsection{Wiring diagrams as interaction patterns}

\index{wiring diagram|see{interaction, wiring diagram}}
\index{interaction!wiring diagram|(}

A \emph{wiring diagram} is a graphical depiction of interactions between systems.
Wiring diagrams depict systems as boxes, showing how they can send inputs and outputs to each other through the wires between them, as well as how multiple systems can combine to form a larger system whenever smaller boxes are nested within a larger box.

\index{interface!monomial}

Formally, and more precisely, we can think of each box in a wiring diagram as an interface given by some monomial.
The box itself is not, per se, a dynamical system as we have defined one; but it becomes a dynamical system once we equip it with a lens from a state system.
Then the entire wiring diagram---specifying how these boxes nest within a larger box---is just an interaction pattern between the interfaces, with the larger box playing the role of the wrapper interface.
Once every nested box is equipped with a lens from a state system, we obtain a dynamical system whose interface is the larger box.

\begin{example}\index{control}
Here is a simple wiring diagram.
\begin{equation}\label{eqn.control_diag}
\begin{tikzpicture}[oriented WD, baseline=(B)]
	\node[bb={2}{1}, fill=blue!10] (plant) {\texttt{Plant}};
	\node[bb={1}{1}, below left=-1 and 1 of plant, fill=blue!10]  (cont) {\texttt{Controller}};
	\node[circle, inner sep=1.5pt, fill=black, right=.1] at (plant_out1) (pdot) {};
	\node[bb={0}{0}, inner ysep=25pt, inner xsep=1cm, fit=(plant) (pdot) (cont)] (outer) {};
	\coordinate (outer_out1) at (outer.east|-plant_out1);
	\coordinate (outer_in1) at (outer.west|-plant_in1);
	\begin{scope}[above, font=\footnotesize]
  	\draw (outer_in1) -- node {$A$} (plant_in1);
  	\draw (cont_out1) to node (B) {$B$} (plant_in2);
  	\draw (plant_out1) to node {$C$} (outer_out1);
  	\draw
  		let
  			\p1 = (cont.south west-| pdot),
  			\p2 = (cont.south west),
  			\n1 = \bby,
  			\n2 = \bbportlen
  		in
  			(pdot) to[out=0, in=0]
  			(\x1+\n2, \y1-\n1) --
  			(\x2-\n2, \y2-\n1) to[out=180, in=180]
  			(cont_in1);
		\end{scope}
	\node[below=0of outer.north] {\texttt{System}};
\end{tikzpicture}
\end{equation}
The plant is receiving information from the world outside the system, as well as from the controller. It's also producing information for the outside world which is being monitored by the controller.\index{control}

There are three boxes shown in \eqref{eqn.control_diag}: the controller, the plant, and the system. Each has a fixed set of inputs and outputs, and so we can consider the box as a monomial interface.\index{interface!monomial}
\begin{equation}\label{eqn.basic_diagram}
	\const{Plant}\coloneqq C\yon^{AB}
	\qquad\quad
	\const{Controller}\coloneqq B\yon^C
	\qquad\quad
	\const{System}\coloneqq C\yon^A.
\end{equation}
The wiring diagram itself is a wrapper
\[
	w\colon\const{Plant}\otimes\const{Controller}\to\const{System},
\]
specifying an interaction pattern between $\const{Plant}$ and $\const{Controller}$ with wrapper interface $\const{System}$.
Concretely, $w$ is a lens $CB\yon^{ABC}\to C\yon^A$ that dictates how wires are feeding from outputs to inputs.
Like all lenses between monomials, $w$ consists of an on-positions function $CB\to C$ and an on-directions function $CBA\to ABC$.

The wiring diagram is a picture that tells us which maps to use.
The on-positions function says ``inside the system you have boxes outputting values of type $C$ and $B$.
The system needs to produce an output of type $C$; how shall I obtain it?''
The answer, according to the wiring diagram, is to send $(c,b)\mapsto c$.

Meanwhile, the on-directions function says ``inside the system you have boxes outputting values of type $C$ and $B$, and the system itself is receiving an input value of type $A$.
The boxes inside need input values of type $A$, $B$, and $C$; how shall I obtain them?''
Again, we can read the answer off the wiring diagram: send $(c,b,a)\mapsto (a,b,c)$.

Note that neither the wiring diagram nor any of the boxes within it are dynamical systems on their own.
Rather, each box is a monomial that could be the interface of a dynamical system.
When we assign to a box a dynamical system having that box as its interface, we say that \emph{give dynamics} to the box.
So the entire wiring diagram is a wrapper that tells us how, given the dynamics for each inner box,
\[
\varphi\colon S\yon^S\to\const{Plant}
\qqand
\psi\colon T\yon^T\to\const{Controller},
\]
we can obtain the dynamics for the outer box:
\[
ST\yon^{ST}\To{\varphi\otimes\psi}\const{Plant}\otimes\const{Controller}\To{w}\const{System}.
\]
\end{example}\index{dynamics}

\begin{exercise}
\begin{enumerate}\index{control}
	\item Make a new wiring diagram like \eqref{eqn.control_diag} except where the controller also receives information of type $A'$ from the outside world.
	\item What are the monomials represented by the boxes in your diagram (replacing \eqref{eqn.basic_diagram})?
	\item What is the interaction pattern represented by this wiring diagram?
	Give the corresponding lens, including its on-positions and on-directions functions.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item Here is the wiring diagram \eqref{eqn.control_diag} modified so that the controller also receives information of type $A'$ from the outside world.
\[
\begin{tikzpicture}[oriented WD, baseline=(B)]
	\node[bb={2}{1}, fill=blue!10] (plant) {\texttt{Plant}};
	\node[bb={2}{1}, below left=-1 and 1 of plant, fill=blue!10]  (cont) {\texttt{Controller}};
	\node[circle, inner sep=1.5pt, fill=black, right=.1] at (plant_out1) (pdot) {};
	\node[bb={0}{0}, inner ysep=25pt, inner xsep=1cm, fit=(plant) (pdot) (cont)] (outer) {};
	\coordinate (outer_out1) at (outer.east|-plant_out1);
	\coordinate (outer_in1) at (outer.west|-plant_in1);
	\coordinate (outer_in2) at (outer.west|-cont_in1);
	\begin{scope}[above, font=\footnotesize]
  	\draw (outer_in1) -- node {$A$} (plant_in1);
  	\draw (outer_in2) -- node {$A'$} (cont_in1);
  	\draw (cont_out1) to node (B) {$B$} (plant_in2);
  	\draw (plant_out1) to node {$C$} (outer_out1);
  	\draw
  		let
  			\p1 = (cont.south west-| pdot),
  			\p2 = (cont.south west),
  			\n1 = \bby,
  			\n2 = \bbportlen
  		in
  			(pdot) to[out=0, in=0]
  			(\x1+\n2, \y1-\n1) --
  			(\x2-\n2, \y2-\n1) to[out=180, in=180]
  			(cont_in2);
		\end{scope}
	\node[below=0of outer.north] {\texttt{System}};
\end{tikzpicture}
\]
    \item The monomials represented by the boxes in this diagram are the same, except that $\const{Controller}$ and $\const{System}$ each have an extra $A'$ exponent:
    \[
	\const{Plant}\coloneqq C\yon^{AB}
	\qquad\quad
	\const{Controller}\coloneqq B\yon^{A'C}
	\qquad\quad
	\const{System}\coloneqq C\yon^{AA'}.
    \]

    \item The interaction pattern represented by this wiring diagram is the lens
    \[
    	w'\colon \const{Plant}\otimes\const{Controller}\to\const{System}
    \]
    consisting of an on-positions function $CB\to C$ given by $(c,b)\mapsto c$ and an on-directions function $CBAA'\to ABA'C$ given by $(c,b,a,a')\mapsto(a,b,a',c)$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Consider the following wiring diagram.
\[
\begin{tikzpicture}[oriented WD, font=\footnotesize, bb port sep=1, bb port length=2.5pt, bb min width=.4cm, bby=.2cm, inner xsep=.2cm, x=.5cm, y=.3cm, text height=1.5ex, text depth=.5ex]
  	\node[bb={2}{1}, fill=blue!10] (Trf) {$\const{Alice}$};
  	\node[bb={1}{2}, fill=blue!10, below=1 of Trf] (Trg) {$\const{Bob}$};
		\node[bb={2}{2}, fill=blue!10] at ($(Trf)!.5!(Trg)+(1.5,0)$) (Trh) {$\const{Carl}$};
  	\node[bb={0}{0}, fit={($(Trf.north west)+(-.25,4)$) (Trg) ($(Trh.north east)+(.25,0)$)}] (Tr) {};
		\node[below] at (Tr.north) {$\const{Team}$};
  	\node[coordinate] at (Tr.west|-Trf_in2) (Tr_in1) {};
  	\node[coordinate] at (Tr.west|-Trg_in1) (Tr_in2) {};
  	\node[coordinate] at (Tr.east|-Trh_out2) (Tr_out1) {};
  	\node at ($(Trg_out2)+(5pt,0)$) (dot) {$\bullet$};
\begin{scope}[font=\tiny]
  	\draw[shorten <=-2pt] (Tr_in1) -- node[below=-3pt] {$A$} (Trf_in2);
  	\draw[shorten <=-2pt] (Tr_in2) -- node[below=-3pt] {$B$} (Trg_in1);
		\draw (Trf_out1) to node[above=-3pt] {$D$} (Trh_in1);
		\draw (Trg_out1) to node[above=-3pt] {$E$} (Trh_in2);
  	\draw (Trg_out2) -- node[below=-3pt] {$F$} (dot.center);
  	\draw[shorten >=-2pt] (Trh_out2) -- node[below=-3pt] {$G$} (Tr_out1);
  	\draw let \p1=(Trh.east), \p2=(Trf.north west), \n1=\bbportlen, \n2=\bby in
  		(Trh_out1) to[in=0] (\x1+\n1,\y2+\n2) -- node[pos=.3, below=-3pt] {$H$} (\x2-\n1,\y2+\n2) to[out=180] (Trf_in1);
	\end{scope}
\end{tikzpicture}
\]
\begin{enumerate}
	\item Write out the polynomials for each of $\const{Alice}$, $\const{Bob}$, and $\const{Carl}$.
	\item Write out the polynomial for the outer box, $\const{Team}$.
	\item The wiring diagram constitutes a lens $f$ in $\poly$; what is its domain and codomain?
	\item What lens is it?
	\item Suppose we have dynamical systems $\alpha\colon A\yon^A\to\const{Alice}$, $\beta\colon B\yon^B\to\const{Bob}$, and $\gamma\colon C\yon^C\to\const{Carl}$. What is the induced dynamical system with interface $\const{Team}$?
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item According to the wiring diagram, we have that $\const{Alice}\coloneqq D\yon^{HA},$ that $\const{Bob}\coloneqq EF\yon^B,$ and that $\const{Carl}\coloneqq HG\yon^{DE}.$
    \item According to the wiring diagram, we have that $\const{Team}\coloneqq G\yon^{AB}.$
    \item The wiring diagram constitutes a wrapper
    \[
        f\colon\const{Alice}\otimes\const{Bob}\otimes\const{Carl}\to\const{Team}.
    \]
    Its domain is $\const{Alice}\otimes\const{Bob}\otimes\const{Carl}\iso DEFHG\yon^{HABDE}$, while its codomain is $\const{Team}=G\yon^{AB}$.
    \item On positions, the lens $f$ is a function $DEFHG\to G$ that sends $(d,e,f,h,g)\mapsto g$.
    On directions, $f$ is a function $DEFHGAB\to HABDE$ that sends $(d,e,f,h,g,a,b)\mapsto(h,a,b,d,e)$.
    \item Given dynamical systems $\alpha\colon A\yon^A\to\const{Alice}$, $\beta\colon B\yon^B\to\const{Bob}$, and $\gamma\colon C\yon^C\to\const{Carl}$, the dynamical system induced by the wiring diagram is given by the composite lens
    \[
        ABC\yon^{ABC}\To{\alpha\otimes\beta\otimes\gamma}\const{Alice}\otimes\const{Bob}\otimes\const{Carl}\To{f}\const{Team}.
    \]
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}[Long division] \label{exc.long_div}\index{long division}
\begin{enumerate}
	\item Let $\fun{divmod}\colon\nn\times\nn_{\geq1}\to\nn\times\nn$ send $(a,b)\mapsto(a\bdiv b, a\bmod b)$; for example, it sends $(10,7)\mapsto(1,3)$ and $(30,7)\mapsto(4,2)$.
	Use \cref{exc.funs_to_moore} to turn it into a dynamical system.
	\item Interpret the following wiring diagram, where we have already given dynamics to each box as indicated by their labels:
\[
\begin{tikzpicture}[oriented WD, bb small]
	\node[bb port sep=3, fill=blue!10, bb={2}{2}] (divmod) {divmod};
	\node[bb={0}{1}, fill=blue!10, left=of divmod_in2] (7) {$7$};
	\node[bb port sep=2, bb={2}{1}, fill=blue!10, below right=-1 and 3 of divmod_out2] (times) {$*$};
	\node[bb={0}{1}, fill=blue!10, below left=-1 and 1 of times_in2] (10) {$10$};
	\node[bb={0}{0}, inner xsep=\bbx, fit=(divmod) (times)(7) (10)] (outer) {};
	\coordinate (outer_in1) at (outer.west|-divmod_in1);
	\coordinate (outer_out1) at (outer.east|-divmod_out1);
	\coordinate (outer_out2) at (outer.east|-times_out1);
	\draw (outer_in1) -- (divmod_in1);
	\draw (7_out1) -- (divmod_in2);
	\draw (10_out1) -- (times_in2);
	\draw (divmod_out1) -- (outer_out1);
	\draw (divmod_out2) to (times_in1);
	\draw (times_out1) -- (outer_out2);
\end{tikzpicture}
\]
	\item Use the above and a diagram of the following form to create a dynamical system that alternates between spitting out $0$'s and the base-$10$ digits of $1/7$ after the decimal point, like so:
\[
\begin{tikzpicture}[oriented WD]
	\node[bb={1}{2}, fill=blue!10] (inner) {};
	\node[bb={0}{0}, inner xsep=1cm, inner ysep=1cm] (outer) {};
	\coordinate (outer_out1) at (outer.east|-inner_out1);
	\draw[shorten >=-3pt] (inner_out1) -- (outer_out1);
	\draw
		let \p1=(inner.south east), \p2=(inner.south west), \n1=\bbportlen, \n2=\bby in
		(inner_out2) to[in=0] (\x1+\n1,\y1-\n2) -- (\x2-\n1,\y1-\n2) to[out=180] (inner_in1);
		\node[right, font=\footnotesize] at (outer_out1) {$0,1,0,4,0,2,0,8,0,5,0,7,0,1,0,4,0,2,0,8,0,5,0,7,0,1,0,4,0,2,0,8,0,5,0,7,\ldots$};
\end{tikzpicture}
\]
We will see in \cref{subsec.comon.sharp.state.run} how to make a dynamical system run twice as fast, then apply this to the above system in \cref{ex.long_div_skip} so that it skips the $0$'s.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item Using \cref{exc.funs_to_moore}, we can turn $\fun{divmod}$ into the dynamical system $\fun{divmod}\colon\nn\times\nn\yon^{\nn\times\nn}\to\nn\times\nn\yon^{\nn\times\nn_{\geq1}}$ whose return function is the identity on $\nn\times\nn$ and whose update function $\nn\times\nn\times\nn\times\nn_{\geq1}\to\nn\times\nn$ sends $(\_,\_,a,b)\mapsto(a\bdiv b,a\bmod b)$.
    \item From left to right, the inner boxes represent monomial interfaces $\nn_{\geq1}\yon, \nn\times\nn\yon^{\nn\times\nn_{\geq1}}, \nn\yon,$ and $\nn\yon^{\nn\times\nn}$.\index{interface!monomial}
    The box labeled $7$ is given dynamics $7\colon\yon\to\nn_{\geq1}\yon$ so that it always returns the output $7$; similarly, the box labeled $10$ is given dynamics $10\colon\yon\to\nn\yon$ so that it always returns the output $10$.
    Meanwhile, the box labeled is given dynamics $\fun{divmod}\colon\nn\times\nn\yon^{\nn\times\nn}\to\nn\times\nn\yon^{\nn\times\nn_{\geq1}}$ from the previous part; while we can again apply \cref{exc.funs_to_moore} to the standard multiplication function $*\colon\nn\times\nn\to\nn$ to give the box labeled $*$ dynamics as well, yielding a dynamical system $*\colon\nn\yon^\nn\to\nn\yon^{\nn\times\nn}$ whose return function is the identity on $\nn$ and whose update function $\nn\times\nn\times\nn\to\nn$ sends $(\_,m,n)\mapsto m*n$.

    Then the outer box is the monomial interface $\nn\times\nn\yon^\nn$, and the wiring diagram is the interaction pattern
    \[
        w\colon\nn_{\geq1}\yon\otimes\left(\nn\times\nn\yon^{\nn\times\nn_{\geq1}}\right)\otimes\nn\yon\otimes\nn\yon^{\nn\times\nn}\to\nn\times\nn\yon^\nn
    \]
    with on-positions function $(s,q,r,t,p)\mapsto(q,p)$ and on-directions function $(s,q,r,t,p,a)\mapsto(a,s,r,t)$.
    So the dynamical system induced by the wiring diagram is the composite lens $\varphi$ given by
    \[
        \yon\otimes\left(\nn\times\nn\yon^{\nn\times\nn}\right)\otimes\yon\otimes\nn\yon^\nn\To{7\otimes\fun{divmod}\otimes10\otimes*}\nn_{\geq1}\yon\otimes\left(\nn\times\nn\yon^{\nn\times\nn_{\geq1}}\right)\otimes\nn\yon\otimes\nn\yon^{\nn\times\nn}\To{w}\nn\times\nn\yon^\nn,
    \]
    whose return function is given by the composite map $(q,r,p)\mapsto(7,q,r,10,p)\mapsto(q,p)$ and whose update function at state $(q,r,p)$ is given by the composite map $a\mapsto(a,7,r,10)\mapsto(a\bdiv7,a\bmod7,r*10)$.

    In other words, the dynamical system $\varphi$ behaves as follows: its state consists of a quotient $q$, a remainder $r$, and a product $p$, of which it returns the quotient and the product.
    Then it is fed a dividend $a$ and evaluates $a\bdiv7$ to obtain the new quotient and $a\bmod7$ to obtain the new remainder.
    Meanwhile, the new product is given by the previous remainder multiplied by $10$.

    \item This second wiring diagram specifies an interaction pattern
    \[
        w'\colon\nn\times\nn\yon^\nn\to\nn\yon
    \]
    with on-positions function $(q,p)\mapsto q$ and on-directions function $(q,p)\mapsto p$.
    So the dynamical system induced by nesting the first wiring diagram within the inner box in this second wiring diagram is a composite lens
    \[
        \left(\nn\times\nn\yon^{\nn\times\nn}\right)\otimes\nn\yon^\nn\To{\varphi}\nn\times\nn\yon^\nn\To{w'}\nn\yon
    \]
    whose return function is given by the composite map $(q,r,p)\mapsto(q,p)\mapsto q$ and whose update function at state $(q,r,p)$ is specifies the new state $(p\bdiv7,p\bmod7,r*10)$.

    In other words, the dynamical system $\varphi$ behaves as follows: its state consists of a quotient $q$, a remainder $r$, and a product $p$, of which it returns just the quotient.
    Then it advances to a new state by evaluating $p\bdiv7$ to obtain the new quotient and $p\bmod7$ to obtain the new remainder.
    Meanwhile, the new product is given by the previous remainder multiplied by $10$.

    If the initial state is $(q,r,p)=(0,0,10)$, then all the states will be as follows, with the values of $q$ in the left column giving us the outputs:
    \begin{table}[hbt!]
        \centering
        \footnotesize
        \begin{tabular}{c|c|c}
            $q$ $(p\bdiv7)$ & $r$ $(p\bmod7)$ & $p$ $(10r)$ \\
            \hline
            0 & 0 & 10 \\
            1 & 3 & 0 \\
            0 & 0 & 30 \\
            4 & 2 & 0 \\
            0 & 0 & 20 \\
            2 & 6 & 0 \\
            0 & 0 & 60 \\
            8 & 4 & 0 \\
            0 & 0 & 40 \\
            5 & 5 & 0 \\
            0 & 0 & 50 \\
            7 & 1 & 0 \\
            0 & 0 & 10 \\
            $\cdots$ & $\cdots$ & $\cdots$
        \end{tabular}
    \end{table}
\end{enumerate}
\end{solution}
\end{exercise}

\begin{example}[Graphs as wiring diagrams and cellular automata]\label{ex.graph_interaction}\index{cellular automata}\index{graph}
Suppose we have a graph $G=(E\tto V)$ as in \cref{def.graph} and a set $\tau(v)$ associated with each vertex $v\in V$:
\[
\begin{tikzcd}
	E\ar[r, shift left=3pt, "s"]\ar[r, shift right=3pt, "t"']&
	V\ar[r, "\tau"]&
	\smset
\end{tikzcd}
\]
We can think of $G$ as an alternative representation of a specific kind of wiring diagram, one where each inner box has exactly one output wire and the outer box is closed.
The vertices $v\in V$ are the inner boxes, the set $\tau(v)$ is the set associated with $v$'s output wire, and each edge $e$ is a wire connecting the output wire of its target $t(e)$ to an input wire of its source $t(e)$.
An edge from a vertex $v_0$ to a vertex $v_1$ indicates that the inputs to $v_0$ depend on the outputs of $v_1$.\footnote{We could have defined the edges in the opposite directions, so that they would point in the direction of data flow rather than in the direction of data dependencies; this was an arbitrary choice.}

In other words, we can associate each vertex $v\in V$ with the monomial
\[
	p_v\coloneqq\tau(v)\yon^{\prod_{e\in E_v}\tau(t(e))}
\]
specifying its inputs and outputs, where $E_v\coloneqq s\inv(v)\ss E$ denotes the set of edges emanating from $v$.
The graph then determines a section
\[
\gamma\colon\bigotimes_{v\in V}p_v\to\yon
\]
given by a function
\[
    \prod_{v\in V}\tau(v)\too\prod_{e\in E}\tau(t(e))
\]
that sends each dependent function $o\colon(v\in V)\to\tau(v)$ to the dependent function $(e\in E)\to\tau(t(e))$ sending $e\mapsto o(t(e))$.
In other words, given the output $o(v)\in\tau(v)$ for every vertex $v\in V$, we know for each edge $e\in E$ that the input $s(e)$ receives is the output of $t(e)$.
\index{dependent function}\index{dynamics}\index{graph}

Hence, once we give dynamics to each $p_v$, namely by specifying a dynamical system $S_v\yon^{S_v}\to p_v$ with outputs in $\tau(v)$ and inputs in $\prod_{e\in E_v}\tau(t(e))$, we will obtain a closed dynamical system that transitions from each vertex's state to the next according to the information that they pass each other along their edges.

Effectively, by interpreting a graph as a wiring diagram and giving each vertex dynamics, we have created what is known as a \emph{cellular automaton}---a network of vertices (or \emph{cells}) with states, such that each vertex $v\in V$ ``listens'' to the signals its \emph{neighbors} in $E_v$ send based on their states, then responds accordingly by updating its own state.

For example, a common graph found in cellular automata is a 2-dimensional integer lattice, with vertices $V\coloneqq\zz\times\zz$. The edges indicate which vertices are neighbors and thus ``hear'' which other vertices. One might use
\[E\coloneqq(\{-1,0,1\}\times\{-1,0,1\}-\{(0,0)\})\times V\]
with $s(i,j,m,n)=(m,n)$ and $t(i,j,m,n)=(m+i, n+j)$, so that the neighbors of each vertex are the eight vertices that surround it.
\end{example}

\begin{exercise}[Conway's Game of Life]\label{exc.conway}\index{graph}\index{interaction!cellular automata and}
Conway's Game of Life is played on a 2-dimensional integer lattice as follows.
Each lattice point is either \emph{live} or \emph{dead}, and each point observes its eight \emph{neighbors} to which it is horizontally, vertically, or diagonally adjacent.
The following occurs at every time step:
\begin{itemize}
    \item Any live point with 2 or 3 live neighbors remains live.
    \item Any dead point with 3 live neighbors becomes live.
    \item All other points either become or remain dead.
\end{itemize}
We can use \cref{ex.graph_interaction} to model Conway's Game of Life as a closed dynamical system.
\begin{enumerate}
	\item What is the appropriate graph $E\tto V$?
	\item What is the appropriate assignment of sets $\tau\colon V\to\smset$?
	\item What are the polynomials $p_v$ from \cref{ex.graph_interaction}?
	\item What is the appropriate state-set $S_v$ for each interface $p_v$?
	\item What is the appropriate dynamical system lens $S_v\yon^{S_v}\to p_v$?
\qedhere
\end{enumerate}
\begin{solution}
We seek to model Conway's Game of Life as a closed dynamical system using \cref{ex.graph_interaction}.
\begin{enumerate}
    \item Following the suggestion from the end of \cref{ex.graph_interaction}, we can use a graph with $V\coloneq\zz\times\zz$ and $E\coloneqq(\{-1,0,1\}\times\{-1,0,1\}-\{(0,0)\})\times V$ with $s(i,j,m,n)=(m,n)$ and $t(i,j,m,n)=(m+i,n+j)$ to model cellular automata like Conway's Game of Life on a 2-dimensional integer lattice in which each point listens only to its eight immediate neighbors.
    \item Each vertex needs only return whether it is live or dead, so we assign $\tau(v)\coloneqq\{\text{live},\text{dead}\}$ for every $v\in V$.
    \item For each $v\in V$, the monomial represented by $v$ from \cref{ex.graph_interaction} can be written as
    \[
        p_v\iso\{\text{live},\text{dead}\}\yon^{\smset(\{-1,0,1\}\times\{-1,0,1\}-\{(0,0)\},\, \{\text{live},\text{dead}\})}.
    \]
    Every vertex returns as output whether it is live or dead and receives as input whether each of its eight neighbors is alive or dead.
    \item Each vertex $v\in V$ only needs to record whether it is live or dead, so $S_v\coloneqq\{\text{live},\text{dead}\}$.
    \item The appropriate dynamical system lens $S_v\yon^S_v\to p_v$ for each vertex $v\in V$ should have the identity function on $\{\text{live},\text{dead}\}$ as its return function, while its update function should be a map
    \[
        \{\text{live},\text{dead}\}\smset(\{-1,0,1\}\times\{-1,0,1\}-\{(0,0)\},\, \{\text{live},\text{dead}\})\to\{\text{live},\text{dead}\}
    \]
    that executes the rules from Conway's Game of Life, using the data of whether the vertex itself is live or dead as well as whether each of its eight neighbors is live or dead to determine whether it should be live or dead in the next time step.
\end{enumerate}
\end{solution}
\end{exercise}

\index{interaction!wiring diagram|)}

\subsection{More examples of general interaction}

While wiring diagrams are a handy visualization tool for certain simple interaction patterns, there are more general interaction patterns that cannot be captured by such a diagram.
For example, here we generalize our previous cellular automata example.

\begin{example}[Generalized cellular automata: voting on who your neighbors are]\label{ex.cell_auto_vote_interaction}\index{cellular automata}\index{graph}
Recall from \cref{ex.graph_interaction} how we constructed a cellular automaton on a graph $G=(E\tto V)$.
For each $v\in V$, the graph specifies the set $N(v)\coloneqq t(E_v)$ of vertices at the ends of edges coming out of $v$.
These vertices are the \emph{neighbors} of $v$, or the vertices that $v$ can ``listen'' to.
We call the map $N\colon V\to\2^V$ from each vertex to the set of its neighbors the \emph{neighbor function}.
For simplicitly, we let each vertex store and return one of two states, so $S_v\coloneqq\tau(v)\coloneqq\2$.

Now just take the vertices and forget the edges of our graph.
Suppose instead that we are given a function $n\colon V\to\nn$ that we think of as specifying the number $n(v)$ of neighbors each $v\in V$ could potentially have.
Let $\ord{n}(v)\coloneqq\{1,2,\ldots,n(v)\}$.
Then we can think of the monomial that each vertex represents as
\[
    p_v\iso\2\yon^{\2^{\ord{n}(v)}},
\]
returning its own state as output and receiving its potential neighbors' states as input.

Say that a neighbor function $N\colon V\to\2^V$ \emph{respects} $n$ if we have an isomorphism $N(v)\iso\ord{n}(v)$ for each $v\in V$.
Now suppose we have a function $N'_-\colon \2^V\to (\2^V)^V$ that sends each set of vertices $S\in\2^V$ to a neighbor function $N'_S\colon V\to \2^V$ that respects $n$.
In other words, each possible state configuration $S$ of all the vertices in $V$ determines a neighbor function $N'_S$.
In the case of \cref{ex.graph_interaction}, when we had a graph, it told us what the neighbor function should always be.
Now we can think of it like all the vertices are voting, via $N'$, on what neighbor function to use to determine which vertices are listening to which others.

We can put this all together by providing a section for all the vertices,
\begin{equation}\label{eqn.polymap_misc9237}
    \bigotimes_{v\in V}p_v\cong\2^V\yon^{\2^{\sum_{v\in V}\ord{n}(v)}}\too\yon.
\end{equation}
Specifying such a section amounts to specifying a function $g\colon \2^V\to\2^{\sum_{v\in V}\ord{n}(v)}$ that sends each possible state configuration $S\in\2^V$ of all the vertices in $V$ to a function $g(S)\colon\sum_{v\in V}\ord{n}(v)\to\2$ specifying the states every vertex hears.
But we already have a neighbor function assigned to $S$ that respects $\ord{n}$, namely $N'_S$, for which $N'_S(v)\iso\ord{n}(v)$ for all $v\in V$.
So we can think of $g(S)$ equivalently as a function $g(S)\colon\sum_{v\in V}N'_S(v)\to\2$ that says for each $v\in V$ what signal in $\2$ it should receive from its neighbor $w\in N'_S(v)$.
But we can just have it receive the current state of its neighbor, as given by $S$:
\[
    g(S)(v,w)\coloneqq S(w).
\]

We have accomplished our goal: the vertices ``vote'' on how they ought to be connected, in that their states together determine the neighbor function.
Of course, we don't mean to imply that this vote needs to be democratic or fair in any way: it is an arbitrary function $N'_-\colon \2^V\to(\2^V)^V$.
It could be dictated by a given vertex $v_0\in V$ in the sense that its state completely determines the neighbor function $V\to\2^V$; this would be expressed by saying that $N'_-$ factors as $\2^V\to\2^{\{v_0\}}\cong\2\To{I_0}(\2^V)^V$ for some $I_0$.
\end{example}

%\begin{exercise}
%We can change \cref{ex.cell_auto_vote_interaction} slightly by replacing the wrapper interface $\yon$ with some other interface.
%\begin{enumerate}
%	\item First change it to $A\yon$ for some set $A$ of your choice, and update \eqref{eqn.polymap_misc9237} so that the system outputs some aspect of the current state configuration of all the vertices $S\in\2^V$.
%	\item What would it mean to change \eqref{eqn.polymap_misc9237} to a map $\bigotimes_{v\in V}p_v\to\yon^A$ for some $A$?
%\qedhere
%\end{enumerate}
%\begin{solution}
%\begin{enumerate}
%    \item **
%    \item **
%\end{enumerate}
%\end{solution}
%\end{exercise}

Here are some more examples of using dependent dynamical systems to model changing wiring diagrams.

\begin{example}\label{ex.bonds_break}\index{interaction!breaking bonds}
In the picture below, forces are being applied to the connected boxes on the left; we would like to model how too much force could cause the connection between the boxes to sever, as on the right.
\[
\begin{tikzpicture}[oriented WD, bb small, bb port length=0]
	\node[bb={1}{1}, fill=blue!10] (x1) {$\varphi_1$};
	\node[bb={1}{1}, fill=blue!10, right=of x1] (x2) {$\varphi_2$};
	\node[bb={1}{1}, fit= (x1) (x2)] (outer) {};
	\draw[->, shorten >= -4mm] (x1_in1) -- (outer_in1) node[left=4.5mm, font=\tiny] {Force};
	\draw (x1_out1) -- (x2_in1);
	\draw[->, shorten >= -4mm] (x2_out1) -- (outer_out1) node[right=4.5mm, font=\tiny] (L) {Force};
%
	\node[bb={1}{1}, fill=blue!10, right=2in of L] (y1) {$\varphi_1$};
	\node[bb={1}{1}, fill=blue!10, right=of y1] (y2) {$\varphi_2$};
	\node[bb={1}{1}, fit= (y1) (y2)] (outer) {};
	\draw[->, shorten >= -4mm] (y1_in1) -- (outer_in1) node[left=4.5mm, font=\tiny] (R){Force};
	\draw[->, shorten >= -4mm] (y2_out1) -- (outer_out1) node[right=4.5mm, font=\tiny] {Force};
	\node[starburst, draw, minimum width=2cm, minimum height=1.5cm,red,fill=orange,line width=1.5pt] at ($(L)!.5!(R)$)
{Snap!};
\end{tikzpicture}
\]
We will imagine the dependent dynamical systems $\varphi_1\colon S\yon^S\to p_1$ and $\varphi_2\colon S\yon^S\to p_2$ as initially connected in space.
They experience forces from the outside world, and---for as long as they are connected---they experience forces from each other.
More precisely, each interface is defined by
\[
	p_1\coloneqq p_2\coloneqq F\yon^{FF}+\{\text{`snapped'}\}\yon^F.
\]
Elements of $F$ will be called \emph{forces}.
We need to be able to add and compare forces, i.e.\ we need $F$ to be an ordered monoid; let's say $F=\nn$ for simplicity.
The idea is that the interface has two kinds of output it can return: either a force $f_i\in F$ on the other system, at which point it can receive an input in $FF$ indicating a force acting on the system from its left and another force acting on it from its right; or `snapped,' indicating that the system is no longer connected to the other system, at which point it only receives a single force in $F$ from the outside.

The wrapper interface is defined to be
\[
    p\coloneqq\yon^{FF};
\]
it takes as input two forces $(f_L, f_R)$ and returns unchanging output.

Though the systems $\varphi_1$ and $\varphi_2$ may be initially connected, if the forces on either one surpass a threshold, that system stops sending and receiving forces from the other. The connection is broken and neither system ever receives forces from the other again. This is what we will implement explicitly below.

\index{interaction pattern}
To do so, we need to define an interaction pattern $p_1\otimes p_2\to p$ that wraps $p$ around $\varphi_1$ and $\varphi_2$.
That is, we need to give a lens
\[
    \kappa\colon (F\yon^{FF}+\{\text{`snapped'}\}\yon^F)\otimes (F\yon^{FF}+\{\text{`snapped'}\}\yon^F)\to\yon^{FF}.
\]
By the distributivity of $\otimes$ over $+$, it suffices to give four lenses:
\begin{equation}\label{eqn.snapped_maps}
\arraycolsep=1.4pt
\begin{array}{lll}
	\kappa_{11}\colon&~ FF\yon^{(FF)(FF)}&\to\yon^{FF}\\
	\kappa_{12}\colon&~ F\{\text{`snapped'}\}\yon^{(FF)F}&\to\yon^{FF}\\
	\kappa_{21}\colon&~ \{\text{`snapped'}\}F\yon^{F(FF)}&\to\yon^{FF}\\
	\kappa_{22}\colon&~ \{\text{`snapped'}\}\{\text{`snapped'}\}\yon^{FF}&\to\yon^{FF}
\end{array}
\end{equation}
The middle two lenses $\kappa_{12}$ and $\kappa_{21}$ won't actually occur in our dynamics, so we take them to be arbitrary.
We take the last lens $\kappa_{22}$ to be the obvious isomorphism, passing the forces from outside to the two internal interfaces.
The first lens $\kappa_{11}$ is equivalent to a function $(FF)(FF)\to (FF)(FF)$ which we take to be $((f_1,f_2),(f_L,f_R))\mapsto((f_L,f_2),(f_1,f_R))$.
While the multiple $F$'s may be a little hard to keep track of, what this map says is that if $\varphi_1$ returns the force $f_1$ on $\varphi_2$ as output and $\varphi_2$ returns the force $f_2$ on $\varphi_1$ as output, then $\varphi_1$ receives the force $f_2$ from the right as input and $\varphi_2$ receives the force $f_1$ from the left as input; and in the meantime the left external force $f_L$ is given to $\varphi_1$ on the left, while the right external force is given to $\varphi_2$ on the right.

Now that we have the interfaces wrapped together, it remains to specify each dynamical system.
The states in the two cases will be identical, namely $S\coloneqq F+\{`snapped'\}$, meaning that at any point the system will either be in the state of applying a force to the other system or not.
The dynamical systems themselves will be identical as well, up to a symmetry swapping left and right; let's just define the left system for now.
It is given by a lens
\[\varphi_1\colon (F+\{\text{`snapped'}\})\yon^{F+\{\text{`snapped'}\}}\to F\yon^{FF}+\{\text{`snapped'}\}\yon^F\]
which we write as the sum of two lenses
\[F\yon^{F+\{\text{`snapped'}\}}\to F\yon^{FF} \qqand \{\text{`snapped'}\}\yon^{F+\{\text{`snapped'}\}}\to\{\text{`snapped'}\}\yon^F.\]
Both lenses are the identity on positions, directly returning their current state.
The second lens corresponds to when the connection is broken, after which the connection should remain broken, so its on-directions function is constant, sending any input to `snapped.'
Meanwhile, the first lens corresponds to the case where the systems are still connected; this system receives two input forces and must update its state---the force it applies---accordingly.
We let the on-directions function $F(FF)\to F+\{\text{`snapped'}\}$ send
\[
(f_1,(f_L,f_2))\mapsto
\begin{cases}
	f_L&\tn{ if }f_1+f_2<100\\
	\text{`snapped'}&\tn{ otherwise}
\end{cases}
\]
Thus, when the sum of forces is high enough, the internal state is updated to the `snapped' state; otherwise, it is sent to the force it receives from outside, which it is now ready to transfer to the other system.
\end{example}

\begin{example}\label{ex.supplier_change}\index{interaction!supplier change}
Consider the case of a company that may change its supplier based on its internal state. The company returns two possible outputs, corresponding to who it wants to receive widgets $W$ from:
\[
\begin{tikzpicture}[oriented WD, every node/.style={fill=blue!10}]
	\node[bb={0}{1}] (s1) {Supplier 1};
	\node[bb={0}{1}, below=of s1] (s2) {Supplier 2};
	\node[bb={1}{0}, right=0.5 of s1] (c) {Company};
	\draw (s1_out1) to node[above, fill=none, font=\tiny] {$W$} (c_in1);
	\draw (s2_out1) to +(5pt,0) node[fill=none] {$\bullet$};
\begin{scope}[xshift=3.5in]
	\node[bb={0}{1}] (s1') {Supplier 1};
	\node[bb={0}{1}, below=of s1'] (s2') {Supplier 2};
	\node[bb={1}{0}, right=0.5 of s2'] (c') {Company};
	\draw (s2'_out1) to node[above, fill=none, font=\tiny] {$W$} (c'_in1);
	\draw (s1'_out1) to +(5pt,0) node[fill=none] {$\bullet$};
\end{scope}
	\node[starburst, draw, minimum width=2cm, minimum height=2cm,align=center,fill=white, font=\small,line width=1.5pt] at ($(c.east)!.5!(s2'.west)$)
{Change\\supplier!};
\end{tikzpicture}
\]
So the company has interface $\2\yon^W$, and each supplier has interface $W\yon$.
Then a section for the company and the suppliers is just a lens $\2\yon^W\otimes W\yon\otimes W\yon\to\yon$, corresponding to a function $\2W^\2\to W$ given by evaluation.
In other words, the company's output determines its supplier.
\end{example}

\begin{example}\label{ex.assemble_machine}\index{interaction!assembling}
When someone assembles a machine, their own positions dictate the interaction pattern of the machine's components.
\begin{equation*}%\label{eqn.someone2}
\begin{tikzpicture}[oriented WD, font=\ttfamily, bb port length=0, every node/.style={fill=blue!10}, baseline=(someone.north)]
	\node[bb port sep=.5, bb={0}{1}] (A) {unit A};
	\node[bb port sep=.5, bb={1}{0}, right=of A] (B) {unit B};
	\coordinate (helper) at ($(A)!.5!(B)$);
	\node[bb={1}{1}, below=2 of helper] (someone) {\tikzsymStrichmaxerl[3]};
	\draw[->, dashed, blue] (someone_in1) to[out=180, in=270] (A.270);
	\draw[->, dashed, blue] (someone_out1) to[out=0, in=270] (B.270);
	\draw[->] (A_out1) -- +(10pt,0);
	\draw (B_in1) -- +(-10pt,0);
%
\begin{scope}[xshift=3.5in]
	\node[bb port sep=.5, bb={0}{1}] (A') {unit A};
	\node[bb port sep=.5, bb={1}{0}, right=.5of A'] (B') {unit B};
	\coordinate (helper') at ($(A')!.5!(B')$);
	\node[bb={1}{1}, below=2 of helper'] (someone') {\tikzsymStrichmaxerl[3]};
	\draw[->, dashed, blue] (someone'_in1) to[out=180, in=270] (A'.270);
	\draw[->, dashed, blue] (someone'_out1) to[out=0, in=270] (B'.270);
	\draw[->] (A'_out1) -- (B'_in1);
\end{scope}
%
	\node[starburst, draw, minimum width=2cm, minimum height=2cm,fill=blue!50,line width=1.5pt, align=center, font=\upshape] at ($(B)!.5!(A')-(0,.6cm)$)
{Attach!};
\end{tikzpicture}
\end{equation*}
In order for the above picture to make sense, the output set of \texttt{unit A} should be the same as the input set of \texttt{unit B}.
Call this set $X$, so that \texttt{unit A} has interface $X\yon$ and \texttt{unit B} has interface $\yon^X$.
We fix a default value $x_0\in X$ for the input to \texttt{unit B} when it is not connected to \texttt{unit A}.
Meanwhile, the person takes no input and dictates whether the units are attached or not, so we give it interface $\2\yon$.

Then a section for the person and the units is a lens $\2\yon\otimes X\yon\otimes \yon^X\to\yon$. The lens $\2X\yon^X\to\yon$, corresponding to a function $\2X\to X$ that maps $(1,x)\mapsto x_0$ and $(2,x)\mapsto x$.
\end{example}

We can easily generalize \cref{ex.assemble_machine}.
Indeed, we will see in the next section that there is an interface $\ihom{q_1\otimes\cdots\otimes q_k\,,\,r}$ that represents all the interaction patterns between $q_1,\ldots,q_k$ with wrapper interface $r$, and that wrapping it around $p$ to it is just a larger interaction pattern:
\[
\poly(p,[q_1\otimes\cdots\otimes q_k\,,\,r])\cong\poly(p\otimes q_1\otimes\cdots\otimes q_k\,,\,r).
\]
In other words, if $p$ is deciding the interaction pattern between $q_1,\ldots,q_k$ with wrapper interface $r$, and gets feedback from that interaction pattern itself, then this is equivalent to an interaction pattern with wrapper interface $r$ that $p$ is part of alongside $q_1,\ldots,q_k$ inside of $r$.

What it also means is that if you want, you can put a little dynamical system inside of $[q_1\otimes\cdots\otimes q_k,r]$ and have it be constantly choosing interaction patterns. Let's see how it works.


\index{interaction|)}
%-------- Section --------%
\section{Closure of $\otimes$}\label{sec.closure}%[-,-]
\index{closed monoidal structure}
\index{monoidal closed structure|see{closed monoidal structure}}
\index{parallel product!closure for}

The parallel monoidal product is closed---we have a monoidal closed structure on $\poly$---meaning that there is a closure operation, which we denote $\ihom{-,-}\colon\poly\op\times\poly\to\poly$, such that there is an isomorphism
\begin{equation}\label{eqn.monoidal_closure}
  \poly(p\otimes q,r) \iso \poly(p,\ihom{q,r})
\end{equation}
natural in $p,q,r$.
The closure operation is defined on $q,r$ as follows:
\begin{equation}\label{eqn.par_hom}
	\ihom{q,r} \coloneqq \prod_{j\in q(\1)}r\circ(q[j]\yon)
\end{equation}
Here $\circ$ denotes standard functor composition; informally, $r \circ (q[j]\yon)$ is the polynomial you get when you replace each appearance of $\yon$ in $r$ by $q[j]\yon$.
Composition, together with the unit $\yon$, is in fact yet another monoidal structure, as we will see in more depth in \cref{part.comon}.

Before we prove that the isomorphism \eqref{eqn.monoidal_closure} holds naturally, let us investigate the properties of the closure operation, starting with some simple examples.

\begin{exercise}
Calculate $\ihom{q,r}$ for $q,r\in\poly$ given as follows.
\begin{enumerate}
	\item $q\coloneqq \0$ and $r$ arbitrary.
	\item $q\coloneqq \1$ and $r$ arbitrary.
	\item $q\coloneqq\yon$ and $r$ arbitrary.
	\item $q\coloneqq A$ for $A\in\smset$ (constant) and $r$ arbitrary.
	\item $q\coloneqq A\yon$ for $A\in\smset$ (linear) and $r$ arbitrary.
	\item $q\coloneqq\yon^\2+\2\yon$ and $r\coloneqq\2\yon^\3+\3$.
\qedhere
\end{enumerate}
\begin{solution}
We compute $\ihom{q,r}$ for various values of $q, r \in \poly$ using \eqref{eqn.par_hom}.
\begin{enumerate}
    \item If $q \coloneqq \0$, then $q(\1) \iso \0$, so $\ihom{q,r}$ is an empty product.
    Hence $\ihom{q,r} \iso \1$.
    \item If $q \coloneqq \1$, then $q(\1) \iso \1$ and $q[1] \iso \0$, so $\ihom{q,r} \iso r \circ (\0\yon) \iso r(\0)$.
    \item If $q \coloneqq \yon$, then $q(\1) \iso \1$ and $q[1] \iso \1$, so $\ihom{q,r} \iso r \circ (\1\yon) \iso r$.
	\item If $q \coloneqq A$ for $A \in \smset$, then $q(\1) \iso A$ and $q[j] \iso \0$ for every $j \in A$, so $\ihom{q,r} \iso \prod_{j \in A} (r \circ (\0\yon)) \iso r(\0)^A$.
	\item If $q \coloneqq A\yon$ for $A\in\smset$, then $q(\1) \iso A$ and $q[j] \iso \1$ for every $j \in A$, so $\ihom{q,r} \iso \prod_{j \in A} (r \circ (\1\yon)) \iso r^A$.
	\item If $q\coloneqq\yon^\2+\2\yon$ and $r\coloneqq\2\yon^\3+\3$, then
	\begin{align*}
	    \ihom{q,r} &\iso (r \circ (\2\yon))(r \circ (\1\yon))^\2 \\
	    &\iso \left(\2(\2\yon)^\3 + \3\right)\left(\2\yon^\3 + \3\right)^\2 \\
	    &\iso \6\4\yon^\9 + \2\0\4\yon^\6 + \1\8\0\yon^\3 + \2\7 \\
	\end{align*}
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.sum_times_closure}
Show that for any polynomials $p_1,p_2,q$, we have an isomorphism
\[
\ihom{p_1 + p_2, q} \iso \ihom{p_1, q} \times \ihom{p_2, q}.
\]
\begin{solution}
We wish to show that for all $p_1, p_2, q \in \poly$, we have $\ihom{p_1 + p_2, q} \iso \ihom{p_1, q} \times \ihom{p_2, q}$.
By \eqref{eqn.par_hom},
\[
    \ihom{p_1 + p_2, q} \iso \left(\prod_{i \in p_1(\1)} q \circ (p_1[i]\yon)\right) \left(\prod_{i \in p_2(\1)} q \circ (p_2[i]\yon)\right) \iso \ihom{p_1, q} \times \ihom{p_2, q}.
\]
\end{solution}
\end{exercise}

\begin{exercise} \label{exc.par_hom_sum}
Show that there is an isomorphism
\begin{equation} \label{eqn.par_hom_sum}
\scalebox{1.3}{$\displaystyle
\ihom{q,r} \iso \sum_{f\colon q\to r}\yon^{\sum_{j\in q(\1)}r[f_\1(j)]}
$}
\end{equation}
where the sum is indexed by $f\in\poly(q,r)$.
\begin{solution}
We may compute
\begin{align*}
    \ihom{q, r} &\iso \prod_{j \in q(\1)} r \circ (q[j]\yon) \tag*{\eqref{eqn.par_hom}} \\
    &\iso \prod_{j \in q(\1)} \, \sum_{k \in r(\1)} (q[j]\yon)^{r[k]} \tag{Replacing each $\yon$ in $r$ by $q[j]\yon$} \\
    &\iso \sum_{f_\1 \colon q(\1) \to r(\1)} \, \prod_{j \in q(\1)} (q[j]\yon)^{r[f_\1(j)]} \tag*{\eqref{eqn.push_prod_sum_set_indep}} \\
    &\iso \sum_{f_\1 \colon q(\1) \to r(\1)} \, \left(\prod_{j \in q(\1)} q[j]^{r[f_\1(j)]} \right)\left(\prod_{j \in q(\1)} \yon^{r[f_\1(j)]} \right) \\
    &\iso \sum_{f_\1 \colon q(\1) \to r(\1)} \; \sum_{f^\sharp \in \prod_{j \in q(\1)} q[j]^{r[f_\1(j)]}} \yon^{\sum_{j \in q(\1)} r[f_\1(j)]} \\
    &\iso \sum_{f \colon q \to r} \yon^{\sum_{j \in q(\1)} r[f_\1(j)]}. \tag*{\eqref{eqn.main_formula}}
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise} \label{exc.dir_hom_p_yon_dir_p}
Verify that \eqref{eqn.dir_hom_p_yon_dir_p} holds.
\begin{solution}
We verify \eqref{eqn.dir_hom_p_yon_dir_p} as follows:
\begin{align*}
    \ihom{p, \yon} \otimes p
    &\iso
    \left(\sum_{f \colon p \to \yon} \yon^{\sum_{i \in p(\1)} \yon[f_1(i)]}\right) \otimes p
    \tag*{\eqref{eqn.par_hom_sum}} \\
    &\iso
    \sum_{f \in \Gamma(p)} \yon^{p(\1)} \otimes \sum_{i \in p(\1)} \yon^{p[i]} \\
    &\iso
    \sum_{f \in \Gamma(p)} \; \sum_{i \in p(\1)} \yon^{p(\1) \times p[i]}
    \tag*{\eqref{eqn.parallel_def}} \\
    &\iso
    \sum_{f \in \prod_{i \in p(\1)} p[i]} \; \sum_{i \in p(\1)} \yon^{p(\1) \times p[i]}.
    \tag*{\eqref{eqn.gamma_prod}}
\end{align*}
\end{solution}
\end{exercise}

\begin{example}\label{ex.parallel_dual}
For any $A\in\smset$ we have
\[
  \ihom{\yon^A,\yon} \iso A\yon
  \qqand
  \ihom{A\yon,\yon} \iso \yon^A.
\]
More generally, for any polynomial $p\in\poly$ we have
\begin{equation}\label{eqn.dir_dual}
  \ihom{p,\yon} \iso \Gamma(p)\yon^{p(\1)}.
\end{equation}
All these facts follow directly from \eqref{eqn.par_hom}.
\end{example}

\begin{exercise}
Verify the three facts above.
\begin{solution}
We have that
\[
    \ihom{\yon^A, \yon} \iso \prod_{j \in \yon^A(\1)} \yon \circ (\yon^A[j]\yon) \iso \prod_{j \in \1} A\yon \iso A\yon,
\]
that
\[
    \ihom{A\yon, \yon} \iso \prod_{j \in A\yon(\1)} \yon \circ ((A\yon)[j]\yon) \iso \prod_{j \in A} \yon \iso \yon^A,
\]
and that
\begin{align*}
    \ihom{p, \yon} &\iso \sum_{f \colon p \to \yon} \yon^{\sum_{i \in p(\1)} \yon[f_1(i)]} \tag{\cref{eqn.par_hom_sum}} \\
    &\iso \sum_{f \in \Gamma(p)} \yon^{\sum_{i \in p(\1)} \1} \\
    &\iso \Gamma(p)\yon^{p(\1)}.
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}
Show that for any $p\in\poly$, if there is an isomorphism $\ihom{\ihom{p,\yon},\yon} \iso p$, then $p$ is either linear $A\yon$ or representable $\yon^A$ for some $A$. Hint: first show that $p$ must be a monomial.
\begin{solution}
Given $p \in \poly$ and an isomorphism $\ihom{\ihom{p,\yon},\yon} \iso p$, we wish to show that $p$ is either linear or representable.
Applying \eqref{eqn.dir_dual} twice, we have that
\[
    p \iso \ihom{\ihom{p,\yon},\yon} \iso \Gamma\left(\Gamma(p)\yon^{p(\1)}\right)\yon^{\Gamma(p)}.
\]
We can compute the coefficient of $p$ via \eqref{eqn.gamma_prod} to obtain
\[
    \Gamma\left(\Gamma(p)\yon^{p(\1)}\right) \iso \prod_{\gamma \in \Gamma(p)} p(\1) \iso p(\1)^{\Gamma(p)}.
\]
Hence
\begin{equation} \label{eqn.p_as_gamma_monomial}
    p \iso p(\1)^{\Gamma(p)}\yon^{\Gamma(p)}.
\end{equation}
In particular, $p$ is a monomial, so we can write $p \coloneqq B\yon^A$ for some $A,B \in \smset$.
Then $p(\1) \iso B$ and \eqref{eqn.gamma_prod} tells us that $\Gamma(p) \iso A^B$.
It follows from \eqref{eqn.p_as_gamma_monomial} that $A \iso A^B$ and that $B \iso B^A$.

We conclude with some elementary set theory.
If either one of $A$ or $B$ were $\1$, then $p$ would be either linear or representable, and we would be done.
Meanwhile, if either one of $A$ or $B$ were $\0$, then the other would be $\1$, and we would again be done.
Otherwise, $|A|,|B| \geq 2$.
But by Cantor's theorem,
\[
    |B| < \big|\2^B\big| \leq \big|A^B\big| = |A| \qqand |A| < \big|\2^A\big| \leq \big|B^A\big| = |B|,
\]
a contradiction.
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.parallel_closure}\index{isomorphism!natural}
With $\ihom{-,-}$ as defined in \eqref{eqn.par_hom}, there is a natural isomorphism
\begin{equation}\label{eqn.poly_closure_brackets}
	\poly(p\otimes q,r)\cong\poly(p,\ihom{q,r}).
\end{equation}
\end{proposition}
\begin{proof}
We have the following chain of natural isomorphisms:
\begin{align*}
	\poly(p\otimes q,r)
	&\iso
	\poly\Big(\sum_{i\in p(\1)}\sum_{j\in q(\1)}\yon^{p[i]q[j]},r\Big) \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}\poly(\yon^{p[i]q[j]},r)
	\tag{Universal property of coproducts} \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}r(p[i]q[j])
	\tag{Yoneda lemma} \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}\poly(\yon^{p[i]},r\circ(q[j]\yon))
	\tag{Yoneda lemma} \\
	&\iso
	\poly\Big(\sum_{i\in p(\1)}\yon^{p[i]},\prod_{j\in q(\1)}r\circ(q[j]\yon)\Big)
	\tag{Universal property of (co)products} \\
	&\iso
	\poly(p,\ihom{q,r}).
\end{align*}
\end{proof}\index{Yoneda lemma}

\begin{exercise}\label{exc.poly_plug_1}
Show that for any $p,q$ we have an isomorphism of sets
\[
\poly(p,q) \iso \ihom{p,q}(\1).
\]
Hint: you can either use the formula \eqref{eqn.par_hom}, or just use
\eqref{eqn.poly_closure_brackets} with the Yoneda lemma and the fact that $\yon\otimes p \iso p$.
\begin{solution}
The isomorphism $\poly(p,q) \iso \ihom{p,q}(\1)$ follows directly from \cref{exc.par_hom_sum} when both sides are applied to $\1$.
Alternatively, we can apply \eqref{eqn.poly_closure_brackets}.
Since $p \iso \yon \otimes p$, we have that
\begin{align*}
    \poly(p, q) &\iso \poly(\yon \otimes p, q) \\
    &\iso \poly(\yon, \ihom{p,q}) \tag*{\eqref{eqn.poly_closure_brackets}} \\
    &\iso \ihom{p,q}(\1). \tag{Yoneda lemma}
\end{align*}
\end{solution}
\end{exercise}\index{Yoneda lemma}

The closure of $\otimes$ implies that for any $p,q\in\poly$, there is a canonical \emph{evaluation} lens
\begin{equation}\label{eqn.eval_parallel}
  \fun{eval}\colon \ihom{p,q}\otimes p\too q.
\end{equation}
As in any closed monoidal category, such an evaluation lens has the universal property that for any $r\in\poly$ and lens $f\colon p\otimes q\to r$, there is a unique lens $f'\colon p\to\ihom{q,r}$ such that the following diagram commutes:
\[
    \begin{tikzcd}
    	p\otimes q\ar[r, "f'\otimes q"]\ar[rr, bend right, "f"']&
    	{\ihom{q,r}}\otimes q\ar[r, "\fun{eval}"]&
    	r
    \end{tikzcd}
\]

\begin{exercise} \label{exc.eval_parallel}
Obtain the evaluation lens $\fun{eval}\colon \ihom{p,q}\otimes p\too q$ from \eqref{eqn.eval_parallel}.
\begin{solution}
To obtain the evaluation lens $\fun{eval}\colon \ihom{p,q}\otimes p\too q$, we consider the following special case of the isomorphism \eqref{eqn.poly_closure_brackets}:
\[
    \poly(\ihom{p,q} \otimes p, q) \iso \poly(\ihom{p,q}, \ihom{p,q}).
\]
Then the evaluation lens is the lens corresponding to the identity lens on $\ihom{p,q}$ under the above isomorphism.
To recover this lens, we can start from the identity lens on $\ihom{p,q}$ and work our way along a chain of natural isomorphisms from $\poly(\ihom{p,q}, \ihom{p,q})$ until we get to $\poly(\ihom{p,q} \otimes p, q)$.
To start, \cref{exc.par_hom_sum} implies that
\begin{align*}\index{isomorphism!natural}
    \poly(\ihom{p,q}, \ihom{p,q})
    &\iso
    \poly\left(\sum_{f \colon p \to q} \, \prod_{i' \in p(\1)} \yon^{q[f_\1(i')]}, \prod_{i \in p(\1)} \, \sum_{j \in q(\1)} (p[i]\yon)^{q[j]}\right) \\
    &\iso
    \prod_{f \colon p \to q} \, \prod_{i \in p(\1)} \poly\left(\prod_{i' \in p(\1)} \yon^{q[f_\1(i')]}, \sum_{j \in q(\1)} (p[i]\yon)^{q[j]}\right),
\end{align*}
where the second isomorphism follows from the universal properties of products and coproducts.
In particular, under this isomorphism, the identity lens on $\ihom{p,q}$ corresponds to a collection of lenses, namely for each $f \colon p \to q$ and each $i \in p(\1)$ the composite
\[
    \prod_{i' \in p(\1)} \yon^{q[f_\1(i')]} \to \yon^{q[f_\1i]} \to \sum_{g \colon q[f_\1i] \to p[i]} \yon^{q[f_\1i]} \iso (p[i]\yon)^{q[f_\1i]} \to \sum_{j \in q(\1)} (p[i]\yon)^{q[j]}
\]
of the canonical projection with index $i' = i$, the canonical inclusion with index $g = f^\sharp_i$, and the canonical inclusion with index $j = f_\1i$.
On positions, this lens picks out the position of $\sum_{j \in q(\1)} (p[i]\yon)^{q[j]}$ corresponding to $j = f_\1i \in q(\1)$ and $f^\sharp_i \colon q[f_\1i] \to p[i]$; on directions, the lens is the canonical inclusion $q[f_\1i] \to \sum_{i' \in p(\1)} q[f_\1(i')]$ with index $i' = i$.

We can reinterpret each of these lenses as a lens
\[
    \yon^{p[i] \times \sum_{i' \in p(\1)} q[f_\1(i')]} \to \sum_{j \in q(\1)} \yon^{q[j]} \iso q
\]
that, on positions, picks out the position $f_\1i \in q(\1)$ of $q$ and, on directions, is the map $q[f_\1i] \to p[i] \times \sum_{i' \in p(\1)} q[f_\1(i')]$ induced by the universal property of products applied to the map $f^\sharp_i \colon q[f_\1i] \to p[i]$ and the inclusion $q[f_\1i] \to \sum_{i' \in p(\1)} q[f_\1(i')]$.
Then by the universal property of coproducts, this collection of lenses induces a single lens $\fun{eval} \colon \ihom{p, q} \otimes p \to q$ that sends each position $f \colon p \to q$ of $\ihom{p,q}$ and position $i \in p(\1)$ of $p$ to the position $f_\1i$ of $q$, with the same behavior on directions as the corresponding lens described previously.
\end{solution}
\end{exercise}

\begin{exercise}
\begin{enumerate}
	\item For any set $S$, obtain the lens $S\yon^S\to\yon$ whose on-directions  is the identity on $S$ using eval and \cref{ex.parallel_dual}.
	\item Show that four lenses in \eqref{eqn.snapped_maps} from \cref{ex.bonds_break}, written equivalently as
	\begin{equation} \label{eqn.snapped_maps2}
	\arraycolsep=1.4pt
    \begin{array}{lll}
    	\kappa_{11}\colon&~ F\yon^{FF}\otimes F\yon^{FF}&\to\yon^F\otimes\yon^F\\
    	\kappa_{12}\colon&~ F\yon^{FF}\otimes \yon^F&\to\yon^F\otimes\yon^F\\
    	\kappa_{21}\colon&~ \yon^F\otimes F\yon^{FF}&\to\yon^F\otimes\yon^F\\
    	\kappa_{22}\colon&~ \yon^F\otimes\yon^F&\to\yon^F\otimes\yon^F,
    \end{array}
	\end{equation}
	can be obtained by taking the parallel product of identity lenses and evaluation lenses.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item Given a set $S$, we wish to obtain the lens $S\yon^S \to \yon$ whose on-directions function is the identity by using eval and \cref{ex.parallel_dual}.
    The example shows that
    \[
        \ihom{S\yon, \yon} \otimes (S\yon) \iso \yon^S \otimes (S\yon) \iso S\yon^S,
    \]
    so by setting $p \coloneqq S\yon$ and $q \coloneqq S$ in \eqref{eqn.eval_parallel}, we obtain an evaluation lens $\fun{eval} \colon S\yon^S \to \yon$.
    By the solution to \cref{exc.eval_parallel}, given a position $s \in S$ of $S\yon^S$, the evaluation lens on directions is the map $\1 \to S$ that picks out $s$.
    In other words, it is indeed the identity on directions.
    \item We wish to write the four lenses in \eqref{eqn.snapped_maps2} from \cref{ex.bonds_break} as the parallel product of identity lenses and evaluation lenses.
    By the solution to \cref{exc.eval_parallel}, the evaluation lens $\ihom{F,\yon^F}\otimes F\to\yon^F$
    is a lens from
    \[
        \ihom{F,\yon^F}\otimes F\iso F\left(\sum_{f\colon F\to\yon^F}\prod_{i\in F}\yon^F\right)\iso F\yon^{FF}
    \]
    to $\yon^F$ that is unique on positions and has the on-directions function $FF\to FF$ given by the identity.
    Then we can verify that $\kappa_{11}$ is equivalent to the parallel product of this evaluation lens with itself.
    We can define $\kappa_{12}$ and $\kappa_{21}$ to be the parallel product of this evaluation lens with the identity on $\yon^F$, while $\kappa_{22}$ is the parallel product of the identity on $\yon^F$ with itself.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{example}[Modeling your environment without knowing what it is]\index{environment!universal}\index{interaction pattern}
Let's imagine a robot whose interface is an arbitrary polynomial $p$. Let's imagine it is part of an interaction pattern
\[
	f\colon (q_1\otimes\cdots\otimes q_n)\otimes p\to r
\]
with some other robots whose interfaces are $q_1,\ldots,q_n$; let $q\coloneqq(q_1\otimes\cdots\otimes q_n)$. The interaction pattern induces a lens $f'\colon q\to \ihom{p,r}$ such that the original system $f$ factors through the evaluation $\ihom{p,r}\otimes p\to r$.

In other words, $\ihom{p,r}$ holds within it all of the possible ways $p$ can interact with other systems when they are all wrapped in $r$.
For example, in the case of $r\coloneqq\yon$, note that $\ihom{p,\yon}\cong\prod_{i\in p(\1)}p[i]\yon$.
That is, for each $p$-position it produces a direction there, which is just what $p$ needs as input in a closed system.

\index{dynamics}\index{environment}

Now suppose we were to populate the interface $p$ with dynamics, a lens $S\yon^S\to p$. One could aim to choose a set $S$ along with an interesting map $g\colon S\to\poly(p,r)$. Then each state $s$ would include a guess $g(s)$ about the state of its environment. This is not the real environment $q$, but just the environment as it affects $p$, namely $\ihom{p,r}$. The robot's states model environmental conditions.
\end{example}

\begin{example}[Chu $\&$]\index{Chu space}
Suppose we have polynomials $p_1,p_2,q_1,q_2,r\in\poly$ and lenses
\[
	\varphi_1\colon p_1\otimes q_1\to r
	\qqand
	\varphi_2\colon p_2\otimes q_2\to r.
\]
One might call these ``$r$-Chu spaces.'' One operation you can do with these as Chu spaces is to return something denoted $\varphi_1\&\varphi_2$, or ``$\varphi_1$ \emph{with} $\varphi_2$'' of the following type:
\[
\varphi_1\&\varphi_2\colon (p_1\times p_2)\otimes (q_1+q_2)\to r
\]
Suppose we are given a position in $p_1$ and a position in $p_2$. Then given a position in either $q_1$ or $q_2$, one evaluates either $\varphi_1$ or $\varphi_2$ respectively to get a position in $r$; given a direction there, one returns the corresponding direction in $q_1$ or $q_2$ respectively, as well as a direction in $p_1\times p_2$ which is either a direction in $p_1$ or in $p_2$.

This sounds complicated, but it can be done formally, once we have monoidal closure. We first rearrange both $\varphi_1,\varphi_2$ to be $p$-centric, using monoidal currying:
\[
\psi_1\colon p_1\to \ihom{q_1,r}
\qqand
\psi_2\colon p_2\to \ihom{q_2,r}
\]
Now we multiply to get $\psi_1\times\psi_2\colon p_1\times p_2\to\ihom{q_1,r}\times\ihom{q_2,r}$. Then we apply \cref{exc.sum_times_closure} to see that $\ihom{q_1,r}\times\ihom{q_2,r}\cong\ihom{q_1+q_2,r}$, and finally monoidal-uncurry to obtain $(p_1\times p_2)\otimes(q_1+q_2)\to r$ as desired.
\end{example}

%-------- Section --------%
\section{Summary and further reading}

In this chapter we explained how discrete dynamical systems can be expressed as certain lenses between polynomial functors. For example, a Moore machine has an input set $A$, an output set $B$, a set of states $S$, a return function $S\to B$, and an update function $A\times S\to S$. All this is captured in a depedent lens
\[S\yon^S\to B\yon^A.\]
We discussed a generalization $S\yon^S\to p$, where the output is an arbitrary polynomial $p\in\poly$. We also talked about how to wire machines in parallel by using the parallel product $\otimes$ and how to add wrapper interfaces by composing with lenses $p\to q$.

Throughout the chapter we gave quite a few different examples. For example, we discussed how every function $A\to B$ counts as a memoryless dynamical system. In fact, it was shown in \cite{beurier2019memoryless} that every dynamical system can be obtained by wiring together memoryless ones. We discussed examples such as file-readers, moving robots, colliding particles, companies that change their suppliers, materials that break when too much force is applied, etc.

For further reading on the mathematics of Moore machines, see \cite{conway2012regular}. For more on mode-dependent interaction, see \cite{spivak2017nesting}. For a similar and complementary categorical approach to dynamical systems, we recommend David Jaz Myers' \emph{Categorical Systems Theory} book, currently in draft form here: \url{http://davidjaz.com/Papers/DynamicalBook.pdf}.

\index{interaction!mode dependent}

%-------- Section --------%
\section{Exercise solutions}
\Closesolutionfile{solutions}
{\footnotesize
	\input{solution-file4}}

\Opensolutionfile{solutions}[solution-file5]

%------------ Chapter ------------%
\chapter{More categorical properties of polynomials} \label{ch.poly.bonus}

The category $\poly$ has very useful formal properties, including completion under colimits and limits, various adjunctions with $\smset$, factorization systems, and so on. Most of the following material is not necessary for the development of our main story, but we collect it here for reference. The reader can skip directly to \cref{part.comon} if so inclined and check back here when needed. Better yet might be to gently leaf through this chapter, to see how well-behaved and versatile the category $\poly$ is.

%-------- Section --------%
\section{Special polynomials and adjunctions} \label{sec.poly.bonus.adj}


There are a few special classes of polynomials that are worth discussing:
\begin{enumerate}[label=\alph*)]
	\item constant polynomials $\0,\1,\2,A$;
	\item linear polynomials $\0,\yon, \2\yon, A\yon$;
	\item representable (or pure power) polynomials $\1, \yon, \yon^\2, \yon^A$; and
	\item monomials $\0, A, \yon, \2\yon^\3, B\yon^A$.
\end{enumerate}
The first two classes, constant and linear polynomials, are interesting because they both put a copy of $\smset$ inside $\poly$, as we'll see in \cref{prop.ff_const_set_to_poly,prop.ff_lin_set_to_poly}.
The third puts a copy of $\smset\op$ inside $\poly$: it is the Yoneda embedding that we saw way back in \cref{exc.finish_proof_yoneda}.
Finally, the fourth puts a copy of bimorphic lenses inside $\poly$, as we saw in \cref{subsec.poly.cat.morph.bimorphic-lens}.

\index{Yoneda embedding|see{Yoneda lemma}}

\begin{exercise}
Which of the four classes above are closed under
\begin{enumerate}
	\item the cocartesian monoidal structure $(\0,+)$ (i.e.\ addition)?
	\item the cartesian monoidal structure $(\1,\times)$ (i.e.\ multiplication)?
	\item the parallel monoidal structure $(\yon,\otimes)$ (i.e.\ taking the parallel product)?
	\item composition of polynomials $p\circ q$? (We have not discussed this yet, so feel free to skip it.)
\qedhere
\end{enumerate}
\begin{solution}
Here $A, B, A', B' \in \smset$.
\begin{enumerate}
    \item We determine whether various classes of polynomials are closed under addition.
    \begin{enumerate}
        \item Constant polynomials are closed under addition: given constants $A, B$, their sum $A + B$ is also a constant polynomial.
        \item Linear polynomials are closed under addition: given linear polynomials $A\yon, B\yon$, their sum $A\yon + B\yon \iso (A + B)\yon$ is also a linear polynomial.
        \item Representable polynomials are \emph{not} closed under addition: for example, $\yon$ is a representable polynomial, but the sum of $\yon$ with itself, $\2\yon$, is not.
        \item Monomials are \emph{not} closed under addition: for example, $\yon$ and $\2\yon^\3$ are monomials, but their sum $\yon + \2\yon^\3$ is not.
    \end{enumerate}
    \item We determine whether various classes of polynomials are closed under multiplication.
    The results below follow from \cref{exc.general_poly_times} \cref{exc.general_poly_times.monomial}.
    \begin{enumerate}
        \item Constant polynomials are closed under multiplication: given constants $A, B$, their product $AB$ is also a constant polynomial.
        \item Linear polynomials are \emph{not} closed under multiplication: for example, $\yon$ and $\2\yon$ are linear polynomials, but their product $\2\yon^\2$ is not.
        \item Representable polynomials are closed under multiplication: given representables $\yon^A, \yon^B$, their product $\yon^{A+B}$ is also a representable polynomial.
        \item Monomials are closed under multiplication: given monomials $B\yon^A, B'\yon^{A'}$, their product $BB'\yon^{A+A'}$ is also a monomial.
    \end{enumerate}
    \item We determine whether various classes of polynomials are closed under taking parallel products.
    The results below follow from \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial}.
    \begin{enumerate}
        \item Constant polynomials are closed under taking parallel products: given constants $A, B$, their parallel product $AB$ is also a constant polynomial.
        \item Linear polynomials are closed under taking parallel products: given linear polynomials $A\yon, B\yon$, their parallel product $AB\yon$ is also a linear polynomial.
        \item Representable polynomials are closed under taking parallel products: given representables $\yon^A, \yon^B$, their parallel product $\yon^{AB}$ is also a representable polynomial.
        \item Monomials are closed under taking parallel products: given monomials $B\yon^A, B'\yon^{A'}$, their parallel product $BB'\yon^{AA'}$ is also a monomial.
    \end{enumerate}
    \item We determine whether various classes of polynomials are closed under composition. (Recall that we can think of computing the composite $p \circ q$ of $p, q \in \poly$ as replacing each appearance of $\yon$ in $p$ with $q$.)
    \begin{enumerate}
        \item Constant polynomials are closed under composition: given constants $A, B$, their composite $A \circ B \iso A$ is also a constant polynomial.
        \item Linear polynomials are closed under composition: given linear polynomials $A\yon, B\yon$, their composite $A\yon \circ B\yon \iso A(B\yon) \iso AB\yon$ is also a linear polynomial.
        \item Representable polynomials are closed under composition: given representables $\yon^A, \yon^B$, their composite $\yon^A \circ \yon^B \iso (\yon^B)^A \iso \yon^{BA}$ is also a representable polynomial.
        \item Monomials are closed under taking parallel products: given monomials $B\yon^A, B'\yon^{A'}$, their composite $B\yon^A \circ B'\yon^{A'} \iso B(B'\yon^{A'})^A \iso BB'^A\yon^{A'A}$ is also a monomial.
    \end{enumerate}
\end{enumerate}
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.ff_const_set_to_poly}
There is a fully faithful functor $\smset\to\poly$ sending $A\mapsto A\yon^\0=A$.
\end{proposition}
\begin{proof}
By \eqref{eqn.colax_poly_map}, a lens $f\colon A\yon^\0\to B\yon^\0$ consists of a function $f\colon A\to B$ and, for each $a\in A$, a function $\0\to\0$. There is only one function $\0\to\0$, so $f$ can be identified with just a function between sets $A\to B$.
\end{proof}

\begin{proposition}\label{prop.ff_lin_set_to_poly}
There is a fully faithful functor $\smset\to\poly$ sending $A\mapsto A\yon$.
\end{proposition}
\begin{proof}
By \eqref{eqn.colax_poly_map}, a lens $f\colon A\yon^\1\to B\yon^\1$ consists of a function $f\colon A\to B$ and for each $a\in A$ a function $\1\to\1$. There is only one function $\1\to\1$, so $f$ can be identified with just a function between sets $A\to B$.
\end{proof}

\index{adjunction!between $\smset$ and $\poly$}
\index{set!as constant polynomial}

\begin{theorem}\label{thm.adjoint_quadruple}
$\poly$ has an adjoint quadruple with $\smset$:
\begin{equation}\label{eqn.adjoints_galore}
\begin{tikzcd}[column sep=60pt, background color=theoremcolor]
  \smset
  	\ar[r, shift left=7pt, "A" description]
		\ar[r, shift left=-21pt, "A\yon"']&
  \poly
  	\ar[l, shift right=21pt, "p(\0)"']
  	\ar[l, shift right=-7pt, "p(\1)" description]
	\ar[l, phantom, "\scriptstyle\Leftarrow"]
	\ar[l, phantom, shift left=14pt, "\scriptstyle\Rightarrow"]
	\ar[l, phantom, shift right=14pt, "\scriptstyle\Rightarrow"]
\end{tikzcd}
\end{equation}
where the functors have been labeled by where they send $A\in\smset$ and $p\in \poly$.

Both rightward functors $\smset\to\poly$ are fully faithful.
\end{theorem}
\begin{proof}\index{Yoneda lemma}
For any set $A$, there is a functor $\poly\to\smset$ given by sending $p$ to $p(A)$; by the Yoneda lemma, it is the functor $\poly(\yon^A,-)$. This, together with \cref{prop.ff_const_set_to_poly,prop.ff_lin_set_to_poly}, gives us the four functors and the fact that the two rightward functors are fully faithful. It remains to provide the following three natural isomorphisms:\index{isomorphism!natural}
\[
\poly(A,p)\iso\smset(A,p(\0))\qquad
\poly(p,A)\iso\smset(p(\1),A)\qquad
\poly(A\yon,p)\iso\smset(A,p(\1)).
\]
All three come from our formula \eqref{eqn.main_formula} for computing general hom-sets in $\poly$; we leave the details to the reader in \cref{exc.adjoint_quadruple}.
\end{proof}

\begin{exercise}\label{exc.adjoint_quadruple}
Here we prove the remainder of \cref{thm.adjoint_quadruple} using \eqref{eqn.main_formula}:
\begin{enumerate}\index{isomorphism!natural}
	\item Provide a natural isomorphism $\poly(A,p)\iso\smset(A,p(\0))$.
	\item \label{exc.adjoint_quadruple.pos_const} Provide a natural isomorphism $\poly(p,A)\iso\smset(p(\1),A)$.
	\item \label{exc.adjoint_quadruple.linear_pos} Provide a natural isomorphism $\poly(A\yon,p)\iso\smset(A,p(\1))$.
\qedhere
\end{enumerate}
\begin{solution}
We complete the proof of \cref{thm.adjoint_quadruple} by exhibiting three natural isomorphisms, all special cases of \eqref{eqn.main_formula}, as follows.
\begin{enumerate}
    \item By \eqref{eqn.main_formula}, we have the natural isomorphism
    \[
        \poly(A, p) % \iso \prod_{a \in A} \sum_{i \in p(\1)} A[a]^{p[i]}
        \iso \prod_{a \in A} \sum_{i \in p(\1)} \0^{p[i]}.
    \]
    As $\0^{p[i]}$ is $\1$ if $p[i] \iso \0$ and $\0$ otherwise, it follows that
    \[
        \poly(A, p) \iso \prod_{a \in A} \{ i \in p(\1) \mid p[i] \iso \0 \} \iso \prod_{a \in A} p(\0) \iso \smset(A, p(0)).
    \]
    \item By \eqref{eqn.main_formula}, we have the natural isomorphism
    \begin{align*}
        \poly(p, A) %&\iso \prod_{i \in p(\1)} \sum_{a \in A} p[i]^{A[a]} \\
        &\iso \prod_{i \in p(\1)} \sum_{a \in A} p[i]^\0 \\
        &\iso \prod_{i \in p(\1)} \sum_{a \in A} \1 \\
        &\iso \prod_{i \in p(\1)} A \\
        &\iso \smset(p(\1), A).
    \end{align*}
    \item By \eqref{eqn.main_formula}, we have the natural isomorphism
    \begin{align*}
        \poly(A\yon, p) %&\iso \prod_{a \in A} \sum_{i \in p(\1)} (A\yon)[a]^{p[i]} \\
        &\iso \prod_{a \in A} \sum_{i \in p(\1)} \1^{p[i]} \\
        &\iso \prod_{a \in A} \sum_{i \in p(\1)} \1 \\
        &\iso \prod_{a \in A} p(\1) \\
        &\iso \smset(A, p(\1)).
    \end{align*}
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.positions_maps_yon}
Show that for any polynomial $p$, its set $p(\1)$ of positions is in bijection with the set of functions $\yon\to p$.
\begin{solution}
Given $p \in \poly$, we wish to show that $p(\1)$ is in bijection with the set of functions $\yon \to p$.
In fact, this follows directly from the Yoneda lemma, but we can also invoke the isomorphism from \cref{exc.adjoint_quadruple} \cref{exc.adjoint_quadruple.linear_pos} with $A \coloneqq \1$ to observe that
\[
    p(1) \iso \smset(\1, p(\1)) \iso \poly(\yon, p).
\]
\end{solution}
\end{exercise}\index{Yoneda lemma}

In \cref{thm.adjoint_quadruple} we see that $p\mapsto p(\0)$ and $p\mapsto p(\1)$ have left adjoints. This is true more generally for any set $A$ in place of $\0$ and $\1$, as we show in \cref{cor.substituting_adj}. However, the fact that $p\mapsto p(\1)$ is itself the left adjoint of the left adjoint of $p\mapsto p(\0)$---and hence that we have the \emph{quadruple} of adjunctions in \eqref{eqn.adjoints_galore}---is special to $A=\0,\1$.

We also have a copower-hom-power two-variable adjunction between $\poly,\smset,$ and $\poly$.

\index{adjunction!two-variable}

\begin{proposition}\label{prop.two_var_adj}
There is a two-variable adjunction between $\poly$, $\smset$, and $\poly$:
\begin{equation}\label{eqn.two_var_adj}
\poly(Ap,q) \iso \smset(A,\poly(p,q)) \iso \poly(p,q^A).
\end{equation}
\end{proposition}
\begin{proof}
Since $Ap$ is the $A$-fold coproduct of $p$ and $q^A$ is the $A$-fold product of $q$, the universal properties of coproducts and products give natural isomorphisms\index{isomorphism!natural}
\[\poly(Ap,q)\cong\prod_{a\in A}\poly(p,q)\cong\poly(p,q^A).\]
The middle set is naturally isomorphic to $\smset(A,\poly(p,q))$, completing the proof.
\end{proof}\index{coproduct!indexed}

Replacing $p$ with $\yon^B$ in \eqref{eqn.two_var_adj}, we obtain the following using the Yoneda lemma.

\begin{corollary}\label{cor.substituting_adj}
For any set $B$ there is an adjunction
\[
\adj{\smset}{A\yon^B}{q(B)}{\poly}
\]
where the functors are labeled by where they send $q\in\poly$ and $A\in\smset$.
\end{corollary}\index{Yoneda lemma}


\begin{exercise}
Prove \cref{cor.substituting_adj} from \cref{prop.two_var_adj}.
\begin{solution}
To prove \cref{cor.substituting_adj}, it suffices to exhibit a natural isomorphism\index{isomorphism!natural}
\[
    \poly(A\yon^B, q) \iso \smset(A, q(B)).
\]
Replacing $p$ with $\yon^B$ in \eqref{eqn.two_var_adj} from \cref{prop.two_var_adj}, we obtain the natural isomorphism
\[
    \poly(A\yon^B, q) \iso \smset(A, \poly(\yon^B, q)).
\]
By the Yoneda lemma, $\poly(\yon^B, q)$ is naturally isomorphic to $q(B)$, yielding the desired result.
\end{solution}
\end{exercise}

\index{Yoneda lemma}
\index{adjunction!between $\smset\op$ and $\poly$}\index{global sections}

\begin{proposition}\label{prop.yoneda_left_adjoint}
The Yoneda embedding $A\mapsto \yon^A$ has a left adjoint
\[
\adjr{\smset\op}{\yon^-}{\Gamma}{\poly}
\]
where $\Gamma(p) \coloneqq \poly(p, \yon) \iso \prod_{i\in p(\1)}p[i]$, as in \eqref{eqn.gamma_def} and \eqref{eqn.gamma_prod}.
That is, there is a natural isomorphism\index{isomorphism!natural}
\begin{equation} \label{eqn.yoneda_left_adjoint}
    \poly(p, \yon^A) \iso \smset(A, \Gamma(p)).
\end{equation}
\end{proposition}
\begin{proof}
By \eqref{eqn.main_formula}, we have the natural isomorphism
\[
    \poly(p, \yon^A) \iso \prod_{i \in p(\1)} p[i]^A,
\]
which in turn is naturally isomorphic to $\smset(A, \Gamma(p))$ by \eqref{eqn.gamma_prod}.
\end{proof}

\begin{exercise}\index{global sections}
  Prove \cref{prop.yoneda_left_adjoint} from \cref{prop.two_var_adj}.
  \begin{solution}
    Replacing $q$ with $\yon$ in the second isomorphism in \eqref{eqn.two_var_adj} from \cref{prop.two_var_adj}, we obtain the natural isomorphism\index{isomorphism!natural}
    \[
    \smset(A,\poly(p,\yon)) \iso \poly(p,\yon^A).
    \]
    As $\Gamma(p)=\poly(p,\yon)$, yields the desired result.
  \end{solution}
\end{exercise}

% Previously an exercise; but already basically done
% Show that $\Gamma(p)\cong\ihom{p,\yon}(\1)$ where $\ihom{-,-}$ is as in \cref{prop.parallel_closure}.

\index{monomial!principal}

\begin{corollary}[Principal monomial]\label{cor.principal_monomial}
There is an adjunction
\[
    \adj{\poly}{{(p(\1),\Gamma(p))}}{A\yon^B}{\smset\times\smset\op}
\]
where the functors are labeled by where they send $p\in\poly$ and $(A,B)\in\smset\times\smset\op$.
That is, there is a natural isomorphism\index{isomorphism!natural}
\begin{equation} \label{eqn.principal_monomial}
    \poly(p, A\yon^B) \iso \smset(p(1), A) \times \smset(B, \Gamma(p)).
\end{equation}
\end{corollary}
\begin{proof}
By the universal property of the product of $A$ and $\yon^B$, we have a natural isomorphism
\[
    \poly(p, A\yon^B) \iso \poly(p, A) \times \poly(p, \yon^B).
\]
Then the desired natural isomorphism follows from \cref{exc.adjoint_quadruple} \cref{exc.adjoint_quadruple.pos_const} and \eqref{eqn.yoneda_left_adjoint}.
\end{proof}

\begin{exercise}\index{global sections}
Use \eqref{eqn.principal_monomial} together with \eqref{eqn.dir_dual} and \eqref{eqn.poly_closure_brackets} to find an alternative proof for \cref{prop.situations2}, i.e.\ that there is an isomorphism
\[
    \Gamma(p\otimes q) \iso \smset\big(q(\1),\Gamma(p)\big) \times \smset\big(p(\1),\Gamma(q)\big).
\]
for any $p,q\in\poly$.
\begin{solution}
% The universal properties of the adjunctions
% \[
%       \adj[40pt]{\poly}{{(-(\1),\Gamma(-))}}{-\yon^-}{\smset\times\smset\op}
%       \qqand
%       \adj{\poly}{-\otimes q}{{\ihom{q,-}}}{\poly}
% \]
% from \cref{cor.principal_monomial,prop.parallel_closure} together with the isomorphism $\ihom{p,\yon} \iso \Gamma(p)\yon^{p(\1)}$ from \eqref{eqn.dir_dual} give
We have the following chain of natural isomorphisms involving global sections: \index{global sections}\index{isomorphism!natural}
\begin{align*}
	\Gamma(p \otimes q) &=
	\poly(p \otimes q,\yon)
	\tag*{\eqref{eqn.gamma_def}} \\
	&\iso
	\poly(p, \ihom{q,\yon})
	\tag*{\eqref{eqn.poly_closure_brackets}} \\
	&\iso
	\poly(p, \Gamma(q)\yon^{q(\1)})
	\tag*{\eqref{eqn.dir_dual}} \\
% 	&\iso
% 	(\smset\times\smset\op)\Big(\big((p(\1),\Gamma(p)\big),\big(\Gamma(q),q(\1)\big)\Big) \\
	&\iso
	\smset\big(p(\1),\Gamma(q)\big) \times \smset\big(q(\1),\Gamma(p)\big).
	\tag*{\eqref{eqn.principal_monomial}}
\qedhere
\end{align*}
\end{solution}
\end{exercise}

%-------- Section --------%
\section{Epi-mono factorization of lenses}

\index{factorization system!epi-mono}

\index{lens!monomorphism}

\begin{proposition}\label{prop.monics_in_poly}
Let $f \colon p \to q$ be a lens in $\poly$. It is a monomorphism if and only if the on-positions function $f_\1 \colon p(\1) \to q(\1)$ is a monomorphism in $\smset$ and, for each $i \in p(\1)$, the on-directions function $f^\sharp_i \colon q[f_\1i]\to p[i]$ is an epimorphism in $\smset$.
\end{proposition}
\begin{proof}
To prove the forward direction, suppose that $f$ is a monomorphism.
Since $p\mapsto p(\1)$ is a right adjoint (\cref{thm.adjoint_quadruple}), it preserves monomorphisms, so the on-positions function $f_\1$ is also a monomorphism.

We now need to show that for any $i\in p(\1)$, the on-directions function $f^\sharp_i \colon q[f_\1i] \to p[i]$ is an epimorphism.
Suppose we are given a set $A$ and a pair of functions $g^\sharp,h^\sharp\colon p[i]\tto A$ with $f^\sharp_i \then g^\sharp = f^\sharp_i \then h^\sharp$.
Then there exist lenses $g,h \colon \yon^A \tto p$ whose on-positions functions both pick out $i$ and whose on-directions functions are $g^\sharp$ and $h^\sharp$, so that $g \then f = h \then f$.
As $f$ is a monomorphism, $g = h$; in particular, their on-directions functions $g^\sharp$ and $h^\sharp$ are equal, as desired.

Conversely, suppose that $f_\1$ is a monomorphism and that, for each $i\in p(\1)$, the function $f^\sharp_i$ is an epimorphism.
Let $r$ be a polynomial and $g,h\colon r\tto p$ be two lenses such that $g \then f = h \then f$.
Then $g_\1 \then f_\1 = h_\1 \then f_\1$, which implies $g_\1 = h_\1$; we'll consider $g_\1$ the default representation.
We also have that $f^\sharp_{g_\1k} \then g^\sharp_k = f^\sharp_{g_\1k} \then h^\sharp_k$ for any $k \in r(\1)$. But $f^\sharp_{g_\1k}$ is an epimorphism, so in fact $g^\sharp_k = h^\sharp_k$, as desired.
\end{proof}

\begin{example}\label{ex.clock_in_N}\index{clock}
Choose a finite nonempty set $\ord{k}$ for $1\leq k\in\nn$, e.g.\ $\ord{k}=\1\2$. There is a monomorphism
\[
f \colon\ord{k}\yon^{\ord{k}}\to\nn\yon^\nn
\]
such that the trajectory ``going around and around the $k$-clock'' comes from the usual counting trajectory $\nn\yon^\nn\to\yon$ from \cref{ex.counting_trajectory}.

On positions, we have $f_\1i=i$ for all $i \in \ord{k}$. On directions, for any $i \in \ord{k}$, we have $f^\sharp_i(n) = n \mod k$ for all $n \in \nn$.
\end{example}

\begin{exercise}
In \cref{ex.clock_in_N}, we gave a lens $\1\2\yon^{\1\2}\to\nn\yon^\nn$. This allows us to turn any dynamical system with $\nn$-many states into a dynamical system with 12 states, while keeping the same interface---say, $p$.

Explain how the behavior of the new system $\1\2\yon^{\1\2}\to p$ would be seen to relate to the behavior of the old system $\nn\yon^\nn\to p$.
\begin{solution}
We are given a monomorphism $f \colon \1\2\yon^{\1\2} \to \nn\yon^\nn$ from \cref{ex.clock_in_N}.
Let $g \colon \nn\yon^\nn \to p$ be a dynamical system with return function $g_\1 \colon \nn \to p(\1)$ and update functions $g^\sharp_n \colon p[g_\1(n)] \to \nn$ for each state $n \in \nn$.
Then the new composite dynamical system $h \coloneqq f \then g$ has a return function $h_\1 \colon \1\2 \to p(\1)$ which sends each state $i \in \1\2$ to the output $h_\1i = g_\1f_\1i = g_\1i$, the same output that the original system returned in the state $i \in \nn$.
Meanwhile, the update function for each state $i \in \1\2$ is a function $h^\sharp_i \colon p[g_\1i] \to \1\2$ which, given an input $a \in p[g_\1i]$, updates the state from $i$ to $h^\sharp_ia = f^\sharp_{g_\1i}(g^\sharp_ia) = g^\sharp_ia \mod 12$, which is where the original system would have taken the same state to, but reduced modulo 12.
In other words, the new system behaves like the old system but with only the states in $\1\2 \ss \nn$ retained, and on any input that would have caused the old system to move to a state outside of $\1\2$, the new system moves to the equivalent state (modulo 12) within $\1\2$ instead.
\end{solution}
\end{exercise}

\index{lens!epimorphism}

\begin{proposition}\label{prop.epis_in_poly}
Let $f \colon p \to q$ be a lens in $\poly$. It is an epimorphism if and only if the function $f_\1 \colon p(\1) \to q(\1)$ is an epimorphism in $\smset$ and, for each $j\in q(\1)$, the induced function
\[
    f^\flat_j \colon q[j] \to \prod_{\substack{i\in p(\1), \\ f_\1i=j}} p[i]
\]
from \eqref{eqn.useful_misc472} is a monomorphism.
\end{proposition}
\begin{proof}
To prove the forward direction, suppose that $f$ is an epimorphism. Since $p \mapsto p(\1)$ is a left adjoint (\cref{thm.adjoint_quadruple}), it preserves epimorphisms, so the on-positions function $f_\1$ is also a epimorphism.

We now need to show that for any $j\in q(\1)$, the induced function $f^\flat_j$ is a monomorphism.
Suppose we are given a set $A$ and a pair of functions $g',h'\colon A\tto q[j]$ with $g' \then f^\flat_j = h' \then f^\flat_j$.
They can be identified with lenses $g,h\colon q\tto \yon^A+\1$, which send the $j$-component to the first component, $\yon^A$, and send all other component to the second component, $\1$. It is easy to check that $fg=fh$, hence $g=h$, and hence $g^\sharp=h^\sharp$ as desired.

Then we can construct lenses $g,h \colon q \tto \yon^A+\1$ whose on-positions functions both send $j$ to the first position, corresponding to $\yon^A$, and all other positions to the second position, corresponding to $\1$.
In addition, we let the on-directions functions be $g^\sharp_j \coloneqq g'$ and $h^\sharp_j \coloneqq h'$.
Then $f \then g = f \then h$.
As $f$ is an epimorphism, $g = h$; in particular, their on-directions functions are equal, so $g' = h'$, as desired.

Conversely, suppose that $f_\1$ is an epimorphism and that, for each $j\in q(\1)$, the function $f^\flat_j$ is a monomorphism.
Let $r$ be a polynomial and $g,h\colon q\tto r$ be two lenses such that $f \then g = f \then h$.
Then $f_\1 \then g_\1 = f_\1 \then h_\1$, which implies $g_\1=h_\1$; we'll consider $g_\1$ the default representation.
We also have that $g^\sharp_{f_\1i} \then f^\sharp_i = h^\sharp_{f_\1i} \then f^\sharp_i$ for any $i\in p(\1)$.
It follows that, for any $j \in q(\1)$, the two composites
\[
\begin{tikzcd}
	r[g_\1j] \ar[r, shift left, "g^\sharp_j"] \ar[r, shift right, "h^\sharp_j"'] & q[j] \ar[r, "f^\flat_j"] & \displaystyle\prod_{\substack{i\in p(\1), \\ f_\1i=j}} p[i]
\end{tikzcd}
\]
are equal, which implies that $g^\sharp_j=h^\sharp_j$ as desired.
\end{proof}

% Insert exercise exploring the difference between the epi proposition and the one about monos

\begin{exercise}
Show that the only way for a lens $p\to\yon$ to \emph{not} be an epimorphism is when $p=0$.
\begin{solution}
Given $p \in \poly$ and a lens $f \colon p \to \yon$, we will use \cref{prop.epis_in_poly} to show that either $f$ is an epimorphism or $p = \0$.
First, note that $f_\1 \colon p(\1) \to \1$ must be an epimorphism unless $p(\1) \iso \0$, in which case $p = \0$.
Next, note that the induced function
\[
    f^\flat \colon \1 \to \prod_{i\in p(\1)} p[i]
\]
from \eqref{eqn.useful_misc472} must be a monomorphism.
So it follows from \cref{prop.epis_in_poly} that either $f$ is an epimorphism or $p = \0$.
\end{solution}
\end{exercise}

\begin{exercise}
Let $A$ and $B$ be sets and $AB$ their product. Find an epimorphism $\yon^A+\yon^B\surj\yon^{AB}$.
\begin{solution}
Given sets $A$ and $B$, by \cref{prop.epis_in_poly}, a lens $f \colon \yon^A + \yon^B \to \yon^{AB}$ is an epimorphism if its on-positions function $f_\1 \colon \2 \to \1$ is an epimorphism (which must be true) and if the induced function
\[
    f^\flat \colon AB \to \prod_{i \in \2} (\yon^A + \yon^B)[i] \iso AB
\]
is a monomorphism.
If we take the on-directions functions $AB \to A$ and $AB \to B$ of $f$ to be the canonical projections, then the induced function $f^\flat \colon AB \to AB$ would be the identity, which is indeed a monomorphism.
So $f$ would be an epimorphism.
\end{solution}
\end{exercise}

\begin{exercise}
Suppose a lens $f\colon p\to q$ is both a monomorphism and an epimorphism; it is then an isomorphism? (That is, is $\poly$ \emph{balanced}?)

Hint: You may use the following facts.
\begin{enumerate}
    \item A function that is both a monomorphism and an epimorphism in $\smset$ is an isomorphism.
    \item A lens is an isomorphism if and only if the on-positions function is an isomorphism and every on-directions function is an isomorphism.
\end{enumerate}
\begin{solution}
Let $f \colon p \to q$ be a lens in $\poly$ that is both a monomorphism and an epimorphism.
We claim that $f$ is an isomorphism.
By \cref{prop.monics_in_poly} and \cref{prop.epis_in_poly}, the on-positions function $f_\1 \colon p(\1) \to q(\1)$ is both a monomorphism and an epimorphism, so it is an isomorphism.
Meanwhile, \cref{prop.epis_in_poly} says that, for each $j \in q(\1)$, the induced function
\[
    f^\flat_j \colon q[j] \to \prod_{\substack{i \in p(\1), \\ f_\1i = j}} p[i]
\]
is a monomorphism.
As $f_\1$ is an isomorphism, it follows that for each $i \in p(\1)$, the function
\[
    f^\flat_{f_\1i} \colon q[f_\1i] \to p[i]
\]
is a monomorphism.
But this is just the on-directions function $f^\sharp_i$ of $f$.
From \cref{prop.monics_in_poly}, we also know that $f^\sharp_i$ is an epimorphism.
It follows that every on-directions function of $f$ is an isomorphism.
Hence $f$ itself is an isomorphism.
\end{solution}
\end{exercise}

We are often interested in whether epimorphisms and monomorphisms form what is called a \emph{factorization system} in a given category, which we define below.

\begin{definition}[Factorization system] \label{def.factor}
Given a category $\cat{C}$ and two classes of morphisms $E$ and $M$ in $\cat{C}$, we say that $(E, M)$ is a \emph{factorization system} of $\cat{C}$ if:
\begin{enumerate}
    \item every morphism $f$ in $\cat{C}$ factors uniquely (up to unique isomorphism) as a morphism $e \in E$ composed with a morphism $m\in M$, so that $f = e\then m$;
    \item $E$ and $M$ each contain every isomorphism; and
    \item $E$ and $M$ are each closed under composition.
\end{enumerate}
If $E$ is the class of epimorphisms and $M$ is the class of monomorphisms (in which case conditions 2 and 3 are automatically satisfied), we say that $\cat{C}$ has \emph{epi-mono factorization}.
\end{definition}

\begin{example}[Epi-mono factorization in $\smset$] \label{ex.epi_mono_set}
The category $\smset$ has epi-mono factorization: a function $f\colon X\to Y$ can be uniquely factored into an epimorphism (surjection) $e$ followed by a monomorphism (injection) $i$, as follows.
The epimorphism $e\colon X\to f(X)$ is given by restricting the codomain of $f$ to its image (also known as \emph{corestricting} $f$), so $e$ sends $x\mapsto f(x)$ for all $x\in X$.
The monomorphism $i\colon f(X)\to Y$ is then given by including the image into the codomain, so $i$ sends $y\mapsto y$ for all $y\in f(X)\ss Y$.
\end{example}

\begin{proposition}
$\poly$ has epi-mono factorization.
\end{proposition}
\begin{proof}
Take an arbitrary lens $\varphi\colon p\to q$.
It suffices to show that there exists a unique polynomial $r$ equipped with an epimorphism $\epsilon\colon p\to r$ and a monomorphism $\mu\colon r\to q$ such that $\varphi=\epsilon\then\mu$.

On positions, we must have $\varphi_\1 = \epsilon_\1\then\mu_\1$, with $\mu_\1$ a monomorphism and $\epsilon_\1$ an epimorphism per \cref{prop.monics_in_poly,prop.epis_in_poly}.
By \cref{ex.epi_mono_set}, since $\smset$ has epi-mono factorization, such $r(\1), \epsilon_\1,$ and $\mu_\1$ uniquely exist.
In particular, we must have that $r(\1)\iso \varphi_\1(p(\1))$, that $\epsilon_\1\colon p(\1)\to\varphi_\1(p(\1))$ is the corestriction of $\varphi_\1$ sending $i\mapsto\varphi_\1(i)$ for each $p$-position $i$, and that $\mu_\1\colon\varphi_\1(p(\1))\to q(\1)$ is the inclusion sending $j\mapsto j$ for each $r$-position $j$.

Then on directions, for any $i\in p(\1)$, we must have that
\[
\begin{tikzcd}
    q[\varphi_\1(i)] \ar[r, "\mu^\sharp_{\varphi_\1(i)}"] \ar[dr, "\varphi^\sharp_i"'] & r[\varphi_\1(i)] \ar[d, "\epsilon^\sharp_i"] \\
    & p[i]
\end{tikzcd}
\]
commutes---or, equivalently, for every $j\in r(\1)\iso\varphi_\1(p(\1))$,
\[
\begin{tikzcd}
    q[j] \ar[r, "\mu^\sharp_j"] \ar[dr, "\varphi^\flat_j"'] & r[j] \ar[d, "\epsilon^\flat_j"] \\
    & \prod\limits_{\substack{i\in p(\1), \\ \varphi_\1(i)=j}} p[i]
\end{tikzcd}
\]
commutes (here $\varphi^\flat_j$ and $\epsilon^\flat_j$ are the induced functions from \eqref{eqn.useful_misc472}), with $\mu_j^\sharp$ an epimorphism and $\epsilon^\flat_j$ a monomorphism per
\cref{prop.monics_in_poly,prop.epis_in_poly}.
So again since $\smset$ has epi-mono factorization, such $r[j], \mu^\sharp_j,$ and $\epsilon^\flat_j$ uniquely exist.
Hence such $p \To{\epsilon} r \To{\mu} q$ uniquely exists overall.
\end{proof}

\index{factorization system!epi-mono}


%-------- Section --------%
\section{Cartesian closure}

\index{polynomial functor!cartesian closed structure}
\index{polynomial functor!exponentials of polynomials}
\index{closed monoidal structure}
\index{exponential|(}

We have already seen in \cref{sec.closure} the closure operation $\ihom{-,-}$ for one monoidal structure on $\poly$, namely $(\yon,\otimes)$.
But this is not the only closed monoidal structure on $\poly$: in fact, we will show that $\poly$ is cartesian closed as well.

For any two polynomials $q,r$, define $r^q\in\poly$ by the formula
\begin{equation}\label{eqn.exponential}
  r^q\coloneqq\prod_{j\in q(\1)}r\circ(\yon+q[j])
\end{equation}
where $\circ$ denotes composition.

Before proving that this really is an exponential in $\poly$, which we do in \cref{thm.poly_cart_closed}, we first get some practice with it.

\begin{example}
Let $A$ be a set. We've been writing the polynomial $A\yon^\0$ simply as $A$, so it better be true that there is an isomorphism
\[
    \yon^A \iso \yon^{A\yon^\0}
\]
in order for the notation to be consistent.
Luckily, this is true.
By \eqref{eqn.exponential}, we have
\[
    \yon^{A\yon^\0} = \prod_{a\in A} \yon \circ (\yon+\0) \iso \yon^A
\]
\end{example}

\begin{exercise}
Compute the following exponentials in $\poly$ using \eqref{eqn.exponential}:
\begin{enumerate}
	\item $p^\0$ for an arbitrary $p\in\poly$.
	\item $p^\1$ for an arbitrary $p\in\poly$.
	\item $\1^p$ for an arbitrary $p\in\poly$.
	\item $A^p$ for an arbitrary $p\in\poly$ and $A\in\smset$.
	\item $\yon^\yon$.
	\item $\yon^{\4\yon}$.
	\item $(\yon^A)^{\yon^B}$ for arbitrary sets $A,B\in\smset$.
\qedhere
\end{enumerate}
\begin{solution}
We use \eqref{eqn.exponential} to compute various exponentials.
Here $p \in \poly$ and $A, B \in \smset$.
\begin{enumerate}
    \item We have that $p^\0$ is an empty product, so $p^\0 \iso \1$ as expected.
	\item We have that $p^\1 \iso p \circ (\yon + \0) \iso p$, as expected.
	\item We have that $\1^p \iso \prod_{i \in p(\1)} \1 \circ (\yon + p[i]) \iso \1$, as expected.
	\item We have that $A^p \iso \prod_{i \in p(\1)} A \circ (\yon + p[i]) \iso A^{p(\1)}$.
	\item We have that $\yon^\yon \iso \yon \circ (\yon + \1) \iso \yon + \1$.
	\item We have that $\yon^{\4\yon} \iso \prod_{j \in \4} \yon \circ (\yon + \1) \iso (\yon + \1)^\4 \iso \yon^\4 + \4\yon^\3 + \6\yon^\2 + \4\yon + \1$.
	\item We have that $(\yon^A)^{\yon^B} \iso (\yon^A) \circ (\yon + B) \iso (\yon + B)^A \iso \sum_{f \colon A \to \2} B^{f\inv(1)} \yon^{f\inv(2)}$.
\end{enumerate}
\end{solution}
\end{exercise}

% \begin{exercise} % Immediate from previous exercise
% Using \eqref{eqn.exponential}, show that the functor $\smset\to\poly$ that sends each set $A$ to the constant polynomial $A$ preserves exponentials.
% That is, given sets $A, B \in \smset$, the set $B^A$ as a constant polynomial coincides with the exponential in $\poly$ that is the constant polynomial $B$ raised to the constant polynomial $A$.
% \begin{solution}
% By \eqref{eqn.exponential}, the exponential that is the constant polynomial $B$ raised to the constant polynomial $A$ can be written as
% \[
%     \prod_{a \in A} B \circ (\yon + A[a]) \iso \prod_{a \in A} B \iso B^A.
% \]
% \end{solution}
% \end{exercise}

\begin{theorem}\label{thm.poly_cart_closed}
The category $\poly$ is cartesian closed. That is, we have a natural isomorphism\index{isomorphism!natural}
\[
    \poly(p,r^q) \iso \poly(p\times q,r),
\]
where $r^q$ is the polynomial defined in \eqref{eqn.exponential}.
\end{theorem}
\begin{proof}
We have the following chain of natural isomorphisms:
\begin{align*}
	\poly(p, r^q) &\iso
	\poly\Big(p, \prod_{j \in q(\1)} r \circ (\yon+q[j])\Big)
	\tag*{\eqref{eqn.exponential}} \\
% 	&\iso
% 	\prod_{j\in q(\1)}\poly(p,r\circ(\yon+q[j]))
% 	\tag{Universal property of products} \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}\poly\big(\yon^{p[i]},r\circ(\yon+q[j])\big)
	\tag{Universal property of (co)products} \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}r\circ(p[i]+q[j])
	\tag{Yoneda lemma} \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}\sum_{k\in r(\1)}(p[i]+q[j])^{r[k]}
	\\
	&\iso
	\prod_{(i,j) \in (p \times q)(\1)} \; \sum_{k\in r(\1)}(p \times q)[(i, j)]^{r[k]}
	\tag*{\eqref{eqn.poly_times}} \\
	&\iso
	\poly(p \times q,r).
	\tag*{\eqref{eqn.main_formula}}
\end{align*}
\end{proof}

\begin{exercise}
Use \cref{thm.poly_cart_closed} to show that for any polynomials $p,q$, there is a canonical evaluation lens
\begin{equation*}%\label{eqn.eval_times}
	\text{eval}\colon p^q \times q \to p.
\end{equation*}
\begin{solution}
By \cref{thm.poly_cart_closed}, there is a natural isomorphism\index{isomorphism!natural}
\[
    \poly(p^q, p^q) \iso \poly(p^q \times q, p).
\]
Under this isomorphism, there exists a lens $\text{eval} \colon p^q \times q \to p$ corresponding to the identity lens on $p^q$.
The lens $\text{eval}$ is the canonical evaluation lens.
\end{solution}
\end{exercise}

\index{exponential|)}\index{exponential|seealso{closed monoidal structure}}\index{Cartesian closed category}


%-------- Section --------%
\section{Limits and colimits of polynomials}
\index{polynomial functor!limits of polynomials|(}

We have already seen that $\poly$ has all coproducts (\cref{prop.poly_coprods}) and products (\cref{prop.poly_prods}).
We will now see that $\poly$ has all small limits and colimits.

\begin{theorem}\label{thm.poly_limits}\index{limit!in $\poly$}
The category $\poly$ has all small limits.
\end{theorem}
\begin{proof}
A category has all small limits if and only if it has products and equalizers, so by \cref{prop.poly_prods}, it suffices to show that $\poly$ has equalizers.

We claim that equalizers in $\poly$ are simply equalizers on positions and coequalizers on directions.
More precisely, let $f,g \colon p \tto q$ be two lenses.
We construct the equalizer $p'$ of $f$ and $g$ as follows.\footnote{If we're being precise, a ``(co)equalizer'' is an object equipped with a morphism, but we will use the term to refer to either just the object or just the morphism when the context is clear.}
We define its position-set $p'(\1)$ to be the equalizer of $f_\1,g_\1 \colon p(\1) \tto q(\1)$ in $\smset$; that is,
\[
    p'(\1) \coloneqq \{i \in p(\1) \mid f_\1i = g_\1i\}.
\]
Then for each $i \in p'(\1)$, we can define the direction-set $p'[i]$ to be the coequalizer of $f^\sharp_i, g^\sharp_i \colon q[f_\1i] \tto p[i]$.
In this way, we obtain a polynomial $p'$ that comes equipped with a lens $e \colon p' \to p$.
One can check that $p'$ together with $e$ satisfies the universal property of the equalizer of $f$ and $g$; see \cref{exc.poly_limits}.
\end{proof}

\begin{exercise}\label{exc.poly_limits}
Complete the proof of \cref{thm.poly_limits} as follows:
\begin{enumerate}
	\item We said that $p'$ comes equipped with a lens $e \colon p' \to p$; what is it?
	\item Show that $e \then f = e \then g$.
	\item Show that $e$ is the equalizer of the pair $f,g$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item The lens $e \colon p' \to p$ can be characterized as follows.
    The on-positions function $e_\1 \colon p'(\1) \to p(\1)$ is the equalizer of $f_\1, g_\1 \colon p(\1) \tto q(\1)$ in $\smset$.
    In particular, $e_\1$ is the canonical inclusion that sends each element of $p'(\1)$ to the same element in $p(\1)$.
    Then for each $i \in p'(\1)$, the on-directions function $e^\sharp_i \colon p[i] \to p'[i]$ is the coequalizer of $f^\sharp_i, g^\sharp_i \colon q[f_\1i] \tto p[i]$ in $\smset$.

    \item To show that $e \then f = e \then g$, it suffices to show that both sides are equal on positions and on directions.
    On positions, $e_\1$ is defined to be the equalizer of $f_\1$ and $g_\1$, so $e_\1 \then f_\1 = e_\1 \then g_\1$.
    Then for each $i \in p'(\1)$, the on-directions function $e^\sharp_i$ is defined to be the coequalizer of $f^\sharp_i$ and $g^\sharp_i$, so $f^\sharp_i \then e^\sharp_i = g^\sharp_i \then e^\sharp_i$.

    \item To show that $e$ is the equalizer of $f$ and $g$, it suffices to show that for any $r \in \poly$ and lens $a \colon r \to p$ satisfying $a \then f = a \then g$, there exists a unique lens $h \colon r \to p'$ for which $a = h \then e$, so that the following diagram commutes.
    \begin{equation*} %\label{eqn.eq_univ_prop}
    \begin{tikzcd}
        p' \ar[r, "e"] & p \ar[r, "f", shift left] \ar[r, "g"', shift right] & q \\
        r \ar[u, "h", dashed] \ar[ur, "a"']
    \end{tikzcd}
    \end{equation*}
    In order for $a = h \then e$ to hold, we must have $a_\1 = h_\1 \then e_\1$ on positions.
    But we have that $a_\1 \then f_\1 = a_\1 \then g_\1$, so by the universal property of $p'(\1)$ and the map $e_\1$ as the equalizer of $f_\1$ and $g_\1$ in $\smset$, there exists a unique $h_\1$ for which $a_\1 = h_\1 \then e_\1$.
    Hence $h$ is uniquely characterized on positions.
    In particular, it must send each $k \in r(\1)$ to $a_\1(k) \in p'(\1)$.

    Then for $a = h \then e$ to hold on directions, we must have that $a^\sharp_k = e^\sharp_{a_\1(k)} \then h^\sharp_k$ for each $k \in r(\1)$.
    But we have that $f^\sharp_{a_\1(k)} \then a^\sharp_{a_\1(k)} = g^\sharp_{a_\1(k)} \then a^\sharp_{a_\1(k)}$, so by the universal property of $p'[a_\1(k)]$ and the map $e^\sharp_{a_\1(k)}$ as the coequalizer of $f^\sharp_{a_\1(k)}$ and $g^\sharp_{a_\1(k)}$ in $\smset$, there exists a unique $h^\sharp_k$ for which $a^\sharp_k = e^\sharp_{a_\1(k)} \then h^\sharp_k$, so that the diagram below commutes.
    \begin{equation*} %\label{eqn.eq_univ_prop_dir}
    \begin{tikzcd}[sep=large]
        p'[a_\1(k)] \ar[d, "h^\sharp_k"', dashed] & p[a_\1(k)] \ar[l, "e^\sharp_{a_\1(k)}"'] \ar[dl, "a^\sharp_k"] & q[f_\1(a_\1(k))] \ar[l, "f^\sharp_{a_\1(k)}"', shift right] \ar[l, "g^\sharp_{a_\1(k)}", shift left] \\
        r[k]
    \end{tikzcd}
    \end{equation*}
    Hence $h$ is also uniquely characterized on directions, so it is unique overall.
    Moreover, we have shown that we can define $h$ on positions so that $a_\1 = h_\1 \then e_\1$, and that we can define $h$ on directions such that $a^\sharp_k = e^\sharp_{a_\1(k)} \then h^\sharp_k$ for all $k \in r(\1)$.
    It follows that there exists $h$ for which $a = h \then e$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{example}[Computing general limits in $\poly$] \label{ex.compute_limits}\index{limit!positions and directions}
The proof of \cref{thm.poly_limits} justifies the following mnemonic for limits in $\poly$:
\slogan{The positions of a limit are the limit of the positions. \\ The directions of a limit are the colimit of the directions.}
We can make this precise as follows: the limit of a functor $p_-\colon\cat{J}\to\poly$ is the polynomial whose position-set is
\begin{equation} \label{eqn.lim_pos}
    \left(\lim_{j\in\cat{J}} p_j\right)(\1) \iso \lim_{j\in\cat{J}} p_j(\1),
\end{equation}
equipped with a canonical projection $\pi_j$ to each $p_j(\1)$, and whose direction-set for each position $i$ is
\begin{equation} \label{eqn.lim_dir}
    \left(\lim_{j\in\cat{J}} p_j\right)[i] \iso \colim_{j\in\cat{J}\op} p_j[\pi_j(i)].
\end{equation}
This notation obscures what is occuring on lenses, but in particular, each lens $\varphi\colon p_j\to p_{j'}$ in the diagram $p_-$ induces an on-positions function $\varphi_\1\colon p_j(\1)\to p_{j'}(\1)$ in the diagram whose limit we take in \eqref{eqn.lim_pos} and, for every position $i$ of the limit, an on-directions function $\varphi^\sharp_{\pi_j(i)}\colon p_{j'}[\pi_{j'}(i)]\to p_j[\pi_j(i)]$ in the diagram whose colimit we take in \eqref{eqn.lim_dir}.
(Note that, by the definition of a limit, $\varphi_\1(\pi_j(i)) = \pi_{j'}(i)$.)

We have seen \eqref{eqn.lim_pos} and \eqref{eqn.lim_dir} to be true for products: the position-set of the product is just the product of the original position-sets, while the direction-set at a tuple of the original positions is just the coproduct of the direction-sets at every position in the tuple.
We have also just shown \eqref{eqn.lim_pos} and \eqref{eqn.lim_dir} to be true for equalizers in the proof of \cref{thm.poly_limits}.
It follows from the construction of any limit as an equalizer of products that it is true for arbitrary limits.
\end{example}

\index{polynomial functor!pullback of polynomials}\index{pullback|seealso{polynomial functor, pullback of polynomials}}

\begin{example}[Pullbacks in $\poly$]\label{ex.pullbacks_in_poly}
Given $q,q',r \in \poly$ and lenses $q\To{f} r\From{f'} q'$, the pullback
\[
\begin{tikzcd}
	p\ar[r, "g'"]\ar[d, "g"']&
	q'\ar[d, "f'"]\\
	q\ar[r, "f"']&
	r\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
is given as follows.
The position-set of $p$ is the pullback of the position-sets of $q$ and $q'$ over that of $r$ in $\smset$.
Then at each position $(i, i') \in p(\1) \ss q(\1) \times q'(\1)$ with $f_\1i=f'_\1i'$, we take the direction-set $p[(i, i')]$ to be the pushout of the direction-sets $q[i]$ and $q'[i']$ over $r[f_\1i]=r[f_\1'i']$ in $\smset$.
These pullback and pushout squares also give the lenses $g$ and $g'$ on positions and on directions:
\begin{equation}\label{eqn.pullback_poly}
\begin{tikzcd}
	p(\1)\ar[r, "g'_\1"]\ar[d, "g_\1"']&
	q'(\1)\ar[d, "f_\1'"]\\
	q(\1)\ar[r, "f_\1"']&
	r(\1)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\qqand
\begin{tikzcd}
	p[(i,i')]\ar[from=r, "(g')^\sharp_{(i,i')}"']\ar[from=d, "g^\sharp_{(i,i')}"]&
	q'[i']\ar[from=d, "(f')^\sharp_{i'}"']\\
	q[i]\ar[from=r, "f^\sharp_i"]&
	r[f_1(i)]\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\end{equation}
\end{example}

\begin{exercise}\index{polynomial functor!pullback of polynomials}
Let $p$ be any polynomial.
\begin{enumerate}
	\item There is a canonical choice of lens $\eta\colon p\to p(\1)$; what is it?
	\item Given an element $i\in p(\1)$, i.e.\ a function (or lens between constant polynomials) $i\colon\1\to p(\1)$, let $p_i$ be the pullback
	\[
	\begin{tikzcd}
	p_i\ar[r, "g"]\ar[d, "f"']&
	p\ar[d, "\eta"]\\
	\1\ar[r, "i"']&
	p(\1)\ar[ul, phantom, very near end, "\lrcorner"]
	\end{tikzcd}
	\]
	What is $p_i$? What are the lenses $f \colon p_i \to \1$ and $g \colon p_i \to p$? \qedhere
\end{enumerate}
\begin{solution}
Here $p \in \poly$.
\begin{enumerate}
    \item The canonical lens $\eta \colon p \to p(\1)$ is the identity $\eta_\1 \colon p(\1) \to p(\1)$ on positions and the empty function on directions.

    \item On positions, we have that $p_i(\1)$ along with $f_\1$ and $g_\1$ form the following pullback square in $\smset$:
    \[
	\begin{tikzcd}
    	p_i(\1) \ar[r, "g_\1"] \ar[d, "f_\1"'] &
    	p(\1) \ar[d, equals] \\
    	\1 \ar[r, "i"'] &
    	p(\1) \ar[ul, phantom, very near end, "\lrcorner"]
	\end{tikzcd}
	\]
	So $p_i(\1) \coloneqq \{(a, i') \in \1 \times p(\1) \mid i = i' \} = \{(1, i)\}$, with $f_\1$ uniquely determined and $g_1$ picking out $i \in p(\1)$.
	Then on directions, we have that $p_i[(1,i)]$ along with $f^\sharp_{(1,i)}$ and $g^\sharp_{(1,i)}$ form the following pushout square in $\smset$:
	\[
	\begin{tikzcd}
    	p_i[(1,i)] \ar[from=r, "g^\sharp_{(1,i)}"'] \ar[from=d, "f^\sharp_{(1,i)}"] &
    	p[i] \ar[from=d, "!"'] \\
    	\0 \ar[from=r, "!"] &
    	\0 \ar[ul, phantom, very near end, "\lrcorner"]
    \end{tikzcd}
    \]
    So $p_i[(1,i)] \coloneqq p[i]$, with $f^\sharp_{(1,i)}$ uniquely determined and $g^\sharp_{(1,i)}$ as the identity.
    It follows that $p_i \coloneqq \{(1,i)\}\yon^{p[i]} \iso \yon^{p[i]}$, where $f \colon p_i \to \1$ is uniquely determined and $g \colon p_i \to p$ picks out $i \in p(\1)$ on positions and is the identity on $p[i]$ on directions.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Let $q\coloneqq \yon^\2+\yon$, $q'\coloneqq\2\yon^\3+\yon^\2$, and $r\coloneq\yon+\1$.
\begin{enumerate}
	\item Choose lenses $f\colon q\to r$ and $f'\colon q'\to r$ and write them down.
	\item Find the pullback of $q\To{f} r\From{f'} q'$.
\qedhere
\end{enumerate}
\begin{solution}\index{polynomial functor!pullback of polynomials}
\begin{enumerate}
    \item There are many possible answers, but one lens $f \colon q \to r$, on positions, sends $1 \in q(\1)$ (corresponding to $\yon^\2$) to $2 \in r(\1)$ (corresponding to $\1$) and $2 \in q(\1)$ (corresponding to $\yon$) to $1 \in r(\1)$ (corresponding to $\yon$).
    Then the on-directions functions $f^\sharp_\1 \colon \0 \to \2$ and $f^\sharp_2 \colon \1 \to \1$ are uniquely determined.
    Another morphism $f' \colon q' \to r$, on positions, sends $1 \in q'(\1)$ (corresponding to one of the $\yon^\3$ terms) to $2 \in r(\1)$ and both $2 \in q'(\1)$ (corresponding to the other $\yon^\3$ term) and $3 \in q'(\1)$ (corresponding to the $\yon^\2$ term) to $1 \in r(\1)$.
    Then the on-directions function $(f')^\sharp_1 \colon \0 \to \3$ is uniquely determined, while we can let $(f')^\sharp_2 \colon \1 \to \3$ pick out $3$ and $(f')^\sharp_3 \colon \1 \to \2$ pick out $1$.

    \item We compute the pullback $p$ along with the lenses $g \colon p \to q$ and $g' \colon p \to q'$ of $q\To{f} r\From{f'} q'$ by following \cref{ex.pullbacks_in_poly}.
    We can compute $p(\1)$ by taking the pullback in $\smset$:
    \[
        p(\1) \coloneqq \{(i, i') \in \2 \times \3 \mid f_\1i = f'_\1(i)\} = \{(1,1), (2,2), (2,3)\}.
    \]
    Moreover, the on-positions functions $g_\1$ and $g'_\1$ send each pair in $p(\1)$ to its left component and its right component, respectively.

    To compute the direction-set at each $p$-position, we must compute a pushout.
    At $(1,1)$, we have $r[f_\1(1)] = r[f'_\1(1)] = r[2] = \0$, so the pushout $p[(1,1)]$ is just the sum $q[1] + q'[1] = \2 + \3 \iso \5$.
    Moreover, the on-directions functions $g^\sharp_{(1,1)}$ and $(g')^\sharp_{(1,1)}$ are the canonical inclusions $\2 \to \2 + \3$ and $\3 \to \2 + \3$.

    At $(2,2)$, we have $r[f_\1(2)] = r[f'_\1(2)] = r[1] = \1$, with $f^\sharp_2$ picking out $1 \in \1 = q[2]$ and $(f')^\sharp_2$ picking out $3 \in \3 = q'[2]$.
    So the pushout $p[(2,2)]$ is the set $\1 + \3 = \{(1,1), (2,1), (2,2), (2,3)\}$ but with $(1,1)$ identified with $(2,3)$; we can think of it as the set of equivalence classes $p[(2,2)] \iso \{\{(1,1), (2,3)\}, \{(2,1)\}, \{(2,2)\}\} \iso \3$.
    Moreover, the on-directions function $g^\sharp_{(2,2)}$ maps $1 \mapsto \{(1,1), (2,3)\}$, while the on-directions function $(g')^\sharp_{(2,2)}$ maps $1 \mapsto \{(2,1)\}, 2 \mapsto \{(2,2)\},$ and $3 \mapsto \{(1,1), (2,3)\}$.

    Finally, at $(2,3)$, we have $r[f_\1(2)] = r[f'_\1(3)] = r[1] = \1$, with $f^\sharp_2$ still picking out $1 \in \1 = q[2]$ and $(f')^\sharp_3$ picking out $1 \in \2 = q'[3]$.
    So the pushout $p[(2,3)]$ is the set $\1 + \2 = \{(1,1), (2,1), (2,2)\}$ but with $(1,1)$ identified with $(2,1)$; we can think of it as the set of equivalence classes $p[(2,3)] \iso \{\{(1,1), (2,1)\}, \{(2,2)\}\} \iso \2$.
    Moreover, the on-directions function $g^\sharp_{(2,3)}$ maps $1 \mapsto \{(1,1), (2,1)\}$, while the on-directions function $(g')^\sharp_{(2,3)}$ maps $1 \mapsto \{(1,1), (2,1)\}$ and $2 \mapsto \{(2,2)\}$.

    It follows that $p \iso \yon^\5 + \yon^\3 + \yon^\2$, with $g$ and $g'$ as described.
\end{enumerate}
\end{solution}
\end{exercise}

\index{polynomial functor!equalizer of polynomials}

\begin{exercise} \label{exc.refl_limits}
An alternative way to prove \cref{thm.poly_limits} would have been to show that the equalizer of two natural transformations between polynomial functors in $\smset^\smset$ is still a polynomial functor---since the full subcategory inclusion $\poly \to \smset^\smset$ reflects these equalizers, it would follow that $\poly$ has equalizers.
But we already know what polynomial the equalizer should be from the proof of \cref{thm.poly_limits}.
So in this exercise, we will show that the equalizer of polynomials we found in $\poly$ is also the equalizer of those same functors in $\smset^\smset$.

Let $f,g\colon p\tto q$ be a pair of natural transformations $f,g\colon p\tto q$ between polynomial functors $p$ and $q$, and let $e\colon p'\to p$ be their equalizer in $\poly$ that we computed in the proof of \cref{thm.poly_limits}.
\begin{enumerate}
    \item Given a set $X$, show that $e_X\colon p'(X)\to p(X)$ is the equalizer of the $X$-components $f_X,g_X\colon p(X)\tto q(X)$ in $\smset$.
    \item Deduce that equalizers in $\poly$ coincide with equalizers in $\smset^\smset$.
    \item Conclude that limits in $\poly$ coincide with limits in $\smset^\smset$. \qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item By \cref{prop.morph_arena_to_func}, $f_X$ (resp.\ $g_X$) sends each $(i,h)\in p(X)$ with $i\in p(\1)$ and $h\colon p[i]\to X$ to $(f_\1i, f^\sharp_i\then h)$ (resp.\ $(g_\1i, g^\sharp_i\then h)$) in $q(X)$.
    So the equalizer of $f_X$ and $g_X$ is the set of all $(i,h)\in p(X)$ for which both $f_\1i = g_\1i$ and $f^\sharp_i\then h = g^\sharp_i\then h$.

    Indeed, by our construction of $p'$, the set $p'(X)$ consists of all pairs $(i,h')$ with $i\in p(\1)$ such that $f_\1i = g_\1i$ and $h'\colon p'[i]\to X$, where $p'[i]$ is the coequalizer of $f^\sharp_i,g^\sharp_i\colon q[f_\1i]\tto p[i]$.
    By the universal property of the coequalizer, functions $h'\colon p'[i]\to X$ precisely correspond to functions $h\colon p[i]\to X$ for which $f^\sharp_i\then h = g^\sharp_i\then h$.
    So $p'(X)$ is indeed the equalizer of $f_X$ and $g_X$.

    The equalizer natural transformation $e'\colon p'\to p$ has the inclusion $e'_X\colon p'(X)\to p(X)$ as its $X$-component, so by \cref{cor.morph_func_to_arena}, it is the lens whose on-positions function is the canonical equalizer inclusion $e'_\1\colon p'(\1)\to p(\1)$, while its on-directions function at $i\in p'(\1)$ is the map $p[i]\to p'[i]$ corresponding to the identity on $p'[i]$ given by the universal property of the coequalizer---which is just the canonical coequalizer map $p[i]\to p'[i]$.
    But this is exactly the lens $e\colon p'\to p$ constructed in the proof of \cref{prop.poly_prods}, as desired.
    \item By \cref{prop.presheaf_lim_ptwise}, limits---including equalizers---in $\smset^\smset$ are computed pointwise.
    So if $e_X\colon p'(X)\to p(X)$ is the equalizer of $f_X,g_X\colon p(X)\tto q(X)$ for every $X\in\smset$, then $e\colon p'\to p$ is the equalizer of $f,g\colon p(X)\tto q(X)$.
    \item We have just shown that equalizers in $\poly$ coincide with equalizers in $\smset^\smset$.
    We saw in the proof of \cref{prop.poly_prods} that products in $\poly$ also coincide with products in $\smset^\smset$.
    Since every limit can be computed as an equalizer of products, we can conclude that limits in $\poly$ coincide with limits in $\smset^\smset$.
\end{enumerate}
\end{solution}
\end{exercise}\index{limits!computed pointwise}

\index{polynomial functor!limits of polynomials|)}
\index{polynomial functor!colimits of polynomials|(}\index{colimit!in $\poly$}

\begin{theorem}\label{thm.poly_colimits}
The category $\poly$ has all small colimits.
\end{theorem}
\begin{proof}
A category has all small colimits if and only if it has coproducts and coequalizers, so by \cref{prop.poly_coprods}, it suffices to show that $\poly$ has coequalizers.

Let $s,t \colon p \tto q$ be two lenses.
We construct the coequalizer $q'$ of $s$ and $t$ as follows.
The pair of functions $s_\1, t_\1 \colon p(\1) \tto q(\1)$ define a graph $G \colon \fbox{$\bullet\tto\bullet$} \to \smset$ with vertices in $q(\1)$, edges in $p(\1)$, sources indicated by $s_\1$, and targets indicated by $t_\1$.
Then the set $C$ of connected components of $G$ is given by the coequalizer $g_\1 \colon q(\1) \to C$ of $s_\1$ and $t_\1$.
We define the position-set of $q'$ to be $C$.
Each direction-set of $q'$ will be a limit of a diagram of direction-sets of $p$ and $q$, but expressing this limit, as we proceed to do, is a bit involved.

\index{graph}

For each connected component $c \in C$, we have a connected subgraph $G_c \ss G$ with vertices $V_c \coloneqq g_\1\inv(c)$ and edges $E_c \coloneqq s_\1\inv(g_\1\inv(c)) = t_\1\inv(g_\1\inv(c))$.
Note that $E_c\ss p(\1)$ and $V_c\ss q(\1)$, so to each $e\in E_c$ (resp.\ to each $v\in V_c$) we have an associated direction-set $p[e]$ (resp.\ $q[v]$).

\index{element!category of elements}\index{category!of elements}

The category of elements $\int G_c$ has objects $E_c+V_c$ and two kinds of (non-identity) morphisms, $e \to s_\1(e)$ and $e \to t_\1(e)$, associated to each $e \in E_c$, all pointing from an object in $E_c$ to an object in $V_c$.
There is a functor $F \colon (\int G_c)\op \to \smset$ sending every $v \mapsto q[v]$, every $e \mapsto p[e]$, and every morphism to a function between them, namely either $s^\sharp_e \colon q[s_\1(e)] \to p[e]$ or $t^\sharp_e \colon q[t_\1(e)] \to p[e]$.
So we can define $q'[c]$ to be the limit of $F$ in $\smset$.

We claim that $q'\coloneqq\sum_{c\in C}\yon^{q'[c]}$ is the coequalizer of $s$ and $t$. We leave the complete proof to the interested reader in \cref{exc.poly_colimits}.
\end{proof}

\begin{exercise}\label{exc.poly_colimits}
Complete the proof of \cref{thm.poly_colimits} as follows:
\begin{enumerate}
	\item Provide a lens $g \colon q \to q'$.
	\item Show that $s \then g = t \then g$.
	\item Show that $g$ is a coequalizer of the pair $s, t$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We define a lens $g \colon q \to q'$ as follows.
    The on-positions function $g_\1 \colon q(\1) \to q'(\1)$ is the coequalizer of $s_\1, t_\1 \colon p(\1) \tto q(\1)$.
    In particular, $g_\1$ sends each vertex in $q(\1)$ to its corresponding connected component in $q'(\1) = C$.
    Then for each $v \in q(\1)$, if we let its corresponding connected component be $c \coloneqq g_\1(v)$, we can define the on-directions function $g^\sharp_v \colon q'[c] \to q[v]$ to be the projection from the limit $q'[c]$ to its component $q[v]$.

    \item To show that $s \then g = t \then g$, we must show that both sides are equal on positions and on directions.
    The on-positions function $g_\1$ is defined to be the coequalizer of $s_\1$ and $t_\1$, so $s_\1 \then g_\1 = t_\1 \then g_\1$.
    So it suffices to show that for all $e \in p(\1)$, if we let its corresponding connected component be $c \coloneqq g_\1(s_\1(e)) = g_\1(t_\1(e))$, then the following diagram of on-directions functions commutes:
    \[
    \begin{tikzcd}[sep=small]
        & q[s_\1(e)] \ar[dl, "s^\sharp_e"'] \\
        p[e] & & q'[c] \ar[ul, "g^\sharp_{s_\1(e)}"'] \ar[dl, "g^\sharp_{t_\1(e)}"] \\
        & q[t_\1(e)] \ar[ul, "t^\sharp_e"]
    \end{tikzcd}
    \]
    But this is automatically true by the definition of $q'[c]$ as a limit---specifically the limit of a functor with $s^\sharp_e$ and $t^\sharp_e$ in its image---and the definitions of $g^\sharp_{s_\1(e)}$ and $g^\sharp_{t_\1(e)}$ as projections from this limit.

    \item To show that $g$ is the coequalizer of $s$ and $t$, it suffices to show that for any $r \in \poly$ and lens $f \colon q \to r$ satisfying $s \then f = t \then f$, there exists a unique lens $h \colon q' \to r$ for which $f = g \then h$, so that the following diagram commutes.
    \begin{equation*} %\label{eqn.eq_univ_prop}
    \begin{tikzcd}
        p \ar[r, "s", shift left] \ar[r, "t"', shift right] & q \ar[r, "g"] \ar[dr, "f"'] & q' \ar[d, "h", dashed] \\
        & & r
    \end{tikzcd}
    \end{equation*}
    In order for $f = g \then h$ to hold, we must have $f_\1 = g_\1 \then h_\1$ on positions.
    But we have that $s_\1 \then f_\1 = t_\1 \then f_\1$, so by the universal property of $q'(\1)$ and the map $g_\1$ as the coequalizer of $s_\1$ and $t_\1$ in $\smset$, there exists a unique $h_\1$ for which $f_\1 = g_\1 \then h_\1$.
    Hence $h$ is uniquely characterized on positions.
    In particular, it must send each connected component $c \in q'(\1)$ to the element in $r(\1)$ to which $f_\1$ sends every vertex $v \in V_c = g_\1\inv(c)$ that lies in the connected component $c$.

    Then for $f = g \then h$ to hold on directions, we must have that $f^\sharp_v = h^\sharp_{g_\1(v)} \then g^\sharp_v$ for each $v \in q(\1)$.
    Put another way, given $c \in q'(\1)$, we must have that $f^\sharp_v = h^\sharp_c \then g^\sharp_v$ for every $v \in V_c$.
    But $s \then f = t \then f$ implies that for each $e \in E_c = s_\1\inv(g_\1\inv(c)) = t_\1\inv(g_\1\inv(c)) \ss p(\1)$, the following diagram of on-directions functions commutes:
    \[
    \begin{tikzcd}[sep=small]
        & q[s_\1(e)] \ar[dl, "s^\sharp_e"'] \\
        p[e] & & r[f_\1(v)] \ar[ul, "f^\sharp_{s_\1(e)}"'] \ar[dl, "f^\sharp_{t_\1(e)}"] \\
        & q[t_\1(e)] \ar[ul, "t^\sharp_e"]
    \end{tikzcd}
    \]
    It follows that $r[f_\1(v)]$ together with the maps $(f^\sharp_v)_{v \in V_c}$ form a cone over the functor $F$.
    So by the universal property of the limit $q'[c]$ of $F$ with projection maps $(g^\sharp_v)_{v \in V_c}$, there exists a unique $h^\sharp_c \colon r[f_\1(v)] \to q'[c]$ for which $f^\sharp_v = h^\sharp_c \then g^\sharp_v$ for every $v \in V_c$.
    Hence $h$ is also uniquely characterized on directions, so it is unique overall.
    Moreover, we have shown that we can define $h$ on positions so that $f_\1 = g_\1 \then h_\1$, and that we can define $h$ on directions such that $f^\sharp_v = h^\sharp_c \then g^\sharp_v$ for all $c \in q'(\1)$ and $v \in V_c$.
    It follows that there exists $h$ for which $f = g \then h$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{example}
Given a diagram in $\poly$, one could either take its (co)limit as a diagram of \emph{polynomial} functors (i.e.\ its (co)limit in $\poly)$ or its (co)limit simply as a diagram of functors (i.e.\ its (co)limit in $\smset^\smset$).
We saw in \cref{exc.refl_limits} that in the case of limits, these yield the same result.
So, too, in the case of coproducts, per \cref{prop.poly_coprods}.

But in the case of general colimits, there are diagrams that yield different results: by the co-Yoneda lemma, \emph{every} functor $\smset \to \smset$---even those that are not polynomials---can be written as the colimit of representable functors in $\smset^\smset$, yet the colimit of the same representables in $\poly$ can only be another polynomial.

For a concrete example, consider the two distinct projections $\yon^\2\to\yon$, which form the diagram
\begin{equation} \label{eqn.2_projs}
    \yon^\2\tto\yon.
\end{equation}
According to \cref{thm.poly_colimits}, the colimit of \eqref{eqn.2_projs} in $\poly$ has the coequalizer of $\1 \tto \1$, namely $\1$, as its position-set, and the limit of the diagram $\1 \tto \2$ consisting of the two inclusions as its sole direction-set.
But this latter limit is just $\0$, so in fact the colimit of \eqref{eqn.2_projs} in $\poly$ is the constant functor $\1\yon^\0\iso\1$.

\index{colimit!$\poly$ disagrees with $\smset^\smset$}

But as functors, by \cref{prop.presheaf_lim_ptwise}, the colimit of \eqref{eqn.2_projs} can be computed pointwise: it is the (nonconstant!) functor
\[
  X\mapsto
  \begin{cases}
  	\0&\tn{ if }X=\0\\
  	\1&\tn{ if }X\neq\0
  \end{cases}
\]
\end{example}

\begin{exercise}\index{global sections}
By \cref{thm.adjoint_quadruple}, for any polynomial $p$, there are canonical lenses involving positions and global sections:
\[
	\epsilon \colon p(\1)\yon\to p
	\qqand
	\eta \colon p\to \yon^{\Gamma(p)}.
\]
\begin{enumerate}
	\item Characterize the behavior of the canonical lens $\epsilon \colon p(\1)\yon\to p$.
	\item Characterize the behavior of the canonical lens $\eta \colon p\to \yon^{\Gamma(p)}$.
	\item Show that the following is a pushout in $\poly$:
    \begin{equation} \label{eqn.pushout_adjoint}
    \begin{tikzcd}
    	p(\1)\yon\ar[r, "!"]\ar[d, "\epsilon"']&
    	\yon\ar[d, "!"]\\
    	p\ar[r, "\eta"']&
    	\yon^{\Gamma(p)}\ar[ul, phantom, very near start, "\ulcorner"]
    \end{tikzcd}
    \end{equation}
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We characterize the lens $\epsilon \colon p(\1)\yon \to p$ as follows.
    On positions, it is the identity on $p(\1)$.
    Then for each $i \in p(\1)$, on directions, it is the unique map $p[i] \to \1$.

    \item We characterize the lens $\eta \colon p \to \yon^{\Gamma(p)}$ as follows.\index{global sections}
    On positions, it is the unique map $p(\1) \to \1$.
    Then for each $i \in p(\1)$, on directions, it is the canonical projection $\Gamma(p) \iso \prod_{i' \in p(\1)} p[i'] \to p[i]$.

    \item Showing that \eqref{eqn.pushout_adjoint} is a pushout square is equivalent to showing that, in the diagram
    \begin{equation} \label{eqn.coeq_adjoint}
    \begin{tikzcd}[sep=large]
        & \yon \ar[d, "\iota"] \ar[dr, "!"] \\
        p(\1)\yon \ar[ur, "!"] \ar[dr, "\epsilon"'] \ar[r, "s", shift left] \ar[r, "t"', shift right] & \yon + p \ar[r, "g"] & \yon^{\Gamma(p)} \\
        & p \ar[u, "\iota'"'] \ar[ur, "\eta"']
    \end{tikzcd}
    \end{equation}
    in which $\iota, \iota'$ are the canonical inclusions and the four triangles commute, $\yon^{\Gamma(p)}$ equipped with the lens $g$ is the coequalizer of $s$ and $t$.
    To do so, we apply \cref{thm.poly_colimits} to compute the coequalizer $q'$ of $s$ and $t$.
    The position-set of $q'$ is the coequalizer of $s_\1 = (! \then \iota)_\1$, which sends every $i \in p(\1)$ to the position of $\yon + p$ corresponding to the summand $\yon$, and $t_\1 = (\epsilon \then \iota')_\1$, which sends each $i \in p(\1)$ to the corresponding position in the summand $p$ of $\yon + p$.
    It follows that the coequalizer of $s_\1$ and $t_\1$ is $\1$, so $q'(\1) \iso \1$.

    Then the direction-set of $q'$ at its sole position is the limit of the functor $F$ whose image consists of lenses of the form $\1 \to \1$ or $p[i] \to \1$ for every $i \in p(\1)$.
    It follows that the limit of $F$ is just a product, namely $\prod_{i \in p(\1)} p[i] \iso \Gamma(p)$.
    Hence $q' \iso \yon^{\Gamma(p)}$, as desired.

    It remains to check that the upper right and lower right triangles in \eqref{eqn.coeq_adjoint} commute.
    The upper right triangle must commute by the uniqueness of morphisms $\yon \to \yon^{\Gamma(p)}$; and the lower right triangle must commute on positions.
    Moreover, the on-directions function of the coequalizer morphism $g$ at each position $i \in p(\1) \ss (\yon+p)(\1)$ must be the canonical projection $\Gamma(p) \to p[i]$, which matches the behavior of the corresponding on-directions function of $\eta$; hence the lower right triangle also commutes on directions.
\end{enumerate}
\end{solution}
\end{exercise}

\index{polynomial functor!pushout of polynomials}

\begin{proposition}\label{prop.tensor_as_pushout}
For polynomials $p,q$, the following is a pushout:
\[
\begin{tikzcd}
	p(\1)\yon\otimes q(\1)\yon\ar[r]\ar[d]&
	p(\1)\yon\otimes q\ar[d]\\
	p\otimes q(\1)\yon\ar[r]&
	p\otimes q\ar[ul, phantom,very near start, "\ulcorner"]
\end{tikzcd}
\]
\end{proposition}
\begin{proof}
All the lenses shown are identities on positions, so the displayed diagram is the coproduct over all $(i,j)\in p(\1)\times q(\1)$ of the diagram shown left
\[
\begin{tikzcd}
	\yon\ar[r]\ar[d]&
	\yon^{q[j]}\ar[d]\\
	\yon^{p[i]}\ar[r]&
	\yon^{p[i]\times q[j]}\ar[ul, phantom, very near start, "\ulcorner"]
\end{tikzcd}
\begin{tikzcd}
	\1\ar[dr, phantom, very near end, "\ulcorner"]&
	q[j]\ar[l]\\
	p[i]\ar[u]&
	p[i]\times q[j]\ar[l]\ar[u]
\end{tikzcd}
\]
where we used $(p\otimes q)[(i,j)]\cong p[i]\times q[j]$. This is the image under the Yoneda embedding of the diagram of sets shown right, which is clearly a pullback. The result follows by \cref{prop.yoneda_left_adjoint}.
\end{proof}\index{polynomial functor!pullback of polynomials}

This means that to give a lens $\varphi\colon p\otimes q\to r$, it suffices to give two lenses $\varphi_p\colon p\otimes q(\1)\yon\to r$ and $\varphi_q\colon p(\1)\yon\otimes q\to r$ that agree on positions. The lens $\varphi_p$ says how information about $q$'s position is transferred to $p$, and the lens $\varphi_q$ says how information about $p$'s position is transferred to $q$.

\index{monoidal structure!parallel product as pushout}
\index{parallel product!as pushout}

\begin{corollary}\label{cor.tensor_as_pushout}
Suppose we have polynomials $p_1,\ldots,p_n\in\poly$. Then $p_1\otimes\cdots\otimes p_n$ is isomorphic to the wide pushout
\[
  \colim
  \left(
  \begin{tikzcd}[column sep=-10pt]
  	&
		p_1(\1)\yon\otimes\cdots\otimes  p_n(\1)\yon\ar[dl]\ar[dr]\\
		p_1\otimes p_2(\1)\yon\otimes\cdots\otimes p_n(\1)\yon&
		\cdots&
		p_1(\1)\yon\otimes\cdots\otimes p_{n-1}(\1)\yon \otimes p_n
  \end{tikzcd}
  \right)
\]
\end{corollary}
\begin{proof}
We proceed by induction on $n\in\nn$.
When $n=0$, the wide pushout has no legs and the empty parallel product is $\yon$, so the result holds.
If the result holds for $n$, then it holds for $n+1$ by \cref{prop.tensor_as_pushout}.
\end{proof}

\index{polynomial functor!colimits of polynomials|)}

%-------- Section --------%
\section{Vertical-cartesian factorization of lenses}

\index{factorization system!vertical-cartesian|(}


Aside from epi-mono factorization, there is another factorization system on $\poly$ that will show up frequently.

\begin{definition}[Vertical and cartesian lenses] \label{def.vert_cart}
Let $f\colon p\to q$ be a lens.
It is called \emph{vertical} if $f_\1\colon p(\1)\to q(\1)$ is an isomorphism.
It is called \emph{cartesian} if, for each $i\in p(\1)$, the function $f^\sharp_i\colon q[f(i)]\to p[i]$ is an isomorphism.
\end{definition}

\index{lens!vertical}
\index{lens!cartesian}


\begin{proposition}\label{prop.vert_cart_factorization}
Vertical and cartesian lenses form a factorization system of $\poly$.
\end{proposition}
\begin{proof}
It is easy to check that isomorphisms are both vertical and cartesian, and that vertical and cartesian lenses are each closed under composition.
It remains to show that every lens in $\poly$ can be uniquely (up to unique isomorphism) factored as a vertical lens composed with a cartesian lens.


Recall from \eqref{eqn.colax_poly_map} that a lens in $\poly$ can be written as to the left; we can thus rewrite it as to the right:
\[
\begin{tikzcd}[column sep=small]
	p(\1)\ar[dr, bend right, "{p[-]}"']\ar[rr, "f_\1"]&~&
	q(\1)\ar[dl, bend left, "{q[-]}"]\\&
	\smset\ar[u, phantom, near end, "\overset{f^\sharp}{\Leftarrow}"]
\end{tikzcd}
\hspace{1in}
\begin{tikzcd}
	p(\1)\ar[dr, bend right, "{p[-]}"']\ar[r, equal, ""' name=equal]&
	p(\1)\ar[d, "{q[f_\1(-)]}"]\ar[r, "f_\1"]&
	q(\1)\ar[dl, bend left, "{q[-]}"]\\&
	|[alias=set]|\smset\ar[from=equal, to=set, pos=.3, phantom, "\overset{f^\sharp}{\Leftarrow}"]
\end{tikzcd}
\]
We can see that the intermediary object $\sum_{i\in p(\1)} \yon^{q[f_\1i]}$ is unique up to unique isomorphism.
\end{proof}

\index{2-out-of-3}

\begin{proposition}
Vertical lenses satisfy 2-out-of-3: given $p\To{f}q\To{g}r$ with $h = f \then g$, if any two of $f,g,h$ are vertical, then so is the third.

If $g$ is cartesian, then $h$ is cartesian if and only if $f$ is cartesian.
\end{proposition}
\begin{proof}
Given $h = f \then g$, we have that $h_\1 = f_\1 \then g_\1$.
Since isomorphisms satisfy 2-out-of-3, it follows that vertical lenses satisfy 2-out-of-3 as well.

Now assume $g$ is cartesian.
On directions, $h = f \then g$ implies that for every $i \in p(\1)$, we have $h^\sharp_i = g^\sharp_{f_\1i} \then f^\sharp_i$.
Since $g^\sharp_{f_\1i}$ is an isomorphism, it follows that every $h^\sharp_i$ is an isomorphism if and only if every $f^\sharp_i$ is an isomorphism, so $h$ is cartesian if and only if $f$ is cartesian.
\end{proof}

\begin{exercise}
Give an example of polynomials $p,q,r$ and lenses $p\To{f}q\To{g}r$ such that $f$ and $f \then g$ are cartesian but $g$ is not.
\begin{solution}
Consider the lenses $\yon \To{f} \yon^\2 + \yon \To{g} \yon$ where $f$ is the canonical inclusion and $g$ is uniquely determined on positions and picks out $1 \in \2$ and $1 \in \1$ on directions.
Then the only on-directions function of $f$ is a function $\1 \to \1$, an isomorphism, so $f$ is cartesian.
Meanwhile, one of the on-directions functions of $g$ is a function $\1 \to \2$, which is not an isomorphism, so $g$ is not cartesian.
Finally, $f \then g$ can only be the unique lens $\yon \to \yon$, namely the identity, which is cartesian.
\end{solution}
\end{exercise}

Here is an alternative characterization of a cartesian lens in $\poly$.
Recall from \cref{exc.deriv-directions} that for any polynomial $p$, there is a corresponding function $\pi_p\colon\dot{p}(\1)\to p(\1)$, i.e.\ the set of all directions mapping to the set of positions.
A lens $(f_\1,f^\sharp)\colon p\to q$ can then be described as a function $f_\1\colon p(\1)\to q(\1)$ along with a function $f^\sharp$ that makes the following diagram in $\smset$ commute:
\begin{equation}\label{eqn.poly_map_usu}
\begin{tikzcd}
	\dot{p}(\1)\ar[d, "\pi_p"']&
	\bullet\ar[l, "f^\sharp"']\ar[r]\ar[d]&
	\dot{q}(\1)\ar[d, "\pi_q"]\\
	p(\1)\ar[r, equal]&
	p(\1)\ar[r, "f_\1"']&
	q(\1)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\end{equation}
Here, the pullback denoted by the dot $\bullet$ is the set of pairs comprised of a $p$-position $i$ and a $q[f_\1i]$-direction $e$.
The function $f^\sharp$ sends each such pair to a direction $f^\sharp_i(e)$ of $p$, and the commutativity of the left square implies that $f^\sharp_i(e)$ is specifically a $p[i]$-direction.
So $f^\sharp_i$ is indeed our familiar on-directions function $q[f_\1i]\to p[i]$, and $f^\sharp$ is just the sum of all these on-directions functions over $i\in p(\1)$.

\index{polynomial functor!pullback of polynomials}

\begin{exercise} \label{exc.cart_pullbacks}
Show that a lens $f\colon p\to q$ in $\poly$ is cartesian if and only if the square on the left hand side of \eqref{eqn.poly_map_usu} is also a pullback:
\[
\begin{tikzcd}
	\dot{p}(\1)\ar[d, "\pi_p"']&
	\bullet\ar[l, "f^\sharp"']\ar[r]\ar[d]&
	\dot{q}(\1)\ar[d, "\pi_q"]\\
	p(\1)\ar[r, equal]\ar[ur, phantom, very near end, "\llcorner"]&
	p(\1)\ar[r, "f_\1"']&
	q(\1)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
\begin{solution}
We wish to show that a lens $f\colon p\to q$ in $\poly$ is cartesian if and only if the square on the left hand side of \eqref{eqn.poly_map_usu} is a pullback.
We already know that that square commutes, so it is a pullback if and only if $f^\sharp$ is an isomorphism.
The right pullback square tells us that the $\bullet$ is $\sum_{i\in p(\1)}q[f_\1i]$.
So $f^\sharp_i\colon q[f_\1i]\to p[i]$ is an isomorphism for every $i\in p(\1)$ if and only if their sum $f^\sharp\colon\sum_{i\in p(\1)}q[f_\1i]\to\sum_{i\in p(\1)}p[i]\iso\dot{p}(\1)$ is an isomorphism as well.
Hence $f$ is cartesian if and only if $f^\sharp$ is an isomorphism, as desired.
\end{solution}
\end{exercise}

\begin{exercise}\index{global sections}
Is the pushout of a cartesian lens always cartesian?
\begin{solution}
The pushout of a cartesian lens is \emph{not} necessarily cartesian.
Take the pushout square \eqref{eqn.pushout_adjoint}.
The lens $!\colon p(\1)\yon\to\yon$ has $\1\to\1$ as every on-directions function, so it is cartesian, but its pushout $\eta\colon p\to\yon^{\Gamma(p)}$ is not going to be cartesian as long as there is some $i\in p(\1)$ for which $\Gamma(p)\not\iso p[i]$.
For instance, when $p\coloneqq\yon+\1$, we have that $\Gamma(p)\iso\0\not\iso\1\iso p[1]$, so $\eta$ is not cartesian.
\end{solution}
\end{exercise}

Why do we use the word \emph{cartesian} to describe cartesian morphisms? It turns out that, as natural transformations, cartesian morphisms are precisely what are known as cartesian natural transformations.

\index{natural transformation!cartesian|seealso{lens!cartesian}}\index{polynomial functor!pullback of polynomials}\index{pullback!cartesian natural transformation and}

\begin{definition}[Cartesian natural transformation] \label{def.cart_nat_trans}
A \emph{cartesian natural transformation} is a natural transformation whose naturality squares are all pullbacks.
That is, given categories $\cat{C},\cat{D}$, functors $F,G$, and natural transformation $\alpha$, we say that $\alpha$ is \emph{cartesian} if for all morphisms $h\colon c\to c'$ in $\cat{C}$,
\[
\begin{tikzcd}
    Fc \ar[d, "Fh"'] \ar[r, "\alpha_c"] & Gc \ar[d, "Gh"] \\
    Fd \ar[r, "\alpha_d"'] & Gd \ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
is a pullback.
\end{definition}

\begin{proposition}\label{prop.cart_as_nt}
Let $f\colon p\to q$ be a morphism in $\poly$. The following are equivalent:
	\begin{enumerate}
		\item viewed as a lens, $f$ is cartesian in the sense of \cref{def.vert_cart}: for each $i\in p(\1)$, the on-directions function $f^\sharp_i$ is a bijection;
		\item the square on the left hand side of \eqref{eqn.poly_map_usu} is also a pullback:
\[
\begin{tikzcd}
	\dot{p}(\1)\ar[d, "\pi_p"']&
	\bullet\ar[l, "f^\sharp"']\ar[r]\ar[d]&
	\dot{q}(\1)\ar[d, "\pi_q"]\\
	p(\1)\ar[r, equal]\ar[ur, phantom, very near end, "\llcorner"]&
	p(\1)\ar[r, "f_\1"']&
	q(\1)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
		\item viewed as a natural transformation, $f$ is cartesian in the sense of \cref{def.cart_nat_trans}: for any sets $A,B$ and function $h\colon A\to B$, the naturality square
\begin{equation} \label{eqn.cart_nt_pullback}
\begin{tikzcd}
	p(A)\ar[r, "f_A"]\ar[d, "p(h)"']&
	q(A)\ar[d, "q(h)"]\\
	p(B)\ar[r, "f_B"']&
	q(B)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\end{equation}
is a pullback.
  \end{enumerate}
\end{proposition}
\begin{proof}
We already showed that the first two are equivalent in \cref{exc.cart_pullbacks}, and we will complete this proof in \cref{exc.cart_as_nt}.
\end{proof}

\begin{exercise} \label{exc.cart_as_nt}
In this exercise, you will complete the proof of \cref{prop.cart_as_nt}.

First, we will show that $1\Rightarrow3$.
In the following, let $f\colon p\to q$ be a cartesian lens in $\poly$ and $h\colon A\to B$ be a function.
\begin{enumerate}
    \item Using \cref{prop.morph_arena_to_func} to translate $f$ from a lens in $\poly$ to a natural transformation and \cref{prop.poly_on_functions} to interpret $q(h)$, characterize the pullback of $p(B)\To{f_B}q(B)\From{q(h)}q(A)$ in $\smset$.
    \item Show that this pullback coincides with the naturality square \eqref{eqn.cart_nt_pullback}, hence proving $1\Rightarrow3$.
\end{enumerate}
Next, we show that $3\Rightarrow1$.
In the following, let $f\colon p\to q$ be a lens in $\poly$ that is cartesian when viewed as a natural transformation, so that \eqref{eqn.cart_nt_pullback} is a pullback for any function $h\colon A\to B$.
Also fix $i\in p(\1)$.
\begin{enumerate}[resume]
    \item Show that the diagram
    \begin{equation} \label{eqn.cart_nt_pullback_cone}
        \begin{tikzcd}[column sep=50pt]
        	\1 \ar[r, "{(f_\1i,\,\id_{q[f_\1i]})}"]\ar[d, "{(i,\,\id_{p[i]})}"']&
        	q(q[f_\1i])\ar[d, "q(f^\sharp_i)"]\\
        	p(p[i])\ar[r, "f_{p[i]}"']&
        	q(p[i])\ar[ul, phantom, very near end, "\lrcorner"]
        \end{tikzcd}
    \end{equation}
    commutes.
    Hint: Use \cref{prop.poly_on_functions}, \cref{prop.morph_arena_to_func}, and/or \cref{cor.morph_func_to_arena}.
    \item Apply the universal property of the pullback \eqref{eqn.cart_nt_pullback} to the diagram \eqref{eqn.cart_nt_pullback_cone} above to exhibit an element of $p(q[f_\1i])$.
    Conclude from the existence of this element that $f^\sharp_i$ is an isomorphism, hence proving $3\Rightarrow1$.\qedhere
\end{enumerate}
\begin{solution}
First, we will show that $1\Rightarrow3$ in \cref{prop.cart_as_nt}.
Here $f\colon p\to q$ is a cartesian lens in $\poly$ and $h\colon A\to B$ is a function.
\begin{enumerate}
    \item An element of $p(B)$ is a pair comprised of a $p$-position $i$ and a function $k\colon p[i]\to B$, and \cref{prop.morph_arena_to_func} tells us that $f_B\colon p(B)\to q(B)$ sends $(i,k)\mapsto(f_\1i,f^\sharp_i\then k)$.
    Meanwhile, an element of $q(A)$ is a pair comprised of a $q$-position $j$ and a function $\ell\colon q[j]\to A$, and \cref{prop.poly_on_functions} tells us that $q(h)$ sends $(j,\ell)\mapsto(j,\ell\then h)$.
    So $((i,k),(j,\ell))$ is in the pullback of $p(B)\To{f_B}q(B)\From{q(h)}q(A)$ if and only if $f_\1i=j$ and $f^\sharp_i\then k=\ell\then h$.

    As $f$ is cartesian, $f^\sharp_i$ is an isomorphism, so we can rewrite the latter equation as $k=g_i\then\ell\then h$, where $g_i$ is the inverse of $f^\sharp_i$.
    In fact, if we let $\ell'\coloneqq g_i\then\ell$, we observe that the values of $j,k,$ and $\ell$ are all already determined by the values of $i$ and $\ell'$: we have that $j=f_\1i$, that $k=\ell'\then h$, and that $\ell=f^\sharp_i\then\ell'$
    It follows that the pullback is equivalently the set of pairs $(i,\ell')$ comprised of a $p$-position $i$ and a function $\ell'\colon p[i]\to A$ (with no other restrictions on $i$ and $\ell'$).
    The projection from the pullback to $p(B)$ sends $(i,\ell')\mapsto(i,\ell'\then h)$, and the projection from the pullback to $q(A)$ sends $(i,\ell')\mapsto(f_\1i,f^\sharp_i\then\ell')$.
    \item The pullback described above---the set of pairs $(i,\ell')$ comprised of a $p$-position $i$ and a function $\ell'\colon p[i]\to A$---is exactly the set $p(A)$.
    Moreover, the projection to $p(B)$ sending $(i,\ell')\mapsto(i,\ell'\then h)$ is $p(h)$, and the projection to $q(A)$ sending $(i,\ell')\mapsto(f_\1i,f^\sharp_i\then\ell')$ is $f_A$ by \cref{prop.morph_arena_to_func}.
    So \eqref{eqn.cart_nt_pullback} is a pullback, as desired.
\end{enumerate}
Next, we will show that $3\Rightarrow1$ in \cref{prop.cart_as_nt}, with $f\colon p\to q$ as a lens in $\poly$ that is a cartesian natural transformation and $i\in p(\1)$.
\begin{enumerate}[resume]
    \item By \cref{cor.morph_func_to_arena}, we have that $f_{p[i]}$ sends $(i,\id_{p[i]})\mapsto(f_\1i,f^\sharp_i)$, and by \cref{prop.poly_on_functions}, we have that $q(f^\sharp_i)$ sends $(f_\1i,\id_{q[f_\1i]})\mapsto(f_\1i,f^\sharp_i)$ as well.
    Hence \eqref{eqn.cart_nt_pullback_cone} commutes.

    \item Taking $A\coloneqq q[f_\1i], B\coloneqq p[i],$ and $h\coloneqq f^\sharp_i$ in \eqref{eqn.cart_nt_pullback} and applying its universal property to \eqref{eqn.cart_nt_pullback_cone} induces an element $(i',g)$ of $p(q[f_\1i])$, with $i'\in p(\1)$ and $g\colon p[i']\to q[f_\1i]$, such that $p(f^\sharp_i)$ sends $(i',g)\mapsto(i,\id_{p[i]})$ and $f_{q[f_\1i]}$ sends $(i',g)\mapsto(f_\1i,\id_{q[f_\1i]})$.
    It follows from the behavior of $p(f^\sharp_i)$ (by \cref{prop.poly_on_functions}) that $i'=i$ and $g\then f^\sharp_i=\id_{p[i]}$, and it follows from the behavior of $f_{q[f_\1i]}$ (by \cref{prop.morph_arena_to_func}) that $f^\sharp_i\then g=\id_{q[f_\1i}$.
    So $g$ is the inverse of $f^\sharp_i$, proving that $f^\sharp_i$ is an isomorphism, as desired.
\end{enumerate}
\end{solution}
\end{exercise}

\index{monoidal structure!preservation of vertical and cartesian maps}

\begin{proposition}\label{prop.monoidal_pres_vert_cart}
The monoidal structures $+$, $\times$, and $\otimes$ preserve both vertical and cartesian morphisms.
\end{proposition}
\begin{proof}
Suppose that $f\colon p\to p'$ and $g\colon q\to q'$ are vertical, so that the on-positions functions $f_\1$ and $g_\1$ are isomorphisms.

We can obtain the on-positions function of a lens by passing it through the functor $\poly\To{p(\1)}\smset$ from \cref{thm.adjoint_quadruple}.
As this functor is both a left adjoint and a right adjoint, it preserves both sums and products, so $(f+g)_\1 = f_\1+g_\1$ and $(f\times g)_1 = f_1\times g_1$.
Hence $f+g$ and $f\times g$ are both vertical.
On-positions, the behavior of $\otimes$ is identical to the behavior of $\times$, so $f\otimes g$ must be vertical as well.

Now suppose that $f\colon p\to p'$ and $g\colon q\to q'$ are cartesian.

A position of $p+q$ is a position $i\in p(\1)$ or a position $j\in q(\1)$, and the map $(f+g)^\sharp$ at that position is either $f^\sharp_i$ or $g^\sharp_j$; either way it is an isomorphism, so $f+g$ is cartesian.

A position of $p\times q$ (resp.\ of $p\otimes q$) is a pair $(i,j)\in p(\1)\times q(\1)$. The lens $(f\times g)^\sharp_{(i,j)}$ (resp.\ $(f\otimes g)^\sharp_{(i,j)}$) is $f^\sharp_i+g^\sharp_j$ (resp.\ $f^\sharp_i\times g^\sharp_j$) which is again an isomorphism if $f^\sharp_i$ and $g^\sharp_j$ are. Hence $f\times g$ (resp.\ $f\otimes g$) is cartesian, completing the proof.
\end{proof}

\begin{proposition}\label{prop.pullback_vert_cart}
Pullbacks preserve vertical (resp.\ cartesian) lenses.
In other words, if $f\colon p\to q$ is a lens and $g\colon q'\to q$ a vertical (resp.\ cartesian) lens, then the pullback $g'$ of $g$ along $p$
\[
\begin{tikzcd}
	p\times_qq'\ar[r]\ar[d, "g'"']&
	q'\ar[d, "g"]\\
	p\ar[r, "f"']&
	q\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
is vertical (resp.\ cartesian).
\end{proposition}
\begin{proof}
This follows from \cref{ex.pullbacks_in_poly}, since the pullback (resp.\ pushout) of an isomorphism is an isomorphism.
\end{proof}

\index{factorization system!vertical-cartesian|)}

%-------- Section --------%
\section{Monoidal $*$-bifibration over $\smset$}

\index{monoidal $*$-bifibraion}

We conclude this chapter by showing that the functor $p\mapsto p(\1)$ has special properties that make it what \cite{shulman2008framed} refers to as a \emph{monoidal $*$-bifibration}.
Roughly speaking, this means that $\smset$ acts as a sort of remote controller on the category $\poly$, grabbing every polynomial by its positions and pushing or pulling it this way and that.
The material in this section is even more technical than the rest of this chapter, and we won't use it again in the book, so the reader may wish to skip to \cref{part.comon}.

As an example, suppose one has a set $A$ and a function $f\colon A\to p(\1)$, which we can also think of as a cartesian lens between constant polynomials.
From $f$, we can obtain a new polynomial $f^*p$ with position-set $A$ via a pullback
\begin{equation}\label{eqn.f^*_defined}
\begin{tikzcd}
	f^*p\ar[r, "\fun{cart}"]\ar[d]&
	p\ar[d, "\eta_p"]\\
	A\ar[r, "f"']&
	p(\1)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\end{equation}
Here $\eta_p$ is the unit of the adjunction $\adjr{\smset}{A}{p(\1)}{\poly}$; it is a vertical lens.
We could evaluate this pullback using \cref{ex.pullbacks_in_poly}.
Alternatively, we can use \cref{prop.pullback_vert_cart} to deduce that the top lens $f^*p\to p$ (which we presciently labeled $\fun{cart}$) is cartesian like $f$ and that the left lens $f^*p\to A$ is vertical like $\eta_p$. Furthermore, $\fun{cart}_1 = f$.
Hence
\[
    f^*p \iso \sum_{a \in A} \yon^{p[f(a)]}.
\]
We'll see this as part of a bigger picture in \cref{prop.basechange,thm.triple_adjoint_basechange}, but first we need the following definitions and a result about cartesian lenses.

\begin{definition}[Slice category] \label{def.slice}
Given an object $c$ in a category $\cat{C}$, the \emph{slice category} of $\cat{C}$ over $c$, denoted $\cat{C}/c$, is the category whose objects are morphisms in $\cat{C}$ with codomain $c$ and whose morphisms are commutative triangles in $\cat{C}$.
\end{definition}

\begin{definition}[Exponentiable morphism]
Given a category $\cat{C}$ with objects $c, d$ and morphism $f \colon c \to d$ such that all pullbacks along $f$ exist in $\cat{C}$, we say that $f$ is \emph{exponentiable} if the functor $f^* \colon \cat{C}/d \to \cat{C}/c$ given by pulling back along $f$ is a left adjoint.
\end{definition}

\index{lens!cartesian}\index{lens!exponentiable}\index{exponentiable lens|see{lens, cartesian}}

\begin{theorem}\label{thm.cart_exponentiable}
Cartesian lenses in $\poly$ are exponentiable.
That is, if $f\colon p\to q$ is cartesian, then the functor $f^*\colon\poly/q\to\poly/p$ given by pulling back along $f$ is a left adjoint:
\[
\begin{tikzcd}[column sep=50pt, background color=theoremcolor]
	\poly/p\ar[r, shift right=7pt, "f_*"']&
	\poly/q\ar[l, shift right=7pt, "f^*"']\ar[l, phantom, "\Leftarrow"]
\end{tikzcd}
\]
\end{theorem}
\begin{proof}
Fix $e\colon p'\to p$ and $g\colon q'\to q$.
\[
\begin{tikzcd}
	p'\ar[d, "e"']&q'\ar[d, "g"]\\
	p\ar[r, "f"']&q
\end{tikzcd}
\]
We need to define a functor $f_*\colon\poly/p\to\poly/q$ and prove the analogous isomorphism establishing it as right adjoint to $f^*$. We first establish some notation. Given a set $Q$ and sets $(P'_i)_{i\in I}$, each equipped with a map $Q\to P'_i$, let $Q/\sum_{i\in I}P'_i$ denote the coproduct in $Q/\smset$, or equivalently the wide pushout of sets $P'_i$ with apex $Q$. Then we give the following formula for $f_*p'$, which we write in larger font for clarity:
\begin{equation}\label{eqn.cart_exp}
f_*p'\coloneqq
\scalebox{1.3}{$\displaystyle
\sum_{j\in q(\1)}\;\sum_{i'\in\prod\limits_{i\in f_\1\inv(j)}e_\1\inv(i)}\;\yon^{q[j]/\sum_{i\in f_\1\inv(j)}p'[i'(i)]}
$}
\end{equation}
Again, $q[j]/\sum_{i\in f_\1\inv(j)}p'[i'(i)]$ is the coproduct of the $p'[i'(i)]$, taken in $q[j]/\smset$. Since $p[i]\cong q[f(i)]$ for any $i\in p(\1)$ by the cartesian assumption on $f$, we have the following chain of natural isomorphisms\index{isomorphism!natural}
\begin{align*}
	(\poly/p)(f^*q', p')&\cong
	\prod_{i\in p(\1)}\;\prod_{\{j'\in q'(\1)\,\mid\,g_\1(j')=f_\1i\}}\;\sum_{\{i'\in p'(\1)\,\mid\,e_\1(i')=i\}}\;(p[i]/\smset)(p'[i'],p[i]+_{q[f(i)]}q'[j'])\\&\cong
	\prod_{i\in p(\1)}\;\prod_{\{j'\in q'(\1)\,\mid\,g_\1(j')=f_\1i\}}\;\sum_{\{i'\in p'(\1)\,\mid\,e_\1(i')=i\}}\;(q[f(i)]/\smset)(p'[i'],q'[j'])\\&\cong
	\prod_{j\in q(\1)}\;\prod_{\{j'\in q'(\1)\,\mid\, g_\1(j')=j\}}\;\prod_{\{i\in p(\1)\,\mid\,f_\1i=j\}}\;\sum_{\{i'\in p'(\1)\,\mid\,e_\1(i')=i\}}\;(q[j]/\smset)(p'[i'],q'[j'])\\&\cong
	\prod_{j\in q(\1)}\;\prod_{\{j'\in q'(\1)\,\mid\, g_\1(j')=j\}}\;\sum_{i'\in\prod_{i\in f_\1\inv(j)}e_\1\inv(i)}\;\prod_{i\in f_\1\inv(j)}\;(q[j]/\smset)(p'[i'(i)],q'[j'])\\&\cong
	\prod_{j\in q(\1)}\;\prod_{\{j'\in q'(\1)\,\mid\, g_\1(j')=j\}}\;\sum_{i'\in\prod_{i\in f_\1\inv(j)}e_\1\inv(i)}\;(q[j]/\smset)\Big(\sum_{i\in f_\1\inv(j)}p'[i'(i)],q'[j']\Big)\\&\cong
	(\poly/q)(q',f_*p')
\end{align*}
\end{proof}

\begin{example}
Let $p\coloneqq\2\yon^\2$, $q\coloneqq\yon^\2+\yon^\0$, and $f\colon p\to q$ the unique cartesian lens between them.
Then for any $e\colon p'\to p$ over $p$, \eqref{eqn.cart_exp} provides the following description for the pushforward $f_*p'$.
%We use the isomorphisms $p(\1)\cong\2$ and $q(\1)\cong\2$ to talk about the positions of $p$ and $q$.

Over the $j=2$ position, $f_\1\inv(2)=\0$ and $q[2]=\0$, so $\prod_{i \in f_\1\inv(2)} e_\1\inv(i)$ is an empty product and $q[2]/\sum_{i\in f_\1\inv(2)} p'[i'(i)]$ is an empty pushout.
Hence the corresponding summand of \eqref{eqn.cart_exp} is simply $\yon^\0\cong\1$.

Over the $j=1$ position, $f_\1\inv(1)=\2$ and $q[1]=p[1]=p[2]=\2$, so $\prod_{i'\in f_\1\inv(1)} e_\1\inv(i) \iso e_\1\inv(1)\times e_\1\inv(2)$.
For $i' \in e_\1\inv(1) \times e_\1\inv(2)$, we have that $q[1]/\sum_{i\in f_\1\inv(2)} p'[i'(i)] \iso X_{i'}$ in the following pushout square:
\[
\begin{tikzcd}
	X_{i'} \ar[from=r] \ar[from=d] &
	p'[i'(2)] \ar[from=d, "e^\sharp_{i'(2)}"'] \\
	p'[i'(1)] \ar[from=r, "e^\sharp_{i'(1)}"] &
	\2 \ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
Then in sum we have
\[
    f_*p' \iso \left(\sum_{i' \in e_\1\inv(1) \times e_2\inv(2)} \yon^{X_{i'}}\right) + \1.
\]
\end{example}

\begin{exercise}
Prove that the unique lens $f\colon\yon\to\1$ is exponentiable.
\begin{solution}
Choose $p\in\poly$ and $q'\in\poly/\yon$. Then there is $q\in\poly$ such that $q'\cong q\yon$, equipped with the projection $q\yon\to\yon$. The pushforward is given by the exponential
\[f_*(q\yon)\coloneqq q^\yon\]
from the cartesian closure; see \eqref{eqn.exponential}. Indeed, we have
\begin{align*}
	\poly/\yon(f^*p,q\yon)&\cong
	\poly/\yon(p\yon,q\yon)\\&\cong
	\poly(p\yon,q)\\&\cong
	\poly(p,q^\yon).
\end{align*}
\end{solution}
\end{exercise}

For any set $A$, let $\poly[A.]$ denote the category whose objects are polynomials $p$ equipped with an isomorphism $A\cong p(\1)$, and whose morphisms are lenses respecting the isomorphisms with $A$.

\begin{proposition}[Base change]\label{prop.basechange}
For any function $f\colon A\to B$, pullback $f^*$ along $f$ induces a functor $\poly[B.]\to\poly[A.]$, which we also denote $f^*$.
\end{proposition}
\begin{proof}
This follows from \eqref{eqn.pullback_poly} with $q\coloneqq A$ and $r\coloneqq B$, since pullback of an iso is an iso.
\end{proof}

\begin{theorem}\label{thm.triple_adjoint_basechange}
For any function $f\colon A\to B$, the pullback functor $f^*$ has both a left and a right adjoint
\begin{equation}\label{eqn.adjoint_triple_monoidal_fib}
\begin{tikzcd}[column sep=large, background color=theoremcolor]
	\poly[A.]\ar[r, shift left=16pt, "f_!"]\ar[r, shift right=16pt, "f_*"']
	\ar[r, phantom, shift left=9pt, "\Rightarrow"]\ar[r, phantom, shift right=9pt, "\Leftarrow"]
&
	\poly[B.]\ar[l, "f^*" description]
\end{tikzcd}
\end{equation}
Moreover $\otimes$ preserves the op-cartesian arrows, making this a monoidal $*$-bifibration in the sense of \cite[Definition 12.1]{shulman2008framed}.
\end{theorem}
\begin{proof}
Let $p$ be a polynomial with $p(\1)\cong A$. Then the formula for $f_!p$ and $f_*p$ are given as follows:
\begin{equation}\label{eqn.f_!andf_*}
f_!p\cong\scalebox{1.3}{$\displaystyle\sum_{b\in B}\yon^{\;\prod\limits_{a\mapsto b}p[a]}$}
\qqand
f_*p\cong\scalebox{1.3}{$\displaystyle\sum_{b\in B}\yon^{\;\sum\limits_{a\mapsto b}p[a]}$}
\end{equation}
It may at first be counterintuitive that the left adjoint $f_!$ involves a product and the right adjoint $f_*$ involves a sum. The reason for this comes from the fact that $\poly$ is equivalent to the Grothendieck construction applied to the functor $\smset\op\to\smcat$ sending each set $A$ to the category $(\smset/A)\op$. The fact that functions $f\colon A\to B$ induces an adjoint triple between $\smset/A$ and $\smset/B$, and hence between $(\smset/A)\op$ and $(\smset/B)\op$ explains the variance in \eqref{eqn.f_!andf_*} and simultaneously establishes the adjoint triple \eqref{eqn.adjoint_triple_monoidal_fib}.

The functor $p\mapsto p(\1)$ is strong monoidal with respect to $\otimes$ and strict monoidal if we choose the lens construction as our model of $\poly$. By \cref{prop.monoidal_pres_vert_cart}, the monoidal product $\otimes$ preserves cartesian lenses; thus we will have established the desired monoidal $*$-bifibration in the sense of \cite[Definition 12.1]{shulman2008framed} as soon as we know that $\otimes$ preserves op-cartesian lenses.

Given $f$ and $p$ as above, the op-cartesian lens is the lens $p\to f_!p$ obtained as the composite $p\to f^*f_!p\to f_!p$ where the first lens is the unit of the $(f_!,f^*)$ adjunction and the second is the cartesian lens for $f_!p$. On positions $p\to f_!p$ acts as $f$, and on directions it is given by projection.

If $f\colon p(\1)\to B$ and $f'\colon p'(\1)\to B'$ are functions then we have
\begin{align*}
	f_!(p)\otimes f'_!(p')&\cong
	\sum_{b\in B}\sum_{b'\in B'}\yon^{\big(\prod_{a\mapsto b}p[a]\big)\times\big(\prod_{a'\mapsto b'}p'[a']\big)}\\&\cong
	\sum_{(b,b')\in B\times B'}\yon^{\big(\prod_{(a,a')\mapsto(b,b')}p[a]\times p[b]\big)}\\&
	\cong (f_!\otimes f'_!)(p\otimes p')
\end{align*}
and the op-cartesian lenses are clearly preserved since projections in the second line match with projections in the first.
\end{proof}

%-------- Section --------%
\section{Summary and further reading}

In this chapter we discussed several of the nice properties of the category $\poly$: it has various adjunctions to $\smset$ and $\smset\op$, is Cartesian closed, has limits and colimits, has an epi-mono factorization system, has a vertical-cartesian factorization system, and comes with a monoidal $*$-bifibration to $\smset$.

The principal monomial functor $p\mapsto p(1)\yon^{\Gamma p}$ discussed in \cref{cor.principal_monomial} is in fact distributive monoidal, and this comes up in work on entropy \cite{spivak2022polynomial} and on noncooperative strategic games \cite{capucci2022diegetic}.

\index{monomial!principal}\index{principal monomial|see{monomial, principal}}


%-------- Section --------%
\section{Exercise solutions}
\Closesolutionfile{solutions}
{\footnotesize
\input{solution-file5}}

\end{document}