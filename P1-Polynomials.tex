% !TeX root = P1-Polynomials.tex
\documentclass[Book-Poly]{subfiles}
\begin{document}


\setcounter{chapter}{0}%Just finished 0.

%---------------- Part ----------------%
\part{The category of polynomial functors}\label{part.poly}

\Opensolutionfile{solutions}[solution-file1]

%------------ Chapter ------------%
\chapter{Introduction: Perspectives~on~polynomials}\label{ch.poly.intro}

\begin{quote}
It is a treasury box!\\
Full of unexpected connections!\\
It is fascinating!\\
I will think about it.\\
\mbox{}\hfill --Andr\'e Joyal, Summer 2020,\\
\mbox{}\hfill personal communication.
\end{quote}

In this book we will investigate a remarkable category called $\poly$. We will see its intimate relationships with dynamic processes, decision-making, and the storage and transformation of data. But our story begins with something quite humble: high school algebra.
\begin{align}\label{eqn.poly_example}
\yon^\2+\2\yon+\1 \quad&\quad
\textit{polynomial}
\intertext{
All our polynomials will involve one variable, $\yon$, chosen for reasons we'll explain soon. Polynomials in one variable can be drawn as a forest of mini-trees:
}
\label{eqn.forest_example}
\begin{tikzpicture}[trees]
  \node (1) {$\bullet$} 
    child {}
    child {};
  \node[right=.5 of 1] (2) {$\bullet$} 
    child {};
  \node[right=.5 of 2] (3) {$\bullet$} 
    child {};
  \node[right=.5 of 3] (4) {$\bullet$};
\end{tikzpicture}
\quad&\quad\textit{forest}
\end{align}
More technically, each mini-tree---a rooted tree whose \emph{leaves} (the arrows) are all children of the \emph{root} (the solid dot)---is called a \emph{corolla}.
So our \emph{forests} are always unions of corollas.
Each corolla in \eqref{eqn.forest_example} corresponds to a \emph{pure-power summand} of the form $\yon^A$ in the polynomial given in \eqref{eqn.poly_example}: the corolla with $2$ leaves corresponds to $\yon^\2$; the two corollas with $1$ leaf each correspond to the two copies of $\yon = \yon^\1$; and the corolla with no leaves corresponds to $\1 = \yon^0$.

We can label the roots and leaves of our forest, like so:
\[
%\label{eqn.labeled_forest}
\begin{tikzpicture}[trees]
  \node["$1$" below] (1) {$\bullet$} 
    child {node {$1$}}
    child {node {$2$}};
  \node["$2$" below, right=.5 of 1] (2) {$\bullet$} 
    child {node {$1$}};
  \node["$3$" below, right=.5 of 2] (3) {$\bullet$} 
    child {node {$1$}};
  \node["$4$" below, right=.5 of 3] (4) {$\bullet$};
\end{tikzpicture}
\]
This suggests yet another depiction of the polynomial \eqref{eqn.poly_example}: as a \emph{dependent set}\footnote{Perhaps better known as an \emph{indexed collection} (or \emph{family}) \emph{of sets}. But we refer to these as \emph{dependent sets} to compare them to sets, the same way dependent types compare to types.} $(p[i])_{i \in I}$.
Here $I = \4 = \{1, 2, 3, 4\}$\footnote{
In standard font, 4 represents the usual natural number. In sans serif font, \4 represents the set $\4=\{1,2,3,4\}$ with 4 elements.
}, the set of roots, and
\begin{align} \label{eqn.arena_example}
p[1] = \2 = \{1, 2\}, \quad p[2] = \1 = \{1\}, \quad p[3] = \1 = \{1\}, \quad p[4] = \0 = \varnothing, \quad&\quad\textit{arena}
\end{align}
so that for each root $i \in I$, the set $p[i]$ consists of the leaves at that root.
We call the entire dependent set $(p[i])_{i \in I}$ an \emph{arena}.
Each element $i \in I$ is a \emph{position} in the arena, and each element $d \in p[i]$ is a \emph{direction} at position $i$.
So the relationship between the arena perspective and the forest picture is that positions are roots and directions are leaves.

We will sometimes refer to the index set $I$ as the \emph{position-set} of $p$, each element $i \in I$ as a \emph{$p$-position}, each set $p[i]$ as the \emph{direction-set} at $i$ of $p$, and each element $d \in p[i]$ as a \emph{$p[i]$-direction}.

Throughout this book, we will see several other ways of interpreting polynomials.
Here is a table of terminology, capturing five different perspectives from which we may view our objects of study.
The first row shows the algebraic notation, as in \eqref{eqn.poly_example};
the second row shows the dependent set terminology, as in \eqref{eqn.arena_example};
the third shows the pictorial terminology of trees, as in \eqref{eqn.forest_example};
the fourth shows dynamical systems terminology, which we will explore in \cref{ch.poly.dyn_sys};
and the fifth row shows decision-making terminology, which we will introduce in \cref{sec.poly.intro.dec}.

\begin{equation}%\label{eqn.table_terminology}
\footnotesize
\begin{tabular}{l|l|l|l}
\multicolumn{4}{c}{\normalsize Polynomial Terminology}\\[3pt]
\textbf{algebra} & $p \coloneqq \sum_{i \in p(\1)} \yon^{p[i]}$ & $i \in p(\1)$ & $d \in p[i]$ \\
\textbf{dependent sets} & arena & position & direction \\
\textbf{tree pictures} & corolla forest & root $\bullet$ & leaf $\uparrow$ \\
\textbf{dynamics} & (mode-dependent) interface & output & input \\
\textbf{decisions} & menu & decision & option \\
\end{tabular}
\end{equation}

\begin{remark}
Though a polynomial will turn out to be a \emph{functor}, while an arena is a \emph{dependent set}, they are so closely related that we often do not make a distinction between a polynomial $p$ and its arena $(p[i])_{i \in I}$; they are essentially two different syntaxes for the same object.
For example, we may often directly refer to the positions and directions of a polynomial, when we mean the positions and directions of its associated arena.
\end{remark}

\begin{exercise}%\label{exc.forest}
Consider the polynomial $p\coloneqq\2\yon^\3+\2\yon+\1$ and the associated corolla forest and arena.
\begin{enumerate}
	\item Draw the polynomial $p$ as a corolla forest.
	\item How many roots does this forest have?
	\item How many positions in the arena does this represent?
	\item For each corolla in the forest, say how many leaves it has.
	\item For each position in the arena, how many directions does it have? \qedhere
\end{enumerate}
\begin{solution}
We consider the polynomial $p\coloneqq\2\yon^\3+\2\yon+\1$.
\begin{enumerate}
	\item Here is $p$ drawn as a forest of corollas (note that the order in which the corollas are drawn does not matter):
	\[
	\begin{tikzpicture}[trees, sibling distance=3mm]
    \node (1) {$\bullet$} 
      child {}
      child {}
      child {};
    \node[right=.7 of 1] (2) {$\bullet$} 
      child {}
      child {}
      child {};
    \node[right=.5 of 2] (3) {$\bullet$} 
      child {};
    \node[right=.3 of 3] (4) {$\bullet$} 
      child {};
    \node[right=.3 of 4] (5) {$\bullet$};
  \end{tikzpicture}
  \]
	\item It has five (5) roots.
	\item It represents five positions, one per root.
	\item \label{sol.forest.leaves} The first and second corollas have three leaves, the third and fourth corollas have one leaf, and the fifth corolla has no leaves.
	\item The set of directions for each position is the same as the set of leaves for each corolla, so just copy the answer from \cref{sol.forest.leaves}, replacing ``corolla'' with ``position'' and ``leaf'' with ``direction.'' Sheesh! Who wrote this.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Consider the polynomial $q\coloneqq\yon^\8+\4\yon$.
\begin{enumerate}
	\item Does the polynomial $q$ have a pure-power summand $\yon^\2$?
	\item Does the polynomial $q$ have a pure-power summand $\yon$?
	\item Does the polynomial $q$ have a pure-power summand $\4\yon$?
	\qedhere
\end{enumerate}
\begin{solution}
We refer to the polynomial $q\coloneqq\yon^\8+4\yon$.
\begin{enumerate}
	\item No, $q$ does not have $\yon^\2$ as a pure-power summand.
	\item Yes, $q$ does have $\yon$ as a pure-power summand.
	\item No, $q$ does not have $\4\yon$ as a pure-power summand, because $\4\yon$ is not a pure-power! But to make amends, we could say that $\4\yon$ is a summand; this means that there is some $q'$ such that $q=q'+\4\yon$. So $\3\yon$ is also a summand, but $\5\yon$ and $\yon^\2$ are not.
\end{enumerate}
\end{solution}
\end{exercise}

One feature that sets our polynomials apart from the polynomials we are familiar with from high school algebra is that the coefficients and exponents are not, strictly speaking, numbers; rather, they are sets, like $\1 = \{1\}$ and $\2 = \{1, 2\}$.
In fact, they can be arbitrary sets, as in $B\yon^A + D\yon^C$ for sets $A, B, C, D$, including infinite ones, as in $\rr\yon^\nn + \2^\rr\yon^\rr$.
Any finite or infinite sum of pure-power summands, each with a finite or infinite set as an exponent, is still a polynomial.
Of course, this makes their forests rather unwieldy to draw, but they can be approximated.
We sketch the polynomial $\yon^\3 + \nn\yon^{[0,1]}$ as a forest below.
\[%\label{eqn.represented_interval}
\begin{tikzpicture}[trees, sibling distance=0.0625mm]
  \node (1) {$\bullet$} 
    child[sibling distance=3mm] foreach \i in {1,2,3}
    ;
  \node[right=1 of 1] (2) {$\bullet$} 
    child foreach \i in {1,...,160}
    ;
  \node[right=1 of 2] (3) {$\bullet$} 
    child foreach \i in {1,...,160}
    ;
  \node[right=1 of 3] (4) {$\bullet$} 
    child foreach \i in {1,...,160}
    ;
  \node[right=.7 of 4] (5) {$\cdots$};
\end{tikzpicture}
\]

\begin{exercise}%\label{exc.suitor_love}
If you were a suitor choosing the corolla forest you love, aesthetically speaking, which would strike your interest? Answer by circling the associated polynomial:
\begin{enumerate}
	\item $\yon^\2+\yon+\1$
	\item $\yon^\2+\3\yon^\2+\3\yon+\1$
	\item $\yon^\2$
	\item $\yon+\1$
	\item $(\nn\yon)^\nn$
	\item $S\yon^S$
	\item $\yon^{\1\0\0}+\yon^\2+\3\yon$
	\item $\yon + \2\yon^\4 + \3\yon^\9 + \4\yon^{\1\6} + \cdots$
	\item Your polynomial's name $p$ here.
\end{enumerate}
Any reason for your choice? Draw a sketch of your forest.
\begin{solution}
Aesthetically speaking, here's the associated polynomial of a beautiful corolla forest:
\[
\yon^\0+\yon^\1+\yon^\2+\yon^\3+\cdots
\]
It's reminiscent (and formally related) to the notion of lists: if $A$ is any set, then $A^\0+A^\1+A^\2+\cdots$ is the set of lists (i.e.\ finite ordered sequences) with entries in $A$. 

Here's a picture of this lovely forest:
\[
	\begin{tikzpicture}[trees, sibling distance=3mm]
    \node (1) {$\bullet$};
    \node[right=.3 of 1] (2) {$\bullet$}
      child {};
    \node[right=.4 of 2] (3) {$\bullet$} 
      child {}
      child {};
    \node[right=.6 of 3] (4) {$\bullet$} 
      child {}
      child {}
      child {};
    \node[right=.6 of 4] {$\cdots$};
  \end{tikzpicture}
\]
\end{solution}
\end{exercise}

Before we can really get into this story, let's summarize where we're going: polynomials are going to have really surprising applications to dynamics, decisions, and data. We speak superlatively of $\poly$:
\slogan{
The category of polynomial functors is a jackpot. Its beauty flows without bound}
but we have not yet begun to deliver. So let's introduce some of the applications and mathematics to come.

%-------- Section --------%
\section{Dynamical systems} \label{sec.poly.intro.dyn_sys}

% ** Explain how people should think about dynamical systems: a clock, computer...
% connecting them
% maybe draw the same picture

You may already be familiar with dynamical systems---machines of various sorts---which have an internal state that can be read out to other systems, as well as updated based on input received from other systems. In this book, we'll be looking mainly at deterministic systems, but with a lot of interesting new options:
\begin{enumerate}
	\item The interface of the system---the way in which it can be interacted with---can change shape through time.
	\item The wiring diagram connecting a bunch of systems can change through time.
	\item One can speed up the dynamics of a system.
	\item One can introduce ``effects,'' i.e.\ as defined by monads on $\smset$.
	\item The dynamical systems on any interface form a topos.
\end{enumerate}
To give some intuition for the first two, imagine yourself as a system, wired up to other systems. You have some input ports: your eyes, your ears, etc., and you have some output ports: your speech, your gestures, etc. And you connect with other systems: your family, your colleagues, the GPS of your phone, etc.
\begin{equation}\label{eqn.wired_forever}
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bb min width =.5cm, bbx=.5cm, bb port sep =1,bb port length=0, bby=.15cm]
	\node[bb={2}{2}, green!25!black] (X11) {\tiny Alice};
	\node[bb={3}{3}, green!25!black, below right=of X11] (X12) {\tiny you};
	\node[bb={2}{1}, green!25!black, above right=of X12] (X13) {\tiny Bob};
	\node[bb={2}{2}, green!25!black, below right = -1 and 1.5 of X12] (X21) {\tiny GPS};
	\node[bb={1}{2}, green!25!black, above right=-1 and 1 of X21] (X22) {\tiny me};
  \node[bb={2}{2}, fit = {($(X11.north east)+(-1,4)$) (X12) (X13) ($(X21.south)$) ($(X22.east)+(.5,0)$)}, bb name = {\small Wired together like this forever?}] (Z) {};
	\draw (X21_out1) to (X22_in1);
	\draw let \p1=(X22.north east), \p2=(X21.north west), \n1={\y1+\bby}, \n2=\bbportlen in
          (X22_out1) to[in=0] (\x1+\n2,\n1) -- (\x2-\n2,\n1) to[out=180] (X21_in1);
	\draw (X11_out1) to (X13_in1);
	\draw (X11_out2) to (X12_in1);
	\draw (X12_out1) to (X13_in2);
	\draw (Z_in1'|-X11_in2) to (X11_in2);	
	\draw (Z_in2'|-X12_in2) to (X12_in2);
	\draw (X12_out2) to (X21_in2);
	\draw (X21_out2) to (Z_out2'|-X21_out2);
	 \draw let \p1=(X12.south east), \p2=(X12.south west), \n1={\y1-\bby}, \n2=\bbportlen in
	  (X12_out3) to[in=0] (\x1+\n2,\n1) -- (\x2-\n2,\n1) to[out=180] (X12_in3);
	\draw let \p1=(X22.north east), \p2=(X11.north west), \n1={\y2+\bby}, \n2=\bbportlen in
          (X22_out2) to[in=0] (\x1+\n2,\n1) -- (\x2-\n2,\n1) to[out=180] (X11_in1);
	\draw (X13_out1) to (Z_out1'|-X13_out1);
\end{tikzpicture}
\end{equation}
We wrote a little question for you at the top of the diagram. Isn't there something a little funny about wiring diagrams? Maybe for old-fashioned machines, you would wire things together once and they'd stay like that for the life of the machine. But my phone connects to different Wi-Fi stations at different times, I drop my connection to Alice for weeks at a time, etc. So wiring diagrams should be able to change in time; $\poly$ will let us do that.

\begin{example}\label{ex.changing_wiring_bonds_supplier_assemble}
Here are some familiar circumstances where we see wiring diagrams changing in time.
\begin{enumerate}[itemsep=0pt]
%	\item Airplanes only communicate when they get near enough;
%	\item A phone is connected to 4G or to wifi depending on circumstances;
%	\item A person can choose when to open (receive input through) their eyes and when to speak (produce output);\goodbreak
	\item When too much force is applied to a material, bonds can break;
\end{enumerate}
\[
\begin{tikzpicture}[oriented WD, bb small, bb port length=0]
	\foreach \i in {0,...,4} {
		\node[bb={1}{1}, fill=blue!10] at (1.7*\i,0) (X\i) {};
	}
%	\node[bb={1}{1}, fit=(X0) (X4)] (X) {};
	\foreach \i in {0,...,3} {
		\draw[thick] (X\i_out1) -- (X\the\numexpr\i+1\relax_in1);
	};
	\draw[thick, ->] (X0_in1) -- node[above, font=\tiny] {Force} +(-2.5,0);
	\draw[thick, ->] (X4_out1) -- node[above, font=\tiny] {Force} +(2.5,0) node (R) {};
%
\def\x{21};
	\foreach \i in {0,...,2} {
		\node[bb={1}{1}, fill=blue!10] at (\x+1.7*\i,0) (Y\i) {};
	}
	\foreach \i in {3,...,4} {
		\node[bb={1}{1}, fill=blue!10] at (\x+1.3+1.7*\i,0) (Y\i) {};
	}
%	\node[bb={1}{1}, fit=(Y0) (Y4)] (Y) {};
	\foreach \i in {0,1,3} {
		\draw[thick] (Y\i_out1) -- (Y\the\numexpr\i+1\relax_in1);
	};
	\draw[thick, ->] (Y0_in1) -- node[above, font=\tiny] {Force} +(-2.5,0) node (L) {};
	\draw[thick, ->] (Y4_out1) -- node[above, font=\tiny] {Force} +(2.5,0);
	\node[starburst, draw, minimum width=2cm, minimum height=1.5cm,red,fill=orange,line width=1.5pt] at ($(L)!.5!(R)$)
{Snap!};
\end{tikzpicture}
\]
\begin{quote}
In materials science the Young's modulus accounts for how much force can be transferred across a material as its endpoints are pulled apart. When the material breaks, the two sides can no longer feel evidence of each other. Thinking of pulling as sending a signal (a signal of force), we might say that the ability of internal entities to send signals to each other---the connectivity of the wiring diagram---is being measured by Young's modulus. It will also be visible within $\poly$.
\end{quote}
\begin{enumerate}[resume]
	\item A company may change its supplier at any time;
\end{enumerate}
\begin{equation*}%\label{eqn.supplier}
\begin{tikzpicture}[oriented WD, font=\ttfamily, every node/.style={fill=blue!10}, baseline=(c)]
	\node[bb={0}{1}] (s1) {Supplier 1};
	\node[bb={0}{1}, below=of s1] (s2) {Supplier 2};
	\coordinate (helper) at ($(s1)!.5!(s2)$);
	\node[bb={1}{0}, right=1.5 of helper] (c) {Company};
	\draw (s1_out1) to (c_in1);
	\draw (s2_out1) to +(5pt,0) node[fill=none] {$\bullet$};
\begin{scope}[xshift=3.5in]
	\node[bb={0}{1}] (s1') {Supplier 1};
	\node[bb={0}{1}, below=of s1'] (s2') {Supplier 2};
	\coordinate (helper') at ($(s1')!.5!(s2')$);
	\node[bb={1}{0}, right=1.5 of helper'] (c') {Company};
	\draw (s2'_out1) to (c'_in1);
	\draw (s1'_out1) to +(5pt,0) node[fill=none] {$\bullet$};
\end{scope}
	\node[starburst, draw, minimum width=2cm, minimum height=2cm,align=center,fill=green!10, font=\small, fill=white, line width=1.5pt] at ($(c)!.5!(helper')$)
{Change\\supplier!};
\end{tikzpicture}
\end{equation*}
\begin{quote}
The company can get widgets either from supplier 1 or supplier 2; we could imagine this choice is completely up to the company. The company can decide based on the quality of widgets it has received in the past, i.e.\ when the company gets a bad widget, it updates an internal variable, and sometimes that variable passes a threshold making the company switch states. Whatever its strategy for deciding, we should be able to encode it in $\poly$.
\end{quote}
\begin{enumerate}[resume]
	\item When someone assembles a machine, their own outputs dictate the connection pattern of the machine's components.
\end{enumerate}
\begin{equation*}%\label{eqn.someone}
\begin{tikzpicture}[oriented WD, font=\ttfamily, bb port length=0, every node/.style={fill=blue!10}, baseline=(someone.north)]
	\node[bb port sep=.5, bb={0}{1}] (A) {unit A};
	\node[bb port sep=.5, bb={1}{0}, right=of A] (B) {unit B};
	\coordinate (helper) at ($(A)!.5!(B)$);
	\node[bb={1}{1}, below=2 of helper] (someone) {\tikzsymStrichmaxerl[3]};
	\draw[->, dashed, blue] (someone_in1) to[out=180, in=270] (A.270);
	\draw[->, dashed, blue] (someone_out1) to[out=0, in=270] (B.270);
	\draw (A_out1) -- +(10pt,0);
	\draw (B_in1) -- +(-10pt,0);
%
\begin{scope}[xshift=3.5in]
	\node[bb port sep=.5, bb={0}{1}] (A') {unit A};
	\node[bb port sep=.5, bb={1}{0}, right=.5of A'] (B') {unit B};
	\coordinate (helper') at ($(A')!.5!(B')$);
	\node[bb={1}{1}, below=2 of helper'] (someone') {\tikzsymStrichmaxerl[3]};
	\draw[->, dashed, blue] (someone'_in1) to[out=180, in=270] (A'.270);
	\draw[->, dashed, blue] (someone'_out1) to[out=0, in=270] (B'.270);
	\draw (A'_out1) -- (B'_in1);
\end{scope}
%
	\node[starburst, draw, minimum width=2cm, minimum height=2cm,fill=blue!50,line width=1.5pt, align=center, font=\upshape] at ($(B)!.5!(A')-(0,.6cm)$)
{Attach!};
\end{tikzpicture}
\end{equation*}
\begin{quote}
Have you ever assembled something? Your internal states dictate the wiring pattern of some other things. We can say this in $\poly$.
\end{quote}

All of the examples discussed here will be presented in some detail once we have the requisite mathematical theory (\cref{ex.bonds_break,ex.supplier_change,ex.assemble_machine}).
\end{example}

\begin{exercise}%\label{exc.changing_types}
Think of another example where systems are sending each other information, but where the sort of information or who it's being sent to or received from can change based on the states of the systems involved. You might have more than two, say $\rr$-many, different wiring patterns in your setting.
\begin{solution}
When using an application, say a drawing program, there are often buttons I can click at the top of the screen. The information I can send the system changes based on which button I click. If no button is clicked, my options for interacting with the program include clicking any one of the buttons. When I click the ``File" button, I can interact with the file system, saving or opening a file, thus interacting with another ``part'' of the computer. The set of things I can do depends on context, including whatever I did before.
\end{solution}
\end{exercise}

But there's more that's intuitively wrong or limiting about the picture in \eqref{eqn.wired_forever}. Ever notice how you can change how you interface with the world? Sometimes I close my eyes, which makes that particular way of sending me information inaccessible: that port vanishes and you need to use your words. Sometimes I'm in a tunnel and my phone can't receive GPS. Sometimes I extend my hand to give or receive an object from another person, but sometimes I don't. Our ports themselves change in time. We will be able to say all this using $\poly$.

And there's even more that's wrong with the above description. Namely, when I move my eyes, that's actually something you can see---e.g.\ whether I'm looking at you. When I turn around, I see different things, and \emph{you can notice I'm turned around}! When I use my muscles or mouth to express things, my very position changes: my tongue moves, my body moves. So my output is actually achieved by changing position. The model in $\poly$ will be able to express this too.

\begin{example}\label{ex.pond_eyeballs}
Imagine a million little eyeballs, each of which has a tiny brain inside it, all together in a pond of eyeballs. All that an individual eyeball $e$ can do is open and close. When $e$ is open, it can make some distinction about all the rest of the eyeballs in view: maybe it can count how many are open, or maybe it can see whether just one certain eyeball $e'$ is open or closed. But when $e$ is closed, it can't see anything; whatever else is happening, it's all the same to $e$. All it can do in that state is process previous information.

Each eyeball in this system will correspond to the polynomial $\yon^\ord{n}+\yon$, which intuitively consists of two settings: one with $n$-many options, and the other with only one option. For simplicity, we could assume $n=2$, so that each eyeball makes a single yes-no distinction whenever it's open.

The point, however, is that any other eyeball may be capable of noticing if $e$ is opened or closed. We can imagine some interesting dynamics in this system, e.g.\ waves of openings or closings sweeping over the group, a ripple of openings expanding through the pond.

Talk about real-world applications! 
\end{example}

Hopefully you now have an idea of what we mean by mode-dependence: interfaces and wiring diagrams changing in time, based on the states of all the systems involved. We'll see that $\poly$ speaks about mode-dependent systems and wiring diagrams in this sense. 

But $\poly$ is very versatile in its applications. In \cref{sec.poly.intro.dec} we'll show how it relates to the making of decisions. First a quick remark.

\begin{remark}
We ended \cref{ex.pond_eyeballs} by joking about ``real-world applications,'' because a pond of eyeballs is about the most bizarre thing one can imagine. But recall Nobel physicist Frank Wilczek's quote from the preface:
\begin{quote}
For me, though, it is difficult to resist the idea that space-time is not essentially different from matter, which we understand more deeply. If so, it will consist of vast numbers of identical units---``particles of space''---each in contact with a few neighbors, exchanging messages, joining and breaking apart, giving birth and passing away.
\end{quote}
Suppose the world was made out of a vast number of identical units, each with its own behavior, able to connect and disconnect with neighbors, and even disappear from the world of cause and effect. We may not even be interested in what our world is actually made of---just what these units are able to do. Is there such an elementary unit that could produce all other dynamical systems? The $\yon^\2+\yon$ eyeballs give a sense of a very simple interface---open and perceiving a single distinction about the world, or closed and making no distinctions---that we could imagine building an entire world from.
\end{remark}


%-------- Section --------%
\section{Decisions} \label{sec.poly.intro.dec}
We return to the example polynomial $\yon^\2 + \2\yon + \1$ from \eqref{eqn.poly_example} and its corresponding corolla forest, in which positions are expressed as roots and directions are represented as leaves:
\begin{equation} \label{eqn.forest2110}
\begin{tikzpicture}[trees]
  \node (1) {$\bullet$} 
    child {}
    child {};
  \node[right=.5 of 1] (2) {$\bullet$} 
    child {};
  \node[right=.5 of 2] (3) {$\bullet$} 
    child {};
  \node[right=.5 of 3] (4) {$\bullet$};
\end{tikzpicture}
\end{equation}
Concretely, we might think of each position as representing a \emph{decision}. Associated to every decision is a set of \emph{options} (directions). The three decisions we exhibit in \eqref{eqn.forest2110} are particularly interesting: they respectively have two options, one option, one option, and no options. Having two options is familiar from life---it's the classic yes/no decision---as well as from Claude Shannon's Information Theory. Having one option is also familiar theoretically and in life: ``sorry, ya just gotta go through it.'' Having no options is when you actually don't get through it: an impossible decision, a sort of ``dead end.'' While the corollas $\1,\yon,$ and $\yon^\2$ are each interesting as decisions, the sum $\yon^\2+\2\yon+\1$ has very little theoretical interest; it's just an example.

Now consider the following three trees, the first two of which are infinite (though that's hard to draw):
\[
\begin{tikzpicture}[trees]
\begin{scope}[
  level 1/.style={sibling distance=20mm},
  level 2/.style={sibling distance=10mm},
  level 3/.style={sibling distance=5mm},
  level 4/.style={sibling distance=2.5mm},
  level 5/.style={sibling distance=1.25mm}]
  \node[dgreen] (a) {$\bullet$}
    child {node[dgreen] {$\bullet$}
    	child {node[dgreen] {$\bullet$}
    		child {node[dgreen] {$\bullet$}
  				child {node[dgreen] {$\bullet$}
    				child {}
    				child {}
    			}
  				child {node[dyellow] {$\bullet$}
    				child {}
    				child {}
    			}
  			}
    		child {node[dyellow] {$\bullet$}
					child {node[dgreen] {$\bullet$}
      			child {}
      			child {}
     			}
    			child  {node[red] {$\bullet$}}
  			}
    	}
    	child {node[dyellow] {$\bullet$}
    		child {node[dgreen] {$\bullet$}
  				child {node[dgreen] {$\bullet$}
    				child {}
    				child {}
    			}
  				child {node[dyellow] {$\bullet$}
    				child {}
    				child {}
    			}
  			}
    		child  {node[red] {$\bullet$}}
    	}
    }
    child {node[dyellow] {$\bullet$}
    	child {node[dgreen] {$\bullet$}
    		child {node[dgreen] {$\bullet$}
  				child {node[dgreen] {$\bullet$}
    				child {}
    				child {}
    			}
  				child {node[dyellow] {$\bullet$}
    				child {}
    				child {}
    			}
  			}
    		child {node[dyellow] {$\bullet$}
					child {node[dgreen] {$\bullet$}
      			child {}
      			child {}
     			}
    			child  {node[red] {$\bullet$}}
  			}
  		}
  		child {node[red] {$\bullet$}
  		}
  	}
  ;
\end{scope}
\begin{scope}[
  level 1/.style={sibling distance=13mm},
  level 2/.style={sibling distance=10mm},
  level 3/.style={sibling distance=5mm},
  level 4/.style={sibling distance=2.5mm},
  level 5/.style={sibling distance=1.25mm}]
\node (b) [right=4 of a, dyellow] {$\bullet$}
  child {node[dgreen] {$\bullet$}
  	child {node[dgreen] {$\bullet$}
  		child {node[dgreen] {$\bullet$}
  			child {node[dgreen] {$\bullet$}
    			child {}
    			child {}
   			}
 				child {node[dyellow] {$\bullet$}
   				child {}
   				child {}
   			}
			}
    		child {node[dyellow] {$\bullet$}
					child {node[dgreen] {$\bullet$}
      			child {}
      			child {}
     			}
    			child  {node[red] {$\bullet$}}
  			}
		}
  	child {node[dyellow] {$\bullet$}
  		child {node[dgreen] {$\bullet$}
  			child {node[dgreen] {$\bullet$}
    			child {}
    			child {}
   			}
 				child {node[dyellow] {$\bullet$}
   				child {}
   				child {}
   			}
			}
  		child  {node[red] {$\bullet$}}
  	}
	}
	child {node[red] {$\bullet$}}	
;
\end{scope}
\node (c) [red, right=2 of b] {$\bullet$};
\end{tikzpicture}
\]
These are patterned examples---and we'll understand what this pattern is more clearly in \cref{**}---of what we will call \emph{decision streams}. Decision streams form the objects in a category that also includes the following level-3 abbreviations of binary decision streams (the third of which is a level-3 abbreviation of a finite stream):
\[
\begin{tikzpicture}[trees]
\begin{scope}[
  level 1/.style={sibling distance=20mm},
  level 2/.style={sibling distance=10mm},
  level 3/.style={sibling distance=5mm},
  level 4/.style={sibling distance=2.5mm}]
  \node (a) {$\bullet$}
    child {node {$\bullet$}
    	child {node {$\bullet$}
    		child {node {$\bullet$}
  			}
    		child {node {$\bullet$}
  				child 
  				child
  			}
    	}
    	child {node {$\bullet$}
  			}
    }
    child {node {$\bullet$}
    	child {node {$\bullet$}
    		child {node {$\bullet$}
  				child
  				child
  			}
    		child {node {$\bullet$}
  				child
  				child
  			}
  		}
  		child {node {$\bullet$}
    		child {node {$\bullet$}
  			}
    		child {node {$\bullet$}
  			}
  		}
  	}
  ;
\end{scope}
\begin{scope}[
  level 1/.style={sibling distance=13mm},
  level 2/.style={sibling distance=10mm},
  level 3/.style={sibling distance=5mm},
  level 4/.style={sibling distance=2.5mm}]
  \node (b) [right=4 of a] {$\bullet$}
    child {node {$\bullet$}
    }
    child {node {$\bullet$}
    	child {node {$\bullet$}
    		child {node {$\bullet$}
  			}
    		child {node {$\bullet$}
  				child
  				child
  			}
  		}
  		child {node {$\bullet$}
    		child {node {$\bullet$}
  				child
  				child
  			}
    		child {node {$\bullet$}
  				child
  				child
  			}
  		}
  	}
  ;
\end{scope}
\begin{scope}[
  level 1/.style={sibling distance=13mm},
  level 2/.style={sibling distance=8mm},
  level 3/.style={sibling distance=5mm},
  level 4/.style={sibling distance=2.5mm}]
  \node (c) [right=4 of b] {$\bullet$}
    child {node {$\bullet$}
    	child {node {$\bullet$}
  		}
  		child {node {$\bullet$}
    		child {node {$\bullet$}
  			}
    		child {node {$\bullet$}
  			}
  		}
    }
    child {node {$\bullet$}
  	}
  ;
\end{scope}
\end{tikzpicture}
\]

The set of such decision streams forms the objects of a category with very nice properties (it's a topos), which we call $\sys(\yon^\2+\1)$. The idea is that every corolla in these diagrams has either two options, corresponding to $\yon^\2$, or no options, corresponding to $\1\iso\yon^\0$.
We say that each such decision stream has type $\yon^\2+\yon^\0$.

\begin{exercise}\label{exc.decision_streams}
\begin{enumerate}
	\item Draw a level-3 abbreviation of a decision stream of type $\yon^\2+\yon^\0$.
	\item Draw a level-4 abbreviation of a decision stream of type $\yon$.
	\item Draw a level-3 abbreviation of a decision stream of type $\nn\yon^\2$ by labeling every node with a natural number.
	\qedhere
\end{enumerate}

\begin{solution}
\begin{enumerate}
	\item Here's a level-3 abbreviation of a decision stream of type $\yon^\2+\yon^\0$.
\[
\begin{tikzpicture}[trees,
  level 1/.style={sibling distance=20mm},
  level 2/.style={sibling distance=10mm},
  level 3/.style={sibling distance=5mm},
  level 4/.style={sibling distance=2.5mm}]
  \node (a) {$\bullet$}
    child {node {$\bullet$}
    	child {node {$\bullet$}
    		child
    		child
    	}
    	child {node {$\bullet$}
  			}
    }
    child {node {$\bullet$}
    	child {node {$\bullet$}}
    	child {node {$\bullet$}
  		}
  	}
  ;
\end{tikzpicture}
\]
	\item Here's a level-4 abbreviation of a decision stream of type $\yon$.
\[
\begin{tikzpicture}[trees]
	\node (a) {$\bullet$}
		child {node {$\bullet$}
			child {node {$\bullet$}
				child {node {$\bullet$}
  				child
			}}};
\end{tikzpicture}
\]
	\item Here's a level-3 abbreviation of a decision stream of type $\nn\yon^\2$ where we indicate the position of each node by labeling it with a natural number.
\[
\begin{tikzpicture}[trees,
  level 1/.style={sibling distance=20mm},
  level 2/.style={sibling distance=10mm},
  level 3/.style={sibling distance=5mm},
  level 4/.style={sibling distance=2.5mm}]
  \node (a) {$27$}
    child {node {$5040$}
    	child {node {$192$}
    		child 
    		child
			}
    	child {node {$0$}
    		child 
    		child
  			}
    }
    child {node {$314159$}
    	child {node {$1000$}
   				child
  				child
  		}
			child {node {$1296$}
				child
				child
			}
  	}
  ;
\end{tikzpicture}
\]
\end{enumerate}
\end{solution}
\end{exercise}

But decisions aren't just about choosing; they're also about trying to accomplish something. The logic of accomplishment is exceptionally rich in this setting. We will concentrate on what we call a \emph{win condition}, which is an induced subgraph of the decision stream with the property that if $n$ is a winning node, then any child of $n$ is also a winning node. 
\[\begin{tikzpicture}[trees,
  level 1/.style={sibling distance=20mm},
  level 2/.style={sibling distance=10mm},
  level 3/.style={sibling distance=5mm},
  level 4/.style={sibling distance=2.5mm}]
  \node (root) {$\bullet$}
    child {node {$\bullet$}
    	child {node {$\bullet$}
    		child {node {$\bullet$}
  			}
    		child {node {$\bullet$}
  				child
  				child
  			}
    	}
    	child {node {$\bullet$}
  			}
    }
    child {node {$\bullet$}
    	child {node {$\bullet$}
    		child {node {$\bullet$}
  				child
  				child
  			}
    		child {node {$\bullet$}
  				child
  				child
  			}
  		}
  		child {node {$\bullet$}
    		child {node {$\bullet$}
  				child
  				child
  			}
    		child {node {$\bullet$}
  			}
  		}
  	}
  ;
 \begin{scope}[every node/.style={circle, inner sep=3pt, blue, draw}]
  \node at (root-1-2)     {};
  \node at (root-2-1-1-1) {};
  \node at (root-2-1-1-2) {};
  \node at (root-2-1-2-1) {};
	\node at (root-2-2) 		{};
  \node at (root-2-2-1) 	{};
  \node at (root-2-2-2) 	{};
  \node at (root-2-2-1-1) {};
  \node at (root-2-2-1-2) {};
 \end{scope}
\end{tikzpicture}
\]
More formally, these are called \emph{sieves}. They form the elements of a logical system called a Heyting algebra: you can take any two sieves and form the intersection or union (which correspond to AND and OR), or even things like implication, negation, and existential and universal quantification. This will give us a calculus of win-conditions for any type $p$ decision stream.

%-------- Section --------%
\section{Data} \label{sec.poly.intro.data}

Data is information, maybe thought of as quantized into atomic pieces, but where these atomic pieces are somehow linked together according to a conceptual structure. When a person or organization uses certain data repeatedly, they often find it useful to put their data in a database. This requires organizing the little pieces into a conceptual structure. So when you hear ``database,'' just think of it as a conceptual structure filled with examples.

To fix a mental image, let's say that you need to constantly look up employees, what department they're in, who the admin person is for that department, who their manager is, etc. Here's an associated database
\begin{equation}\label{eqn.mytables}
\begin{tabular}{ c | c  c  c}
  \textbf{Employee}&\textbf{FirstName}&\textbf{WorksIn}&\textbf{Mngr}\\\hline
  1&Alan&101&2\\
  2&Ruth&101&2\\
  3&Carla&102&3
\end{tabular}
\hspace{.3in}
\begin{tabular}{ c | c  c}
  \textbf{Department}&\textbf{Name}&\textbf{Admin}\\\hline
  101&Sales&1\\
  102&IT&3\\~
\end{tabular}
%\hspace{.3in}
%\begin{tabular}{ c |}
%	\textbf{String}\\\hline
%	Alan\\
%	IT\\[-3pt]
%	\resizebox{!}{10pt}{$\vdots$}
%\end{tabular}
\end{equation}
We can see it as being associated to the following conceptual scheme, also called a \emph{schema}:
\begin{equation}\label{eqn.myschema}
\cat{C}\coloneqq
\boxCD{white}{
\begin{tikzcd}[row sep=large, ampersand replacement = \&]
 	\LTO{Employee}\ar[rr, shift left, "\text{WorksIn}"]\ar[dr, bend right, "\text{FirstName}"']\ar[loop left, "\text{Mngr}"]\&\&
  \LTO{Department}\ar[ll, shift left, "\text{Admin}"]\ar[dl, bend left, "\text{Name}"]\\
  \&\LTO[\circ]{String}
\end{tikzcd}
\leavevmode\\\bigskip
\text{Department.Admin.WorksIn = Department}
}
\end{equation}
The equation at the bottom says that for any department $d$, if you ask for the admin person and see which department they work in, it's required to be $d$.

What's called $\cat{C}$ in \eqref{eqn.myschema} is a \emph{finitely presented category}.
The objects of the category are the points, while each arrow corresponds to a morphism of $\cat{C}$ (there are additional morphisms, including identity morphisms, that are not depicted).
The equation at the bottom indicates that composing the morphism Admin with the morphism WorksIn yields the identity morphism on the object Department.

The database instance presented in \eqref{eqn.mytables} then corresponds to a functor $I\colon\cat{C}\to\smset$.
For example, $I$ sends the object $\text{Employee}\in\cat{C}$ to the set $I(\text{Employee})\coloneqq\{1,2,3\}$, the entries in the Employee column of the left table.

The functor also sends the morphism $\text{Mngr}\colon\text{Employee}\to\text{Employee}$ to the function $I(\text{Mngr})\colon\{1,2,3\}\to\{1,2,3\}$ that sends each entry in the Employee column to its corresponding entry in the Mngr column.
So $I(\text{Mngr})(1)=2$, $I(\text{Mngr})(2)=2$, and $I(\text{Mngr})(3)=3$.

A functor $\cat{C}\to\smset$ is called a \emph{copresheaf on $\cat{C}$}. So the story of database schemas and their data can be based on the story of categories and their copresheaves.

\begin{exercise} %\label{exc.my_schema_and_tables}
As above, we define the finitely presented category $\cat{C}$ according to \eqref{eqn.myschema} and the copresheaf $I$ on $\cat{C}$ according to \eqref{eqn.mytables}.
\begin{enumerate}
    \item What is $I(\text{Department})$?
    \item What is $I(\text{Admin})$?
    \item Composing Admin with FirstName yields a morphism from Department to String that we denote by Admin.FirstName.
    What is $I(\text{Admin.FirstName})$?
    \item Say we require that managers work in the same department as the employees they oversee.
    Write down an equation in $\cat{C}$ (like the one at the bottom of \eqref{eqn.myschema} that expresses this condition.
    \item How might we define $I(\text{String})$? \qedhere
\end{enumerate}

\begin{solution}
We refer to \eqref{eqn.myschema} and \eqref{eqn.mytables} to characterize the category $\cat{C}$ and the functor $I \colon \cat{C} \to \smset$.
\begin{enumerate}
    \item Here Department is an object of $\cat{C}$, so $I(\text{Department})$ is a set. From the Department column in the table on the right of \eqref{eqn.mytables}, we observe that $I(\text{Department}) = \{101, 102\}$.
    \item Here Admin is an morphism of $\cat{C}$ from Department to Employee, so $I(\text{Admin})$ is a function from $I(\text{Department}) = \{101, 102\}$ to $I(\text{Employee}) = \{1, 2, 3\}$. From the Admin column in the table on the right of \eqref{eqn.mytables}, we observe that $I(\text{Admin})(101) = 1$ and $I(\text{Admin})(102) = 3$.
    \item By functoriality, $I(\text{Admin.FirstName})$ is the function $I(\text{Admin})$ composed with $I(\text{FirstName})$.
    We have that $I(\text{Admin})$ sends $101$ to $1$ and $102$ to $3$, while the FirstName column tells us that $I(\text{FirstName})$ sends $1$ to Alan and $3$ to Carla.
    So $I(\text{Admin.FirstName})$ sends $101$ to Alan and $102$ to Carla.
    \item If every employee works in the same department as their manager, then Employee.WorksIn = Employee.Mngr.WorksIn.
    \item The name suggests that $I(\text{String})$ is the set of all possible strings of characters.
    Perhaps this could be defined as $\bigcup_{n \in \nn} A^n$, where $A$ is our alphabet of allowed characters, which may include the English letters, spaces, digits, and whatever other characters we allow.
    At the very least, since FirstName and Name are both morphisms to String, every entry in the FirstName and Name columns must be in $I(\text{String})$.
    So all we know for sure is that $\{\text{Alan, Ruth, Carla, Sales, IT}\} \ss I(\text{String})$.
\end{enumerate}
\end{solution}
\end{exercise}

There's a very important thing that we do with databases: we query them. We ask them questions like ``tell me the First Name of every Employee that's either the Admin of the Sales department or their Manager.''
\begin{minted}{mysql}
 FOR    d: Department, e: Employee
 WHERE  Name(d)="Sales" AND
        (e=Admin(d) OR e=Mngr(Admin(d)))
 RETURN FirstName(e)
\end{minted}
This sort of question is formally called a ``union of conjunctive queries.'' We will see this sort of query is intimately connected with $\poly$.
We will also see how databases can be conceived in terms of dynamical systems.

%-------- Section --------%
\section{Implementation} \label{sec.poly.intro.code}

Everything we talk about can actually be implemented in a computer without much difficulty, at least if you have access to a language that supports dependent types, such as Agda.

What we have been calling polynomials---things like $\yon^\2+\2\yon+\1$---are often called \emph{containers} in the computer science literature. A container consists of a type $S$, usually called the type of \emph{shapes}, and a type $P(s)$ for each term $s:S$, called the type of \emph{positions} in shape $s$. It's mildly unfortunate that the names clash with our own: for us a container-shape is a position and a container-position is a direction.

Luckily, the Agda code is pretty easy to understand.
\begin{agda}
record Arena : Set where  -- an arena consists of 
   field                      -- two fields
     pos : Set                -- one called pos, a set, and
     dir : pos -> Set         -- one called dir,
                              -- a set for each element of the set pos
\end{agda}

%-------- Section --------%
\section{Mathematical theory} \label{sec.poly.intro.math_theory}

The applications of $\poly$ are quite diverse and interesting, including dynamics, data, and decisions. However it is how the mathematics of $\poly$ supports these applications that is so fantastic. For experts, here are some reasons for the excitement.

\begin{proposition}
$\poly$ has all products and coproducts and is completely distributive.
$\poly$ also has exponential objects, making it a biCartesian closed category.
It therefore supports the simply typed lambda calculus.
\end{proposition}
\begin{proof}
We will prove that $\poly$ has coproducts in \cref{prop.poly_coprods}, that it has products in \cref{prop.poly_prods}, that it is completely distributive in \cref{**}, and that $\poly$ has exponential objects in \cref{thm.poly_cart_closed}.
\end{proof}

\begin{proposition}
Beyond the coCartesian and Cartesian monoidal structures $(\0,+)$ and $(\1,\times)$, the category $\poly$ has two additional monoidal structures, denoted $(\yon,\otimes)$ and $(\yon,\circ)$, which are together duoidal.\footnote{We will follow the convention of writing the tensor unit before the tensor product when specifying a monoidal structure.} Moreover $\otimes$ is also a closed monoidal structure that distributes over coproducts.
\end{proposition}
\begin{proof}
We will define $\otimes$ in \cref{def.dirichlet} and prove that $(\yon, \otimes)$ is a monoidal structure on $\poly$ in \cref{prop.dirichlet_monoidal}, and we will define $\circ$ in \cref{**} and prove that $(\yon, \circ)$ is a monoidal structure on $\poly$ in \cref{**}.
Then we will show that $\circ$ is duoidal over $\otimes$ in \cref{**}.
In \cref{prop.day}, we will show that $\otimes$ distributes over coproducts.
Then in \cref{prop.dirichlet_closure}, we will prove that $\otimes$ is closed.
\end{proof}

\begin{proposition}\label{prop.adjoint_quadruple}
$\poly$ has an adjoint quadruple with $\smset$ and an adjoint pair with $\smset\op$:\footnote{
	We use the notation 
	$\begin{tikzcd}[ampersand replacement=\&]
		C\ar[r, shift left=4pt, "L"]\&
		D\ar[l, shift left=4pt, "R"]\ar[l, phantom, "\scriptstyle\Rightarrow"]
	\end{tikzcd}$
	to denote an adjunction $L \dashv R$. The double arrow, always pointing in the same direction as the left adjoint, indicates both the unit $C\Rightarrow R\circ L$ and the counit $L\circ R\Rightarrow D$ of the adjunction.
}
\begin{equation*}%\label{eqn.adjoints_galore}
\begin{tikzcd}[column sep=60pt]
  \smset
  	\ar[r, shift left=7pt, "A" description]
		\ar[r, shift left=-21pt, "A\yon"']&
  \poly
  	\ar[l, shift right=21pt, "p(\0)"']
  	\ar[l, shift right=-7pt, "p(\1)" description]
	\ar[l, phantom, "\scriptstyle\Leftarrow"]
	\ar[l, phantom, shift left=14pt, "\scriptstyle\Rightarrow"]
	\ar[l, phantom, shift right=14pt, "\scriptstyle\Rightarrow"]
\end{tikzcd}
\hspace{.6in}
\adjr[50pt]{\smset\op}{\yon^A}{\Gamma(p)}{\poly}.
\end{equation*}
Each functor is labeled by where it sends $p\in\poly$ or $A\in\smset$; in particular, $\Gamma(p) \coloneqq \poly(p, \yon)$.
\end{proposition}
\begin{proof}
We will prove that $\poly$ has an adjoint quadruple with $\smset$ in \cref{thm.adjoint_quadruple}, and that it has an adjoint quadruple with $\smset\op$ in \cref{prop.yoneda_left_adjoint}.
\end{proof}

There's a lot we're leaving out of this summary, just so we can hit the highlights.

\begin{proposition}
The functor $\poly\to\smset$ given by $p\mapsto p(\1)$ is a monoidal fibration.
\end{proposition}

In fact it's a monoidal $*$-bifibration in the sense of \cite{shulman2008framed}. But here's where things get really interesting.

\begin{proposition}[Ahman-Uustalu]\label{prop.ahman_uustalu1}
Comonoids in $(\poly,\yon,\circ)$ are categories (up to isomorphism).
\end{proposition}

\begin{proposition}
The category $\comon$ has finite coproducts and products, and coproducts in $\comon$ agree with those in $\smcat$.
\end{proposition}

\begin{proposition}
The functor $\comon\to\poly$ has a right adjoint
\[
\adjr{\poly}{\com{K}_-}{u_-}{\comon},
\]
called the \emph{cofree comonoid} construction. It is lax monoidal with respect to $\otimes$.
\end{proposition}

\begin{proposition}
The category $\comon$ has a third symmetric monoidal structure $(\yon,\otimes)$, and the functor $u_-\colon(\comon,\yon,\otimes)\to(\poly,\yon,\otimes)$ is strong monoidal.
\end{proposition}



\begin{proposition}\label{prop.sysp[i]s_topos}
For any polynomial $p$, the category
\[\sys(p)\cong\com{K}_p\set\]
of dynamical systems on $p$ forms a topos.
\end{proposition}

The proposition above indicates that there is a full-fledged logic of dynamical systems inhabiting any interface, while the following proposition implies that these logics can be combined and compared.

\begin{proposition}\label{prop.poly_map_pregeo_topos}
A morphism $p\to q$ of polynomials induces a pre-geometric morphism between their respective toposes
\[
\adj{\sys(p)}{}{}{\sys(q)}.
\]
\end{proposition}

The following propositions suggest that the whole story of dynamics carries a strong connection to database theory.

\begin{proposition}
Suppose that the category $\cat{C}$ corresponds under \cref{prop.ahman_uustalu1} to the comonoid $\com{C}$. Then there is an equivalence of categories
\[
\Cat{Bimod}(\com{C},0)\cong\cat{C}\set
\]
between $(\com{C},0)$-bimodules and $\cat{C}$-copresheaves.
\end{proposition}
We will use the convention that the comonoid $\com{C}$ corresponds to category $\cat{C}$, and similarly for $\com{D}$ and $\cat{D}$, etc. 

\begin{proposition}[Garner]
For any categories $\cat{C}$ and $\cat{D}$, there is an equivalence of categories
\[
\Cat{Bimod}(\com{C},\com{D})\cong\Cat{pra}(\cat{C}\set,\cat{D}\set)
\]
between that of bimodules between comonoids in $\poly$ and parametric right adjoints between copresheaf categories.
\end{proposition}

\begin{proposition}
For any category $\cat{C}$ the category of left $\com{C}$-modules is equivalent to the category of functors $\cat{C}\to\poly$,
\[\Cat{Bimod}(\com{C},\yon)\cong\Cat{Fun}(\cat{C},\poly).\]
\end{proposition}

If you skipped over any of that---or all of that---it'll be no problem whatsoever! We will cover each of the above results in detail over the course of this book. There are many avenues for study, but we need to push forward.

We'll begin in the next chapter.

\Closesolutionfile{solutions}

%-------- Section --------%
\section{Exercise solutions}
{\footnotesize
\input{solution-file1}}

\Opensolutionfile{solutions}[solution-file2]

%------------ Chapter ------------%
\chapter{Polynomial functors and natural~transformations} \label{ch.poly.func_nat}

In this chapter, we will set down the basic category-theoretic story of $\poly$, so that we can have a firm foundation from which to speak about dynamical systems, decisions, and data. We begin with the category $\smset$ of sets and functions, and what is arguably the fundamental theorem of category theory, the Yoneda lemma.

%-------- Section --------%
\section{Representable functors and the Yoneda lemma} \label{sec.poly.func_nat.yon}

\begin{definition} \label{def.representable}
Given any set $S$, we denote by $\yon^S\colon\smset\to\smset$ the functor that sends any set $X$ to the set $X^S=\smset(S,X)$, and sends any function $h\colon X\to Y$ to the function $h^S\colon X^S\to Y^S$, the one that sends $g\colon S\to X$ to $g\then h\colon S\to Y$.\footnote{Throughout this text, in any category, given objects $A, B, C$ and morphisms $f \colon A \to B$ and $g \colon B \to C$, we denote their composite morphism $A \to C$ as $f \then g$ (instead of the usual $g \circ f$).}

We refer to functors of this form as \emph{representable functors}, or simply as \emph{representables}.
In particular, we call $\yon^S$ the functor \emph{represented by} $S$.
\end{definition}

The symbol $\yon$ stands for Yoneda, for reasons we will get to in \cref{lemma.yoneda}. For now, here are some pictures for your eyes to gaze at; they are the polynomials corresponding to various representables, namely the pure powers:
\begin{equation}\label{eqn.trees_for_gazing}
\begin{tikzpicture}[trees, sibling distance=2mm]
  \node["$\yon^{\5}$" below] (1) {$\bullet$} 
    child foreach \i in {1,...,5}
    ;
\end{tikzpicture}
\qquad
\begin{tikzpicture}[trees, sibling distance=1mm]
  \node["$\yon^{\1\0}$" below] (1) {$\bullet$} 
    child foreach \i in {1,...,10}
    ;
\end{tikzpicture}
\qquad
\begin{tikzpicture}[trees, sibling distance=0.5mm]
  \node["$\yon^{\2\0}$" below] (1) {$\bullet$} 
    child foreach \i in {1,...,20}
    ;
\end{tikzpicture}
\qquad
\begin{tikzpicture}[trees, sibling distance=0.25mm]
  \node["$\yon^{\4\0}$" below] (1) {$\bullet$} 
    child foreach \i in {1,...,40}
    ;
\end{tikzpicture}
\qquad
\begin{tikzpicture}[trees, sibling distance=0.0625mm]
  \node["$\yon^{[0,1]}$" below] (1) {$\bullet$} 
    child foreach \i in {1,...,160}
    ;
\end{tikzpicture}
\end{equation}

\begin{example}
The functor that sends every set $X$ to $X\times X$, and sends $h\colon X\to Y$ to $(h\times h)\colon (X\times X)\to(Y\times Y)$, is representable. After all, $X \times X \iso X^\2$, so this functor is just a pure power we are already familiar with: $\yon^\2$.
\end{example}

\begin{exercise}\label{exc.representable_fun}
For each of the following functors $\smset\to\smset$, say if it is representable or not; if it is, say what the representing set is.
\begin{enumerate}
	\item The identity functor $X\mapsto X$, which sends each function to itself.
	\item The constant functor $X\mapsto\2$, which sends every function to the identity on $\2$.
	\item The constant functor $X\mapsto\1$, which sends every function to the identity on $\1$.
	\item The constant functor $X\mapsto\0$, which sends every function to the identity on $\0$.
	\item A functor $X\mapsto X^\nn$.
	If it were representable, where would it send each function?
	\item A functor $X\mapsto 2^X$.
	If it were representable, where would it send each function?
\qedhere
\end{enumerate}

\begin{solution}
Our goal is to say whether various functors are representable (of the form $X\mapsto\smset(S,X)$ for some $S$, called the representing set).
\begin{enumerate}
	\item The identity functor $X\mapsto X$ is represented by $S=\1$: a function $\1 \to X$ is just an element of $X$, so $\smset(\1,X) \iso X$.
	Alternatively, note that $X^1 \iso X$.
	\item \label{sol.representable_fun.2} The constant functor $X\mapsto\2$ is not representable: the functor sends $\1$ to $\2$, but $\1^S \iso \1 \not\iso \2$ for any set $S$.
	\item The constant functor $X\mapsto\1$ is representable by $S=\0$: there is exactly one function $\0 \to X$, so $\smset(\0,X) \iso \1$.
	Alternatively, note that $X^0 \iso \1$.
	\item The constant functor $X\mapsto\0$ is not representable, for the same reason as in \cref{sol.representable_fun.2}.
	\item The functor $\yon^\nn$ that sends $X\mapsto X^\nn$ is represented by $S=\nn$, by definition.
	It sends each function $h \colon X \to Y$ to the function $h^\nn \colon X^\nn \to Y^\nn$ that sends each $g \colon \nn \to X$ to $g \then h \colon \nn \to Y$.
	\item No $\smset \to \smset$ functor $X\mapsto \2^X$ is representable, for the same reason as in \cref{sol.representable_fun.2}.
	(There \emph{is}, however, a functor $\smset\op \to \smset$ sending $X \mapsto 2^X$ that is understood to be representable in a more general sense.)
\end{enumerate}
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.representable_nt}
For any function $f\colon R\to S$, there is an induced natural transformation $\yon^f\colon\yon^S\to \yon^R$; on any set $X$ the $X$-component $X^f\colon X^S\to X^R$ is given by sending $g\colon S\to X$ to $f\then g\colon R\to X$.
\end{proposition}

\begin{exercise} \label{exc.representable_nt}
Prove that for any function $f\colon R\to S$, what we said was a natural transformation in \cref{prop.representable_nt} really is natural. That is, for any function $h\colon X\to Y$, show that the following diagram commutes:
\[
\begin{tikzcd}[bottom base]
	X^S\ar[r, "h^S"]\ar[d, "X^f"']&
	Y^S\ar[d, "Y^f"]\\
	X^R\ar[r, "h^R"']&
	Y^R\ar[ul, phantom, "?"]
\end{tikzcd}
\qedhere
\]

\begin{solution}
To show that
\[
\begin{tikzcd}[ampersand replacement=\&]
	X^S\ar[r, "h^S"]\ar[d, "X^f"']\&
	Y^S\ar[d, "Y^f"]\\
	X^R\ar[r, "h^R"']\&
	Y^R\ar[ul, phantom, "?"]\&
	\qedhere
\end{tikzcd}
\]
commutes, we note that by \cref{prop.representable_nt}, both vertical maps compose functions from $S$ with $f \colon R \to S$ from the left, and by \cref{def.representable}, both horizontal maps compose functions to $X$ with $h \colon X \to Y$ on the right.
So by the associativity of composition, the diagram commutes.
\end{solution}
\end{exercise}

\begin{exercise} \label{exc.representable_nt_components}
Let $X$ be an arbitrary set. For each of the following sets $R,S$ and functions $f\colon R\to S$, describe the $X$-component of, i.e.\ the function $X^S\to X^R$ coming from, the natural transformation $\yon^f\colon\yon^S\to\yon^R$.
\begin{enumerate}
	\item \label{exc.representable_nt_components.id} $R=\5$, $S=\5$, $f=\id$.  (Here you're supposed to give a function called $X^{\id_\5}\colon X^\5\to X^\5$.)
	\item $R=\2$, $S=\1$, $f$ is the unique function.
	\item $R=\1$, $S=\2$, $f(1)=1$.
	\item $R=\1$, $S=\2$, $f(1)=2$.
	\item $R=\0$, $S=\5$, $f$ is the unique function.
	\item $R=\nn$, $S=\nn$, $f(n)=n+1$.
\qedhere
\end{enumerate}

\begin{solution}
In each case, given $f \colon R \to S$, we can find the $X$-component $X^f \colon X^S \to X^R$ of the natural transformation $\yon^f\colon\yon^S\to\yon^R$ by applying \cref{prop.representable_nt}, which says that $X^f$ sends each $g \colon S \to X$ to $f \then g \colon R \to X$.
\begin{enumerate}
    \item If $R=\5$, $S=\5$, and $f=\id$, then $X^f$ is the identity function on $X^\5$.
    \item If $R=\2$, $S=\1$, and $f$ is the unique function, then $X^f$ sends each $g \in X$ (i.e.\ function $g \colon \1 \to X$) to the function that maps both elements of $\2$ to $g$.
    We can think of $X^f$ as the diagonal $X \to X \times X$.
	\item If $R=\1$, $S=\2$, and $f(1)=1$, then $X^f$ sends each $g \colon \2 \to X$ to $g(1)$, viewed as a function $\1 \to X$.
	We can think of $X^f$ as the left projection $X \times X \to X$.
	\item If $R=\1$, $S=\2$, and $f(1)=2$, then $X^f$ sends each $g \colon \2 \to X$ to $g(2)$, viewed as a function $\1 \to X$.
	We can think of $X^f$ as the right projection $X \times X \to X$.
	\item If $R=\0$, $S=\5$, and $f$ is the unique function, then $X^f$ is the unique function $X^\5 \to X^\0 \iso \1$.
	\item If $R=\nn$, $S=\nn$, and $f(n)=n+1$, then $X^f$ sends each $g \colon \nn \to X$ to the function $h \colon \nn \to X$ satisfying $h(n) = g(n+1)$ for all $n \in \nn$.
	We can think of $X^f$ as removing the first term of an infinite sequence of elements of $X$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise} \label{exc.representable_nt_functorial}
Show that the construction in \cref{prop.representable_nt} is functorial
\begin{equation}
\yon^-\colon\smset\op\to\smset^\smset,
\end{equation}
as follows.
\begin{enumerate}
	\item Show that for any set $S$, we have $\yon^{\id_S}\colon\yon^S\to\yon^S$ is the identity.
	\item Show that for any functions $f\colon R\to S$ and $g\colon S\to T$, we have $\yon^g\then\yon^f=\yon^{f\then g}$.
\qedhere
\end{enumerate}

\begin{solution}
\begin{enumerate}
    \item The fact that $\yon^{\id_S}\colon\yon^S\to\yon^S$ is the identity is just a generalization of \cref{exc.representable_nt_components} \cref{exc.representable_nt_components.id}.
    For any set $X$, the $X$-component $X^{\id_S} \colon X^S \to X^S$ of $\yon^{\id_S}$ sends each $h \colon S \to X$ to $\id_S \then h = h$, so $X^{\id_S}$ is the identity on $X^S$.
    Hence $\yon^{\id_S}$ is the identity on $\yon^S$.
    \item Fix $f \colon R \to S$ and $g \colon S \to T$; we wish to show that $\yon^g \then \yon^f = \yon^{f \then g}$.
    It suffices to show component-wise that $X^g \then X^f = X^{f \then g}$ for every set $X$.
    Indeed, $X^g$ sends each $h \colon T \to X$ to $g \then h$; then $X^f$ sends $g \then h$ to $f \then g \then h = X^{f \then g}(h)$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{lemma}[Yoneda lemma]\label{lemma.yoneda}
Given a functor $F\colon\smset\to\smset$ and a set $S$, there is an isomorphism
\begin{equation}\label{eqn.yoneda}
F(S)\cong\nat(\yon^S,F)
\end{equation}
where $\nat$ denotes the set of natural transformations. Moreover, \eqref{eqn.yoneda} is natural in both $S$ and $F$.
\end{lemma}
\begin{proof}[Sketch of proof]
For any natural transformation $m\colon\yon^S\to F$, consider the component $m_S\colon S^S\to F(S)$. Applying it to the identity on $S$ as an element of $S^S$, we get an element $m_S(\id_S)\in F(S)$.

For any element $a\in F(S)$, there is a natural transformation $m^a\colon\yon^S\to F$ whose $X$-component is the function $X^S\to F(X)$ given by sending $g\colon S\to X$ to $F(g)(a)$. In \cref{exc.finish_proof_yoneda} we ask you to show that this is natural in $X$ and that these two constructions are mutually inverse.
\end{proof}

\begin{exercise}\label{exc.finish_proof_yoneda}
Whoever solves this exercise can say they've proved the Yoneda lemma.
\begin{enumerate}
	\item Show that for any $a\in F(S)$, the maps $X^S\to F(X)$ given as in the proof sketch of \cref{lemma.yoneda} are natural in $X$.
	\item Show that the two mappings from the proof sketch of \cref{lemma.yoneda} are mutually inverse.
	\item Show that \eqref{eqn.yoneda} is natural in $F$.
	\item Show that \eqref{eqn.yoneda} is natural in $S$.
	\item As a corollary of \cref{lemma.yoneda}, show that $\yon^-\colon\smset\op\to\smset^\smset$ is fully faithful, in particular that there is an isomorphism $\nat(\yon^S,\yon^T)\cong S^T$.
\qedhere
\end{enumerate}

\begin{solution}
\begin{enumerate}
    \item Given $a \in F(S)$, naturality of the maps $X^S \to F(X)$ that send $g \colon S \to X$ to $F(g)(a)$ amounts to the commutativity of
    \[
    \begin{tikzcd}[ampersand replacement=\&]
    	X^S\ar[r, "h^S"]\ar[d, "F(-)(a)"']\&
    	Y^S\ar[d, "F(-)(a)"]\\
    	F(X)\ar[r, "F(h)"']\&
    	F(Y)
    \end{tikzcd}
    \]
    for all $h \colon X \to Y$.
    The top map $h^S$ sends any $g \colon X \to S$ to $g \then h$ (\cref{def.representable}), which is then sent to $F(g \then h)(a)$ by the right map.
    Meanwhile, the left map sends $g$ to $F(g)(a)$, which is then sent to $F(h)(F(g)(a))$ by the bottom map.
    So by the functoriality of $F$, the square commutes.
    
    \item We seek to show that the two maps from the proof sketch of \cref{lemma.yoneda} are mutually inverse. First, we show that for any natural transformation $m \colon \yon^S \to F$, we have that $m^{m_S(\id_S)} = m$.
    Given a set $X$, the $X$-component of $m^{m_S(\id_S)}$ sends each $g \colon S \to X$ to $F(g)(m_S(\id_S))$; it suffices to show that this is also where the $X$-component of $m$ sends $g$.
    Indeed, by the naturality of $m$, the square
    \[
    \begin{tikzcd}[ampersand replacement=\&]
    	S^S\ar[r, "g^S"]\ar[d, "m_S"']\&
    	X^S\ar[d, "m_X"]\\
    	F(S)\ar[r, "F(g)"']\&
    	F(X)
    \end{tikzcd}
    \]
    commutes, so in particular
    \begin{equation} \label{eqn.finish_proof_yoneda}
        F(g)(m_S(\id_S)) = m_X(g^S(\id_S)) = m_X(\id_S \then g) = m_X(g).
    \end{equation}
    In the other direction, we show that for any $a \in F(S)$, we have $m^a_S(\id_S) = a$: by construction, $m^a_S \colon S^S \to F(S)$ sends $\id_S$ to $F(\id_S)(a) = a$.
    
    \item Given functors $F, G \colon \smset^\smset$ and a natural transformation $\alpha \colon F \to G$, we wish to show that the naturality square
    \[
    \begin{tikzcd}[ampersand replacement=\&]
    	\nat(\yon^S,F)\ar[d, "- \then \alpha"']\ar[r, "\sim"]\&
    	F(S)\ar[d, "\alpha_S"]\\
    	\nat(\yon^S,G)\ar[r, "\sim"]\&
    	G(S)
    \end{tikzcd}
    \]
    commutes.
    The top map sends any $m \colon \yon^S \to F$ to $m_S(\id_S)$, which in turn is sent by the right map to $\alpha_S(m_S(\id_S)) = (m \then \alpha)_S(\id_S)$.
    This is also where the bottom map sends $m \then \alpha$, so the square commutes.
    
    \item Given a function $g \colon S \to X$, we wish to show that the naturality square on the left side of the diagram
    \[
    \begin{tikzcd}[ampersand replacement=\&]
    	\nat(\yon^S,F)\ar[d, "\yon^g \then -"']\ar[r, "\sim"]\&
    	F(S)\ar[d, "F(f)"]\\
    	\nat(\yon^X,F)\ar[r, "\sim"]\&
    	F(X)
    \end{tikzcd}
    \]
    commutes.
    The left map sends any $m \colon \yon^S \to F$ to $\yon^g \then m$, which is sent by the bottom map to $(\yon^g \then m)_X(\id_X) = m_X(X^g(\id_X)) = m_X(f \then \id_X) = m_X(g)$.
    Meanwhile, the top map sends $m$ to $m_S(\id_S)$, which is sent by the right map to $F(g)(m_S(\id_S))$.
    So the square commutes by \eqref{eqn.finish_proof_yoneda}.
    
    \item To show that $\nat(\yon^S, \yon^T) \iso S^T$, just take $F = \yon^T$ in \cref{lemma.yoneda}.
\end{enumerate}
\end{solution}
\end{exercise}

%-------- Section --------%
\section{Polynomials: sums of representables} \label{sec.poly.func_nat.repr_sum}

We've seen that for any set $A$, the symbol $\yon^A$ represents a functor $\smset\to\smset$. We will generalize this by adding representable functors together to form polynomials. In some sense the name ``polynomial'' doesn't quite fit, because in algebra polynomials are often taken to be finite sums, whereas we will use sums that may be infinite. However, we are not the first by far to use the term ``polynomial'' in this way.

So here's the deal. All of our polynomials will be polynomials in one variable, $\yon$; every other letter or number that shows up will represent a set.%
\footnote{For those who clamor for polynomials in many variables, we will see in \cref{**} that the multivariable story falls out of the one-variable story.}
For example, in the following bizarre polynomial
\begin{equation}\label{eqn.biz_poly}
p\coloneqq\rr\yon^\zz+\3\yon^{\3}+A\yon+\sum_{i\in I}Q_i\yon^{R_i+Q_i^2},
\end{equation}
$\rr$ denotes the set of real numbers, $\zz$ denotes the set of integers, $\3$ denotes the set $\{1,2,3\}$, and $A$, $I$, $Q_i$, and $R_i$ denote some arbitrary sets that should have already been defined in order for \eqref{eqn.biz_poly} to make sense.

The polynomials we already understand at this point are the pure power polynomials $\yon^A$ for some set $A$: they are representable functors $\yon^A\colon\smset\to\smset.$
So to understand general polynomials like $p$, we just need to understand what the sums of functors are. It will be useful to discuss products of functors at the same time.

To undertand the sums and products of set-valued functors, we will first need to understand the sums and products of sets.

%---- Subsection ----%
\subsection{Dependent sums and products of sets} \label{subsec.poly.func_nat.repr_sum.dep_sum_prod_set}

Let $I$ be a set, and let $X_i$ be a set for each $i\in I$. We denote this
\emph{$I$-indexed collection of sets} --- a \emph{dependent set} --- in the form of a functor as $X\colon I\to\smset$ (where we view the set $I$ as a discrete category) or more classically as $(X_i)_{i\in I}$.
Indeed, when a functor $X \colon I \to \smset$ is understood to be a dependent set, we will denote $X(i)$ as $X_i$ for $i \in I$.
Choosing one element from one set in the collection would be denoted $(i,x)$ where $x\in X_i$. Choosing an element from
each set in the collection would give us a function $i \mapsto x_i$ where each
$x_i\in X_i$. This is a sort of function we haven't seen before, at least in
this form --- its codomain \emph{depends} on the element we are applying it to.
Think of a vector field $v$: to each point $p$ it assignes a tangent vector
$v_p$ in the tangent space \emph{at $p$}. We can write the signature of such a
function as
\[f \colon (i \in I) \to X_i.\]
We call this a \emph{dependent function}, since its codomain depends on the
element of its domain we are applying it to.

\begin{definition}[Dependent sums and products of sets]
Let $I$ be a set and $X\colon I\to\smset$ be an $I$-indexed collection of sets. The \emph{sum} $\sum_{i\in I}X_i$ and \emph{product} $\prod_{i\in I}X_i$ of this collection are the sets
\[
\sum_{i\in I}X_i:=\{(i,x)\mid i\in I\text{ and }x\in X_i\}
\qqand
\prod_{i\in I}X_i:=\{f \colon (i \in I) \to X_i\}.
\]
\end{definition}

\begin{example}\label{ex.two_sums_and_prods}
If $I=\2=\{1,2\}$ then a collection $X\colon I\to \smset$ is just two sets, say $X_1=\{a,b,c\}$ and $X_2=\{c,d\}$. Their sum is the disjoint union
\[\sum_{i\in \2}X_i=X_1+X_2=\{(1,a),(1,b),(1,c),(2,c),(2,d)\}.\]
Its cardinality (i.e.\ the number of elements it contains) will always be the sum of the cardinalities of $X_1$ and $X_2$.

Meanwhile, their product is the usual Cartesian product
\[\prod_{i\in \2}X_i \cong X_1\times X_2=\{(a,c),(a,d),(b,c),(b,d),(c,c),(c,d)\}.\]
Its cardinality will always be the product of the cardinalities of $X_1$ and $X_2$.
\end{example}


\begin{exercise}\label{exc.on_sums_prods_sets}
Let $I$ be a set and let $X_i\coloneqq\1$ be a one-element set for each $i\in I$. 
\begin{enumerate}
	\item \label{exc.on_sums_prods_sets.sum} Show that there is an isomorphism of sets $I\cong\sum_{i\in I}\1$.
	\item \label{exc.on_sums_prods_sets.prod} Show that there is an isomorphism of sets $\1\cong\prod_{i\in I}\1$.
\end{enumerate}
As a special case, suppose $I\coloneqq\varnothing$ and $X\colon \varnothing\to\smset$ is the unique empty collection of sets.
\begin{enumerate}[resume]
	\item Is it true that $X_i=\1$ for each $i\in I$?
	\item Show that there is an isomorphism of sets $\0\cong\sum_{i\in\varnothing}X_i$.
	\item Show that there is an isomorphism of sets $\1\cong\prod_{i\in\varnothing}X_i$.
\qedhere
\end{enumerate}

\begin{solution}
We are given a set $I$ and a dependent set $(X_i)_{i \in I}$ for which $X_i \coloneqq \1$ for every $i \in I$.
\begin{enumerate}
    \item \label{sol.on_sums_prods_sets.sum}
    To show that $I \iso \sum_{i \in I} \1$, we note that $x \in \1 = \{1\}$ if and only if $x = 1$, so $\sum_{i \in I} \1 = \{(i, 1) \mid i \in I\}$.
    Then function $I \to \sum_{i \in I} \1$ that sends each $i \in I$ to $(i, 1)$ is clearly an isomorphism.
    
    \item \label{sol.on_sums_prods_sets.prod}
    To show that $\1 \iso \prod_{i \in I} \1$, it suffices to demonstrate that there is a unique dependent function $f \colon (i \in I) \to \1$.
    As $\1 = \{1\}$, such a function $f$ must always send $i \in I$ to $1$.
    This completely characterizes $f$, so there is only one such dependent function.
    
\end{enumerate}
Now $I \coloneqq \varnothing$ and $X \colon \varnothing \to \smset$ is the unique empty collection of sets.
\begin{enumerate}[resume]
    \item \label{sol.on_sums_prods_sets.vac} Yes: since $I$ is empty, there are no $i \in I$.
    So it is true that $X_i = 1$ holds whenever $i \in I$ holds, because $i \in I$ never holds. 
    We say that this sort of statement is ``vacuously true.''
    
    \item As $I = \varnothing = \0$, we have $\0 = I \iso \sum_{i \in I} \1 = \sum_{i \in \varnothing} X_i$, where the middle isomorphism follows from \cref{sol.on_sums_prods_sets.sum} and the last equation follows from \cref{sol.on_sums_prods_sets.vac}.
    
    \item As $I = \varnothing = \0$, we have $\1 \iso \prod_{i \in I} \1 = \prod_{i \in \varnothing} X_i$, where the isomorphism on the left follows from \cref{sol.on_sums_prods_sets.prod} and the equation on the right follows from \cref{sol.on_sums_prods_sets.vac}.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.dependent_product_as_sections}
  Let $X \colon I \to \smset$ be a set depending on an $i \in I$. There is a
  projection function
  $\pi_1 \colon \sum_{i \in I} X_i \to I$
  defined by $\pi_1(i, x) = i$.
  \begin{enumerate}
    \item What is the signature of the second projection $\pi_2(i, x) = x$?
    (Hint: it's a dependent function.)
    \item A \emph{section} of a function $r \colon A \to B$ is a function $s \colon B \to A$ such that $s \then r = \id_B$.
    Show that the dependent product is isomorphic to the set of sections of $\pi_1$:
    \[\prod_{i \in I} X_i \cong \left\{s \colon I \to \sum_{i \in I} X_i \,\middle|\, s \then \pi_1 = \id_I\right\}.\]
    \qedhere
  \end{enumerate}
\begin{solution}
We have a dependent set $X \colon I \to \smset$ and a projection function $\pi_1 \colon \sum_{i \in I} X_i \to I$ defined by $\pi_1(i, x) = i$.
\begin{enumerate}
    \item The second projection $\pi_2(i, x) = x$ sends each pair $p = (i, x) \in \sum_{i \in I} X_i$ to $x$, an element of $X_i$.
    Note that we can write $i$ in terms of $p$ as $\pi_1(p)$.
    This allows us to write the signature of $\pi_2$ as $\pi_2 \colon (p \in \sum_{i \in I} X_i) \to X_{\pi_1(p)}$.
    
    \item Let $S := \{s \colon I \to \sum_{i \in I} X_i \mid s \then \pi_1 = \id_I\}$ be the set of sections of $\pi_1$. To show that $\prod_{i \in I} X_i \cong S$, we will exhibit maps in either direction and show that they are mutually inverse.
    For each $f \colon (i \in I) \to X_i$ in $\prod_{i \in I} X_i$, we have that $f(i) \in X_i$ for all $i \in I$, so we can define a function $s_f \colon I \to \sum_{i \in I} X_i$ that sends each $i \in I$ to $(i, f(i))$.
    Then $\pi_1(s_f(i)) = \pi_1(i, f(i)) = i$, so $s_f$ is indeed a section of $\pi_1$.
    Hence $f \mapsto s_f$ is a map $\prod_{i \in I} X_i \to S$.
    
    In the other direction, for each section $s \colon I \to \sum_{i \in I} X_i$ we have $\pi_1(s(i)) = i$ for all $i \in I$, so we can write $s(i)$ as an ordered pair $(i, \pi_2(s(i)))$ with $\pi_2(s(i)) \in X_i$.
    It follows that we can define a function $f_s \colon (i \in I) \to X_i$ that sends each $i \in I$ to  $\pi_2(s(i))$.
    Then $s \mapsto f_s$ is a map $S \to \prod_{i \in I} X_i$.
    By construction $s_{f_s}(i) = (i, f_s(i)) = (\pi_1(s(i)), \pi_2(s(i))) = s(i)$ and $f_{s_f}(i) = \pi_2(s_f(i)) = \pi_2(i, f(i)) = f(i)$, so these maps are mutually inverse.
\end{enumerate}
\end{solution}
\end{exercise}

A helpful way to think about sum or product sets is by considering what choices must be made to specify an element of such a set.
In the following examples, say that we have a dependent set $X \colon I \to \smset$.

Here we give the instructions for choosing an element of $\sum_{i \in I} X_i$.

\begin{quote}
To choose an element of $\sum_{i \in I} X_i$: 
\begin{enumerate}
    \item choose an element $i \in I$;
    \item choose an element of $X_i$.
\end{enumerate}
\end{quote}

Then the projection $\pi_1$ from \cref{exc.dependent_product_as_sections} sends each element of $\sum_{i \in I} X_i$ to the element of $i \in I$ chosen in step 1, while the projection $\pi_2$ sends each element of $\sum_{i \in I} X_i$ to the element of $X_i$ chosen in step 2.

Now we give the instructions for choosing an element of $\prod_{i \in I} X_i$.

\begin{quote}
To choose an element of $\prod_{i \in I} X_i$: 
\begin{enumerate}
    \item for each element $i \in I$:
    \begin{enumerate}[label=\arabic*.]
        \item choose an element of $X_i$.
    \end{enumerate}
\end{enumerate}
\end{quote}

Armed with these interpretations, we can tackle more complicated expressions, including those with nested $\sum$'s and $\prod$'s like
\begin{equation}\label{eqn.misc98339}
A \coloneqq \sum_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k).
\end{equation}
We can give the instructions for choosing an element of $A$ as a nested list, as follows.

\begin{quote}
To choose an element of $A$:
\begin{enumerate}
    \item choose an element $i \in I$;
    \item for each element $j \in J(i)$:
    \begin{enumerate}[label=\arabic*.]
        \item choose an element $k \in K(i,j)$;
        \item choose an element of $X(i,j,k)$.
    \end{enumerate}
\end{enumerate}
\end{quote}

Note that the choice of $k\in K(i,j)$ can depend on $i$ and $j$; it must be able to, because different values of $i$ and $j$ may lead to different sets $K(i,j)$.

By describing $A$ like this, it is clear that each $a \in A$ can be projected to an element $\pi_1(a) \in I$ from step 1 and a dependent function $\pi_2(a)$ from step 2.
This dependent function in turn sends each $j \in J(i)$ to a pair that can be projected to an element $\pi_1(\pi_2(a)(j)) \in K(i, j)$ from step 2.1 and an element $\pi_2(\pi_2(a)(j)) \in X(i,j,k)$ from step 2.2.

\begin{example}%[Notation for $\sum\prod$ stuff]
\label{ex.notation_sum_prod}
% Here we give notation for the elements of a set involving $\sum$'s and $\prod$'s such as that in \eqref{eqn.misc98339}.

Let $I=\{1,2\}$, let $J(1)=\{j\}$ and $J(2)\coloneqq\{j,j'\}$, let $K(1,j)\coloneqq\{k_1,k_2\}$, $K(2,j)\coloneqq\{k_1\}$, and $K(2,j')\coloneqq\{k'\}$, and let $X(i,j,k)=\{x,y\}$ for all $i,j,k$. Now the formula 
\[\sum_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k)\]
from \eqref{eqn.misc98339} has been given meaning as an actual set. Here is a list of all eight of its elements:
\[
\left\{
\begin{gathered}
	\big(1, j\mapsto(k_1,x)\big),\qquad
	\big(1, j\mapsto(k_1,y)\big),\qquad
	\big(1, j\mapsto(k_2,x)\big),\qquad
	\big(1, j\mapsto(k_2,y)\big),\\
	\big(2, j\mapsto(k_1,x), j'\mapsto(k',x)\big),\qquad
	\big(2, j\mapsto(k_1,x), j'\mapsto(k',y)\big),\\
	\big(2, j\mapsto(k_1,y), j'\mapsto(k',x)\big),\qquad
	\big(2, j\mapsto(k_1,y), j'\mapsto(k',y)\big)
\end{gathered}
\right\}
\]
In each case, we first chose an element $i\in I$, either 1 or 2. Then for each $j\in J(i)$ we chose an element $k\in K(i,j)$; then we concluded by choosing an element of $X(i,j,k)$.
\end{example}

\begin{exercise}
Consider the set
\[B \coloneqq \prod_{i\in I}\sum_{j\in J(i)}\prod_{k\in K(i,j)}X(i,j,k).\]
\begin{enumerate}
	\item Give the instructions for choosing an element of $B$ as a nested list, like we did for $A$ just below \eqref{eqn.misc98339}.
	\item With $I$, $J$, $K$, and $X$ as in \cref{ex.notation_sum_prod}, how many elements are in $B$?
	\item Write out three of these elements in the style of \cref{ex.notation_sum_prod}.
\qedhere
\end{enumerate}
\begin{solution}
We are given the set
\[
    B \coloneqq \prod_{i\in I}\sum_{j\in J(i)}\prod_{k\in K(i,j)}X(i,j,k).
\]
\begin{enumerate}
	\item Here are the instructions for choosing an element of $B$ as a nested list.
	\begin{quote}
	    To choose an element of $B$:
	    \begin{enumerate}[label=\arabic*.]
	        \item for each element $i \in I$:
	        \begin{enumerate}[label=\arabic*.]
	            \item choose an element $j \in J(i)$;
	            \item for each element $k \in K(i, j)$:
	            \begin{enumerate}[label=\arabic*.]
	                \item choose an element of $X(i,j,k)$.
	            \end{enumerate}
	        \end{enumerate}
	    \end{enumerate}
	\end{quote}
	\item Given $I\coloneqq\{1,2\}$, $J(1)\coloneqq\{j\}$, $J(2)\coloneqq\{j,j'\}$, $K(1,j)\coloneqq\{k_1,k_2\}$, $K(2,j)\coloneqq\{k_1\}$, $K(2,j')\coloneqq\{k'\}$, and $X(i,j,k)=\{x,y\}$ for all $i,j,k$, our goal is to count the number of elements in $B$.
	To compute the cardinality of $B$, we can use the fact that the cardinality of a sum (resp.\ product) is the sum (resp.\ product) of the cardinalities.
	So
	\begin{align*}
	    |B| &= \prod_{i\in I}\sum_{j\in J(i)}\prod_{k\in K(i,j)}|X(i,j,k)| \\
	    &= \prod_{i\in \{1,2\}}\sum_{j\in J(i)}\prod_{k\in K(i,j)}2 \\
	    &= \left(\sum_{j\in J(1)} 2^{|K(1,j)|}\right)\left(\sum_{j\in J(2)} 2^{|K(2,j)|}\right) \\
	    &= \left(2^2\right)\left(2^1 + 2^1\right) = 16.
	\end{align*}
	\item Here are three of the elements of $B$ (you may have written down others):
	\begin{itemize}
	    \item $(1 \mapsto (j, k_1 \mapsto x, k_2 \mapsto y), 2 \mapsto (j', k' \mapsto x))$
	    \item $(1 \mapsto (j, k_1 \mapsto y, k_2 \mapsto y), 2 \mapsto (j, k_1 \mapsto y))$
	    \item $(1 \mapsto (j, k_1 \mapsto y, k_2 \mapsto x), 2 \mapsto (j', k' \mapsto y))$
	\end{itemize}
\qedhere
\end{enumerate}
\end{solution}
\end{exercise}

%---- Subsection ----%
\subsection{Expanding products of sums} \label{subsec.poly.func_nat.repr_sum.expand}

We will often encounter sums of dependent sets nested within products.
The following proposition helps us work with these; it is sometimes called the \emph{type-theoretic axiom of choice} or the \emph{completely distributive property}, in this case of $\smset$. It is almost trivial, once you understand what it's saying; in particular, once the statement is written in Agda, its proof is one short line of Agda code. 

\begin{proposition}[Pushing $\prod$ past $\sum$]\label{prop.push_prod_sum_set}
For any set $I$, collection of sets $\{J(i)\}_{i\in I}$, and collection of sets $\{X(i,j)\}_{i\in i, j\in J(i)}$, we have a bijection
\begin{equation}\label{eqn.set_completely_distributive}
\prod_{i\in I}\sum_{j\in J(i)}X(i,j)
\cong
\sum_{\bar{j}\in \prod_{i\in I}J(i)}\;\prod_{i\in I}X(i,\bar{j}(i)).
\end{equation}
\end{proposition}
\begin{proof}
We'll do this the old-fashioned way: by giving a map from left to right, a
map from right to left, and a proof that the two maps are mutually inverse. 

First, let's go from left to right. An element of the set on the left is a dependent function $f \colon (i \in I) \to \sum_{j \in J(i)} X(i, j)$, which we can compose with projections from its codomain to yield $\pi_1(f(i)) \in J(i)$ and $\pi_2(f(i)) \in X(i, \pi_1(f(i)))$ for every $i \in I$.
We can then form the following pair in the right hand set:
\[
    (i \mapsto \pi_1(f(i)), i \mapsto \pi_2(f(i))).
\]

Now let's go from right to left. An element of the set on the right is a pair of dependent functions, $\bar{j} \colon (i \in I) \to J(i)$ and $g \colon
(i \in I) \to X(i, \bar{j}(i))$. We then get an element of the set on the left as follows:
\[i \mapsto (\bar{j}(i), g(i)).\]

Now, we just need to check that a round trip takes us back where we were. If we
start on the right from $(\bar{j}, g)$, our round trip gives us the pair
\[(i \mapsto \pi_1(\bar{j}(i), g(i)), i \mapsto \pi_2(\bar{j}(i), g(i))).\]
But $\pi_1(\bar{j}(i), g(i)) = \bar{j}(i)$ and $\pi_2(\bar{j}(i), g(i)) = g(i)$ by definition, so
we're back where we started. On the other hand, starting on the left from $f$ gives us the function
\[i \mapsto (\pi_1(f(i)), \pi_2(f(i))).\]
But again, since $f(i)$ is a pair whose components are $\pi_1(f(i))$ and $\pi_2(f(i))$, we're back where we started.

\erase{
We start with the function from left to right. To give such a function it suffices by the universal properties of products and coproducts to fix an arbitrary $j_0\in\prod_{i\in I}J(i)$ and $i_0\in I$, and provide a function $\prod_{i\in I}X(i,j_0(i))\to\sum_{j\in J(i_0)}X(i_0,j)$. We do so by first projecting onto the $i_0$ factor and then including into the $j_0(i_0)$ summand
\[
  \prod_{i\in I}X(i,j_0(i))\to 
  X(i_0,j_0(i_0))\to
  \sum_{j\in J(i_0)}X(i_0,j).
\]
We have now given the function from left to right. To go the other way..
}
\end{proof}

When $J(i)=J$ does not depend on $i\in I$, the formula in \eqref{eqn.set_completely_distributive} becomes much easier.

\begin{corollary} \label{cor.push_prod_sum_set_indep}
For any set $I$, set $J$, and collection of sets $\{X(i, j)\}_{i \in I, j \in J}$, we have a bijection
\begin{equation} \label{eqn.push_prod_sum_set_indep}
    \prod_{i\in I}\sum_{j\in J}X(i,j)\cong\sum_{\bar{j}\colon I\to J}\prod_{i\in I}X(i,\bar{j}(i)).
\end{equation}
\end{corollary}
\begin{proof}
Just take $J(i) = J$ for all $i \in I$ in \eqref{eqn.set_completely_distributive}.
Note that dependent functions $\bar{j}$ in $\prod_{i \in I} J(i)$ then become standard functions $\bar{j} \colon I \to J$.
\end{proof}

Below, e.g.\ in \cref{exc.practice_sum_prod}, you'll often see alternating products and sums; using \eqref{eqn.set_completely_distributive}, you can always write it as a sum of products, in which every $\sum$ appears before every $\prod$ (i.e.\ in ``disjunctive normal form'').
This is analogous to how products of sums in high school algebra can always be expanded into sums of products via the distributive property.

% \begin{exercise} \label{exc.push_prod_sum_set}
% \begin{enumerate}
%     \item Rewrite
%     \[
%         \sum_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k)
%     \]
%     so that every $\sum$ appears before every $\prod$.
%     \item Rewrite
%     \[
%         \prod_{i\in I}\sum_{j\in J(i)}\prod_{k\in K(i,j)}X(i,j,k)
%     \]
%     so that every $\sum$ appears before every $\prod$.\qedhere
% \end{enumerate}
% \begin{solution}
% Our goal is to rewrite each expression so that every $\sum$ appears before every $\prod$.
% \begin{enumerate}
%     \item By applying \eqref{eqn.set_completely_distributive}, we can rewrite
%     \[
%         \sum_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k)
%     \]
%     as
%     \[
%         \sum_{i\in I}\sum_{\bar{k}\in \prod_{j\in J}K(i,j)}\prod_{j\in J(i)}X(i,j,\bar{k}(j)).
%     \]
%     \item By applying \eqref{eqn.set_completely_distributive}, we can rewrite
%     \[
%         \prod_{i\in I}\sum_{j\in J(i)}\prod_{k\in K(i,j)}X(i,j,k)
%     \]
%     as
%     \[
%         \sum_{\bar{j}\in \prod_{i\in I}J(i)}\;\prod_{i\in I}X(i,\bar{j}(i))\prod_{k\in K(i,\bar{j}(i))}X(i,\bar{j}(i),k).
%     \]
% \end{enumerate}
% \end{solution}
% \end{exercise}

%---- Subsection ----%
\subsection{Dependent sums and products of functors $\smset\to\smset$} \label{subsec.poly.func_nat.repr_sum.dep_sum_prod_func}

Where are we, and where are we going? We've defined dependent sums and products of sets; that's where we are. Our goal is to define polynomial functors, e.g.\ $\yon^\2+\2\yon+\1$, and the maps between them. Since $\yon^2$, $\yon$, and $\1$ are functors, we just need to define sums of functors $\smset\to\smset$. But we might as well define products of functors at the same time, because they'll very much come in handy.

\begin{definition}[Dependent sums and products of functors $\smset\to\smset$]\label{def.sum_prod}
For any two functors $F,G\colon\smset\to\smset$, let
\[
  (F+G)\colon\smset\to\smset
  \qqand
  (F\times G)\colon\smset\to\smset
\]
denote the functors that respectively assign to each $X\in\smset$ the sets
\[
  (F+G)(X)\coloneqq F(X)+G(X)
  \qqand
	(F\times G)(X)\coloneqq F(X)\times G(X).
\]
We may also denote the product functor $F \times G$ by $FG$.

More generally, for any set $I$ and functors $(F_i)_{i\in I}$, let
\[
\sum_{i\in I}F_i\colon\smset\to\smset
\qqand
\prod_{i\in I}F_i\colon\smset\to\smset
\]
denote the functors that respectively assign to each $X\in\smset$ the sum and product of sets
\[
	\Big(\sum_{i\in I}F_i\Big)(X)\coloneqq\sum_{i\in I} F_i(X)
	\qqand
	\Big(\prod_{i\in I}F_i\Big)(X)\coloneqq\prod_{i\in I} F_i(X).
\]
Given a set $I\in\smset$, we will also use $I$ to denote the constant functor that assigns $I$ to each $X\in\smset$.
In particular, we denote by $\0,\1\colon\smset\to\smset$ the constant functors that respectively assign $\0$ and $\1$ to each $X\in\smset$.
\end{definition}

\begin{exercise} \label{exc.repeated_sum_is_product}
\begin{enumerate}
	\item Show that for a set $I \in \smset$ and a functor $F \colon \smset \to \smset$, the sum of $I$ copies of $F$ is isomorphic to the product of the constant functor $I$ and $F$:
    \[
        \sum_{i \in I} F \iso IF.
    \]
    (Note that this is analogous to the fact from basic arithmetic that adding up $n \in \nn$ copies of number is equal to multiplying that same number by $n$.)
	\item So does $\2\yon$ denote $\2\times\yon$ or $\yon+\yon$, or does it not matter for some reason?
\qedhere
	\end{enumerate}
\begin{solution}
\begin{enumerate}
	\item It suffices to show that for all $X \in \smset$,
    \[
      \sum_{i \in I} F(X) \iso (IF)(X).
    \]
    The left hand side is isomorphic to the set $\{(i,s)\mid i\in I\text{ and }s\in F(X)\} \iso I \times F(X)$, while the right hand side is also isomorphic to the set $I(X) \times F(X) \iso I \times F(X)$. (Alternatively, the result also follows from \eqref{eqn.set_completely_distributive}.)
    \item It doesn't matter: $\2\yon$ can denote either one. Indeed, taking $I\coloneqq\2$, the above says that there is an isomorphism $\2\times\yon\cong\yon+\yon$. 
\end{enumerate}
\end{solution}
\end{exercise}

\begin{proposition} \label{prop.prods_coprods_set_endofuncs}
Referring to the notation in \cref{def.sum_prod}, the functors $\0$ and $\1$ are respectively an initial object and a terminal object in $\smset^\smset$, the operations $+$ and $\times$ are respectively a binary coproduct and a binary product in $\smset^\smset$, and the operations $\sum_{i\in I}$ and $\prod_{i\in I}$ are arbitrary coproducts and products in $\smset^\smset$.
\end{proposition}
\begin{proof}
By \cref{ex.two_sums_and_prods,exc.on_sums_prods_sets}, it suffices to show that $\sum_{i\in I}F_i$ and $\prod_{i\in I}F_i$ are a sum and product in $\smset^\smset$. This itself is a special case of a more general fact, where sums and products are replaced by arbitrary colimits and limits, and where $\smset^\smset$ is replaced by an arbitrary functor category $\cat{C}^\cat{D}$, where $\cat{C}$ is a category that (like $\smset$) has limits and colimits; see \cite[page 22 -- 23, displays (24) and (25)]{macLane1992sheaves}.
\end{proof}

We've finally arrived: we can define polynomial functors!

%---- Subsection ----%
\subsection{What is a polynomial functor?} \label{subsec.poly.func_nat.repr_sum.what}

\begin{definition}[Polynomial functors]
A \emph{polynomial functor} (or simply a \emph{polynomial}) is a functor $p\colon\smset\to\smset$ such that there exists a set $I$, sets $(p[i])_{i\in I}$, and an isomorphism
\[p\cong\sum_{i\in I}\yon^{p[i]}\]
to a sum of representables.
\end{definition}

So (up to isomorphism), a polynomial functor is just a sum of representables.

\begin{remark}
Given sets $I, A \in \smset$, it follows from \cref{exc.repeated_sum_is_product} that we have an isomorphism of polynomials
\[
    \sum_{i \in I} \yon^A \iso I\yon^A.
\]
So when we write down a polynomial, we will often combine identical representable summands $\yon^A$ by writing them in the form $I\yon^A$.
In particular, the constant functor $\1$ is a representable functor ($\1 \iso \yon^\0$), so every constant functor $I$ is a polynomial functor: $I \iso \sum_{i \in I} \1$.
\end{remark}

\begin{example}\label{ex.pedantic_poly_eval}
Consider the polynomial $p \coloneqq \yon^\2+\2\yon+\1$. It denotes a functor $\smset\to\smset$; what does this functor do to the set $X\coloneqq\{a,b\}$? 
To be very precise and pedantic, let's say
\[
I\coloneqq\4\qqand
  p[1]\coloneqq\2,\quad
  p[2]\coloneqq\1,\quad
  p[3]\coloneqq\1,\quad
  p[4]\coloneqq\0\
\]
so that $p\cong\sum_{i\in I}\yon^{p[i]}$. Now we have
\[
p(X)\cong
\{(1,a,a),(1,a,b),(1,b,a),(1,b,b),(2,a),(2,b),(3,a),(3,b),(4)\}.
\]
It has $(2^2+2+2+1)$-many, i.e.\ $9$, elements. The representable summand $\yon^A$ throws in all $A$-tuples from $X$, but it's indexed by the name of the summand. In particular if $A=\0$ then it just records the empty tuple at that summand.
\end{example}

\begin{exercise}
In the pedantic style of \cref{ex.pedantic_poly_eval}, write out all the elements of $p(X)$ for $p$ and $X$ as follows:
\begin{enumerate}
	\item $p\coloneqq\yon^\3$ and $X\coloneqq\{4,9\}.$
	\item $p\coloneqq\3\yon^\2+\1$ and $X\coloneqq\{a\}$.
	\item $p\coloneqq\0$ and $X\coloneqq\nn$.
	\item $p\coloneqq\4$ and $X\coloneqq\nn$.
	\item $p\coloneqq\yon$ and $X\coloneqq\nn$.
\qedhere
\end{enumerate}

\begin{solution}
Our goal is to write out the elements of $p(X)$ for each polynomial $p$ and set $X$ that we are given.
\begin{enumerate}
    \item If $p\coloneqq\yon^\3$ and $X\coloneqq\{4,9\}$, then let $I \coloneqq \1$ and $p[1] \coloneqq \3$ so that $p \iso \sum_{i \in I} \yon^{p[i]}$.
    So
    \[
        p(X) \iso \{(1, 4, 4, 4), (1, 4, 4, 9), (1, 4, 9, 4), (1, 4, 9, 9), (1, 9, 4, 4), (1, 9, 4, 9), (1, 9, 9, 4), (1, 9, 9, 9)\}.
    \]
    
    \item If $p\coloneqq\3\yon^\2+\1$ and $X\coloneqq\{a\}$, then let $I \coloneqq \4$, $p[1] \coloneqq p[2] \coloneqq p[3] \coloneqq \2$, and $p[4] \coloneqq \1$ so that $p \iso \sum_{i \in I} \yon^{p[i]}$.
    So $p(X) \iso \{(1, a, a), (2, a, a), (3, a, a), (4)\}$.
	
	\item If $\coloneqq\0$ and $X\coloneqq\nn$, then let $I \coloneqq \0$ so that $p \iso \sum_{i \in I} \yon^{p[i]}$.
	So $p(X) \iso \0$.
	Alternatively, we note that $\0$ is the constant functor that assigns $\0$ to every set. 
	
	\item If $p\coloneqq\4$ and $X\coloneqq\nn$, then let $I \coloneqq \4$ and $p[i] \coloneqq \0$ for every $i \in I$.
	So $p(X) \iso \{(1), (2), (3), (4)\} \iso \4$.
	
	\item If $p\coloneqq\yon$ and $X\coloneqq\nn$, then let $I \coloneqq \1$ and $p[1] \coloneqq \1$ so that $p \iso \sum_{i \in I} \yon^{p[i]}$.
    So $p(X) \iso \{(1, n) \mid n \in \nn\}$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{example}
Suppose $p\coloneqq\yon^\2+\2\yon+1$. As a functor $\smset\to\smset$ it should be able to act not only on sets but on functions. Let $X\coloneqq\{a_1,a_2,b_1\}$, $Y\coloneqq\{a,b,c\}$, and $f\colon X\to Y$ be the function sending $a_1,a_2\mapsto a$ and $b_1\mapsto b$. The induced function $p(f)\colon p(X)\to p(Y)$ is shown below
\[
\begin{tikzcd}[row sep=2pt, column sep=3pt, shorten <=-5pt, shorten >=-5pt, dashed]
\LMO{(1,a_1,a_1)}\ar[rrrr, blue, bend left=25pt]&\LMO{(1,a_1,a_2)}\ar[rrr, blue, bend left=25pt]&\LMO{(1,a_1,b_1)}\ar[rrr, blue, bend left=25pt]&[30pt]&
\LMO{(1,a,a)}&\LMO{(1,a,b)}&\LMO{(1,a,c)}&&
\\
\LMO{(1,a_2,a_1)}\ar[rrrru, blue, bend left=10pt]&\LMO{(1,a_2,a_2)}\ar[rrru, blue, bend left=10pt]&\LMO{(1,a_2,b_1)}\ar[rrru, blue, bend left=10pt]&&
\LMO{(1,b,a)}&\LMO{(1,b,b)}&\LMO{(1,b,c)}&&
\\
\LMO{(1,b_1,a_1)}\ar[rrrru, blue, bend left=10pt]&\LMO{(1,b_1,a_2)}\ar[rrru, blue, bend left=10pt]&\LMO{(1,b_1,b_1)}\ar[rrru, blue, bend left=10pt]&&
\LMO{(1,c,a)}&\LMO{(1,c,b)}&\LMO{(1,c,c)}&&
\\
\LMO{(2,a_1)}\ar[rrrr, blue, bend left=25pt]&\LMO{(2,a_2)}\ar[rrr, blue, bend left=25pt]&\LMO{(2,b_1)}\ar[rrr, blue, bend left=25pt]&&
\LMO{(2,a)}&\LMO{(2,b)}&\LMO{(2,c)}&&
\\
\LMO{(3,a_1)}\ar[rrrr, blue, bend left=25pt]&\LMO{(3,a_2)}\ar[rrr, blue, bend left=25pt]&\LMO{(3,b_1)}\ar[rrr, blue, bend left=25pt]&&
\LMO{(3,a)}&\LMO{(3,b)}&\LMO{(3,c)}&&
\\&
\LMO{(4)}\ar[rrrr, blue]&&&&\LMO{(4)}
\end{tikzcd}
\]
\end{example}

\begin{exercise}
Let $p\coloneqq\yon^\2+\yon$. Choose a function $f\colon\1\to\2$ and write out the induced function $p(f)\colon p(\1)\to p(\2)$.
\begin{solution}
Given $p\coloneqq\yon^\2+\yon$, we seek the induced function $p(f)\colon p(\1)\to p(\2)$ for a function $f\colon\1\to\2$ of our choice.
We will choose $1 \mapsto 2$.
We can evaluate
\[
    p(\1) \iso \{(1, 1, 1), (2, 1)\} \text{ and } p(\2) \iso \{(1, 1, 1), (1, 1, 2), (1, 2, 1), (1, 2, 2), (2, 1), (2, 2)\}.
\]
So $p(f)$ sends $(1, 1, 1) \mapsto (1, 2, 2)$ and $(2, 1) \mapsto (2, 2)$.
(If we had instead picked $1 \mapsto 1$ as our function $f$, then $p(f)$ would send $(1, 1, 1) \mapsto (1, 1, 1)$ and $(2, 1) \mapsto (2, 1)$.)
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.apply1}
Let $p\coloneqq\sum_{i\in I}\yon^{p[i]}$ be an arbitrary polynomial functor. Then $I\cong p(\1)$, so there is an isomorphism of functors
\begin{equation}\label{eqn.sum_p1}
p\cong\sum_{i\in p(\1)}\yon^{p[i]}.
\end{equation}
\end{proposition}
\begin{proof}
We need to show that $I\iso p(\1)$; the latter claim follows directly. In \cref{exc.on_sums_prods_sets} it was shown that $I\iso\sum_{i\in I}\1$, so we just need to show that $(\yon^{p[i]})(\1)\iso\1$ for every $i \in I$. But $\1^{p[i]}\iso \1$ because there is a unique function $p[i]\to \1$ for any $p[i]$.
\end{proof}
We can draw an analogy between \cref{prop.apply1} and evaluating $p(1)$ for a polynomial $p$ from high school algebra, which yields the sum of the coefficients of $p$.
The notation in \eqref{eqn.sum_p1} will be how we denote arbitrary polynomials from now on.

\begin{exercise}\label{exc.apply0}
We saw in \cref{prop.apply1} that for any polynomial $p$, e.g.\ $p\coloneqq\yon^\3+\3\yon^\2+\4$, the set $p(\1)$ gives back the set of summands, in this case $\8$. 

What does $p(\0)$ give you?
\begin{solution}
We consider $p(\0)$ for arbitrary polynomials $p$.
A representable functor $\yon^S$ for some $S \in \smset$ sends $\0 \mapsto \0$ if $S \neq \0$ (as there are then no functions $S \to \0$), but sends $\0 \mapsto \1$ if $S = \0$ (as there is a unique function $\0 \to \0$).
So
\[
    p(\0) \iso \sum_{i \in p(\1)} (\yon^{p[i]})(\0) \iso \sum_{\substack{i \in p(\1), \\ p[i] \neq \0}} \0 + \sum_{\substack{i \in p(\1), \\ p[i] = \0}} \1 \iso \{i \in p(\1) \mid p[i] = \0\}.
\]
In other words, $p(\0)$ is the set of \emph{constant} representable summands of $p$.
For example, if $p\coloneqq\yon^\3+\3\yon^\2+\4$, then $p(\0) = \4$.
In the language of high school algebra, we might call $p(\0)$ the ``constant term'' of $p$.
\end{solution}
\end{exercise}

%-------- Section --------%
\section{Morphisms between polynomial functors}
\label{sec.poly.func_nat.morph}

Before we define the category $\poly$ of polynomial functors, we notice that polynomial functors already live inside a category, namely the category $\smset^\smset$ of functors $\smset \to \smset$, whose morphisms are natural transformations.
This leads to a very natural (if you will) definition of morphisms between polynomial functors, from which we can derive a category of polynomial functors for free.

\begin{definition}[Polynomial morphisms, $\poly$]
Given polynomial functors $p$ and $q$, a \emph{morphism of polynomial functors} (or a \emph{polynomial morphism}) is a natural transformation $p\to q$.
Then $\poly$ is the category whose objects are polynomial functors and whose morphisms are polynomial morphisms.
\end{definition}

In other words, $\poly$ is the full subcategory of $\smset^\smset$ spanned by the polynomial functors: we take the category $\smset^\smset$, throw out all the objects that are not polynomials, but keep all the same morphisms between any two polynomial functors.

%---- Subsection ----%
\subsection{Coproducts of polynomials}
\label{subsec.poly.func_nat.morph.coprod}

Since polynomial functors are defined as arbitrary sums of representables, coproducts in $\poly$ are quite easy to understand.

\begin{proposition} \label{prop.poly_coprods}
The category $\poly$ has arbitrary coproducts, given by the operation $\sum_{i \in I}$.
\end{proposition}
\begin{proof}
By \cref{prop.prods_coprods_set_endofuncs}, the category $\smset^\smset$ has arbitrary coproducts given by $\sum_{i \in I}$.
The full subcategory inclusion $\poly \to \smset^\smset$ reflects these coproducts, and by definition $\poly$ is closed under the operation $\sum_{i \in I}$.
\end{proof}

Explicitly, given polynomials $\{p_i\}_{i \in I}$, their coproduct is
\begin{equation} \label{eqn.poly_coprod}
    \sum_{i \in I} p_i \iso \sum_{i \in I} \sum_{j \in p_i(\1)} \yon^{p_i[j]} \iso \sum_{(i,j) \in \sum_{i \in I} p_i(\1)} \yon^{p_i[j]},
\end{equation}
which coincides with our notion of sums of functors $\smset \to \smset$ from \cref{def.sum_prod}.
Binary coproducts are thus given by binary sums of functors, denoted by $+$.
In particular, \eqref{eqn.poly_coprod} implies that for any polynomials $p$ and $q$, their coproduct $p+q$ is given as follows.
The set of positions of $p+q$ is the coproduct of sets $p(\1) + q(\1)$.
At position $(1,i) \in p(\1) + q(\1)$ with $i \in p(\1)$, the directions of $p+q$ are just the directions of $p$ at $i$; at position $(2,j) \in p(\1) + q(\1)$ with $j \in q(\1)$, the directions of $p+q$ are just the directions of $q$ at $j$.
% It follows that the initial object of $\poly$ is $\0$, and that binary coproducts are given by
% \begin{equation} \label{eqn.poly_plus}
%     p + q = ???
% \end{equation}

%---- Subsection ----%
\subsection{Polynomial morphisms, concretely} \label{subsec.poly.func_nat.morph.concrete}

A natural transformation between polynomials $p \to q$ consists of a function $p(X) \to q(X)$ for every set $X \in \smset$ such that the naturality squares commute.
That's a lot of data to keep track of!
Fortunately, there is a much simpler way to think about these polynomial morphisms, which we will discover with some help from our old friend, the Yoneda lemma.

\begin{exercise} \label{exc.poly_morph_yoneda}
Given a set $S$ and a polynomial $q$, show that a polynomial morphism $\yon^S \to q$ can be identified with an element of the set $q(S)$.
That is, there is an isomorphism
\[
    \poly(\yon^S, q) \iso q(S).
\]
Moreover, show that this isomorphism is natural in both $S$ and $q$.
Hint: Use the Yoneda lemma (\cref{lemma.yoneda}).
\begin{solution}
As $\poly(\yon^S, q)$ is defined to be $\nat(\yon^S, q)$\, the natural isomorphism $\poly(\yon^S, q) \iso q(S)$ follows directly from the Yoneda lemma (\cref{lemma.yoneda}) with $F = q$.
\end{solution}
\end{exercise}

Before we present our alternative characterization of polynomial morphisms, recall that every polynomial $p \coloneqq \sum_{i \in p(\1)} \yon^{p[i]}$ is uniquely associated to a dependent set, $(p[i])_{i \in p(\1)}$, which we call its \emph{arena}, as in \eqref{eqn.arena_example}.
Alternatively, we could write such a dependent set as a functor $p[-] \colon p(\1) \to \smset$, where we view the set $p(\1)$ as a discrete category.
Below, we make use of this functor notation to express the arenas of polynomials.

\begin{proposition}\label{prop.poly_maps_prod_sum}
Let $p\coloneqq\sum_{i\in p(\1)}\yon^{p[i]}$ and $q\coloneqq\sum_{j\in q(\1)}\yon^{q[j]}$ be polynomials.
Then we have an isomorphism
\begin{equation}\label{eqn.main_formula}
\poly(p,q)\cong\prod_{i\in p(\1)}\sum_{j\in q(\1)}{p[i]}^{q[j]}.
\end{equation}
% **Naturality???
In other words, a morphism $p\to q$ can be identified with a pair $(f_1,f^\sharp)$
\begin{equation}\label{eqn.colax_poly_map}
\begin{tikzcd}[column sep=small]
	p(\1)\ar[dr, "p{[-]}"']\ar[rr, "f_1"]&~&
	q(\1)\ar[dl, "q{[-]}"]\\&
	\smset\ar[u, phantom, near end, "\overset{f^\sharp}{\Leftarrow}"]
\end{tikzcd}
\end{equation}
where $f_1 \colon p(\1) \to q(\1)$ is a function (or functor between discrete categories) and $f^\sharp \colon q[f_1(-)] \to p[-]$ is a natural transformation: for each $i\in p(\1)$ with $j\coloneqq f_1(i)$, there is a function $f^\sharp_i\colon q[j]\to p[i]$. % **Maybe write an exercise about viewing dependent functions as natural transformations?
\end{proposition}
\begin{proof}
By the universal property of the coproduct, we have an isomorphism
\[
    \poly\left(\sum_{i \in p(\1)}\yon^{p[i]}, q\right) \iso \prod_{i \in p(\1)} \poly(\yon^{p[i]}, q),
\]
so applying \cref{exc.poly_morph_yoneda} (i.e.\ the Yoneda lemma) and unraveling the definitions of $p$ and $q$ yields \eqref{eqn.main_formula}.

The expression on the right hand side of \eqref{eqn.main_formula} is the set of dependent functions $f \colon (i \in p(\1)) \to \sum_{j \in q(\1)} p[i]^{q[j]}$, each of which is uniquely determined by its components: $\pi_1 \circ f$, which sends $i \in p(\1)$ to $\pi_1(f(i)) \in q(\1)$, and $\pi_2 \circ f$, which sends $i \in p(\1)$ with $j \coloneqq \pi_1(f(i))$ to an element of $p[i]^{q[j]}$, i.e.\ a function $q[j] \to p[i]$.
These can be identified respectively with a (non-dependent) function $f_1 \coloneqq \pi_1 \circ f$ from $p(\1) \to q(\1)$ and a natural transformation $f^\sharp \colon q[f_1(-)] \to p[-]$.
\end{proof}

We have now greatly simplified our characterization of polynomial morphisms $f \colon p \to q$: rather than infinitely many functions satisfying infinitely many naturality conditions, they can be specified simply as a function $f_1 \colon p(\1) \to q(\1)$ and, for each $i \in p(\1)$, a function $f^\sharp_i \colon q[f_1(i)] \to p[i]$, without any additional restrictions.

Here is where we begin to see the advantages of viewing polynomials as arenas.
As a reminder, from the arena perspective, we call the elements of $p(\1)$ the \emph{positions} of $p$, and for each position $i \in p(\1)$, we call the elements of $p[i]$ the \emph{directions} of $p$ at $i$.
We can see that our characterization of a polynomial morphism can be written entirely in the language of positions and directions: when polynomials $p$ and $q$ are viewed as arenas, a morphism $f \colon p \to q$ consists of a ``forwards'' \emph{on-positions} function $f_1$ from the positions of $p$ to the positions of $q$, along with, for every position $i$ of $p$, a ``backwards'' \emph{on-directions} function $f^\sharp_i$ from the directions of $q$ at $f_1(i)$ to the directions of $p$ at $i$.

When we wish to emphasize the arena perspective, we will call the data of $(f_1, f^\sharp)$ a \emph{morphism of arenas} or \emph{arena morphism} between $p$ and $q$; but since \cref{prop.poly_maps_prod_sum} tells us that polynomial morphisms and arena morphisms carry the same data, we may use these terms interchangeably.

Here is how you would implement such a morphism in Agda:
\begin{agda}
record ArenaMorphism (p : Arena) (q : Arena) : Set where
   field
     onPos : (pos p) -> (pos q)
     onDir : (i : pos p) -> dir q (onPos i) -> dir p i
\end{agda}

This forwards on-positions/backwards on-directions formulation may still seem a little complicated, so here is some decision-making intuition.
Recall from \cref{sec.poly.intro.dec} that we may view each position of an arena as a decision and the directions at that position as the options available for that decision.
The morphisms $f\colon p\to q$ are then the ways to \emph{delegate} $p$'s decisions to $q$. Every one of $p$'s decisions, say $i\in p(\1)$, is passed forward to a decision $j\in q(\1)$ for $q$ to make, and every choice $d\in q[j]$ that $q$ could make among its options is passed back as some choice $c\in p[i]$ among $p$'s options.

%In fact, this formula will be so important to us that we will often blur the
%difference between a dependent lens $\lens{f^\sharp}{f} \colon \lens{p[i]}{i \in
%  p(\1)} \fromto \lens{q[j]}{j \in q(\1)}$ with the natural transformation $p \to
%q$ it corresponds to.
%\end{remark}

\begin{example}\label{ex.practice_with_poly_morphisms}
Let $p\coloneqq \yon^\3+\2\yon$ and $q\coloneqq\yon^\4+\yon^\2+\2$. Here they are, depicted as corolla forests:
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, blue!50!black, "$p$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$} 
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$} 
      child {};
    \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$}
      child {};
  \end{tikzpicture}
  };
%
	\node (p2) [draw, red!75!black, right=2 of p1, "$q$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$} 
      child {}
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$} 
      child {}
      child {};
    \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$}
    ;
    \node[right=.5 of 3,"\tiny 4" below] (4) {$\bullet$}
    ;
  \end{tikzpicture}
  };
\end{tikzpicture}
\]
To give a map of polynomials $p\to q$, one sends each position $i\in p(\1)$ of $p$ to a position $j\in q(\1)$ of $q$, then sends each direction in $q[j]$ back to one in $p[i]$.

How many ways are there to do this? Before answering this, let's just pick one.
\[
\begin{tikzpicture}
	\node (p1) {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 1" below] (1) {$\bullet$} 
      child[blue!50!black] {coordinate (11)}
      child[blue!50!black] {coordinate (12)}
      child[blue!50!black] {coordinate (13)};
    \node[right=1.5 of 1, red!75!black, "\tiny 1" below] (2) {$\bullet$} 
      child[red!75!black] {coordinate (21)}
      child[red!75!black] {coordinate (22)}
      child[red!75!black] {coordinate (23)}
      child[red!75!black] {coordinate (24)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (13);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (13);
      \draw[postaction={decorate}] (24) to (13);
    \end{scope}
  \end{tikzpicture}	
	};	
%
	\node (p2) [right=1 of p1] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 2" below] (1) {$\bullet$} 
      child[blue!50!black] {coordinate (11)};
    \node[right=of 1, red!75!black, "\tiny 1" below] (2) {$\bullet$} 
      child[red!75!black] {coordinate (21)}
      child[red!75!black] {coordinate (22)}
      child[red!75!black] {coordinate (23)}
      child[red!75!black] {coordinate (24)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (11);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (11);
      \draw[postaction={decorate}] (24) to (11);
    \end{scope}
  \end{tikzpicture}	
	};	
%
	\node (p3) [below right=-1.05cm and 1 of p2] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 3" below] (1) {$\bullet$} 
      child[blue!50!black] {};
    \node[right=of 1, red!75!black, "\tiny 4" below] (2) {$\bullet$} 
		;
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
  \end{tikzpicture}	
	};	
\end{tikzpicture}
\]
This represents one morphism $p\to q$.

So how many different morphisms are there from $p$ to $q$? The first position of $p$ can be sent to any position of $q$: 1, 2, 3, or 4. Sending it to $1$ requires choosing how each of the four options $(q[1]=\4)$ are to be assigned one of $p[1]=\3$ options; there are $3^4$ ways to do this. Similarly, we can calculate all the ways to handle the first position of $p$: there are $3^4+3^2+3^0+3^0=92$. 

The second position of $p$ can also be sent to 1, 2, 3, or 4, before sending back directions; there are $1^4+1^2+1^0+1^0=4$ ways to do this. 
Similarly there are four ways to send the third position of $p$ to a position of $q$ and send back directions.

In total, there are $92 \cdot 4 \cdot 4=1472$ morphisms $p\to q$.

Unsurprisingly, this is exactly what is given by \eqref{eqn.main_formula}:
\begin{align*}
    |\poly(p, q)| &= \prod_{i \in p(\1)} |q(p[i])| \\
    &= \prod_{i \in p(\1)} |p[i]|^4 + |p[i]|^2 + 2 \\
    &= (3^4 + 3^2 + 2)(1^4 + 1^2 + 2)^2 \\
    &= 92 \cdot 4^2 = 1472.
\end{align*}
\end{example}

\begin{exercise}\label{exc.practice_poly_maps}
\begin{enumerate}
	\item Draw the corolla forests associated to $p\coloneqq\yon^\3+\yon+\1$, $q\coloneqq \yon^\2+\yon^\2+\2$, and $r\coloneqq\yon^\3$.
	\item Give an example of a morphism $p\to q$ and draw it as we did in \cref{ex.practice_with_poly_morphisms}.
	\item Explain your morphism intuitively as a delegation of decisions.
	\item Explain in those terms why there can't be any morphisms $p\to r$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item Here are the corolla forests associated to $p\coloneqq\yon^\3+\yon+\1$, $q\coloneqq \yon^\2+\yon^\2+\2$, and $r\coloneqq\yon^\3$ (with each root labeled for convenience).
    \[
    \begin{tikzpicture}[rounded corners]
    	\node (p) [draw, blue!50!black, "$p$" above] {
    	\begin{tikzpicture}[trees, sibling distance=2.5mm]
            \node["\tiny 1" below] (1) {$\bullet$} 
              child {}
              child {}
              child {};
            \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$} 
              child {};
            \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$};
        \end{tikzpicture}
        };
    %
    	\node (q) [draw, red!75!black, right=2 of p, "$q$" above] {
    	\begin{tikzpicture}[trees, sibling distance=2.5mm]
            \node["\tiny 1" below] (1) {$\bullet$} 
              child {}
              child {};
            \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$} 
              child {}
              child {};
            \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$};
            \node[right=.5 of 3,"\tiny 4" below] (4) {$\bullet$};
        \end{tikzpicture}
        };
    %
    	\node (r) [draw, green!50!black, right=2 of q, "$r$" above] {
    	\begin{tikzpicture}[trees, sibling distance=2.5mm]
        \node["\tiny 1" below] (1) {$\bullet$} 
          child {}
          child {}
          child {};
        \end{tikzpicture}
      };
    \end{tikzpicture}
    \]
	\item Here is one possible morphism $p \to q$ (you may have drawn others).
	\[
    \begin{tikzpicture}
    	\node (p1) {
        	\begin{tikzpicture}[trees, sibling distance=2.5mm]
                \node[blue!50!black, "\tiny 1" below] (1) {$\bullet$} 
                  child[blue!50!black] {coordinate (11)}
                  child[blue!50!black] {coordinate (12)}
                  child[blue!50!black] {coordinate (13)};
                \node[right=1.5 of 1, red!75!black, "\tiny 2" below] (2) {$\bullet$}
                  child[red!75!black] {coordinate (21)}
                  child[red!75!black] {coordinate (22)};
                \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
                \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
                  \draw[postaction={decorate}] (21) to (13);
                  \draw[postaction={decorate}] (22) to (11);
                \end{scope}
            \end{tikzpicture}	
    	};	
        %
    	\node (p2) [right=1 of p1] {
        	\begin{tikzpicture}[trees, sibling distance=2.5mm]
                \node[blue!50!black, "\tiny 2" below] (1) {$\bullet$} 
                  child[blue!50!black] {coordinate (11)};
                \node[right=of 1, red!75!black, "\tiny 4" below] (2) {$\bullet$};
                \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
            \end{tikzpicture}	
    	};	
        %
    	\node (p3) [below right=-1.05cm and 1 of p2] {
        	\begin{tikzpicture}[trees, sibling distance=2.5mm]
                \node[blue!50!black, "\tiny 3" below] (1) {$\bullet$};
                \node[right=of 1, red!75!black, "\tiny 3" below] (2) {$\bullet$};
                \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
            \end{tikzpicture}	
    	};	
    \end{tikzpicture}
    \]
    \item As depicted, our morphism delegates the first decision of $p$ to the second decision of $q$, whose first and second options are passed back to the third and first options, respectively, of the first decision of $p$.
    Then the second decision of $p$ is delegated to the fourth decision of $q$, which has no options; effectively, the second decision of $p$ has been canceled.
    Finally, the third decision of $p$ is delegated to the third decision of $q$, neither of which has any options.
    \item There cannot be a morphism $p \to r$ for the following reason: if we delegate the third decision of $p$, which has no options, to the sole decision of $r$, which has $3$ options, then there is no way to pass a choice of one of the $3$ options back to any of the options associated with the third decision of $p$, as there are no such options.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
For any polynomial $p$ and set $A$, e.g.\ $A=\2$, the Yoneda lemma gives an isomorphism $p(A)\cong \poly(\yon^A,p)$.
\begin{enumerate}
	\item Choose a polynomial $p$ and draw both $\yon^\2$ and $p$ as corolla forests.
	\item Count all the maps $\yon^2\to p$. How many are there?
	\item Is the previous answer equal to $p(\2)$?
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We let $p \coloneqq \yon^3 + 1$ (you could have selected others) and draw both $p$ and $\yon^\2$ as corolla forests, labeling each root for convenience.
    \[
    \begin{tikzpicture}[rounded corners]
    	\node (y2) [draw, blue!50!black, "$\yon^\2$" above] {
    	\begin{tikzpicture}[trees, sibling distance=2.5mm]
            \node["\tiny 1" below] (1) {$\bullet$} 
              child {}
              child {};
        \end{tikzpicture}
        };
    %
    	\node (p) [draw, red!75!black, right=2 of y2, "$p$" above] {
    	\begin{tikzpicture}[trees, sibling distance=2.5mm]
            \node["\tiny 1" below] (1) {$\bullet$} 
              child {}
              child {}
              child {};
            \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$};
        \end{tikzpicture}
        };
    \end{tikzpicture}
    \]
    \item We count all the maps from $\yon^\2$ to $p$.
    The unique position of $\yon^\2$ can be sent to either position of $p$.
    If it is sent to the first position of $p$, then there are $2$ directions of $\yon^\2$ for each of the $3$ directions at the first position of $p$ to be sent to, for a total of $2^3 = 8$ maps.
    Otherwise, the unique position of $\yon^\2$ is sent to the second position of $p$---at which there are no directions, so there is exactly $1$ way to do this.
    So we have $8 + 1 = 9$ maps from $\yon^\2$ to $p$.
    \item Yes, the previous answer is equal to $p(\2) = 2^3 + 1 = 9$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
For each of the following polynomials $p,q$, compute the number of morphisms $p\to q$.
\begin{enumerate}
	\item $p=\yon^\3$,\quad $q=\yon^\4$.
	\item $p=\yon^\3+\1$,\quad $q=\yon^\4$.
	\item $p=\yon^\3+\1$,\quad $q=\yon^\4+\1$.
	\item $p=\4\yon^\3+\3\yon^\2+\yon$,\quad $q=\yon$.
	\item $p=\4\yon^\3$,\quad $q=\3\yon$.
\qedhere
\end{enumerate}
\begin{solution}
Our goal is to compute the number of natural transformations $p\to q$ for each of the following polynomials $p,q$.
By \eqref{eqn.main_formula}, we always have
\[
    |\poly(p, q)| = \prod_{i \in p(\1)} |q(p[i])|.
\]
\begin{enumerate}
	\item If $p=\yon^\3$ and $q=\yon^\4$, then
	\[
	    |\poly(p, q)| = \prod_{i \in \1} |p[i]|^4 = 3^4 = 81.
	\]
	\item If $p=\yon^\3+\1$ and $q=\yon^\4$, then
	\[
	    |\poly(p, q)| = \prod_{i \in \2} |p[i]|^4 = 3^4 \cdot 0^4 = 0.
	\]
	\item If $p=\yon^\3+\1$ and $q=\yon^\4+\1$, then
	\[
	    |\poly(p, q)| = \prod_{i \in \2} |p[i]|^4 + 1 = (3^4 + 1)(0^4 + 1) = 82.
	\]
	\item If $p=\4\yon^\3+\3\yon^2\+\yon$ and $q=\yon$, then
	\[
	    |\poly(p, q)| = \prod_{i \in \8} |p[i]| = 3^4 \cdot 2^3 \cdot 1 = 648.
	\]
	\item If $p=\4\yon^\3$ and $q=\3\yon$, then
	\[
	    |\poly(p, q)| = \prod_{i \in \4} 3|p[i]| = (3 \cdot 3)^4 = 6561.
	\]
\qedhere
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.practice_sum_prod}
\begin{enumerate}
\item Is it true that the following are isomorphic?
\begin{equation}\label{eqn.poly_p_q}
  \poly(p,q)
  \cong^?
  \prod_{i\in p(\1)}\sum_{j\in q(\1)}\prod_{d\in q[j]}\sum_{c\in p[i]}\1
\end{equation}
\item \label{exc.practice_sum_prod.useful} Is it true that the following are isomorphic?
	\begin{equation}\label{eqn.useful_misc472}
	\poly(p,q)\cong^?\sum_{f_1\colon p(\1)\to q(\1)}\prod_{j\in q(\1)}\smset\Bigg(q[j],\prod_{\substack{i \in p(\1), \\ f_1(i) = j}}p[i]\Bigg)
	\end{equation}
\item If the answer to \#2 is ``yes,'' then describe in the language of decision-making how any element of the right-hand side gives a way of delegating decisions from $p$ to $q$. If ``no,'' give intuition for why the two sets are not isomorphic.
\qedhere
\end{enumerate}
\begin{solution}
\begin{longenum}
\item We will show that, yes, the following isomorphism does hold:
\[
  \poly(p,q)
  \cong
  \prod_{i\in p(\1)}\sum_{j\in q(\1)}\prod_{d\in q[j]}\sum_{c\in p[i]}\1.
\]
By \eqref{eqn.main_formula}, it suffices to show that for all $i \in p(\1)$ and $j \in q(\1)$, we have
\[
    p[i]^{q[j]} \iso \prod_{d \in q[j]} \sum_{c \in p[i]} \1.
\]
Indeed, by \eqref{eqn.push_prod_sum_set_indep} and \cref{exc.on_sums_prods_sets}, we have
\begin{align*}
    \prod_{d \in q[j]} \sum_{c \in p[i]} \1 &\iso \sum_{\bar{c} \colon q[j] \to p[i]} \, \prod_{d \in q[j]} \1 \tag*{\eqref{eqn.push_prod_sum_set_indep}} \\
    &\iso \sum_{\bar{c} \colon q[j] \to p[i]} \1 \tag{\cref{exc.on_sums_prods_sets} \cref{exc.on_sums_prods_sets.prod}} \\
    &\iso \smset(q[j], p[i]) \tag{\cref{exc.on_sums_prods_sets} \cref{exc.on_sums_prods_sets.sum}} \\
    &\iso p[i]^{q[j]}.
\end{align*}

\item We will show that, yes, the following isomorphism does hold:
\[
	\poly(p,q) \iso \sum_{f_1\colon p(\1)\to q(\1)} \; \prod_{j\in q(\1)}\smset\Bigg(q[j],\prod_{\substack{i \in p(\1), \\ f_1(i) = j}}p[i]\Bigg).
\]
By \eqref{eqn.main_formula} and \eqref{eqn.push_prod_sum_set_indep}, we have
\begin{align*}
    \poly(p, q) &\iso \prod_{i \in p(\1)} \sum_{j \in q(\1)} p[i]^{q[j]} \tag*{\eqref{eqn.main_formula}} \\
    &\iso \sum_{f_1 \colon p(\1) \to q(\1)} \; \prod_{i \in p(\1)} p[i]^{q[f_1(i)]} \tag*{\eqref{eqn.push_prod_sum_set_indep}} \\
    &\iso \sum_{f_1 \colon p(\1) \to q(\1)} \; \prod_{j \in q(\1)} \; \prod_{\substack{i \in p(\1), \\ f_1(i) = j}} p[i]^{q[j]} \tag{$\ast$} \\
    &\iso \sum_{f_1\colon p(\1)\to q(\1)} \; \prod_{j\in q(\1)}\smset\Bigg(q[j],\prod_{\substack{i \in p(\1), \\ f_1(i) = j}}p[i]\Bigg) \tag{Universal property of products}
\end{align*}
where ($\ast$) follows from the fact that for any function $f_1 \colon p(\1) \to q(\1)$, the set $p(\1)$ can be written as the disjoint union of sets of the form $f_1\inv(j) = \{i \in p(1) \ | \ f_1(i) = j\}$ for each $j \in q(\1)$.

\item To explain how the set
\[
	D_{p,q} \coloneqq \sum_{f_1\colon p(\1)\to q(\1)}\prod_{j\in q(\1)}\smset\Bigg(q[j],\prod_{\substack{i \in p(\1), \\ f_1(i) = j}}p[i]\Bigg)
\]
specifies a way of delegating decisions from $p$ to $q$, we first give the instructions for choosing an element of $D_{p,q}$ as a nested list:
\begin{quote}
To choose an element of $D_{p,q}$:
\begin{longenum}
    \item choose a function $f_1 \colon p(\1) \to q(\1)$;
    \item for each element $j \in q(\1)$:
    \begin{longenum}
        \item for each element of $q[j]$:
        \begin{longenum}
            \item for each element $i \in p(\1)$ satisfying $f_1(i) = j$:
            \begin{longenum}
                \item choose an element of $p[i]$.
            \end{longenum}
        \end{longenum}
    \end{longenum}
\end{longenum}
\end{quote}
So $f_1$ delegates each of $p$'s decisions to one of $q$'s decisions.
Then for every option of every decision $j$ of $q$, we choose an option of each of $p$'s decisions that has been delegated to $j$ by $f_1$.
\end{longenum}
\end{solution}
\end{exercise}

\begin{exercise}%\label{exc.poly_coprod}
Use \eqref{eqn.poly_coprod} and \eqref{eqn.main_formula} to verify that
\[
    \poly\left(\sum_{i \in I} p_i, q\right) \iso \prod_{i \in I} \poly(p_i, q)
\]
for all polynomials $\{p_i\}_{i \in I}$ and $q$, as expected from the universal property of coproducts.
\begin{solution}
Given $q \in \poly$ and $p_i \in \poly$ for each $i \in I$ for some set $I$, we use \eqref{eqn.poly_coprod} and \eqref{eqn.main_formula} to verify that
\begin{align*}
    \poly\left(\sum_{i \in I} p_i, q\right)
    &\iso \poly\left(\sum_{(i,j) \in \sum_{i \in I} p_i(\1)} \yon^{p_i[j]}, q\right)
    \tag*{\eqref{eqn.poly_coprod}} \\
    &\iso \prod_{(i,j) \in \sum_{i \in I} p_i(\1)} q(p_i[j])
    \tag*{\eqref{eqn.main_formula}} \\
    &\iso \prod_{i \in I} \prod_{j \in p_i(\1)} q(p_i[j]) \\
    &\iso \prod_{i \in I} \poly(p_i, q).
    \tag*{\eqref{eqn.main_formula}}
\end{align*}
\end{solution}
\end{exercise}

\begin{example}[Derivatives]\label{ex.derivatives}
The \emph{derivative} of a polynomial $p$, denoted $\dot{p}$, is defined as follows:
\[
\dot{p}\coloneqq\sum_{i\in p(\1)}\sum_{d\in p[i]}\yon^{p[i]-\{d\}}.
\]
For example, if $p\coloneq\yon^{\{U,V,W\}}+\{A,B\}\yon^{\{X\}}$ then 
\[\dot{p}=\{U\}\yon^{\{V,W\}}+\{V\}\yon^{\{U,W\}}+\{W\}\yon^{\{U,V\}}+\{(A,X),(B,X)\}\yon^\0.\]
Up to isomorphism $p\cong\yon^\3+\2\yon$ and $\dot{p}\cong\3\yon^\2+\2$.
Unsurprisingly, this coincides with the familiar notion of derivatives of polynomials from calculus.

Thus we get a canonical map $\dot{p}\yon\to p$, because we have an isomorphism
\[
\dot{p}\yon\cong\sum_{i\in p(\1)}\sum_{d\in p[i]}\yon^{p[i]}.
\]
This natural transformation comes up in computer science in the context of ``plugging in to one-hole contexts''; we will not explore that here, but see \cite{mcbride} and \cite{abbot2003derivatives} for more info.%The Derivative of a Regular Type is its Type of One-Hole Contexts. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.22.8611&rep=rep1&type=pdf.
%https://www.cs.nott.ac.uk/~psztxa/publ/tlca03.pdf

A morphism $f\colon p\to \dot{q}$ can be interpreted as something like an arena morphism from $p$ to $q$, except that each position of $p$ explicitly selects a direction of $q$ to remain unassigned. More precisely, for each $i\in p(\1)$ we have $f_1(i)=(j,d)\in \sum_{j\in q(\1)}q[j]$, i.e.\ a choice of position $j$ of $q$, as usual, together with a chosen direction $d\in q[j]$. Then every direction of $q$ at $j$ \emph{other than $d$} is sent back to an direction of $p$ at $i$.
\end{example}

\begin{exercise} \label{exc.deriv_directions}
Show that $\dot{p}(\1)$ is isomorphic to the set of all directions of $p$ (i.e.\ the union of all direction-sets of $p$), so there is a canonical function $\pi_p \colon \dot{p}(\1) \to p(\1)$ that sends each direction $d$ of $p$ to the position $i$ of $p$ for which $d \in p[i]$.
\begin{solution}
We can evaluate $\dot{p}(\1)$ directly from the definition of $\dot{p}$ to obtain
\[
    \dot{p}(\1) = \sum_{i \in p(\1)} \sum_{d \in p[i]} \1^{p[i]-\{d\}} \iso \sum_{i \in p(\1)} p[i],
\]
which is isomorphic to the set of all directions of $p$.
Then $\pi_p \colon \dot{p}(\1) \to p(\1)$ is the canonical projection, sending each direction $d$ of $p$ to the position $i$ of $p$ for which $d \in p[i]$.
\end{solution}
\end{exercise}

\begin{exercise}
The derivative is not very well-behaved category-theoretically.
However, it is intriguing.
Below $p, q \in \poly$.
\begin{enumerate}
	\item Explain the canonical map $\dot{p}\yon\to p$ from \cref{ex.derivatives} in more detail.
	\item Is there always a canonical map $p\to \dot{p}$?
	\item Is there always a canonical map $\dot{p}\to p$?
	\item If given a map $p\to q$, does one get a map $\dot{p}\to\dot{q}$?
	\item We will define the binary operations $\otimes$ and $\ihom{-,-}$ on $\poly$ later on in \eqref{eqn.parallel_def} and \eqref{eqn.dir_hom}, and in \cref{exc.dir_hom_p_yon_dir_p}, you will be able to use \cref{exc.dir_hom_sum} to deduce that
	\begin{equation} \label{eqn.dir_hom_p_yon_dir_p}
	    \ihom{p, \yon} \otimes p \iso \sum_{f \in \prod_{i \in p(\1)} p[i]} \; \sum_{i \in p(\1)} \yon^{p(\1) \times p[i]},
	\end{equation}
% 	Using this, find a formula for a map $p\otimes\ihom{p,\yon}\to \dot{p}$ that works for any $p\in\poly$.
	Is there always a canonical map $\ihom{p,\yon}\otimes p\to \dot{p}$?
	\item When talking to someone who explains maps $p\to\dot{q}$ in terms of ``unassigned directions,'' how might you describe what is modeled by a map $p\yon\to q$?
	\qedhere
\end{enumerate}
\begin{solution}
Here $p, q \in \poly$.
\begin{enumerate}
	\item Our goal is to characterize the canonical map $\dot{p}\yon\to p$.
	If we unravel the definitions, this is a map
	\[
	    \sum_{i \in p(\1)} \sum_{d \in p[i]} \yon^{p[i]} \to \sum_{i \in p(\1)} \yon^{p[i]}.
	\]
	We observe that there is always such a map sending every position $(i, d) \in \sum_{i \in p(\1)} p[i]$ of $\dot{p}\yon$ to its first projection $i \in p(\1)$ and is the identity on directions.
	This is the canonical map.

	\item There cannot always be a canonical map $p\to \dot{p}$, for if $p \coloneqq \1$, then $\dot{p} \coloneqq \0$, and there is no map $\1 \to \0$.
	
	\item We show that there cannot always be a canonical map $\dot{p}\to p$.
	Take $p \coloneqq \yon$, so $\dot{p} \coloneqq \1$.
	A map $\1 \to \yon$ must have an on-directions function $\1 \to \0$, but such a function does not exist.
	
	\item We show that even when there is a map $p \to q$, there is not necessarily a map $\dot{p}\to\dot{q}$.
	Take $p \coloneqq \yon$ and $q \coloneqq \1$.
	Then there is a map $p \to q$ that sends the unique position of $\yon$ to the unique position of $\1$ and is the empty function on directions.
	But $\dot{p} = \1$ and $\dot{q} = \0$, and there is no map $\1 \to \0$.
	
	\item We show that there is a canonical map $g \colon \ihom{p,\yon} \otimes p \to \dot{p}$, where $\ihom{p,\yon} \otimes p$ is given by \eqref{eqn.dir_hom_p_yon_dir_p}.
	The on-positions function $g_1$ takes $f \in \prod_{i \in p(\1)} p[i]$ and $i \in p(\1)$ and sends the pair of them to the position of $\dot{p}$ corresponding to $i \in p(\1)$ and $f(i) \in p[i]$.
	We then have $\dot{p}[(i, f(i))] \iso p[i]$ and $(\ihom{p,\yon} \otimes p)[(f, i)] \iso p(\1) \times p[i]$, so the on-directions function $g^\sharp_{(f,i)}$ can send each $d \in p[i]$ to $(i, d) \in p(\1) \times p[i]$.

	\item We wish to describe a map $p\yon \to q$ in terms of ``unassigned to directions.''
	Observe that as an arena, $p\yon$ has the same positions as $p$ but has one more direction than $p$ does at each position.
	We denote this extra direction at each position $i \in p(\1)$ of $p\yon$ by $\ast_i$.
	So a map $f \colon p\yon\to q$ sends each position $i$ of $p$ to a position $j$ of $q$, but every direction of $q$ at $j$ could either be sent back to a direction of $p$ at $i$ or the extra direction $\ast_i$.
	We can say that an arena morphism $f \colon p\yon \to q$ is like an arena morphism $p \to q$, except that any of the directions of $q$ may remain un assigned---i.e.\ we may have partial on-directions functions.
\end{enumerate}
\end{solution}
\end{exercise}


%\subsubsection{Back to maps of polynomials}
%After that interlude, you are hopefully more comfortable with our main formula
%\cref{eqn.main_formula} describing the set of morphisms between two arbitrary
%polynomial functors as dependent lenses. Let's practice.

% \begin{example}[Constants are sent to constants]\label{ex.const_to_const}
% Any natural transformation $p\to q$ must send constants in $p$ to constants in $q$. So if $p$ has constant terms and $q$ doesn't, e.g.\ if $p=\yon^\2+\1$ and $q=\yon^\3$, then there are no maps $p\to q$. 

% **Finish**
% \end{example}

%---- Subsection ----%
\subsection{Translating between natural transformations and arena morphisms} \label{subsec.poly.func_nat.morph.translate}
We now know that we can specify a morphism of polynomials $p \to q$ in two ways: in the language of functors, by specifying a natural transformation from $p$ to $q$; or in the language of arenas, by specifying a function $f_1 \colon p(\1) \to q(\1)$ and, for each $i \in p(\1)$, a function $f^\sharp_i \colon q[f_1(i)] \to p[i]$.
But what is the relationship between these two formulations?
If you told me an arena morphism, and I told you a natural transformation between polynomial functors, how could we tell if we were talking about the same morphism or not?
We want to be able to translate between these two languages.

Our Rosetta Stone turns out to be the proof of the Yoneda lemma.
The lemma itself forms the crux of the proof of \cref{prop.poly_maps_prod_sum}, that these two formulations of polynomial morphisms are equivalent; so unraveling this proof reveals the translation we seek.

\begin{proposition} \label{prop.morph_arena_to_func}
Let $p$ and $q$ be polynomial functors, and let $(f_1, f^\sharp)$ be a morphism between their associated arenas.
Then the isomorphism in \eqref{eqn.main_formula} sends $(f_1, f^\sharp)$ to the natural transformation $f \colon p \to q$ whose $X$-component $f_X \colon p(X) \to q(X)$ for each $X \in \smset$ sends every
\[
    (i, g) \in \sum_{i \in p(\1)} X^{p[i]} \iso p(X),
\]
with $i \in p(\1)$ and $g \colon p[i] \to X$, to
\[
    (f_1(i), f^\sharp_i \then g) \in \sum_{j \in q(\1)} X^{q[j]} \iso q(X).
\]
\end{proposition}
\begin{proof}
As an element of the product over $I$ on the right hand side of \eqref{eqn.main_formula}, the pair $(f_1, f^\sharp)$ can equivalently be thought of as a set of pairs $\{(f_1(i), f^\sharp_i)\}_{i \in I}$.
Fixing $i \in I$, the pair $(f_1(i), f^\sharp_i)$ is an element of
\[
    \sum_{j \in q(\1)} p[i]^{q[j]} = q(p[i])
\]
(so $f_1(i) \in q(\1)$ and $f^\sharp_i \colon q[f_1(i)] \to p[i]$).
By the Yoneda lemma (\cref{lemma.yoneda}), we have an isomorphism $q(p[i]) \iso \poly(\yon^{p[i]}, q)$, and by the proof of the Yoneda lemma, this isomorphism sends $(f_1(i), f^\sharp_i)$ to the natural transformation $f^i \colon \yon^{p[i]} \to q$ whose $X$-component is the function $f^i_X \colon X^{p[i]} \to q(X)$ given by sending $g \colon p[i] \to X$ to
\[
    q(g)(f_1(i), f^\sharp_i) = \left(\sum_{j \in q(\1)} g^{q[j]}\right)(f_1(i), f^\sharp_i) = \left(f_1(i), g^{q[f_1(i)]}(f^\sharp_i)\right) = (f_1(i), f^\sharp_i \then g).
\]
Taken together, the collection of natural transformations $\{f^i\}_{i \in I}$ is an element of $\prod_{i \in I} \poly(\yon^{p[i]}, q)$.
Applying the universal property of coproducts, as in the proof of \cref{prop.poly_maps_prod_sum}, we find that $\{f^i\}_{i \in I}$ corresponds to the natural transformation $f \colon p \to q$ we desire.
\end{proof}

\begin{example}
Let us return to the polynomials $p \coloneqq \yon^\3 + \2\yon$ and $q \coloneqq \yon^\4 + \yon^\2 + \2$ from \cref{ex.practice_with_poly_morphisms} and the morphism $f \colon p \to q$ depicted below:
\[
\begin{tikzpicture}
	\node (p1) {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 1" below] (1) {$\bullet$} 
      child[blue!50!black] {coordinate (11)}
      child[blue!50!black] {coordinate (12)}
      child[blue!50!black] {coordinate (13)};
    \node[right=1.5 of 1, red!75!black, "\tiny 1" below] (2) {$\bullet$} 
      child[red!75!black] {coordinate (21)}
      child[red!75!black] {coordinate (22)}
      child[red!75!black] {coordinate (23)}
      child[red!75!black] {coordinate (24)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (13);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (13);
      \draw[postaction={decorate}] (24) to (13);
    \end{scope}
  \end{tikzpicture}	
	};	
%
	\node (p2) [right=1 of p1] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 2" below] (1) {$\bullet$} 
      child[blue!50!black] {coordinate (11)};
    \node[right=of 1, red!75!black, "\tiny 1" below] (2) {$\bullet$} 
      child[red!75!black] {coordinate (21)}
      child[red!75!black] {coordinate (22)}
      child[red!75!black] {coordinate (23)}
      child[red!75!black] {coordinate (24)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (11);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (11);
      \draw[postaction={decorate}] (24) to (11);
    \end{scope}
  \end{tikzpicture}	
	};	
%
	\node (p3) [below right=-1.05cm and 1 of p2] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 3" below] (1) {$\bullet$} 
      child[blue!50!black] {};
    \node[right=of 1, red!75!black, "\tiny 4" below] (2) {$\bullet$} 
		;
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
  \end{tikzpicture}	
	};	
\end{tikzpicture}
\]
Fix a set $X \coloneqq \{a,b,c,d,e\}$.
When viewed as a natural transformation, the morphism $f$ has as its $X$-component a function $f_X \colon p(X) \to q(X)$.
In other words, for any element of $p(X)$, the morphism $f$ should be able to give us an element of $q(X)$.

What does an element of $p(X)$ look like?
Well, to specify such an element, we would need to choose a position $i$ of $p$ and a function $p[i] \to X$.
We can depict this by selecting one of the corollas in the forest of $p$ and labeling each leaf of that corolla with an element of $X$.
For example, here we depict an element $(1, g)$ of $p(X)$, where $g \colon p[1] \to X$ is given by $1 \mapsto c, 2 \mapsto e,$ and $3 \mapsto a$:
\[
\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 1" below] (1) {$\bullet$} 
      child[blue!50!black] {node {$c$}}
      child[blue!50!black] {node {$e$}}
      child[blue!50!black] {node {$a$}};
\end{tikzpicture}
\]
Similarly, an element of $q(X)$ can be drawn as a corolla in the forest of $q$, with each leaf labeled by an element of $X$.
So what element of $q(X)$ is $f_X(1, g)$?

\cref{prop.morph_arena_to_func} tells us that $f_X(1, g)$ can be read off of the forest depiction of $f$ at position $1$:
\[
\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 1" below] (1) {$\bullet$} 
      child[blue!50!black] {coordinate (11)}
      child[blue!50!black] {coordinate (12)}
      child[blue!50!black] {coordinate (13)};
    \node[right=1.5 of 1, red!75!black, "\tiny 1" below] (2) {$\bullet$} 
      child[red!75!black] {coordinate (21)}
      child[red!75!black] {coordinate (22)}
      child[red!75!black] {coordinate (23)}
      child[red!75!black] {coordinate (24)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (13);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (13);
      \draw[postaction={decorate}] (24) to (13);
    \end{scope}
\end{tikzpicture}	
\]
To draw $f_X(1, g)$, we first draw the corolla in the forest of $q$ corresponding to $f_1(1)$: the corolla on the right hand side above.
Then we label each leaf of that corolla by following the arrow from that leaf (as given by $f^\sharp_i$) to a leaf of $p$ at $1$, and use the label there that is given by $(1, g)$.
So $f_X(1, g)$ looks like
\[
\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[red!75!black, "\tiny 1" below] (1) {$\bullet$} 
      child[red!75!black] {node {$a$}}
      child[red!75!black] {node {$c$}}
      child[red!75!black] {node {$a$}}
      child[red!75!black] {node {$a$}};
\end{tikzpicture}
\]
\end{example}

% ** Add an exercise—labeled trees?

\cref{prop.morph_arena_to_func} lets us translate from arena morphisms to natural transformations.
The following corollary tells us how to go in the other direction.
In particular, it justifies the notation $f_1$ for the on-positions function of $f$.

\begin{corollary} \label{prop.morph_func_to_arena}
Let $p$ and $q$ be polynomial functors, and let $f \colon p \to q$ be a natural transformation between them.
Then the isomorphism in \eqref{eqn.main_formula} sends $f$ to the arena morphism $(f_1, f^\sharp)$ for which $f_1 \colon p(\1) \to q(\1)$ is the $\1$-component of $f$ and, for each $i \in p(\1)$, we have
\[
    (f_1(i), f^\sharp_i) = f_{p[i]}(i, \id_{p[i]}).
\]
\end{corollary}
\begin{proof}
By \cref{prop.morph_arena_to_func}, the $\1$-component $f_\1 \colon p(\1) \to q(\1)$ sends every $i \in p(\1)$ to $f_1(i) \in q(\1)$, so the on-positions function $f_1$ is equal to the $\1$-component $f_\1$.
Also, the $p[i]$-component $f_{p[i]} \colon p(p[i]) \to q(p[i])$ sends every $(i, \id_{p[i]}) \in p(p[i])$, with $i \in p(\1)$, to $(f_1(i), f^\sharp_i \then \id_{p[i]}) = (f_1(i), f^\sharp_i)$.
\end{proof}

% ** More exercises??

%---- Subsection ----%
\subsection{Identity and composition of arena morphisms} \label{subsec.poly.func_nat.morph.id_comp}

Thus far, we have seen how the category $\poly$ of polynomial functors and natural transformations can just as easily be thought of as the category of arenas and arena morphisms.
But in order to actually discuss the latter category, we need to be able to give identity arena morphisms and describe how these arena morphisms compose.
To do so, we can leverage our ability to translate back and forth between arena morphisms and natural transformations.

For instance, given a polynomial $p$, the identity morphism of its associated arena should correspond to the identity natural transformation of $p$ as a functor.

\begin{exercise}[Identity arena morphisms] \label{exc.arena_morph_id}
Let $p$ be a polynomial and let $\id_p \colon p \to p$ be its identity natural transformation, whose $X$-component $(\id_p)_X \colon p(X) \to p(X)$ for each $X \in \smset$ is the identity function on $p(X)$; that is, $(\id_p)_X = \id_{p(X)}$.

Use \cref{prop.morph_func_to_arena} to show that the arena morphism $((\id_p)_1, (\id_p)^\sharp)$ associated to $\id_p$ is such that $(\id_p)_1 \colon p(\1) \to p(\1)$ and $(\id_p)^\sharp_i \colon p[(\id_p)_1(i)] \to p[i]$ for $i \in p(\1)$ are all identity functions.
\begin{solution}
We wish to show that the arena morphism $((\id_p)_1, (\id_p)^\sharp)$ associated to the identity natural transformation $\id_p$ of a polynomial $p$ is such that $(\id_p)_1$ and every $(\id_p)^\sharp_i$ are all identity functions.
Indeed, by \cref{prop.morph_func_to_arena}, for each $i \in p(\1)$, we have
\[
    ((\id_p)_1(i), (\id_p)^\sharp_i) = (\id_p)_{p[i]}(i, \id_{p[i]}) = (i, \id_{p[i]}),
\]
as $(\id_p)_{p[i]}$ is the identity function on $p(p[i])$.
\end{solution}
\end{exercise}

Similarly, we should be able to deduce how two arena morphisms compose by translating them to natural transformations, composing those, then translating back to arena morphisms.

\begin{exercise}[Composing arena morphisms] \label{exc.arena_morph_comp}
Let $p,q,$ and $r$ be polynomials, let $f \colon p \to q$ and $g \colon q \to r$ be natural transformations, and let $h \coloneqq f \then g$ be their composite, whose $X$-component $h_X \colon p(X) \to r(X)$ for each $X \in \smset$ is the composite of the $X$-components of $f$ and $g$; that is, $h_X = f_X \then g_X$.

Use \cref{prop.morph_func_to_arena} to show that the arena morphism $(h_1, h^\sharp)$ associated to $h$ satisfies $h_1 = f_1 \then g_1$ and $h^\sharp_i = g^\sharp_{f_1(i)} \then f^\sharp_i$ for all $i \in p(\1)$.
\begin{solution}
Given polynomial morphisms $f \colon p \to q, g \colon q \to r,$ and their composite $h \coloneqq f \then g$, we wish to show that the arena morphism $(h_1, h^\sharp)$ associated to $h$ satisfies $h_1 = f_1 \then g_1$ and $h^\sharp_i = g^\sharp_{f_1(i)} \then f^\sharp_i$ for all $i \in p(\1)$.
Indeed, by \cref{prop.morph_func_to_arena} and \cref{prop.morph_arena_to_func}, for each $i \in p(\1)$, we have
\begin{align*}
    (h_1(i), h^\sharp_i) &= h_{p[i]}(i, \id_{p[i]}) \\
    &= g_{p[i]}(f_{p[i]}(i, \id_{p[i]})) \tag{$h = f \then g$} \\
    &= g_{p[i]}(f_1(i), f^\sharp_i) \tag{\cref{prop.morph_func_to_arena}} \\
    &= (g_1(f_1(i)), g^\sharp_{f_1(i)} \then f^\sharp_i). \tag{\cref{prop.morph_arena_to_func}}
\end{align*}
\end{solution}
\end{exercise}

\begin{example}[Commutative diagrams in $\poly$] \label{ex.comm_poly}
The above exercise tells us how to interpret commutative diagrams in $\poly$ as commutative diagrams in the more familiar setting of $\smset$.
Given polynomials $p, q, r$ and natural transformations $f \colon p \to q, g \colon q \to r,$ and $h \colon p \to r$, the diagram
\[
\begin{tikzcd}
    p \ar[r, "f"] \ar[dr, "h"'] & q \ar[d, "g"] \\
    & r
\end{tikzcd}
\]
commutes in $\poly$ if and only if the forwards on-positions diagram
\[
\begin{tikzcd}
    p(\1) \ar[r, "f_1"] \ar[dr, "h_1"'] & q(\1) \ar[d, "g_1"] \\
    & r(\1)
\end{tikzcd}
\]
commutes in $\smset$ and, for each $i \in p(\1)$, the backwards on-directions diagram
\[
\begin{tikzcd}
    p[i] & q[f_1(i)] \ar[l, "f^\sharp_i"'] \\
    & r[h_1(i)] \ar[u, "g^\sharp_{f_1(i)}"'] \ar[ul, "h^\sharp_i"]
\end{tikzcd}
\]
commutes in $\smset$.
We can use this fact to determine whether a given diagram in $\poly$ commutes.
\end{example}

\begin{exercise}
Verify that, for $p, q \in \poly$, the polynomial $p+q$ given by the binary sum of $p$ and $q$ satisfies the universal property of the coproduct of $p$ and $q$.
That is, provide morphisms $\iota \colon p \to p + q$ and $\kappa \colon q \to p + q$, then show that for any other polynomial $r$ with morphisms $f \colon p \to r$ and $g \colon q \to r$, there exists a unique morphism $h \colon p+q \to r$---shown dashed---making the following diagram commute:
\begin{equation} \label{eqn.coprod_univ_prop}
\begin{tikzcd}
	p \ar[r, "\iota"] \ar[dr, "f"'] &
	p + q \ar[d, "h", dashed] &
	q \ar[l, "\kappa"'] \ar[dl, "g"] \\
	& r
\end{tikzcd}
\end{equation}
Hint: Use \cref{ex.comm_poly} to determine whether a diagram commutes.
\begin{solution}
We provide $\iota \colon p \to p + q$ and $\kappa \colon q \to p + q$ as follows.
On positions, they are the canonical inclusions $\iota_1 \colon p(\1) \to p(\1)+q(\1)$ and $\kappa_1 \colon q(\1) \to p(\1)+q(\1)$; on directions, they are identities.
We wish to show that, for $p, q \in \poly$, the polynomial $p+q$ along with $\iota$ and $\kappa$ satisfy the universal property of the coproduct.
That is, we must show that for any $r \in \poly$ and maps $f \colon r \to p$ and $g \colon r \to q$, there exists a unique map $h \colon r \to p+q$ for which the diagram \eqref{eqn.coprod_univ_prop} commutes.
% \begin{equation} \label{eqn.coprod_univ_prop}
% \begin{tikzcd}
% 	r & q \ar[l, "g"'] \ar[d, "\kappa"] \\
% 	p \ar[u, "f"] \ar[r, "\iota"'] & p+q \ar[ul, "h", dashed].
% \end{tikzcd}
% \end{equation}

We apply \cref{ex.comm_poly}.
In order for \eqref{eqn.coprod_univ_prop} to commute, it must commute on positions---that is, the following diagram of sets must commute:
\begin{equation} \label{eqn.coprod_univ_prop_pos}
\begin{tikzcd}
	p(\1) \ar[r, "\iota_1"] \ar[dr, "f_1"'] &
	p(\1) + q(\1) \ar[d, "h_1", dashed] &
	q(\1) \ar[l, "\kappa_1"'] \ar[dl, "g_1"] \\
	& r(\1)
\end{tikzcd}
\end{equation}
But since $p(1)+q(\1)$ along with the inclusions $\iota_1$ and $\kappa_1$ form the coproduct of $p(\1)$ and $q(\1)$ in $\smset$, there exists a unique $h_1$ for which \eqref{eqn.coprod_univ_prop_pos} commutes.
Hence $h$ is uniquely characterized on positions.
In particular, it must send each $(1,i) \in p(\1)+q(\1)$ with $i \in p(\1)$ to $f_1(i)$ and each $(2,j) \in p(\1)+q(\1)$ with $j \in q(\1)$ to $g_1(j)$.

Moreover, if \eqref{eqn.coprod_univ_prop} is to commute on directions, then for every $i \in p(\1)$ and $j \in q(\1)$, the following diagrams of sets must commute:
\begin{equation} \label{eqn.coprod_univ_prop_dir}
\begin{tikzcd}[sep=large]
	p[i] & (p+q)[(1,i)] \ar[l, "\iota^\sharp_i"'] & (p+q)[(2,j)] \ar[r, "\kappa^\sharp_j"] & q[j] \\
	& r[f_1(i)] \ar[ul, "f^\sharp_i"] \ar[u, "h^\sharp_{(1,i)}"', dashed] & r[g_1(j)] \ar[u, "h^\sharp_{(2,j)}", dashed] \ar[ur, "g^\sharp_j"']
\end{tikzcd}
\end{equation}
But $(p+q)[(1,i)] \iso p[i]$ and $\iota^\sharp_i$ is the identity, so we must have $h^\sharp_{(1,i)} = f^\sharp_i$.
Similarly, $(p+q)[(2,j)] \iso q[j]$ and $\kappa^\sharp_j$ is the identity, so we must have $h^\sharp_{(2,j)} = g^\sharp_j$.
Hence $h$ is also uniquely characterized on directions, so it is unique overall.
Moreover, we have shown that we can define $h$ on positions so that \eqref{eqn.coprod_univ_prop_pos} commutes, and that we can define $h$ on directions such that the diagrams in \eqref{eqn.coprod_univ_prop_dir} commute.
As the commutativity of the diagrams in \eqref{eqn.coprod_univ_prop_pos} and \eqref{eqn.coprod_univ_prop_dir} together imply the commutativity of \eqref{eqn.coprod_univ_prop}, it follows that there exists $h$ for which \eqref{eqn.coprod_univ_prop} commutes.
\end{solution}
\end{exercise}

%% **Examples/exercises of composing two polynomial morphisms?

\begin{exercise}[A functor $\Cat{Top}\to\poly$] \label{exc.top_poly_func}
This exercise is for those who know what topological spaces and continuous maps are. It will not be used again in this book.
\begin{enumerate}
	\item Suppose that $X$ is a topological space. Organize its points and their neighborhoods into a polynomial $p_X$.
	\item Give a formula by which any continuous map $X\to Y$ induces a map of polynomials $p_X\to p_Y$.
	\item Show that your formula defines a functor.
	\item Is it full? Faithful?
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
	\item Given a topological space $X$, we can define a polynomial $p_X$ whose positions are the points in $X$ and whose directions at $x \in X$ are the open neighborhoods of $x$.
	In other words,
	\[
	    p_X \coloneqq \sum_{x \in X} \yon^{\{ U \ss X \ | \ x \in U, \, U \text{ open} \}}.
	\]
	\item \label{exc.top_poly_func.morphs} For every continuous map $f \colon X \to Y$, we give a polynomial morphism $p_f \colon p_X \to p_Y$.
	The on-positions function is just $f$, while for each position $x \in X$ of $p_X$, the on-directions function $(p_f)^\sharp_x \colon p_Y[f(x)] \to p_X[x]$ sends each open neighborhood $U$ of $f(x)$ to the open neighborhood $f\inv(U)$ of $x$.
	
	\item To show that $p_X$ is functorial in $X$, it suffices to show that sending continuous maps $f \colon X \to Y$ to their induced polynomial morphisms $p_f \colon p_X \to p_Y$ preserves identities and composition.
	First, we show that for any topological space $X$, the polynomial morphism $p_{\id_X}$, where $\id_X$ is the identity map on $X$, is an identity morphism.
	By \cref{exc.top_poly_func.morphs}, the on-positions function of $p_{\id_X}$ is $\id_X$, and for each $x \in X$ the on-directions function $(p_f)^\sharp_x \colon p_X[x] \to p_X[x]$ sends $U \in p_X[x]$ to $(\id_X)\inv(U) = U$.
	Hence $p_{\id_X}$ is the identity on both positions and directions; it follows from \cref{exc.arena_morph_id} that $p_{\id_X}$ is an identity morphism.
	
	We now show that for topological spaces $X,Y,$ and $Z$ and continuous maps $f \colon X \to Y$ and $g \colon Y \to Z$, we have $p_f \then p_g = p_{f \then g}$.
	By \cref{exc.top_poly_func.morphs} and \cref{exc.arena_morph_comp}, the on-positions function of either side is equal to $f \then g$, so it suffices to show that for all $x \in X$,
	\[
	    (p_{f \then g})^\sharp_x = (p_g)^\sharp_{f(x)} \then (p_f)^\sharp_x.
	\]
	Again by \cref{exc.top_poly_func.morphs}, the left hand side sends each $U \in p_Z[g(f(x))]$ to $(f \then g)\inv(U)$, while the right hand side sends $U$ to $f\inv(g\inv(U))$, but by elementary set theory, these sets are equal.
	
	\item The functor is not full.
	Consider the spaces $X = \2$ with the indiscrete topology (i.e.\ the only open sets are the empty set and $X$) and $Y = \2$ with the discrete topology (i.e.\ all subsets are open).
	Then $p_X \iso \2\yon$ and $p_Y \iso \2\yon^\2$, so our functor induces a function from the set of continuous functions $X \to Y$ to the set of polynomial morphisms $\2\yon \to \2\yon^\2$.
	We claim that this function is not surjective: in particular, consider the polynomial morphism $h \colon \2\yon \to \2\yon^\2$ that is the identity on positions (and uniquely defined on directions).
	Then a continuous function $f \colon X \to Y$ that our functor sends to $h$ must also be the identity on the underlying sets of $X$ and $Y$.
	But such a function cannot be continuous, as the preimage under $f$ of a singleton set of $Y$, which is open, would be a singleton set of $X$, which would not be open. 
	So our functor sends no continuous function $X \to Y$ to $h$, and therefore is not full.
	
	The functor is, however, faithful: for any spaces $X$ and $Y$ and continuous function $f \colon X \to Y$, we can uniquely recover $f$ from $p_f$ by taking the on-positions function $(p_f)_1$.
\end{enumerate}
\end{solution}
\end{exercise}

%-------- Section --------%
\section{Prepare for dynamics} \label{sec.poly.func_nat.prepare_dyn}

As beautiful as the category $\poly$ is---and to be clear we have not really begun to say what is so special about it---discussing its virtues is not our goal. We want to use it!

Polynomial functors are a setting in which to speak about dynamics, decisions, and data. We want the reader to understand this deeply as they go through the mathematics. So in order to make the story a bit more seamless, we discuss a few relevant aspects of $\poly$ that we can use immediately.

%---- Subsection ----%
\subsection{The categorical product} \label{subsec.poly.func_nat.prepare_dyn.prod}
The category $\poly$ has limits and colimits, is Cartesian closed, has epi-mono factorizations, is completely distributive, etc., etc. However, in order to tell a good story of dynamics, we only need products right now. These will be useful for letting many different interfaces control the same internal dynamics.

\begin{proposition}\label{prop.poly_prods}
The category $\poly$ has arbitrary products. 
\end{proposition}
\begin{proof}
The proof is very similar to that of \cref{prop.poly_coprods}.

By \cref{prop.prods_coprods_set_endofuncs}, the category $\smset^\smset$ has arbitrary products given by $\prod_{i \in I}$.
The full subcategory inclusion $\poly \to \smset^\smset$ reflects these products.
It remains to show that $\poly$ is closed under the operation $\prod_{i \in I}$.

Indeed, given polynomials $\{p_i\}_{i \in I}$, by \eqref{eqn.set_completely_distributive}, their product is
\begin{equation} \label{eqn.poly_prod}
    \prod_{i \in I} p_i \iso \prod_{i \in I} \sum_{j \in p_i(\1)} \yon^{p_i[j]} \iso \sum_{\bar{j} \in \prod_{i \in I} p_i(\1)} \prod_{i \in I} \yon^{p_i[\bar{j}(i)]} \iso \sum_{\bar{j} \in \prod_{i \in I} p_i(\1)} \yon^{\sum_{i \in I} p_i[\bar{j}(i)]},
\end{equation}
which, as a coproduct of representables, is in $\poly$.
% We will see that $\1$ is a terminal object and that the product of $p$ and $q$ in $\poly$ is the usual product of $p$ and $q$ as polynomials. That is, if $p\coloneqq\sum_{i\in p(\1)}\yon^{p[i]}$ and $q\coloneqq\sum_{j\in q(\1)}\yon^{q[j]}$ are in standard notation, then
% \begin{equation}\label{eqn.poly_times}
% p\times q\cong\sum_{i\in p(\1)}\sum_{j\in q(\1)}\yon^{p[i]+q[j]}.
% \end{equation}
% We leave the proof as an exercise; see \cref{exc.poly_times}.
\end{proof}

\begin{exercise}%\label{exc.poly_prod}
% \begin{enumerate}
% 	\item Use \eqref{eqn.main_formula} to verify that $\1$ is terminal in $\poly$.
	
% 	\item Use \eqref{eqn.main_formula} and  \eqref{eqn.poly_times} to verify that
% 	\[
% 	    \poly(r, p \times q) \iso \poly(r, p) \times \poly(r, q)
% 	\]
% 	for all polynomials $p,q,r$.
	
% 	\item
	Use \eqref{eqn.main_formula}
% 	and \eqref{eqn.poly_prod}
	to verify that
	\[
	    \poly\left(q, \prod_{i \in I} p_i\right) \iso \prod_{i \in I} \poly(q, p_i)
	\]
	for all polynomials $\{p_i\}_{i \in I}$ and $q$, as expected from the universal property of products.
\qedhere
% \end{enumerate}
\begin{solution}
% \begin{enumerate}
    % \item To verify that $\1$ is terminal in $\poly$, we use \eqref{eqn.main_formula} to show that $\poly(p, \1) \iso \1$ for all $p \in \poly$:
    % \[
    %     \poly(p, \1) \iso \prod_{i \in p(\1)} \sum_{j \in \1} p[i]^\0 \iso \prod_{i \in p(\1)} \1 \iso \1.
    % \]
    
    % \item Given $p,q,r \in \poly$, we use \eqref{eqn.main_formula} and  \eqref{eqn.poly_times} to verify that
    % \begin{align*}
    %     \poly(r, p \times q) &\iso \poly\left(r, \sum_{i \in p(\1)} \sum_{j \in q(\1)} \yon^{p[i] + q[j]} \right)
    %     \tag*{\eqref{eqn.poly_times}} \\
    %     &\iso \prod_{k \in r(\1)} \sum_{i \in p(\1)} \sum_{j \in q(\1)} r[k]^{p[i] + q[j]} \tag*{\eqref{eqn.main_formula}} \\
    %     &\iso \prod_{k \in r(\1)} \left(\sum_{i \in p(\1)} r[k]^{p[i]}\right) \times \left(\sum_{j \in q(\1)} r[k]^{q[j]}\right)  \\
    %     &\iso \left(\prod_{k \in r(\1)} \sum_{i \in p(\1)} r[k]^{p[i]}\right) \times \left(\prod_{k \in r(\1)} \sum_{j \in q(\1)} r[k]^{q[j]}\right) \\
    %     &\iso \poly(r, p) \times \poly(r, q).
    %     \tag*{\eqref{eqn.main_formula}}
    % \end{align*}
    
    % \item
    Given $q \in \poly$ and $p_i \in \poly$ for each $i \in I$ for some set $I$, we use \eqref{eqn.main_formula} to verify that
    \begin{align*}
        \poly\left(q, \prod_{i \in I} p_i\right) &\iso \prod_{k \in q(\1)} \left(\prod_{i \in I} p_i\right)(q[k])
        \tag*{\eqref{eqn.main_formula}} \\
        &\iso \prod_{k \in q(\1)} \prod_{i \in I} p_i(q[k]) \\
        &\iso \prod_{i \in I} \prod_{k \in q(\1)} p_i(q[k]) \\
        &\iso \prod_{i \in I} \poly(q, p_i).
        \tag*{\eqref{eqn.main_formula}}
    \end{align*}
% \end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Let $p_1\coloneqq\yon+\1, p_2\coloneqq\yon+\2,$ and $p_3\coloneqq\yon^\2$.
What is $\prod_{i\in\3}p_i$ according to \eqref{eqn.poly_prod}? Is the answer what you would expect?
\begin{solution}
Given $p_1\coloneqq\yon+\1,p_2\coloneqq\yon+\2,$ and $p_3\coloneqq\yon^\2$, we compute $\prod_{i\in\3}p_i$ via \eqref{eqn.poly_prod} as follows:
\begin{align*}
    \prod_{i\in\3} p_i
    &\iso
    \sum_{\bar{j} \in \prod_{i\in\3} p_i(\1)} \yon^{\sum_{i\in\3} p_i[\bar{j}(i)]}
    \tag*{\eqref{eqn.poly_prod}} \\
    &\iso
    \sum_{\bar{j} \colon (i\in\3) \to p_i(\1)} \yon^{p_1[\bar{j}(1)] + p_2[\bar{j}(2)] + p_3[\bar{j}(3)]} \\
    &\iso
    \yon^{p_1[1] + p_2[1] + p_3[1]}
    + \yon^{p_1[1] + p_2[2] + p_3[1]}
    + \yon^{p_1[1] + p_2[3] + p_3[1]} \\
    &+ \yon^{p_1[2] + p_2[1] + p_3[1]}
    + \yon^{p_1[2] + p_2[2] + p_3[1]}
    + \yon^{p_1[2] + p_2[3] + p_3[1]} \\
    &\iso
    \yon^{\1 + \1 + \2}
    + \yon^{\1 + \0 + \2}
    + \yon^{\1 + \0 + \2} \\
    &+ \yon^{\0 + \1 + \2}
    + \yon^{\0 + \0 + \2}
    + \yon^{\0 + \0 + \2} \\
    % &\iso
    % \yon^\4 + \yon^\3 + \yon^\3 + \yon^\3 + \yon^\2 + \yon^\2 \\
    &\iso
    \yon^\4 + \3\yon^\3 + \2\yon^\2,
\end{align*}
as we might expect from standard polynomial multiplication.
\end{solution}
\end{exercise}

It follows from \eqref{eqn.poly_prod} that the terminal object of $\poly$ is $\1$, and that binary products are given by
\begin{equation}\label{eqn.poly_times}
    p \times q \iso \sum_{i \in p(\1)} \sum_{j \in q(\1)} \yon^{p[i] + q[j]}.
\end{equation}

We will sometimes write $pq$ rather than $p\times q$:
\begin{equation} \tag{Notation}
pq\coloneqq p\times q
\end{equation}

\begin{example}
We can draw the product of two polynomials in terms of their associated forests. Let $p\coloneqq\yon^\3+\yon$ and $q\coloneqq\yon^\4+\yon^\2+\1$.
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, blue!50!black, "$p$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$} 
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$} 
      child {};
  \end{tikzpicture}
  };
%
	\node (p2) [draw, red!75!black, right=2 of p1, "$q$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$} 
      child {}
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$} 
      child {}
      child {};
    \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$}
    ;
  \end{tikzpicture}
  };
\end{tikzpicture}
\]
Then $pq\cong\yon^\7+\2\yon^\5+\2\yon^\3+\yon$.
As arenas, we take all pairs of positions, and for each pair we take the disjoint union of the directions.
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, "$pq$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny {(1,1)}" below] (11) {$\bullet$} 
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 11, "\tiny {(1,2)}" below] (12) {$\bullet$} 
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 12, "\tiny {(1,3)}" below] (13) {$\bullet$} 
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[blue!50!black] {};
    \node[right=1.5 of 13, "\tiny {(2,1)}" below] (21) {$\bullet$} 
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 21, "\tiny {(2,2)}" below] (22) {$\bullet$} 
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 22, "\tiny {(2,3)}" below] (23) {$\bullet$} 
      child[blue!50!black] {};
	\end{tikzpicture}
	};
\end{tikzpicture}
\]
\end{example}

In practice, we can multiply polynomial functors the same way we would multiply two polynomials in high school algebra.

\begin{exercise} \label{exc.general_poly_times}
\begin{enumerate}
    \item \label{exc.general_poly_times.monomial} Show that for sets $A_1, B_1, A_2, B_2$, we have
    \[
        A_1\yon^{B_1} \times A_2\yon^{B_2} \iso A_1 A_2\yon^{B_1 + B_2}.
    \]
    \item \label{exc.general_poly_times.polynomial} Show that for sets $\{A_i\}_{i \in I}$ and $\{B_j\}_{j \in J}$, we have
    \[
        \left(\sum_{i \in I} A_i\yon^{B_i}\right) \times \left(\sum_{j \in J} A_j\yon^{B_j}\right) \iso \sum_{i \in I} \sum_{j \in J} A_i A_j \yon^{B_i + B_j}.
    \]
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We compute the product $A_1\yon^{B_1} \times A_2\yon^{B_2}$ using \eqref{eqn.poly_times}:
    \begin{align*}
        A_1\yon^{B_1} \times A_2\yon^{B_2} &\iso \left(\sum_{i \in A_1} \yon^{B_1}\right) \times \left(\sum_{j \in A_2} \yon^{B_2}\right) \\
        &\iso \sum_{i \in A_1} \sum_{j \in A_2} \yon^{B_1 + B_2} \\
        &\iso A_1 A_2\yon^{B_1 + B_2}.
    \end{align*}

    \item We expand the product $\left(\sum_{i \in I} A_i\yon^{B_i}\right) \times \left(\sum_{j \in J} A_j\yon^{B_j}\right)$ by applying \eqref{eqn.set_completely_distributive}, with $I_1 \coloneqq I$ and $I_2 \coloneqq J$:
    \begin{align*}
        \left(\sum_{i \in I} A_i\yon^{B_i}\right) \times \left(\sum_{j \in J} A_j\yon^{B_j}\right) &\iso \prod_{k \in \2} \sum_{i \in I_k} A_i\yon^{B_i} \\
        &\iso \sum_{\bar{i} \in \prod_{k \in \2} I_k} \prod_{k \in \2} A_{\bar{i}(k)}\yon^{B_{\bar{i}(k)}} \\
        &\iso \sum_{i \in I} \sum_{j \in J} A_i\yon^{B_i} \times A_j\yon^{B_j} \\
        &\iso \sum_{i \in I} \sum_{j \in J} A_i A_j \yon^{B_i + B_j}
    \end{align*}
    where the last isomorphism follows from \cref{exc.general_poly_times.monomial}.
\end{enumerate}
\end{solution}
\end{exercise}

As arena morphisms, the canonical projections $\pi \colon pq \to p$ and $\phi \colon pq \to q$ behave as you might expect: on positions, they are the projections from $(pq)(\1) \iso p(\1) \times q(\1)$ to $p(\1)$ and $q(\1)$, respectively; on directions, they are the inclusions $p[i] \to p[i] + q[j]$ and $q[j] \to p[i] + q[j]$ for each position $(i, j)$ of $pq$.

\begin{exercise} \label{exc.poly_prod}
Verify that, for $p, q \in \poly$, the polynomial $pq$ given by \eqref{eqn.poly_times} along with the maps $\pi \colon pq \to p$ and $\phi \colon pq \to q$ described above satisfy the universal property of the product of $p$ and $q$.
\begin{solution}
We wish to show that, for $p, q \in \poly$, the polynomial $pq$ along with the maps $\pi \colon pq \to p$ and $\phi \colon pq \to q$ as described in the text satisfy the universal property of the product.
That is, we must show that for any $r \in \poly$ and maps $f \colon r \to p$ and $g \colon r \to q$, there exists a unique map $h \colon r \to pq$ for which the following diagram commutes:
\begin{equation} \label{eqn.prod_univ_prop}
\begin{tikzcd}
	r \ar[d, "f"'] \ar[r, "g"] \ar[dr, "h", dashed] & q \\
	p & pq. \ar[l, "\pi"] \ar[u, "\phi"'] 
\end{tikzcd}
\end{equation}
We apply \cref{ex.comm_poly}.
In order for \eqref{eqn.prod_univ_prop} to commute, it must commute on positions---that is, the following diagram of sets must commute:
\begin{equation} \label{eqn.prod_univ_prop_pos}
\begin{tikzcd}
	r(\1) \ar[d, "f_1"'] \ar[r, "g_1"] \ar[dr, "h_1", dashed] & q(\1) \\
	p(\1) & (pq)(\1). \ar[l, "\pi_1"] \ar[u, "\phi_1"'] 
\end{tikzcd}
\end{equation}
But since $(pq)(\1) \iso p(1) \times q(\1)$ along with the projections $\pi_1$ and $\phi_1$ form the product of $p(\1)$ and $q(\1)$ in $\smset$, there exists a unique $h_1$ for which \eqref{eqn.prod_univ_prop_pos} commutes.
Hence $h$ is uniquely characterized on positions.
In particular, it must send each $k \in r(\1)$ to the pair $(f_1(k), g_1(k)) \in (pq)(\1)$.

Moreover, if \eqref{eqn.coprod_univ_prop} is to commute on directions, then for every $k \in r(\1)$, the following diagram of sets must commute:
\begin{equation} \label{eqn.prod_univ_prop_dir}
\begin{tikzcd}[sep=large]
	r[k] & q[g_1(k)] \ar[l, "g^\sharp_k"'] \ar[d, "\phi^\sharp_{(f_1(k), g_1(k))}"] \\
	p[f_1(k)] \ar[u, "f^\sharp_k"] \ar[r, "\pi^\sharp_{(f_1(k), g_1(k))}"'] & (pq)[(f_1(k), g_1(k))]. \ar[ul, "h^\sharp_k"', dashed] 
\end{tikzcd}
\end{equation}
As $(pq)[(f_1(k), g_1(k))] \iso p[f_1(k)] + q[g_1(k)]$ along with the inclusions $\pi^\sharp_{(f_1(k), g_1(k))}$ and $\phi^\sharp_{(f_1(k), g_1(k))}$ form the coproduct of $p[f_1(k)]$ and $q[g_1(k)]$ in $\smset$, there exists a unique $h^\sharp_k$ for which \eqref{eqn.prod_univ_prop_dir} commutes.
Hence $h$ is also uniquely characterized on directions, so it is unique overall.
Moreover, we have shown that we can define $h$ on positions so that \eqref{eqn.prod_univ_prop_pos} commutes, and that we can define $h$ on directions such that \eqref{eqn.prod_univ_prop_dir} commutes.
As the commutativity of \eqref{eqn.prod_univ_prop_pos} and \eqref{eqn.prod_univ_prop_dir} together imply the commutativity of \eqref{eqn.prod_univ_prop}, it follows that there exists $h$ for which \eqref{eqn.prod_univ_prop} commutes.
\end{solution}
\end{exercise}

%---- Subsection ----%
\subsection{The parallel product} \label{subsec.poly.func_nat.prepare_dyn.par}
There is a closely related monoidal structure on $\poly$ that will be useful for putting dynamical systems in parallel and then wiring them together.

\begin{definition}\label{def.dirichlet}
Let $p$ and $q$ be polynomials. When they are expressed in standard notation, their \emph{parallel product}, denoted $p\otimes q$, is given by the formula:
\begin{equation}\label{eqn.parallel_def}
p\otimes q\cong\sum_{i\in p(\1)}\sum_{j\in q(\1)}\yon^{p[i]\times q[j]}.
\end{equation}
% On arenas, this is defined by
% \begin{equation}\label{eqn.parallel_product_dependent}
% \sum_{i \in p(\1)}\yon^{p[i]} \otimes \sum_{j \in q(\1)}\yon^{q[j]} := \sum_{(i,j) \in p(\1) \times q(\1)}\yon^{p[i]\times q[j]}.
% \end{equation}
\end{definition}

One should compare this with the formula for the product of polynomials shown in \eqref{eqn.poly_times}. The difference is that the parallel product multiplies exponents where the categorical product adds them.

% \begin{remark}
%   The reason we call this the parallel product is because it generalizes the parallel product for lenses in \cref{sec.lens_discrete}.
% \end{remark}

\begin{example}
We can draw the parallel product of two polynomials in terms of their associated forests. Let $p\coloneqq\yon^\3+\yon$ and $q\coloneqq\yon^\4+\yon^\2+\1$.
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, blue!50!black, "$p$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$} 
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$} 
      child {};
  \end{tikzpicture}
  };
%
	\node (p2) [draw, red!75!black, right=2 of p1, "$q$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$} 
      child {}
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$} 
      child {}
      child {};
    \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$}
    ;
  \end{tikzpicture}
  };
\end{tikzpicture}
\]
Then $p\otimes q\cong\yon^{\1\2}+\yon^\6+\yon^\4+\yon^\2+\2$.
As arenas, we take all pairs of positions, and for each pair we take the product of the directions.
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, "$p \otimes q$" above] {
	\begin{tikzpicture}[trees, sibling distance=2mm]
    \node["\tiny {(1,1)}" below] (11) {$\bullet$} 
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
    ;
    \node[right=2 of 11, "\tiny {(1,2)}" below] (12) {$\bullet$} 
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
    ;
    \node[right=1.5 of 12, "\tiny {(1,3)}" below] (13) {$\bullet$} 
    ;
   \node[right=1.5 of 13, "\tiny {(2,1)}" below] (21) {$\bullet$} 
      child {}
      child {}
      child {}
      child {}
 		;    
		\node[right=1.5 of 21, "\tiny {(2,2)}" below] (22) {$\bullet$} 
      child {}
      child {}
 		;    
    \node[right=1.5 of 22, "\tiny {(2,3)}" below] (23) {$\bullet$} 
 		;    
	\end{tikzpicture}
	};
\end{tikzpicture}
\]
\end{example}

% \begin{exercise}
% \begin{enumerate}
%     \item Compute the parallel product of monomials $A_1\yon^{B_1}\otimes A_2\yon^{B_2}$.
%     \item 
% \end{enumerate}
% \begin{solution}
% The parallel product of monomials $A_1\yon^{B_1}\otimes A_2\yon^{B_2}$ is $A_1 A_2\yon^{B_1 B_2}$. 
% \end{solution}
% \end{exercise}


\begin{exercise} \label{exc.general_poly_parallel_times}
\begin{enumerate}
    \item \label{exc.general_poly_parallel_times.monomial} Show that for sets $A_1, B_1, A_2, B_2$, we have
    \[
        A_1\yon^{B_1} \otimes A_2\yon^{B_2} \iso A_1 A_2\yon^{B_1 B_2}.
    \]
    \item \label{exc.general_poly_parallel_times.polynomial} Show that for sets $\{A_i\}_{i \in I}$ and $\{B_j\}_{j \in J}$, we have
    \[
        \left(\sum_{i \in I} A_i\yon^{B_i}\right) \otimes \left(\sum_{j \in J} A_j\yon^{B_j}\right) \iso \sum_{i \in I} \sum_{j \in J} A_i A_j \yon^{B_i B_j}.
    \]
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We compute the product $A_1\yon^{B_1} \otimes A_2\yon^{B_2}$ using \eqref{eqn.parallel_def}:
    \begin{align*}
        A_1\yon^{B_1} \otimes A_2\yon^{B_2} &\iso \left(\sum_{i \in A_1} \yon^{B_1}\right) \otimes \left(\sum_{j \in A_2} \yon^{B_2}\right) \\
        &\iso \sum_{i \in A_1} \sum_{j \in A_2} \yon^{B_1 \times B_2} \\
        &\iso A_1 A_2\yon^{B_1 B_2}.
    \end{align*}

    \item We expand the product $\left(\sum_{i \in I} A_i\yon^{B_i}\right) \otimes \left(\sum_{j \in J} A_j\yon^{B_j}\right)$ as follows:
    \begin{align*}
        \left(\sum_{i \in I} A_i\yon^{B_i}\right) \otimes \left(\sum_{j \in J} A_j\yon^{B_j}\right) &\iso \left(\sum_{i \in I} \sum_{i' \in A_i} \yon^{B_i}\right) \otimes \left(\sum_{j \in J} \sum_{j' \in A_j} \yon^{B_j}\right) \\
        &\iso \sum_{i \in I} \sum_{i' \in A_i} \sum_{j \in J} \sum_{j' \in A_j} \yon^{B_i \times B_j} \\
        &\iso \sum_{i \in I} \sum_{j \in J} \sum_{i' \in A_i} \sum_{j' \in A_j} \yon^{B_i B_j} \\
        &\iso \sum_{i \in I} \sum_{j \in J} A_i A_j \yon^{B_i B_j}.
    \end{align*}
\end{enumerate}
\end{solution}
\end{exercise}




\begin{exercise}
Let $p\coloneqq\yon^\2+\yon$ and $q\coloneqq\2\yon^\4$.
\begin{enumerate}
	\item Draw $p$ and $q$ as corolla forests.
	\item Draw $pq=p\times q$ as a corolla forest.
	\item Draw $p\otimes q$ as a corolla forest.
\qedhere
\end{enumerate}
\begin{solution}
Here $p\coloneqq\yon^\2+\yon$ and $q\coloneqq\2\yon^\4$.
\begin{enumerate}
\item Here are $p$ and $q$ drawn as corolla forests:
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, blue!50!black, "$p$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$} 
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$} 
      child {};
  \end{tikzpicture}
  };
%
	\node (p2) [draw, red!75!black, right=2 of p1, "$q$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$} 
      child {}
      child {}
      child {}
      child {};
    \node[right=1 of 1,"\tiny 2" below] (2) {$\bullet$} 
      child {}
      child {}
      child {}
      child {};
  \end{tikzpicture}
  };
\end{tikzpicture}
\]

\item Here is $pq$ drawn as a corolla forest:
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, "$pq$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny {(1,1)}" below] (11) {$\bullet$} 
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 11, "\tiny {(1,2)}" below] (12) {$\bullet$} 
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 12, "\tiny {(2,1)}" below] (21) {$\bullet$} 
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 21, "\tiny {(2,2)}" below] (22) {$\bullet$} 
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
	\end{tikzpicture}
	};
\end{tikzpicture}
\]
\item Here is $p \otimes q$ drawn as a corolla forest:
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, "$pq$" above] {
	\begin{tikzpicture}[trees, sibling distance=2mm]
    \node["\tiny {(1,1)}" below] (11) {$\bullet$} 
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
    ;
    \node[right=2 of 11, "\tiny {(1,2)}" below] (12) {$\bullet$} 
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
    ;
   \node[right=1.5 of 12, "\tiny {(2,1)}" below] (21) {$\bullet$} 
      child {}
      child {}
      child {}
      child {}
 	;    
	\node[right=1.5 of 21, "\tiny {(2,2)}" below] (22) {$\bullet$} 
      child {}
      child {} 
      child {}
      child {}
 	;
	\end{tikzpicture}
	};
\end{tikzpicture}
\]
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Consider the polynomials $p\coloneqq \2\yon^\2+\3\yon$ and $q\coloneqq\yon^\4+\3\yon^\3$.
\begin{enumerate}
	\item What is $p\times q$?
	\item What is $p\otimes q$?
	\item What is the product of the following purely formal expression we'll see \emph{only this once!}:
	\[
	(2\mdot2^\yon+3\mdot 1^\yon) \cdot 
	(1\mdot4^\yon+3\mdot 3^\yon)
	\]
    The factors of the above product are called Dirichlet series.
	\item Describe the connection between the last two parts. (An alternative name we give for the parallel product $\otimes$ is the \emph{Dirichlet product}.) \qedhere
\end{enumerate}
\begin{solution}
Here $p\coloneqq \2\yon^\2+\3\yon$ and $q\coloneqq\yon^\4+\3\yon^\3$.
\begin{enumerate}
    \item We compute $p \times q$ using \cref{exc.general_poly_times} \cref{exc.general_poly_times.polynomial}:
    \begin{align*}
        p \times q &\iso \2\yon^{\2 + \4} + (\2 \times \3)\yon^{\2 + \3} + \3\yon^{\1 + \4} + (\3 \times \3)\yon^{\1 + \3} \\
        &\iso \2\yon^\6 + \6\yon^\5 + \3\yon^\5 + \9\yon^\4 \\
        &\iso \2\yon^\6 + \9\yon^\5 + \9\yon^\4.
    \end{align*}
    
    \item We compute $p \otimes q$ using \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.polynomial}:
    \begin{align*}
        p \otimes q &\iso \2\yon^{\2 \times \4} + (\2 \times \3)\yon^{\2 \times \3} + \3\yon^\4 + (\3 \times \3)\yon^\3 \\
        &\iso \2\yon^\8 + \6\yon^\6 + \3\yon^\4 + \9\yon^\3.
    \end{align*}
    
    \item We evaluate $(2\mdot2^\yon+3\mdot 1^\yon+1) \cdot 
	(1\mdot4^\yon+3\mdot 3^\yon+2)$ using standard high school algebra:
    \begin{align*}
	    (2\mdot2^\yon+3\mdot 1^\yon) \cdot (1\mdot4^\yon+3\mdot 3^\yon) &= 2\mdot1\mdot2^\yon\mdot4^\yon + 2\mdot3\mdot2^\yon\mdot3^\yon + 3\mdot1\mdot1^\yon\mdot4^\yon + 3\mdot3\mdot1^\yon\mdot3^\yon \\
	    &= 2\mdot8^\yon + 6\mdot6^\yon + 3\mdot4^\yon + 9\mdot3^\yon.
	\end{align*}
	
	\item We describe the connection between the last two parts as follows.
	Given a polynomial $p$, we let $d(p)$ denote the Dirichlet series $\sum_{i \in p(\1)} |p[i]|^\yon$.
	Then by \eqref{eqn.parallel_def},
	\begin{align*}
	    d(p \otimes q) &= \sum_{i \in p(\1)} \sum_{j \in q(\1)} |p[i] \times q[j]|^\yon \\
	    &= \sum_{i \in p(\1)} |p[i]|^\yon \sum_{j \in q(\1)} |q[j]|^\yon \\
	    &= d(p) \cdot d(q).
	\end{align*}
	The last two parts are simply an example of this identity for a specific choice of $p$ and $q$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
What is $(\3\yon^\5+\6\yon^\2)\otimes\4$? Hint: $\4=\4\yon^\0$.
\begin{solution}
We compute $(\3\yon^\5+\6\yon^\2)\otimes\4$ using \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.polynomial} and the fact that $\4=\4\yon^\0$:
\begin{align*}
    (\3\yon^\5+\6\yon^\2)\otimes\4\yon^\0 &\iso (\3\times\4)\yon^{\5\times\0} + (\6\times\4)\yon^{\2\times\0} \\
    &\iso \1\2\yon^\0 + \2\4\yon^\0 \\
    &\iso \3\6.
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.prepare_poly_smc}
Let $p,q,r\in\poly$ be any polynomials.
\begin{enumerate}
  \item Show that there is an isomorphism $p\otimes\yon\cong p$.
  \item Show that there is an isomorphism $(p\otimes q)\otimes r\cong p\otimes (q\otimes r)$.
  \item Show that there is an isomorphism $p\otimes q \cong q\otimes p$.
 \qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
  \item We show that $p\otimes\yon\cong p$:
  \begin{align*}
      p \otimes y &\iso \sum_{i \in p(\1)} \sum_{j \in \1} \yon^{p[i] \times \1} \tag*{\eqref{eqn.parallel_def}} \\
      &\iso \sum_{i \in p(\1)} \yon^{p[i]} \iso p.
  \end{align*}
  
  \item We show that $(p\otimes q)\otimes r\cong p\otimes (q\otimes r)$:
  \begin{align*}
      (p \otimes q) \otimes r &\iso \left(\sum_{i \in p(\1)} \sum_{j \in q(\1)} \yon^{p[i] \times q[j]}\right) \otimes r \tag*{\eqref{eqn.parallel_def}} \\
      &\iso \sum_{i \in p(\1)} \sum_{j \in q(\1)} \left(\sum_{k \in r(\1)} \yon^{(p[i] \times q[j]) \times r[k]}\right) \tag*{\eqref{eqn.parallel_def}} \\
      &\iso \sum_{i \in p(\1)} \left(\sum_{j \in q(\1)} \sum_{k \in r(\1)} \yon^{p[i] \times (q[j] \times r[k])}\right) \tag{Associativity of $\sum$ and $\times$} \\
      &\iso p \otimes \left(\sum_{j \in q(\1)} \sum_{k \in r(\1)} \yon^{q[j] \times r[k]}\right) \tag*{\eqref{eqn.parallel_def}} \\
      &\iso p \otimes (q \otimes r). \tag*{\eqref{eqn.parallel_def}} \\
  \end{align*}
  
  \item We show that $(p\otimes q)\cong(q\otimes p)$:
  \begin{align*}
      p \otimes q &\iso \sum_{i \in p(\1)} \sum_{j \in q(\1)} \yon^{p[i] \times q[j]} \tag*{\eqref{eqn.parallel_def}} \\
      &\iso \sum_{j \in q(\1)} \sum_{i \in p(\1)} \yon^{q[j] \times p[i]} \tag{Commutativity of $\sum$ and $\times$} \\
      &\iso q \otimes p. \tag*{\eqref{eqn.parallel_def}} \\
  \end{align*}
\end{enumerate}
\end{solution}
\end{exercise}

In \cref{exc.prepare_poly_smc}, we have gone most of the way to proving that $(\poly,\yon,\otimes)$ is a symmetric monoidal category. 

\begin{proposition}\label{prop.dirichlet_monoidal}
The category $\poly$ has a symmetric monoidal structure $(\yon,\otimes)$ where $\otimes$ is the parallel product from \cref{def.dirichlet}.
\end{proposition}
\begin{proof}[Sketch of proof]
Given maps of polynomials $f\colon p\to p'$ and $g\colon q\to q'$, we need to give a map $(f\otimes g)\colon (p\otimes q)\to (p'\otimes q')$. On positions, define
\[
(f\otimes g)_1(i,j)\coloneqq \big(f_1(i),g_1(j)\big)
\]
On directions at $(i,j)\in p(\1)\times q(\1)$, define
\[
  (f\otimes g)^\sharp_{(i,j)}(d,e)\coloneqq
  \big(f^\sharp_i(d),g^\sharp_j(e)\big).
\]
Then \cref{exc.prepare_poly_smc} gives us the unitors, associator, and braiding.
We have not proven the functoriality of $\otimes$, the naturality of the isomorphisms from \cref{exc.prepare_poly_smc}, or all the coherences between these isomorphisms, but we ask the reader to take them on trust or to check them for themselves.
Alternatively, we may invoke the Day convolution to obtain the monoidal structure $(\yon, \otimes)$ directly (see \cref{prop.day}).
\end{proof}

\begin{exercise} \label{exc.some_parallel_prods}
\begin{enumerate}
	\item \label{exc.some_parallel_prods.const} If $p=A$ and $q=B$ are constant polynomials, what is $p\otimes q$?
	\item If $p=A$ is constant and $q$ is arbitrary, what can you say about $p\otimes q$?
	\item \label{exc.some_parallel_prods.lin} If $p=A\yon$ and $q=B\yon$ are linear polynomials, what is $p\otimes q$?
	\item \label{exc.some_parallel_prods.pos_prod} For arbitrary $p,q\in\poly$, what is the relationship between the sets $(p\otimes q)(\1)$ and $p(\1)\times q(\1)$?
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item By \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial}, we have $A \otimes B \iso AB$.
    \item We use \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.polynomial} to compute $A \otimes q$:
    \[
        A \otimes q \iso \sum_{j \in q(\1)} A\yon^{\0 \times q[i]} \iso \sum_{j \in q(\1)} A \iso A \times q(\1).
    \]
    \item By \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial}, we have $A\yon \otimes B\yon \iso AB\yon$.
    \item We show that $(p\otimes q)(\1)$ and $p(\1)\times q(\1)$ are isomorphic. By \eqref{eqn.parallel_def},
    \[
        (p \otimes q)(\1) \iso \sum_{i \in p(\1)} \sum_{j \in q(\1)} \1^{p[i] \times q[j]} \iso p(\1) \times q(\1).
    \]
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.dir_closed_classes}
Which of the following classes of polynomials are closed under $\otimes$? Note also whether they contain $\yon$.
\begin{enumerate}
	\item The set $\{A\yon^\0\mid A\in\smset\}$ of constant polynomials.
	\item The set $\{A\yon\mid A\in\smset\}$ of linear polynomials.
	\item The set $\{A\yon+B\mid A,B\in\smset\}$ of affine polynomials.
	\item The set $\{A\yon^\2+B\yon+C\mid A,B,C\in\smset\}$ of quadradic polynomials.
	\item The set $\{A\yon^B\mid A,B\in\smset\}$ of monomials.
	\item The set $\{S\yon^S\mid S\in\smset\}$ of systematic polynomials.
	\item The set $\{p\in\poly\mid p(\1)\text{ is finite}\}$. \qedhere
\end{enumerate}
\begin{solution}
For each of the following classes of polynomials, we determine whether they are closed under $\otimes$ and whether they contain $\yon$.
\begin{enumerate}
	\item The set $\{A\yon^\0\mid A\in\smset\}$ of constant polynomials is closed under $\otimes$ by the solution to \cref{exc.some_parallel_prods} \cref{exc.some_parallel_prods.const}.
	But the set does not contain $\yon$, as $\yon$ is not a constant polynomial.
	\item The set $\{A\yon\mid A\in\smset\}$ of linear polynomials is closed under $\otimes$ by the solution to \cref{exc.some_parallel_prods} \cref{exc.some_parallel_prods.lin} and does contain $\yon$, as $\yon \iso \1\yon$.
	\item The set $\{A\yon+B\mid A,B\in\smset\}$ of affine polynomials is closed under $\otimes$, for \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.polynomial} yields
	\[
	    (A\yon + B) \otimes (A'\yon + B') \iso AA'\yon + AB' + BA' + BB'.
	\]
	The set contains $\yon$, as $\yon \iso \1\yon + \0 $.
	\item The set $\{A\yon^\2+B\yon+C\mid A,B,C\in\smset\}$ of quadradic polynomials is not closed under $\otimes$, for even though $\yon^\2 \iso \1\yon^\2 + \0\yon + \0$ is a quadratic polynomial, \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial} implies that
	\[
	    \yon^\2 \otimes \yon^\2 \iso \yon^\4,
	\]
	which is not quadratic.
	The set contains $\yon$, as $\yon \iso \0\yon^\2 + \1\yon + \0$.
	\item The set $\{A\yon^B\mid A,B\in\smset\}$ of monomials is closed under $\otimes$ by \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial} and does contain $\yon$, as $\yon \iso \1\yon^\1$.
	\item The set $\{S\yon^S\mid S\in\smset\}$ of systematic polynomials is closed under $\otimes$, for \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial} yields
	\[
	    S\yon^S \otimes T\yon^T \iso ST\yon^{ST}.
	\]
	The set contains $\yon$, as $\yon \iso \1\yon^\1$.
	\item The set $\{p\in\poly\mid p(\1)\text{ is finite}\}$ is closed under $\otimes$ by the solution to \cref{exc.some_parallel_prods} \cref{exc.some_parallel_prods.pos_prod}.
	The set contains $\yon$, as $\yon(\1) \iso \1$ is finite.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
What is the smallest class of polynomials that's closed under $\otimes$ and contains $\yon$?
\begin{solution}
The smallest class of polynomials that's closed under $\otimes$ and contains $\yon$ is just $\{\yon\}$.
This is because by \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial}, we have $\yon \otimes \yon \iso \yon$.
\end{solution}
\end{exercise}

\begin{exercise}
Show that for any $p_1,p_2,q\in\poly$ there is an isomorphism
\[
(p_1+p_2)\otimes q\cong (p_1\otimes q)+(p_2\otimes q).
\qedhere
\]
\begin{solution}
We show that $(p_1 + p_2) \otimes q \iso (p_1 \otimes q) + (p_2 \otimes q)$ using \eqref{eqn.parallel_def}:
\begin{align*}
    (p_1 + p_2) \otimes q &\iso \sum_{k \in \2} \sum_{i \in p_k(\1)} \sum_{j \in q(\1)} \yon^{p_k[i] \times q[j]} \\
    &\iso \sum_{i \in p_1(\1)} \sum_{j \in q(\1)} \yon^{p_1[i] \times q[j]} + \sum_{i \in p_2(\1)} \sum_{j \in q(\1)} \yon^{p_2[i] \times q[j]} \\
    &\iso (p_1 \otimes q) + (p_2 \otimes q).
\end{align*}
\end{solution}
\end{exercise}

\begin{proposition} \label{prop.day}
For any monoidal structure $(I,\star)$ on $\smset$, there is a corresponding monoidal structure $(\yon^I, \odot)$ on $\poly$, where $\odot$ is the Day convolution.
Moreover, $\odot$ distributes over coproducts. 

In the case of $(\0,+)$ and $(\1,\times)$, this procedure returns the $(\1,\times)$ and $(\yon,\otimes)$ monoidal structures respectively.
\end{proposition}
\begin{proof}
Any monoidal structure $(I, \odot)$ on $\smset$ induces a monoidal structure on $\smset^\smset$ with the Day convolution $\odot$ as the tensor product and $\yon^I$ as the unit.
To prove that this monoidal structure restricts to $\poly$, it suffices to show that $\poly$ is closed under the Day convolution.

Given polynomials $p\coloneqq\sum_{i \in p(\1)} \yon^{p[i]}$ and $q\coloneqq\sum_{j \in q(\1)} \yon^{q[j]}$, their Day convolution is given by the coend
\begin{equation} \label{eqn.day_conv.coend}
    p \odot q \iso \int^{(A,B)\in\smset^\2} \yon^{A \star B} \times p(A) \times q(B).
\end{equation}
We can rewrite the product $p(A) \times q(B)$ as
\[
    p(A) \times q(B) \iso \left(\sum_{i \in p(\1)} A^{p[i]}\right) \times \left(\sum_{j \in q(\1)} B^{q[i]}\right) \iso \sum_{(i,j) \in p(\1) \times q(\1)} A^{p[i]} \times B^{q[i]} %\\
    % &\iso \sum_{(i,j) \in p(\1) \times q(\1)} \smset(p[i], A) \times \smset(q[j], B) \\
    % &\iso \sum_{(i,j) \in p(\1) \times q(\1)} \smset^\2((p[i], q[j]), (A, B))
\]
So because products distribute over coproducts in $\smset$ and coends commute with coproducts, we can rewrite \eqref{eqn.day_conv.coend} as
\[
    p \odot q \iso \sum_{(i,j) \in p(\1) \times q(\1)} \int^{(A,B)\in\smset^\2} \yon^{A \star B} \times A^{p[i]} \times B^{q[i]},
\]
which, by the co-Yoneda lemma, can be rewritten as
\begin{equation} \label{eqn.day_conv.poly}
    p \odot q \iso \sum_{(i,j) \in p(\1) \times q(\1)} \yon^{p[i] \star q[j]}
\end{equation}
in $\poly$.
That the Day convolution distributes over coproducts also follows from the fact that products distribute over coproducts in $\smset$ and coends commute with coproducts; or, alternatively, directly from \eqref{eqn.day_conv.poly}.

We observe that \eqref{eqn.day_conv.poly} gives $(\yon^I, \odot) = (\1, \times)$ when $(I, \star) = (\0, +)$ and $(\yon^I, \odot) = (\yon, \otimes)$ when $(I, \star) = (\1, \times)$.
\end{proof}

\begin{exercise}
\begin{enumerate}
	\item Show that the operation $(A, B)\mapsto A+AB+B$ on $\smset$ is associative.
	\item Show that $\0$ is unital for the above operation.
	\item Let $(\1,\odot)$ denote the corresponding monoidal structure on $\poly$. Compute the monoidal product $(\yon^\3+\yon)\odot(\2\yon^\2+\2)$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item To show that the operation $(A, B)\mapsto A+AB+B$ on $\smset$ is associative, we observe that
    \begin{align*}
        (A + AB + B) + (A + AB + B)C + C &\iso A + AB + B + AC + ABC + BC + C \\
        &\iso A + AB + ABC + AC + B + BC + C \\
        &\iso A + A(B + BC + C) + (B + BC + C).
    \end{align*}
    \item To show that $\0$ is unital for this operation, we observe that
    \[
        (A, \0) \mapsto A + A\0 + \0 \iso A
    \]
    and
    \[
        (\0, B) \mapsto \0 + \0B + B \iso B.
    \]
    \item We let $(\1,\odot)$ denote the corresponding monoidal product on $\poly$ and evaluate $(\yon^\3+\yon)\odot(\2\yon^\2+\2)$.
    By \eqref{eqn.day_conv.poly}, with $A \star B \iso A + AB + B$, we have
    \begin{align*}
        (\yon^\3+\yon)\odot(\2\yon^\2+\2) &\iso (\yon^\3+\yon^\1)\odot(\yon^\2+\yon^\2+\yon^\0+\yon^\0) \\
        &\iso \yon^{\3\star\2} + \yon^{\3\star\2} + \yon^{\3\star\0} + \yon^{\3\star\0} + \yon^{\1\star\2} + \yon^{\1\star\2} + \yon^{\1\star\0} + \yon^{\1\star\0} \\
        &\iso \2\yon^{\1\1} + \2\yon^\3 + \2\yon^\5 + \2\yon^\1.
    \end{align*}
\end{enumerate}
\end{solution}
\end{exercise}

%---- Subsection ----%
\subsection{$\otimes$-monoids in $\poly$} \label{subsec.poly.func_nat.prepare_dyn.par_monoid}

\begin{definition}
A \emph{$\otimes$-monoid in} $\poly$ consists of a polynomial $m$, called the \emph{carrier}; a position $\eta\colon\yon\to m$, called the \emph{unit}; and a map $\mu\colon m\otimes m\to m$, called the \emph{multiplication}, that form a monoid in $(\poly,\yon,\otimes)$---that is, they must satisfy certain equations.
We sometimes say that $m$ \emph{carries a $\otimes$-monoid structure}, and we refer to the unit and multiplication together as a \emph{$\otimes$-monoid structure on} $m$.
\end{definition}
% ** Show that a map y -> m is a position of m?

These $\otimes$-monoids have a kind of swarm-like semantics, letting supervisors summarize the positions of a crew of subordinates and distribute instructions to those subordinates in a coherent way.

\begin{equation}\label{eqn.swarm}
\begin{tikzpicture}[oriented WD]
	\node[bb={0}{0}] at (1.6,4)     (x1) {$m$};
	\node[bb={0}{0}] at (1.2,8) (x2) {$m$};
	\node[bb={0}{0}] at (2,7)     (x3) {$m$};
	\node[bb={0}{0}] at (2.5,10)     (x4) {$m$};
	\node[bb={0}{0}] at (3,2)     (x5) {$m$};
	\node[bb={0}{0}] at (4,4)     (x6) {$m$};
	\node[bb={0}{0}] at (4,8)     (x7) {$m$};
	\node[bb={0}{0}] at (5,5)   (x8) {$m$};
	\node[bb={0}{0}] at (5.8,6)   (x9) {$m$};
	\node[bb={0}{0}] at (4.6,11)     (x0) {$m$};
	\node[bb={0}{0}, bb name=$m$, bbx=1, bby=1, fit=(x2) (x3) (x4)] (o1) {};
	\node[bb={0}{0}, bb name=$m$, bbx=1, bby=1, fit=(x6) (x7) (x8) (x9)] (o2) {};
	\node[bb={0}{0}, bb name=$m$, bbx=1, bby=1, fit=(o1) (o2) (x1) (x5) (x0)] (o2) {};
\end{tikzpicture}
\end{equation}
% ** Might be too general? Should be a linear row of boxes if noncommutative
Each box in this picture is labeled $m$. Whenever a box has $n$ subboxes, it represents the canonical map
\[m^{\otimes n} \to m\]
given by the monoid structure on $m$.
We can think of this map as an (associative, unital) $n$-ary multiplication operation on $m$.
Formally we might denote it by $\mu^{n-1}$, so that $\mu^1=\mu\colon m\otimes m\to m$,\footnote{Here $\mu\inv = \eta$ and $\mu^n = (\mu^{n-1} \otimes \id_m) \then \mu$ for $n \in \nn$, so that $\mu^0 = \id_m$ and $\mu^1 = \mu$.} but for typographical reasons it is nicer to just write $\mu$ for all of them.

The on-positions function $\mu_1 \colon m(\1)^n \to m(\1)$ sends every $n$-tuple $i\coloneqq (i_1,\ldots,i_n)$ of positions to a single position $i'\coloneqq\mu_1(i)$.
In the language of decision-making, we can think of this as a crew of $n$ subordinates, each with a decision to make, summarizing all of their decisions as a single decision for their supervisor to make.

Meanwhile, the on-directions function
\[
    \mu^\sharp_{i} \colon m[i'] \to m[i_1]\times\cdots\times m[i_n]
\] 
at the $n$-tuple $i$ sends every direction at $i'$ to a direction at each of the $n$ positions in $i$.
Continuing our decision-making analogy, the supervisor chooses a single option to distribute to all the subordinates that tells them which option to pick for each of their individual decisions.
This relationship between subordinates and their supervisor gives us our swarm-like semantics for $\otimes$-monoids.

The coherence of the monoid ensures that if we have a nested hierarchy in which intermediary supervisors of smaller crews are themselves subordinates of a higher supervisor, we may reassociate some of the crews or even forget about some of the intermediary supervisors without altering the relationship.
In other words, we may safely ignore the intermediary subboxes in \eqref{eqn.swarm}.

\begin{example}[Monoids in $\smset$ give linear $\otimes$-monoids]\label{ex.tensor_monoid_linear}
If $(M,e,*)$ is a monoid in $(\smset,\1,\times)$, then the linear polynomial $M\yon$ carries a corresponding $\otimes$-monoid structure.
Here the unit $\eta \colon \yon \to M\yon$ specifies the position $e \in M$, and since $M\yon \otimes M\yon \iso (M \times M)\yon$, the multiplication is a map $\mu \colon (M \times M)\yon \to M\yon$.
This map is equal to $* \colon M \times M \to M$ on positions and the identity on directions.

We may interpret the swarm-like semantics of this $\otimes$-monoid.
Every decision has but a single option, so no choices need to be made.
So we can think of the decisions simply as straightforward actions; any crew of subordinates can send their actions to their supervisor as a one-action summary via $*$, and there is no need for the supervisor to send any directions back.

This construction is functorial and fully faithful: a map between monoids in $\smset$ can be identified with a map between $\otimes$-monoids.
\end{example}

\begin{example}[Sets give representable $\otimes$-monoids]\label{ex.tensor_monoid_rep}
For any set $S$, the polynomial $\yon^S$ carries a canonical $\otimes$-monoid structure: the unit is the unique map $\eta\colon\yon\to\yon^S$, while the multiplication $\mu\colon\yon^S\otimes\yon^S\to\yon^S$ is the identity on positions and the diagonal $S\to S\times S$ on directions.

We may interpret the swarm-like semantics of this $\otimes$-monoid.
There is only one kind of decision to be made, always with the same options.
So there is no need for the subordinates to tell their supervisor what decision they are making; all the supervisor has to do is select an option, and every subordinate will know to select that option, too.

This construction is again (contravariantly) functorial and fully faithful.
\end{example}

\begin{example}\label{ex.swarm_reals}
Let $m\coloneqq\rr_{\geq0}\yon^{\rr_{\geq0}}$. If we take $\eta\coloneqq0$ and take $\mu_1(a,b) \coloneqq a+b$ and $\mu^\sharp_{(a,b)}(t) \coloneqq \left(\frac{at}{a+b},\frac{bt}{a+b}\right)$, then $(m, \eta, \mu)$ is a $\otimes$-monoid.
\end{example}

\begin{exercise}
Give an interpretation of the swarm-like semantics of the $\otimes$-monoid in \cref{ex.swarm_reals}.
\begin{solution}
Here is one interpretation of the swarm-like semantics of the $\otimes$-monoid in \cref{ex.swarm_reals}.
In a crew of $n$ subordinates, each subordinate $i \in \ord{n}$ must determine how much of Product T to give back to the $i^\text{th}$ stakeholder, who has has contributed some quantity $a_i \in \rr_{\geq0}$ of Resource A.
The subordinates pool together the total amount $\sum_{i \in \ord{n}} a_i$ of Resource A that they have received, which they pass on to their supervisor.
The supervisor, who sees only this total, uses it to decide (through unspecified means, perhaps via another morphism we compose with) the total quantity $t \in \rr_{\geq0}$ of Product T to be split between the stakeholders.
Upon receiving this quantity, each subordinate then gives their stakeholder their cut $a_i t / \sum_{i \in \ord{n}} a_i$ of Product T proportional to their contribution of Resource A.
% Looking at the picture \eqref{eqn.swarm}, suppose a big box $m$ has $n$-many subordinates, which we call little boxes; we're considering a map $\mu\colon m^{\otimes n}\to m$. Suppose each little box has its own dynamics. Then the map $\mu$ gives us dynamics on the whole. What is it?

% Each little box is publishing its position, which is a nonnegative real number. The big box adds up these real numbers, and that is its position. Now position might be the wrong word, given that map, so maybe ``energy usage'' is better. The energy usage of the big box is the sum of the energy usages of its subordinates.

% Now a direction comes in to the big box; it is a nonnegative real number. Perhaps this represents how much energy the big box is being granted at the next time step. This energy is then distributed according to whatever fraction of the whole the current boxes are currently using. This operation is monoidal, meaning that no matter how many subordinates each box has, it will be coherent with forgetting the intermediary boxes that are doing the summarizing.
\end{solution}
\end{exercise}

\begin{example}\label{ex.list_monoids}
Here are two examples of $\otimes$-monoid structures carried by 
\[\Set{List} \coloneqq \sum_{n\in\nn} \yon^{\ord{n}} = \1+\yon+\yon^\2+\cdots,\]
the ``list'' polynomial, whose position-set is $\nn$ and whose direction-set at $n\in\nn$ is $\ord{n}$.\footnote{We call this the ``list'' polynomial because, when viewed as a functor $\smset\to\smset$, it sends each set $A$ to the set of finite lists (i.e.\ sequences) of elements in $A$, often denoted $\Set{List}(A)$ or $A^*$.
You may know this polynomial as the list monad, the free monoid (on a set), or the Kleene star.}
The first $\otimes$-monoid we'll discuss is commutative, while the second $\otimes$-monoid is not.
We will check that they satisfy the necessary conditions of an $\otimes$-monoid in \cref{exc.list_monoids}.

We have one $\otimes$-monoid carried by $\ell$ that is commutative, as follows: its unit specifies the position $1\in\nn$, and its multiplication is a map $\ell\otimes\ell \to \ell$ whose on-positions function $\cdot \colon \nn\times\nn \to \nn$ is standard integer multplication (e.g.\ $2\mdot3=6$) and whose on-directions function $\ord{mn} \to \ord{m} \times \ord{n}$ for each $(m,n)\in\nn$ is the canonical isomorphism between the set $\{1,\ldots,mn\}$ and the set $\{1,\ldots,m\}\times\{1,\ldots,n\}$.

We have another $\otimes$-monoid carried by $\ell$ that is not commutative, as follows: its unit specifies the position $\0\in\nn$, and its multiplication is a map $\ell\otimes\ell \to \ell$ whose on-positions function $+ \colon \nn\times\nn \to \nn$ is standard integer addition and whose on-directions function $\ord{m+n} \to \ord{m} \times \ord{n}$ is given by
\[
    d \mapsto \big(\min(d,m), \max(0,d-m)\big)
\]
for each $(m,n)\in\nn$.
For example, take the position $(m,n)\coloneqq(3,5)$; it is sent to position $8$. Given a direction $d \in \ord{8}$ there, it is distributed to a pair $(d_1, d_2)$ of directions $d_1 \in \ord{3}$ and $d_2 \in \ord{5}$.
% One can imagine a measuring cup with height 8 and a line at 3:
If $1 \leq d \leq 3$, then $d_1=d$ and $d_2=0$; if $3 \leq d \leq 8$, then $d_1=3$ and $d_2=d-5$.
\end{example}

\begin{exercise}\label{exc.list_monoids}
\begin{enumerate}
	\item Check that the first $\otimes$-monoid described in \cref{ex.list_monoids} really satisfies the necessary equations.
	\item Show that the first $\otimes$-monoid in \cref{ex.list_monoids} really is commutative.
	\item Give an interpretation of the swarm-like semantics of the first $\otimes$-monoid in \cref{ex.list_monoids}.
	\item Check that the second $\otimes$-monoid described in \cref{ex.list_monoids} really satisfies the necessary equations.
	\item Show that the second $\otimes$-monoid in \cref{ex.list_monoids} really is noncommutative.
	\item Give an interpretation of the swarm-like semantics of the second $\otimes$-monoid in \cref{ex.list_monoids}.
\qedhere
\end{enumerate}
\begin{solution}
**
\end{solution}
\end{exercise}

\begin{exercise}
Find a $\otimes$-monoid carried by the polynomial $\yon+\1$.
\begin{solution}
**
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.tensor_tensor_monoids}
If $m,n$ are the carriers of $\otimes$-monoids, then $m\otimes n$ naturally also carries a $\otimes$-monoid structure.
\end{proposition}

\begin{exercise}
Sketch a proof of \cref{prop.tensor_tensor_monoids}.
\begin{solution}
Given that $m, n$ carry $\otimes$-monoid structures, we give an $\otimes$-monoidal structure carried by $m\otimes n$.
In fact, the following works in any symmetric monoidal category.
The unit $\yon\to m\otimes n$ is given by the tensor of the units,
\[\yon\cong\yon\otimes\yon\To{\eta_m\otimes\eta_n}m\otimes n,\]
and the monoidal product is given by swapping (that's where the symmetry is used) and using the monoidal products
\[
(m\otimes n)\otimes(m\otimes n)\to (m\otimes m)\otimes(n\otimes n)\To{\mu_m\otimes\mu_n}m\otimes n.
\]
The laws are easy to verify.
\end{solution}
\end{exercise}

\begin{exercise}
Give a $\otimes$-monoid structure on $\nn\yon^\5$.
\begin{solution}
We give one of many examples of a $\otimes$-monoid structure on $\nn\yon^\5$.
We have that $(\nn, 0, +)$ is a monoid in $(\smset, \1, \times)$, so by \cref{ex.tensor_monoid_linear}, there is a $\otimes$-monoid whose carrier is $\nn\yon$, whose unit is the position $0$, and whose multiplication $\mu$ is given by $+$.
Meanwhile, by \cref{ex.tensor_monoid_rep}, $\yon^\5$ carries a canonical $\otimes$-monoid structure with a uniquely determined unit and a multiplication $\mu'$ that is the identity on positions and the diagonal on directions.
So we can apply \cref{prop.tensor_tensor_monoids} to obtain an $\otimes$-monoid whose carrier is $\nn\yon^\5$, whose unit is $0$, and whose multiplication is the map
\[
\nn\yon^\5 \otimes \nn\yon^\5 \iso (\nn\yon \otimes \yon^\5)\otimes(\nn\yon \otimes \yon^\5) \to (\nn\yon \otimes \nn\yon)\otimes(\yon^\5 \otimes \yon^\5) \To{\mu \otimes \mu'} \nn\yon \otimes \yon^\5 \iso \nn\yon^\5
\]
given by $+$ on positions and the diagonal on directions.
The subordinates report the sum of the natural number labels of their individual decisions to their supervisor, who selects one of $5$ options for all subordinates to select.

There are other solutions; for one thing, $(\nn, 0, +)$ is not the only monoidal structure on $\nn$ in $\smset$.
\end{solution}
\end{exercise}

\begin{proposition}[Free $\otimes$-monoid] \label{prop.free_parallel_monoid}
The forgetful functor from $\otimes$-monoids to $\poly$ has a left adjoint sending $p$ to a $\otimes$-monoid whose carrier is $\sum_{n \in \nn} p^{\otimes n}$.
\end{proposition}
\begin{proof}
By \cref{prop.day}, $\otimes$ distributes over coproducts, so the result follows from \cite[Chapter~VII, Theorem~2]{MacLane:1998a} (or \cite[Proposition~2.2]{Nlab:category_of_monoids}).
In particular, the unit of the free $\otimes$-monoid is the unique position of the summand $p^{\otimes 0} \iso \yon$ of $\sum_{n \in \nn} p^{\otimes n}$, while the multiplication has as its components the canonical inclusions
\[
    p^{\otimes i} \otimes p^{\otimes j} \iso p^{\otimes (i + j)} \to \sum_{n \in \nn} p^{\otimes n}
\]
for $i, j \in \nn$.
\end{proof}

\cref{prop.free_parallel_monoid} tells us that the carrier of the free $\otimes$-monoid on $p$ has as its positions elements of $\Set{List}(p(\1))$, i.e.\ lists of positions of $p$.
Given a position $\ell=(i_1,\ldots,i_n)$, the set of directions there is given by $\prod_{j\in\ord{n}} p[i_j]$; i.e., the directions of the carrier at a list of positions of $p$ are just directions of $p$ at each position in the list.

\begin{exercise}
Give an interpretation of the swarm-like semantics of the free $\otimes$-monoid on a polynomial $p\in\poly$.
\begin{solution}
We give an interpretation of the swarm-like semantics of the free $\otimes$-monoid on a polynomial $p\in\poly$.
From \cref{prop.free_parallel_monoid}, we know that the carrier of the free $\otimes$-monoid on $p$ has as its positions lists of positions of $p$.
So each subordinate faces a list of decisions of $p$, and these lists are all concatenated into a single list of decisions of $p$ that is sent to their supervisor.
For each decision of $p$ in the list, the supervisor selects an option, which is then sent back to the subordinate who asked about that decision.

% So a $p'$-dynamical system can readout any list $i'\coloneqq[i_1,\ldots,i_n]$ of $p$-positions as its output and given such a list it takes in a list $[d_1,\ldots,d_n]$ of directions at those positions. 

Put another way, if a larger box contains a bunch of smaller boxes as subordinates, the ``shorter'' lists of positions from the smaller boxes are concatenated to form the ``longer'' list of positions for the larger box.
Given a longer list of directions there, it is then chopped up into shorter lists of the appropriate lengths so that they can be dispersed back to the subordinates.
\end{solution}
\end{exercise}

%\begin{proposition}
%Let $\cat{O}$ be an operad (with one object), so that $\cat{O}_n$ denotes the set of $n$-ary operations for any $n\in\nn$. Then the polynomial $\sum_{n\in\nn}\cat{O}_n$ carries a $\otimes$-monoid structure.
%\end{proposition}

%---- Subsection ----%
\subsection{Bimorphic lenses}\label{subsec.poly.func_nat.prepare_dyn.bimorphic_lens}

Monomials are special polynomials: those of the form $A\yon^B$ for sets $A,B$. Here's a picture of $\5\yon^{\1\2}$:
\[
\begin{tikzpicture}
\node[draw, rounded corners, "$\5\yon^{\1\2}$"] {
	\begin{tikzpicture}[trees, sibling distance=1mm]
	\foreach \i in {1,...,5}
	{
    \node["\tiny \i" below] at (1.8*\i,0) {$\bullet$} 
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
    ;
	};
	\end{tikzpicture}
};
\end{tikzpicture}
\]

The formula for morphisms between these is particularly simple:
\begin{align*}
  \poly\left(A_1\yon^{B_1},\,A_2\yon^{B_2}\right) &\iso \prod_{a \in A_1} \sum_{a' \in A_2} B_1^{B_2} \tag*{\eqref{eqn.main_formula}} \\
  &\iso \smset(A_1, A_2 \times B_1^{B_2}) \\
  &\iso \smset(A_1,A_2)\times\smset(A_1\times B_2,B_1).
\end{align*}
It says that to give a morphism from one monomial to another, you just need to give two (non-dependent!) functions. Let's rewrite it to make those two functions explicit---they are the familiar on-positions and on-directions functions:
\[
  \poly\left(A_1\yon^{B_1},\,A_2\yon^{B_2}\right)
  \cong
  \left\{
    (f_1,f^\sharp)
  \;\middle|\;
  	\parbox{1.2in}{
    $
    \begin{aligned}
  	  f_1&\colon A_1\to A_2\\
  	  f^\sharp&\colon A_1\times B_2\to B_1
    \end{aligned}
    $
  }.
  \right\}
\]
Ordinarily, $f^\sharp$ is more involved: its type depends on the directions at each input position and output position of $f_1$.
But for monomials, every position has the same set of directions, so $f^\sharp$ is just a standard function.

The monomials in $\poly$ and the morphisms between them forms a full subcategory of $\poly$, and it has been called the \emph{the category of bimorphic lenses} \cite{hedges2018limits}. It comes up in functional programming. The functions $f_1, f^\sharp$ corresponding to a morphism $f \colon A_1\yon^{B_1}\to A_2\yon^{B_2}$ are given special names:
\begin{equation}\label{eqn.bimorphic_lens}
\begin{aligned}
	\text{get} \coloneqq f_1 &\colon A_1\to A_2\\
	\text{set} \coloneqq f^\sharp &\colon A_1\times B_2\to B_1
\end{aligned}
\end{equation}
The idea is that each position $a \in A_1$ of $A_1\yon^{B_1}$ ``gets'' a position $f_1(a) \in A_2$ of $A_2\yon^{B_2}$, and given $a \in A_1$, every direction at $f_1(a)$ in $B_2$ ``sets'' a direction back at $a$ in $B_1$.

We will call a polynomial morphism between two monomials a \emph{lens} when we want to remind the reader that morphisms between monomials are much simpler than those between arbitrary polynomials.

\begin{remark}
Consider the monomial $S\yon^S$. As an arena, the set of positions is $S$, and the set of directions at each position $s\in S$ is again just $S$.
In the language of decision-making, each $s \in S$ is a decision where the options you have to choose from are always just the decisions in $S$ again.
Notice that there is a natural way to string together a series of such decisions into a cycle: at each step, you start at some element of $S$, and the option you select is the element of $S$ that you will move to next.
We will formalize this idea in \cref{**}.

A lens (i.e.\ a polynomial map) $(\text{get, set})\colon S\yon^S\to T\yon^T$ is as usual a way to delegate decisions of $S\yon^S$ to decisions of $T\yon^T$.
When you need to make a decision at $s \in S$, you ask your friend at $\text{get}(s) \in T$ for help.
If your friend selects option $t \in T$, then you know to select option $\text{set}(s, t) \in S$.

But what happens when we string together these decisions into cycles?
Now you are moving between elements of $S$, looking to your friend for help at each step as they move between elements of $T$.
In this scenario, there are a few conditions that a lens $S\yon^S \to T\yon^T$ should satisfy to ensure that the associated delgation behaves well with respect to the movements of both you and your friend:
\begin{enumerate}
    \item If your friend chooses to stay put, then you should stay put, too.
    This is reflected by the equation
    \[
        \text{set}(s,\text{get}(s))=s.
    \]
    
    \item After your friend moves, and you move accordingly, you should delegate the decision at your new location to the decision at your friend's new location.
    This is reflected by the equation
    \[
        \text{get}(\text{set}(s,t))=t.
    \]

    \item If your friend moves to $t$, then to $t'$, the place where you end up should be where you would have ended up if your friend had moved directly to $t'$ in the first place.
    This is reflected by the equation
    \[
        \text{set}(\text{set}(s,t),t')=\text{set}(s,t')
    \]
\end{enumerate}
We will see these three conditions emerge from more general theory in \cref{**}.
\end{remark}

%-------- Section --------%
\section{Summary and further reading}

Thanks to Joachim Kock for telling us about the derivative $\dot{p}$ of a polynomial and the relationship between $\dot{p}(\1)$ and the total number of leaves of $p$.

\Closesolutionfile{solutions}

%-------- Section --------%
\section{Exercise solutions}
{\footnotesize
\input{solution-file2}}

\Opensolutionfile{solutions}[solution-file3]

%------------ Chapter ------------%
\chapter{Dynamical systems as polynomial~morphisms} \label{ch.poly.dyn_sys}

Let's start putting all this $\poly$ stuff to use. 

%-------- Section --------%
\section{Moore machines}

We begin with our simplest example of a dynamical system: a deterministic machine with internal states that can yield output and be updated according to input.

\begin{definition}\label{def.moore_machine}
If $A$, $B$, and $S$ are sets, an $(A,B)$-Moore machine with states $S$ consists of two functions
\begin{align*}
	\text{yield}&\colon S\to B\\
	\text{update}&\colon S\times A\to S 
\end{align*}
\end{definition}

We can visualize a Moore machine as a set $S$ of possible states.
At any point in time, the machine is in one of those states: say $s \in S$.
While there, whenever we ask the machine to produce output, it will give us $\text{yield}(s) \in B$.
But if we feed the machine input $a \in A$, the machine will switch to a new state, $\text{update}(s, a)$.

\begin{example}\label{ex.Moore_three}
Here's a picture of a Moore machine with $S\coloneqq\3$ states:
\[
\begin{tikzpicture}
	\node[draw] {
  \begin{tikzcd}[column sep=small]
  	\LMO{b_1}\ar[rr, dgreen, thick, bend left]\ar[loop left, thick, orange]&&
  	\LMO{b_2}\ar[ll, thick, orange, bend left]\ar[dl, bend left, thick, dgreen]\\&
  	\LMO{b_2} \ar[ul, thick, orange, bend left] \ar[loop left, thick, dgreen]
  \end{tikzcd}
  };
\end{tikzpicture}
\]
Each state is labeled by the value it yields, an element of $B\coloneqq\{b_1,b_2\}$. Each state has two outgoing arrows, one orange and one green, so $A\coloneqq\{{\color{orange}\text{orange}},{\color{dgreen}\text{green}}\}$.

You can imagine barking ``orange! orange! green! orange!'' etc. at this machine to make it run through its states.
\end{example}
%% ** Explain more, maybe? Add an exercise?

Does \cref{def.moore_machine} look familiar?
It's easy to see that an $(A,B)$-Moore machine with states $S$ is just a lens (i.e.\ map of monomials)
\[
S\yon^S\to B\yon^A,
\]
with $\text{get} \coloneqq \text{yield}$ and $\text{set} \coloneqq \text{update}$.
Given such a Moore machine, we will call the monomial $B\yon^A$ the \emph{interface}, because it encodes the possible inputs and outputs; and we will call the monomial $S\yon^S$ the \emph{state system}.

%% ** Exercise to turn the example above into a lens?

\begin{example}\label{ex.counting_trajectory}
There is a dynamical system that takes unchanging input and produces as output the sequence of natural numbers $0,1,2,3,...$. It is a Moore machine with states $\nn$ and interface $\nn\yon$. The associated polynomial map $\nn\yon^\nn\to\nn\yon$ is given by the identity $\nn \to \nn$ on positions and the function $\nn \iso \nn \times \1 \to \nn$ sending $n\mapsto n+1$ on directions.
% \[
% \begin{tikzpicture}[polybox, mapstos]
% 	\node[poly, dom] (s) {$n+1$\nodepart{two}$n$};
% 	\node[poly, cod, linear, right=of s] (p) {$\vphantom{1}$\nodepart{two}$n$};
% 	\draw (s_pos) to[first] (p_pos);
% 	\draw (p_dir) to[last] (s_dir);
% \end{tikzpicture}
% \]
\end{example}
%% ** Introduce polyboxes??

We can generalize both $S\yon^S$ and the interface $B\yon^A$ in \cref{def.moore_machine}, though the latter is much simpler---we'll spend much of the chapter on it. 
After some examples, we'll briefly preview something we have already hinted at and will spend a lot of time on later, namely what's special about $S\yon^S$.
Then we'll discuss how to model regular languages using Moore machines and how products of polynomials allow multiple interfaces to act on the same state system.

\begin{example}\label{ex.R2_moore}
Here's a(n infinite) Moore machine with states $\rr^2$:
\[
\rr^2\yon^{\rr^2}\to\rr^2\yon^{[0,1]\times[0,2\pi)}%]
\]
Its output type is $\rr^2$, which we might think of as a location in the 2-dimensional plane, and its input type is $[0,1]\times[0,2\pi)$, which we might think of as a command to move a certain distance in a certain direction. The map itself is given by the lens
\begin{align*}
  \rr^2&\To{\text{yield}}\rr^2&
  \rr^2\times[0,1]\times[0,2\pi)&\To{\text{update}}\rr^2
  	%\nonumber\\\label{eqn.r2moore}
  	\\
  (x,y)&\mapsto(x,y)&
  (x,y,r,\theta)&\mapsto(x+r\cos\theta, y+r\sin\theta)
\end{align*}
\end{example}

\begin{exercise}
Explain in words what the Moore machine in \cref{ex.R2_moore} does.
\begin{solution}
At any point in time, the Moore machine in \cref{ex.R2_moore} is located somewhere on the $2$-dimensional plane, say at the coordinates $(x, y) \in \rr^2$, as recorded by its state.
Whenever we ask the machine to produce output, it will tell us those coordinates, since $\text{yield}(x, y) = (x, y)$.
But if we give the machine input of the form $(r, \theta)$ for some distance $r \in [0,1]$ and direction $\theta \in [0, 2\pi)$, the machine will move by that distance, in that direction, going from $(x, y)$ to
\[
    \text{update}(x,y,r,\theta) = (x,y) + r(\cos\theta, \sin\theta)
\]
(here we treat $\rr^2$ as a vector space, so that $r(\cos\theta, \sin\theta)$ is a vector of length $r$ in the direction of $\theta$).
\end{solution}
\end{exercise}

\begin{example}[From functions to memoryless Moore machines]\label{ex.funs_to_moore}
For any function $f\colon A\to B$, there is a corresponding $(A,B)$-Moore machine with states $B$ that takes in a stream of $A$'s and outputs the stream of $B$'s obtained by applying $f$. 

It is given by the map $(\id_B,\texttt{const} \ f)\colon B\yon^B\to B\yon^A$. That is, it is the identity on positions, yielding the state directly as output, and on directions it is the function $B\times A\To{\pi}A\To{f} B$, which ignores the current state and applies $f$ to the input to compute the new state.

If the machine begins in state $b_0$ and is given a stream $(a_1,a_2,\ldots)$, the machine's outputs will be $(b_0,f(a_1),f(a_2),\ldots)$. We could say this machine is \emph{memoryless}, because at no point does the state of the machine depend on any previous states.
\end{example}

\begin{exercise}\label{exc.funs_to_moore}
Suppose we are given a function $f\colon A\times B\to B$.
\begin{enumerate}
	\item Find a corresponding $(A,B)$-Moore machine $B\yon^B\to B\yon^A$.
	\item Would you say the machine is memoryless?
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We seek an $(A,B)$-Moore machine $B\yon^B\to B\yon^A$ corresponding to the function $f\colon A\times B\to B$.
    We know that an $(A,B)$-Moore machine $B\yon^B \to B\yon^A$ consists of a function $\text{yield} \colon B \to B$ and a function $\text{update} \colon B \times A \to B$.
    So we can simply let the yield function be the identity on $B$ and the update function be $B \times A \iso A \times B \To{f} B$, i.e.\ the function $f$ with its inputs swapped.
    
    \item Generally, such a machine is not memoryless.
    Unlike in \cref{ex.funs_to_moore}, the update function $B \times A \iso A \times B \To{f} B$ does appear to depend on its first input, namely the previous state, which $f$ takes as its second input.
    
    However, if $f$ factors through the projection $\pi \colon A \times B \to A$, i.e.\ if $f$ can be written as a composite $A \times B \To{\pi} A \To{f'} B$ for some $f' \colon A \to B$, then the resulting machine \emph{is} memoryless: it is the memoryless Moore machine corresponding to $f'$, as in \cref{ex.funs_to_moore}.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Find $A,B\in\smset$ such that the following can be identified with a morphism $S\yon^S\to B\yon^A$, and explain in words what the corresponding Moore machine does (there may be multiple possible solutions):
\begin{enumerate}
	\item a \emph{discrete dynamical system}, i.e.\ a set of states $S$ and a transition function $S\to S$ that tells us how to move from state to state.
	\item a \emph{magma}, i.e.\ a set $S$ and a function $S\times S\to S$.
	\item a set $S$ and a subset $S'\ss S$.\qedhere
\end{enumerate}
\begin{solution}
For each of the following constructs, we find $A, B \in \smset$ such that the construct can be identified with a morphism $S\yon^S \to B\yon^A$, i.e.\ a function $\text{yield} \colon S \to B$ and a function $\text{update} \colon S \times A \to S$.
\begin{enumerate}
    \item Given a discrete dynamical system, consisting of a set of states $S$ and a transition funtion $n \colon S \to S$, we can set $A \coloneqq B \coloneqq \1$; so $\text{yield} \colon S \to \1$ is unique, while $\text{update} \colon S \times \1 \to S$ is given by $S \times 1 \iso S \To{n} S$.
    The corresponding Moore machine has a single choice of input (you could think of it as a button that says ``advance to the next state'') and always produces the same output (which effectively tells us nothing).
    So it is just a set of states, and a deterministic way to move from state to state.
    
    We could have also set $A \coloneqq \0$ and $B \coloneqq S$, so that $\text{yield} \coloneqq n$ and $\text{update} \colon S \times \0 \to S$ is unique, but this formulation is somewhat less satisfying: this is a Moore machine that never moves between its states, effectively functioning as a lookup table between whatever state the machine happens to be in and its output, which happens to refer to some state.
    
    \item Given a magma, consisting of a set $S$ and a function $m \colon S \times S \to S$, we can set $A \coloneqq S$ and $B \coloneqq 1$.
    Then $\text{yield} \colon S \to \1$ is unique, while $\text{update} \colon S \times S \to S$ is equal to $m$.
    The corresponding Moore machine always produces the same output.
    It uses the binary operation $m$ to combine the current state with the input---which also refers to a state---to obtain the new state.
    
    Note that, since $m$ is not guaranteed to be commutative, we could also set the update function to be $m$ with its inputs swapped.
    The difference here is that the new state is given by applying $m$ with the input on the left and the current state on the right, rather than the other way around.
    
    We could have also set $A \coloneqq \0$ and $B \coloneqq S^S$, so that $\text{update} \colon S \times \0 \to S$ is unique, while currying $m$ gives $\text{yield}$, so that $\text{yield}(s)$ is the function $S \to S$ given by $s' \mapsto m(s, s')$.
    Alternatively, $\text{yield}(s)$ could be the function $s' \mapsto m(s', s)$.
    Either way, this is a Moore machine that never moves between its states, functioning as a lookup table between the machine's current state and the function $m$ partially applied to that state on one side or the other.
    
    \item Given a set $S$ and a subset $S' \ss S$, we can set $A \coloneqq \0$ and $B \coloneqq \2$.
    Then $\text{update} \colon S \times \0 \to S$ is unique, while we define $\text{yield} \colon S \to 2$ by
    \[
        \text{yield}(s) =
        \begin{cases}
            1 & \text{if } s \in S' \\
            2 & \text{if } s \notin S',
        \end{cases}
    \]
    so that $S'$ can be recovered from the yield function as its fiber over $1$.
    Alternatively, we could define the yield function so that $S'$ is instead its fiber over $2$.
    The corresponing Moore machine never moves between its states, but gives one of two outputs indicating whether or not the current state is in the subset $S'$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Consider the Moore machine in \cref{ex.R2_moore}, and think of it as a robot. Using the terminology from that example, modify the robot as follows.

Add to its state a ``health meter,'' which has a real value between 0 and 10. Make the robot lose half its health whenever it moves to a location whose $x$-coordinate is negative. Do not output its health; instead, use its health $h$ as a multiplier, allowing it to move a distance of $hr$ given an input of $r$.
\begin{solution}
We modify the Moore machine from \cref{ex.R2_moore} as follows.
The original Moore machine had a state set $\rr^2$, so to add a health meter with values in $[0,10]$, we take the Cartesian product to obtain the new state set $\rr^2 \times [0,10]$.
The inputs and outputs are unchanged, so the Moore machine is a lens
\[
    \rr^2 \times [0,10] \yon^{\rr^2 \times [0,10]} \to \rr^2 \yon^{[0,1] \times [0,2\pi)}.
\]
Its yield function $\rr^2 \times [0,10] \to \rr^2$ is the canonical projection, as the machine only outputs its location in $\rr^2$ and not its health; while its update function
\[
    \rr^2 \times [0,10] \times [0,1] \times [0,2\pi) \to \rr^2 \times [0,10]
\]
sends $(x, y, h, r, \theta)$ to
\[
    (x + hr\cos\theta, y + hr\sin\theta, h'),
\]
where $h' = h/2$ if the machine's new $x$-coordinate $x + hr\cos\theta < 0$ and $h' = h$ otherwise.
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.file_reader}
Let's say a file of length $n$ is a function $f\colon\ord{n}\to\Set{ascii}$, where $\Set{ascii}\coloneqq\2\5\6$.
We refer to elements of $\ord{n}=\{1,\ldots,n\}$ as positions in the file and, for each position $i \in \ord{n}$, the value $f(i)$ as the character at position $i$.

Given a file $f$, make a robot (Moore machine) whose output type is $\Set{ascii} + \{\text{done}\}$
and whose input type is
\[
\{(s,t)\mid 1\leq s\leq t\leq n\}+\{\text{continue}\}.
\]
For any input, if it is of the form $(s,t)$, then the robot should go to position $s$ in the file and read the character at that position.
If the input is ``continue,'' the robot should move to the next position (i.e.\ from $s$ to $s+1$) and read that character—unless the new position would be greater than $t$, in which case the robot should continually output ``done'' until it receives another $(s,t)$ pair.
\begin{solution}
Given a file $f \colon \ord{n} \to \Set{ascii}$, we construct our file-reading robot as a Moore machine as follows.
There are many options for what states the machine should record, but we will use pairs of values $(i, t) \in \ord{n}^\2$, where $i$ tracks the robot's current position in the file, while $t$ tracks the position where the robot should stop.
But we will also include a ``done'' state to record when the robot has passed the stop position.
So the Moore machine is a lens
\[
    (\ord{n}^\2+\{\text{done}\})\yon^{\ord{n}^\2+\{\text{done}\}} \to (\Set{ascii} + \{\text{done}\})\yon^{\{(s,t)\mid 1\leq s\leq t\leq n\}+\{\text{continue}\}}.
\]
If the robot's current state is ``done,'' then the robot should output ``done.'' Otherwise, the robot should output the character at its current position in the file.
So its yield function $\ord{n}^\2+\{\text{done}\} \to \Set{ascii} + \{\text{done}\}$ sends $(i, t)$ to $f(i)$ and ``done'' to ``done.''
Meanwhile, the update function
\[
    (\ord{n}^\2+\{\text{done}\}) \times (\{(s,t)\mid 1\leq s\leq t\leq n\}+\{\text{continue}\}) \to \ord{n}^\2+\{\text{done}\}
\]
sends any state with input $(s,t)$ to the state $(s,t)$.
On the other hand, if the input is ``continue,'' an old state $(i,t)$ is sent to the new state $(i+1,t)$ if $i + 1 \leq t$ and ``done'' otherwise.
Finally, if the old state is ``done'' and the input is ``continue,'' the new state is still ``done.''
\end{solution}
\end{exercise}

While \cref{exc.file_reader} gives us a functioning file-reading robot, it is a little strange that we are still able to give the input ``continue'' even when the output is ``done,'' or input a new range of positions before the robot has finished reading from the previous range.
When we introduce generalized Moore machines, we will be able to let the robot ``close its port,'' so that it can't receive signals while it's busy reading, but open its port again once it's ``done''; see \cref{ex.generalized_file_reader}.

\begin{exercise}[Tape of a Turing machine]
A Turing machine has a tape. The tape has a position for each integer, and each position holds a value $v\in V=\{0,1,-\}$ of 0,1, or blank. At any given time the tape not only holds this function $f\colon\zz\to V$ from positions to values, but also a distinguished choice $c\in\zz$ of the ``current'' position. Thus the set of states of the tape is $V^\zz\times\zz$.

The Turing machine interacts with the tape by asking for the value at the current position, an element of $V$, and by telling it to change the value there as well as whether to move left (i.e.\ decrease the current position by $1$) or right (i.e.\ increase by $1$). Thus the set of outputs of the tape is $V$ and the set of inputs is $V\times\{L,R\}$.

\begin{enumerate}
	\item Give the form of the tape as a Moore machine, i.e.\ map of polynomials $t\colon S\yon^S\to p$ for appropriate $S\in S$ and $p\in\poly$.
	\item Write down the specific $t$ that makes it act like a tape as specified above.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item A Turing machine has states $V^\zz \times \zz$, outputs $V$, and inputs $V \times \{L, R\}$, so as a Moore machine, it is a map of polynomials
    \[
        t \colon (V^\zz \times \zz)\yon^{V^\zz \times \zz} \to V\yon^{V \times \{L,R\}}.
    \]
    \item The yield function of $t$ should output the value at the current position of the tape. So $\text{yield} \colon V^\zz \times \zz \to V$ is the evaluation map: it sends $(f,c)$ with $f \colon \zz \to V$ and $c \in \zz$ to $f(c)$.
    Meanwhile, the update function of $t$ should write the input value of $V$ at the current position of the tape, then move according to whether the second input value is $L$ (left) or $R$ (right).
    So
    \[
        \text{update} \colon (V^\zz \times \zz) \times (V \times \{L,R\}) \to V^\zz \times \zz
    \]
    sends old tape $f \colon \zz \to V$, old position $c \in \zz$, new value $v \in V$, and direction $D \in \{L,R\}$ to the new tape $f' \colon \zz \to V$ satisfying
    \[
        f'(n) =
        \begin{cases}
            v & \text{if } n = c \\
            f(n) & \text{if } n \neq c
        \end{cases}
    \]
    and the new position $c-1$ if $D=L$ and $c+1$ if $D = R$.
\end{enumerate}
\end{solution}
\end{exercise}

%---- Subsection ----%
\subsection{Regular languages}

Regular languages are very important in computer science. One way to express what they are is to say that they are exactly the languages recognizable by a deterministic finite state automaton. What is that?

\begin{definition}\label{def.dfa}
A \emph{deterministic finite state automaton} consists of
\begin{enumerate}
	\item a finite set $S$, elements of which are called \emph{states}
	\item a finite set $A$, elements of which are called \emph{input symbols},
	\item a function $u\colon S\times A\to S$, called the \emph{update function},
	\item an element $s_0\in S$, called the \emph{initial state},
	\item a subset $F\ss S$, called the \emph{accept states}.
\end{enumerate}
\end{definition}

\begin{proposition}
A deterministic finite state automaton with a set of states $S$ and a set of input symbols $A$ can be identified with a pair of maps
\[
\yon\to S\yon^S\to \2\yon^A.
\]
\end{proposition}
\begin{proof}
A map $\yon\to S\yon^S$ can be identified with an element $s_0\in S$.
A map $S\yon^S\to\2\yon^A$ consists of a function $S\to\2$, which can be identified with a subset $F \ss S$, together with a function $u \colon S\times A\to S$, forming the rest of the required structure.
\end{proof}

One could also make a version of this story where, whenever the machine hits an accept-state, it stops; again, to do this requires polynomials (in this case, $\yon^A+1$), which we'll get to in \cref{sec.dependent_systems}.

% ** More about regular languages, either in this section or the next. Maybe this section should just be called ``deterministic finite state automata.''}

%---- Subsection ----%
\subsection{Products: interfaces operating on the same system}

One thing we can use right away in our thinking about dynamical systems is products of polynomials.
Products give us a way to combine multiple polynomials into one, and in particular, the product of monomials is still a monomial.
So it should not come as a surprise that they allow us to combine multiple Moore machines.
What is interesting is that the behavior of this new Moore machine has a natural interpretation in terms of the behaviors of the original machines.

For any polynomials $s,p_1,p_2$ and maps $s\to p_1$ and $s\to p_2$, the universal property of products gives us a map $s\to p_1p_2$. In the context of Moore machines, we have the following.

\begin{proposition} \label{prop.moore_prod}
Suppose we have an $(A_1,B_1)$-Moore machine and an $(A_2,B_2)$-Moore machine, each with state set $S$. Then there is an induced $(A_1+A_2,B_1 B_2)$-Moore machine, again with state set $S$.
\end{proposition}
\begin{proof}
We are given maps of polynomials $S\yon^S\to B_1\yon^{A_1}$ and $S\yon^S\to B_2\yon^{A_2}$. Hence, by the universal property of products, we have a map
\[
  S\yon^S\to
  (B_1\yon^{A_1})\times(B_2\yon^{A_2})
  \cong(B_1B_2)\yon^{A_1+A_2},
\] 
as desired.
\end{proof}

\begin{corollary}
Given $n \in \nn$, suppose we have an $(A_i,B_i)$-Moore machine with state set $S$ for each $i \in \ord{n}$. Then there is an induced $\left(\sum_{i \in \ord{n}} A_i, \prod_{i \in \ord{n}} B_i\right)$-Moore machine, again with state set $S$.
\end{corollary}
\begin{proof}
The result follows from \cref{prop.moore_prod} by induction on $n$.
\end{proof}

By \cref{exc.poly_prod}, if each $(A_i,B_i)$-Moore machine has $\text{yield}_i$ as its yield function and $\text{update}_i$ as its update function, then the induced $\left(\sum_{i \in \ord{n}} A_i, \prod_{i \in \ord{n}} B_i\right)$-Moore machine has a yield function that sends $s \in S$ to the $n$-tuple $(\text{yield}_i(s))_{i \in \ord{n}}$ and an update function that sends $(s, a) \in S \times \sum_{i \in \ord{n}} A_i$ to $\text{update}_i(s, a)$ if $a \in A_i$.

In other words, if there are multiple interfaces that can drive the same set of states, we can view them as a single interface that drives the states together.
This single Moore machine can receive inputs from any one of the original machines' input sets and update its state accordingly; it then yields output in all of the original machines at once. It's as though each of the machines can see where the combined system is at any time, but only one of them can actually operate it at any given time.

\begin{example} \label{ex.labeled_transition}
Consider two four-state dynamical systems $e \colon 4\yon^4\to \rr\yon^{\{r,b\}}$ and $f \colon 4\yon^4\to \rr\yon^{\{g\}}$, each of which gives outputs in $\rr$; we think of $r,b,g$ as red, blue, and green, respectively. We can draw such morphisms as labeled transition systems, e.g.
\[
\begin{tikzpicture}
	\node[draw] (1) {
  \begin{tikzcd}[row sep=15pt]
  	\LMO{\pi}\ar[r, bend left=15pt, red]\ar[loop left=15pt, blue]&
  	\LMO{0}\ar[l, bend left=15pt, red]\ar[d, bend left=15pt, blue]\\
  	\LMO[under]{-1.41}\ar[u,bend left=15pt, red]\ar[r, bend right=15pt, blue]&
  	\LMO[under]{2.72}\ar[l, bend right=15pt, red]\ar[loop right=15pt, blue]
  \end{tikzcd}
	};
	\node[draw, right=of 1] {
  \begin{tikzcd}[row sep=15pt]
  	\LMO{-2}\ar[d, green!50!black]&
  	\LMO{4}\ar[l, green!50!black]\\
  	\LMO[under]{-8}\ar[loop left, green!50!black]&
  	\LMO[under]{16}\ar[ul, green!50!black]
  \end{tikzcd}
  };
 \end{tikzpicture}
\]

The universal property of products provides a unique way to put these systems together to obtain a morphism $4\yon^4\to(\rr\yon^{\{r,b\}}\times \rr\yon^{\{g\}})=(\rr^2)\yon^{\{r,b,g\}}$. With the examples above, it looks like this:
\[
\begin{tikzpicture}
	\node[draw] (1) {
  \begin{tikzcd}
  	\LMO{(\pi,-2)}\ar[r, bend left=15pt, red]\ar[loop left, blue]\ar[d, bend left=15pt, green!50!black]&
  	\LMO{(0,4)}\ar[l, bend left=15pt, red]\ar[d, bend left=15pt, blue]\ar[l, green!50!black]\\
  	\LMO[under]{(-1.41,-8)}\ar[u,bend left=15pt, red]\ar[r, bend right=15pt, blue]\ar[loop left, green!50!black]&
  	\LMO[under]{(2.72,16)}\ar[l, bend right=15pt, red]\ar[loop right=15pt, blue]\ar[ul, green!50!black]
  \end{tikzcd}
  };
\end{tikzpicture}
\]
Each state now gives two outputs: one according to the output function of $e$, and another according to the output function of $f$.
As for the possible inputs, we now have the option of giving either an input from the input set of $e$ (either $r$ or $b$), in which case the machine will update its state according to the update function of $e$, or an input from the input set of $f$ (only $g$), in which case the machine will update its state according to the update function of $f$.
\end{example}

\begin{exercise}[Toward event-based systems]
Let $f\colon S\yon^S\to B\yon^A$ be a Moore machine. It is constantly needing input at each time step. An event-based system is one that doesn't always get input, and only reacts when it does.

So suppose we want to allow our machine not to do anything. That is, rather than needing to press a button in $A$ at each time step, we want to be able to \emph{not} press any button, in which case the machine just stays where it is. We want a new machine $f'\colon S\yon^S\to p$ that has this behavior; what is $p$ and what is $f'$?
\begin{solution}
Given a Moore machine $f\colon S\yon^S\to B\yon^A$, we seek a new machine $f'\colon S\yon^S\to p$ that has the added option to provide no input at a step so that the machine does not change.
We can think of this as having two different interfaces acting on the same system: the original interface $B\yon^A$ of $f$, and a new interface with only one possible input---namely the option to provide no input at all---that does not change the system.
This latter interface also does not need to distinguish between its outputs; it should have just one possible output that says nothing.
So the interface we want to add on is $\yon$.

If $\yon$ were the only interface acting on the system, we would have a Moore machine $g \colon S\yon^S \to \yon$ whose yield function is the unique function $S \to \1$ and whose update function is the identity function on $S$, since the input never changes the system.
Then $p$ is the product of the two interfaces $B\yon^A$ and $\yon$, while $f' \colon S\yon^S \to p$ is the unique map induced by $f \colon S\yon^S \to B\yon^A$ and $g \colon S\yon^S \to \yon$.
In particular, $p \iso B\yon^{A + \1}$, while $f'$ consists of a yield function $S \to B$ that is the same as the yield function of $f$ and an update function $S \times (A + \1) \to S$ that behaves like the update function of $f$ when the input is from $A$ but does not change the state when the input is from $\1$.
\end{solution}
\end{exercise}

%---- Subsection ----%
\subsection{Parallel products: juxtaposing machines}

Another way to combine two polynomials is by taking their parallel product, as in \cref{subsec.poly.func_nat.prepare_dyn.par}.
As with the categorical product, the parallel product of monomials is still a monomial.
So parallel products give us another way to create new Moore machines from old ones by combining them. In fact it's about as easy as one could hope: you just multiply the set of states, multiply the set of inputs, and multiply the set of outputs.

\begin{proposition} \label{prop.moore_parallel_prod}
Suppose we have an $(A_1,B_1)$-Moore machine with state set $S_1$ and an $(A_2,B_2)$-Moore machine with state set $S_2$. Then there is an induced $(A_1 A_2,B_1 B_2)$-Moore machine with state set $S_1 S_2$.
\end{proposition}
\begin{proof}
We are given maps of polynomials $S_1\yon^{S_1}\to B_1\yon^{A_1}$ and $S_2\yon^{S_2}\to B_2\yon^{A_2}$, so we can take their parallel product to get a map
\[
  S_1 S_2\yon^{S_1 S_2} \cong (S_1 \yon^{S_1}) \otimes (S_2 \yon^{S_2}) \to
  (B_1\yon^{A_1})\otimes(B_2\yon^{A_2})
  \cong(B_1 B_2)\yon^{A_1 A_2},
\] 
as desired.
\end{proof}

Roughly speaking, the new system has inputs $A_1\times A_2$, outputs $B_1\times B_2$, and states $S_1\times S_2$. It just runs each of the two systems in parallel. We'll give the more general $n$-ary case and then explain it in detail.

\begin{corollary}
Given $n \in \nn$, suppose we have an $(A_i,B_i)$-Moore machine with state set $S_i$ for each $i \in \ord{n}$. Then there is an induced $\left(\prod_{i \in \ord{n}} A_i, \prod_{i \in \ord{n}} B_i\right)$-Moore machine with state set $\prod_{i \in \ord{n}} S_i$.
\end{corollary}
\begin{proof}
The result follows from \cref{prop.moore_parallel_prod} by induction on $n$.
\end{proof}

By \cref{prop.dirichlet_monoidal}, if each $(A_i,B_i)$-Moore machine has $\text{yield}_i$ as its yield function and $\text{update}_i$ as its update function, then the induced $\left(\prod_{i \in \ord{n}} A_i, \prod_{i \in \ord{n}} B_i\right)$-Moore machine has a yield function that sends the $n$-tuple of states $(s_i)_{i \in \ord{n}} \in \prod_{i \in \ord{n}} S$ to the $n$-tuple of outputs $(\text{yield}_i(s))_{i \in \ord{n}}$ and an update function that sends $((s_i)_{i \in \ord{n}}, (a_i)_{i \in \ord{n}}) \in S \times \prod_{i \in \ord{n}} A_i$ to $(\text{update}_i(s_i, a_i))_{i \in \ord{n}}$.
In other words, if there are multiple interfaces that drive the same set of states, we can view them as a single interface that drives the states together.
This single Moore machine can receive inputs from any one of the original machines' input sets and update its state accordingly; it then yields output in all of the original machines at once.

%** Example here!
% \begin{example} \label{ex.labeled_transition}
% Consider two four-state dynamical systems $e \colon 4\yon^4\to \rr\yon^{\{r,b\}}$ and $f \colon 4\yon^4\to \rr\yon^{\{g\}}$, each of which gives outputs in $\rr$; we think of $r,b,g$ as red, blue, and green, respectively. We can draw such morphisms as labeled transition systems, e.g.
% \[
% \begin{tikzpicture}
% 	\node[draw] (1) {
%   \begin{tikzcd}[row sep=15pt]
%   	\LMO{\pi}\ar[r, bend left=15pt, red]\ar[loop left=15pt, blue]&
%   	\LMO{0}\ar[l, bend left=15pt, red]\ar[d, bend left=15pt, blue]\\
%   	\LMO[under]{-1.41}\ar[u,bend left=15pt, red]\ar[r, bend right=15pt, blue]&
%   	\LMO[under]{2.72}\ar[l, bend right=15pt, red]\ar[loop right=15pt, blue]
%   \end{tikzcd}
% 	};
% 	\node[draw, right=of 1] {
%   \begin{tikzcd}[row sep=15pt]
%   	\LMO{-2}\ar[d, green!50!black]&
%   	\LMO{4}\ar[l, green!50!black]\\
%   	\LMO[under]{-8}\ar[loop left, green!50!black]&
%   	\LMO[under]{16}\ar[ul, green!50!black]
%   \end{tikzcd}
%   };
%  \end{tikzpicture}
% \]

% The universal property of products provides a unique way to put these systems together to obtain a morphism $4\yon^4\to(\rr\yon^{\{r,b\}}\times \rr\yon^{\{g\}})=(\rr^2)\yon^{\{r,b,g\}}$. With the examples above, it looks like this:
% \[
% \begin{tikzpicture}
% 	\node[draw] (1) {
%   \begin{tikzcd}
%   	\LMO{(\pi,-2)}\ar[r, bend left=15pt, red]\ar[loop left, blue]\ar[d, bend left=15pt, green!50!black]&
%   	\LMO{(0,4)}\ar[l, bend left=15pt, red]\ar[d, bend left=15pt, blue]\ar[l, green!50!black]\\
%   	\LMO[under]{(-1.41,-8)}\ar[u,bend left=15pt, red]\ar[r, bend right=15pt, blue]\ar[loop left, green!50!black]&
%   	\LMO[under]{(2.72,16)}\ar[l, bend right=15pt, red]\ar[loop right=15pt, blue]\ar[ul, green!50!black]
%   \end{tikzcd}
%   };
% \end{tikzpicture}
% \]
% Each state now gives two outputs: one according to the output function of $e$, and another according to the output function of $f$.
% As for the possible inputs, we now have the option of giving either an input from the input set of $e$ (either $r$ or $b$), in which case the machine will update its state according to the update function of $e$, or an input from the input set of $f$ (only $g$), in which case the machine will update its state according to the update function of $f$.
% \end{example}

%---- Subsection ----%
\subsection{Composing morphisms: wrapper interfaces} % migrating interfaces? overlaying interfaces?

\begin{example}[Paddling]\label{ex.paddler}
Consider a Moore machine with interface $\nn\yon$; its output could be any stream of natural numbers.
We may interpret each natural number as the machine's current location.
What if we don't want this machine to jump around wildly?
Instead, suppose we want to be very strict about what how far the machine can move and what makes it move.

To accomplish this, we introduce two intermediary interfaces, which we call the \emph{paddler} and the \emph{tracker}:%
\footnote{Perhaps one could refer to the tracker as the \emph{demiurge}; it is responsible for maintaining the material universe.}
\[
  \textit{paddler}\coloneqq\2\yon
  \qqand
  \textit{tracker}\coloneqq\nn\yon^\2
\]
The paddler has interface $\2\yon$ because it is blind (i.e.\ takes no inputs) and can only move (i.e.\ output) its paddle left or right: $\2\cong\{\text{left, right}\}$. The tracker has interface $\nn\yon^\2$ because it will announce the location of the machine (as an element $n\in\nn$) and watch the paddler's direction (as an element of $\2$). Their enclosure together
\[
\textit{tracker}\otimes\textit{paddler}\to\nn\yon
\]
whichi is a map $\2\yon\otimes\nn\yon^\2\cong\2\nn\yon^\2\to\nn\yon$ is the obvious rearrangement of the identity function $\nn\times\2\to\nn\times\2$. This lets us see the tracker's announcement (in $\nn$) and let's tracker see the paddler's direction.

Let's leave the paddler's dynamics alone---how you make that paddler behave is totally up to you---and instead we'll focus here on the dynamics of the tracker. We want it to watch for when the paddle switches from left to right or from right to left; at that moment it should push the paddler forward one unit. Thus the states of the tracker are given by $S\coloneqq\2\nn$, and its dynamics
\[\varphi\colon S\yon^S\to\nn\yon^\2\]
are given by the map given on elements $d,d'\in\2$ and $i\in\nn$ by the formula:
\[
  (d,i)\mapsto i,d'\mapsto
	\begin{cases}
		i&\tn{if }d=d'\\
		i+1&\tn{if }d\neq d'.
	\end{cases}
\]
As advertised, when the paddle switches, the tracker announces that the machine has moved forward one unit; when the paddle stays still, the tracker announces that the machine itself also stays still.
\end{example}

\begin{exercise}
Change the dynamics and state-set of the tracker in \cref{ex.paddler} so that it exhibits the following behavior.

When the paddle switches once and stops, the tracker increases its position by one unit and stops, as before in \cref{ex.paddler}. But when the paddle switches twice in a row, the tracker increases its position by two units on the second switch! So if it is quiet for a while and then switches three times in a row, the tracker will increase its position by one then two then two.
\begin{solution}
**
\end{solution}
\end{exercise}

%---- Subsection ----%
\subsection{Situations} % Fields? Enclosures?
Let $p$ be a polynomial. We will refer to a map $p\to\yon$ as a \emph{situation} for $p$, and denote the set of them by
\begin{equation} \label{eqn.gamma_def}
\Gamma(p)\coloneqq\poly(p,\yon)
\end{equation}
as we did in \cref{prop.adjoint_quadruple}.
By \eqref{eqn.main_formula}, we have that
\begin{equation} \label{eqn.gamma_prod}
    \Gamma(p) \iso \prod_{i \in p(\1)} p[i],
\end{equation}
so a situation $\gamma \in \Gamma(p)$ can be thought of as a dependent function that takes every position $i\in p(\1)$ and returns a direction $\gamma(i)\in p[i]$. The notation $\Gamma(p)$ comes from the common mathematical term ``global section''. But the idea for the name we use is that a situation $\gamma$ dictates what you'll see (the input you receive), given anything you might do (the output you provide). If the situation is that you're at a party, then that dictates what direction you'll receive, given what position you take. A situation is that which gives you input, no matter where you happen to be looking.

\begin{exercise}
Let $S\yon^S\to B\yon^A$ be a $(A,B)$-Moore machine.
\begin{enumerate}
	\item Is it true that a situation $\gamma\colon B\yon^A\to\yon$ can be identified with a function $A\to B$?
	\item What would a situation $\gamma\colon B\yon^A\to\yon$ represent semantically?
	\item Given such a $\gamma$, how do you interpret the composite $S\yon^S\to\yon$?
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
	\item No, it represents a function $B\to A$!
	\item A map $\gamma\colon B\yon^A\to\yon$, i.e.\ a function $B\to A$ represents a situation in which we automatically respond an input $A$ for each output $B$.
	\item Given such a $\gamma$, as well as our original Moore-machine $S\yon^S\to B\yon^A$, we interpret the composite $S\yon^S\to\yon$, i.e.\ function $S\to S$ as a function that sends each state to the updated state, based on how it outputs via the moore machine and the corresponding input via the situation.
\end{enumerate}
\end{solution}
\end{exercise}

A map $p_1\otimes \cdots \otimes p_n\to\yon$ puts these $n$ interfaces in a situation together. The following proposition helps explain why situations are a useful concept.

\begin{proposition}\label{prop.situations2}
Given polynomials $p,q\in\poly$, there is a bijection
\begin{equation} \label{eqn.situations2}
\Gamma(p\otimes q)\cong\smset\big(q(\1),\Gamma(p)\big)\;\times\;\smset\big(p(\1),\Gamma(q)\big).
\end{equation}
\end{proposition}
The idea is that for $p$ and $q$ to be enclosed together in a system simply means that each $p$-position gives a situation for $q$, and each $q$-position gives a situation for $p$. 
\begin{proof}[Proof of \cref{prop.situations2}]
This is a direct calculation:
\begin{align*}
	\Gamma(p\otimes q) &\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}(p[i]\times q[j]) \\
	&\iso
	\left(\prod_{j\in q(\1)}\prod_{i\in p(\1)}p[i]\right)\times
		 \left(\prod_{i\in p(\1)}\prod_{j\in q(\1)}q[j]\right) \\
	&\iso
	\smset(q(\1),\Gamma(p))\times\smset(p(\1),\Gamma(q)).
\end{align*}
\end{proof}

\begin{exercise}
\begin{enumerate}
	\item State and prove a generalization of \eqref{eqn.situations2} from \cref{prop.situations2} for $n$-many polynomials $p_1,\ldots,p_n\in\poly$.
	\item Generalize the ``idea'' statement between \cref{prop.situations2} and its proof.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We generalize \eqref{eqn.situations2} for $n$ polynomials as follows.
    Given polynomials $p_1,\ldots,p_n\in\poly$, we claim there is a bijection
    \[
        \Gamma\left(\bigotimes_{i=1}^n p_i \right) \iso \prod_{i=1}^n \smset\left(\prod_{\substack{1 \leq j \leq n, \\ j \neq i}} p_j(\1), \Gamma(p_i)\right).
    \]
    The $n=1$ case is clear, and the $n=2$ case is given by \eqref{eqn.situations2}.
    Then by induction on $n$, we have
    \begin{align*}
        \Gamma\left(\bigotimes_{i=1}^n p_i \right) &\iso \smset\left(p_n(\1), \Gamma\left(\bigotimes_{i=1}^{n-1} p_i \right)\right) \times \smset\left(\prod_{i=1}^{n-1} p_i(\1), \Gamma(p_n)\right) \tag*{\eqref{eqn.situations2}} \\
        &\iso \smset\left(p_n(\1), \prod_{i=1}^{n-1} \smset\left(\prod_{\substack{1 \leq j \leq n-1, \\ j \neq i}} p_j(\1), \Gamma(p_i)\right)\right) \times \smset\left(\prod_{i=1}^{n-1} p_i(\1), \Gamma(p_n)\right) \tag{Inductive hypothesis} \\
        &\iso \prod_{i=1}^{n-1} \smset\left(\prod_{\substack{1 \leq j \leq n, \\ j \neq i}} p_j(\1), \Gamma(p_i)\right) \times \smset\left(\prod_{i=1}^{n-1} p_i(\1), \Gamma(p_n)\right) \tag{Universal properties of products and internal homs},
    \end{align*}
    and the result follows.
    \item **
\end{enumerate}
\end{solution}
\end{exercise}

% todo: change where this goes
\begin{exercise}
We will use \eqref{eqn.situations2} to consider the interaction $\varphi$ between \textit{you} and \textit{chalk} from \cref{ex.pickup_chalk} as a pair of functions $\textit{you}(\1)\to\Gamma(\textit{chalk})$ and $\textit{chalk}(\1)\to\Gamma(\textit{you})$.
\begin{enumerate}
	\item How is the chalk's position a situation for you? That is, write the map $\textit{chalk}(\1)\to\Gamma(\textit{you})$.
	\item How is your position a situation for the chalk? That is, write the map $\textit{you}(\1)\to\Gamma(\textit{chalk})$.
\qedhere
\end{enumerate}
\begin{solution}
**
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.gamma_pres_coproduct}
The situations functor $\Gamma\colon\poly\to\smset\op$ sends $(0,+)$ to $(1,\times)$:
\[
	\Gamma(\0)\iso\1
	\qqand
	\Gamma(p+q)\iso\Gamma(p)\times\Gamma(q).
\]
\end{proposition}
Technically, one could say that $\Gamma$ preserves coproducts, since coproducts in $\smset\op$ are products in $\smset$.

\begin{exercise}
Prove \cref{prop.gamma_pres_coproduct}.
\begin{solution}
\cref{prop.gamma_pres_coproduct} follows directly from \cref{prop.poly_coprods}: we have that $\Gamma(\0) = \poly(\0,\yon) \iso \1$ since $\0$ is initial in $\poly$, and $\Gamma(p + q) = \poly(p+q,\yon) \iso \poly(p+q,\yon) = \Gamma(p) \times \Gamma(q)$ since $+$ gives coproducts in $\poly$. 
\end{solution}
\end{exercise}

The situations functor $\Gamma\colon\poly\to\smset\op$ is also normal lax monoidal in the sense that there are canonical functions
\[
	\1\cong\Gamma(\yon)
	\qqand
	\Gamma(p)\times\Gamma(q)\to\Gamma(p\otimes q)
\]
satisfying certain well-known laws. We won't prove this unless or until we need it.

% todo: explanation/example of what a situation means in terms of dynamical systems


% \subsubsection{The polynomial $S\yon^S$ as a comonad on $\smset$}\label{page.poly_comonad}

% A \emph{comonad} on $\smset$ is a functor $F\colon\smset\to\smset$, equipped with two natural transformations $\epsilon\colon F\to\id$ and $\delta\colon F\to F\circ F$, satisfying three equations. We don't need this now, so we won't get into it here. But we will note that every comonad comes from an adjunction, and the adjunction corresponding to $S\yon^S$ is
% \[
% \adj{\smset}{-\times S}{-^S}{\smset}
% \]
% the ``curry/uncurry'' adjunction. In functional programming, the comonad $S\yon^S$ is called the \emph{state comonad},%
% \footnote{The comonad $S\yon^S$ is sometimes called the \emph{store} comonad.} 
% and the elements of $S$ are called states. \niu{Are they??} It is no coincidence that we also refer to elements of $S$ as states. \niu{How does the interpretation of the store comonad as a data structure indexed by $S$ along with a ``current location'' element in $S$ actually relate to this?}

% Again, we will be \emph{very} interested in polynomial comonads later---as mentioned in \cref{prop.ahman_uustalu1}, they are exactly categories!!---but for now we move on to things we can use right away in our story about dynamical systems.%
% \footnote{If you're curious what category the comonad $S\yon^S$ corresponds to, it's the one with object set $S$ and a unique morphism $s_1\to s_2$ for every pair of objects $s_1,s_2\in S$.}

%---- Section ----%
\section{Dependent systems}\label{sec.dependent_systems}

Everything we've done above was for interfaces of the form $B\yon^A$, i.e.\ for monomials. But the theory works just as well for an arbitrary $p$, a sum of monomials.

\begin{definition}[Dependent systems]\label{def.gen_moore}
A \emph{dependent system} is a map of polynomials
\[S\yon^S\to p\]
for some $S\in\smset$ and $p\in\poly$. The set $S$ is called the set of \emph{states} and the polynomial $p$ is called the \emph{(poly-) interface}.
\end{definition}

We could also call these \emph{generalized Moore machines}, since Moore machines were seen to be given by the special case $p\coloneqq B\yon^A$ for sets $A,B$.
For the standard Moore machines that we have been working with, the set of inputs was always fixed; if the interface were $B\yon^A$, the set of inputs would always be $A$.

On the other hand, an arbitrary polynomial looks like $p\coloneqq\sum_{j \in J} B_j\yon^{A_j}$.
So the output of a dependent system with interface $p$ is an element of the coproduct $\sum_{j \in J} B_j$.
Then which $B_j$ the output is in determines the sort of input $A_j$ you get.
The set of inputs is no longer constant; it varies depending on the current output.
What kind of system has that kind of a relationship between its inputs and outputs?

Well, we can begin to think of outputs not only as outward expressions, but as positions that one takes within one's arena---terminology we've been using all along.
If the arena is your body, then your outputs would be the positions that your body could take.
This includes where you go, as well as the direction you're looking with your head and eyes, whether your lips are pursed or not, etc.
In fact, every form of output you provide, from talking to gesturing to moving another object, is performed by changing your position. And your position determines what inputs you'll notice: if your eyes are closed, your input type is different than if your eyes are open.

\slogan{The position you're in is itself a sensory organ: a hand outstretched, an eye open or closed.}

If we squint, we could even see an output more as a sensing apparatus than anything else. This is pretty philosophical, but imagine your outputs---what you say and do---are more there as a question to the world, a way of sensing what the world is like.

But however you think of dependent systems, we need to get a feel for how they work mathematically. Let's start with something familiar.

\begin{example}\label{ex.regular_lang_stop}
Recall regular languages from \cref{def.dfa}. Often one does not want to ``keep going'' after recognizing a word in one's language. For that, rather than use a map $S\yon^S\to \2\yon^A$, we could use a map
\[
(f_1,f^\sharp)\colon S\yon^S\to \yon^A+\1
\]
To give such a map, one in particular provides a function $f_1\colon S\to\2$; here we are thinking of $\2$ as the set of positions of $\yon^A+\1$. A function $f_1$ sends some elements of $S$ to $1$ and sends others to $2$; those that are sent to $2$ are said to be ``accepted.'' This is captured by the fact that there are no options in the term $\1$, no inputs available there. In other words, the function $f^\sharp$ is trivial there.

On the other hand, $f^\sharp_s$ is not trivial on those elements $s\in S$ for which $f_1(s)=1$. They must come equipped with a function $f^\sharp_s\colon A\to S$, saying how the machine updates on each element of $A$, starting at state $s$. Again, this is in line with the way state machines encode regular languages.
\end{example}

\begin{exercise}\label{exc.det_fsa_misc_398}
Consider the deterministic finite state automaton shown below:
\[
\begin{tikzcd}[column sep=small]
	\LMO{}\ar[rr, bend left, orange]\ar[loop left, dgreen]&&
	\LMO{}\ar[dl, bend left, orange]\ar[ll, dgreen, bend left]\\&
	\LMO{}
\end{tikzcd}
\]
The state without any outgoing arrows is the ``accept'' state for a regular language, and the left-most state is the start state. Answer the following questions, in keeping with the notation from \cref{ex.regular_lang_stop}.

\begin{enumerate}
	\item What is $S$?
	\item What is $A$?
	\item In terms of regular languages, what is the alphabet here?
	\item Specify the morphism of polynomials $S\yon^S\to\yon^A+1$.
	\item Name a word that is accepted by this machine.
	\item Name a word that is not accepted by this machine.
\qedhere
\end{enumerate}
\begin{solution}
**
\end{solution}
\end{exercise}

\begin{example}[Graphs] \label{ex.graph_dyn}
Given a graph $G = (E \tto V)$ with source map $s \colon E \to V$ and target map $t \colon E \to V$, there is an associated polynomial
\[
    g := \sum_{v \in V} \yon^{s\inv(v)}.
\]
We call this the \emph{emanation polynomial of $G$}.

The graph itself can be seen as a dynamical system $f \colon V\yon^V \to g$, where $f_1 = \id_V$ and $f^\sharp(v, e) = t(e)$.
\end{example}

\begin{exercise}
Pick your favorite graph $G$, and consider the associated dynamical system as in \cref{ex.graph_dyn}.
Draw the associated labeled transition system as in \cref{ex.labeled_transition}.
\begin{solution}
**
\end{solution}
\end{exercise}

\begin{example}[Adding a pause button]\label{ex.pause}
Given any dependent dynamical system $f\colon S\yon^S\to p$, we can ``add a pause button,'' meaning that for any state (and any position), we add an input that keeps the state where it is.

To do this, note that we have a map $\epsilon\colon S\yon^S\to\yon$ given by identity $\id_S\colon S\to S$ (see \cref{exc.pause}). By the universal property of products, we can pair $f$ and $\epsilon$ to get a map $(f,\epsilon)\colon S\yon^S\to p\yon\cong p\times \yon$. This process is actually universal in a way we'll find important later. Indeed, it's called \emph{copointing}; see \cref{prop.copointing}.
\end{example}

\begin{exercise}\label{exc.pause}
What does it mean in \cref{ex.pause} that the map $\epsilon\colon S\yon^S\to\yon$ is given by the identity $\id_S$?
\begin{solution}
A map of polynomials $\epsilon\colon S\yon^S\to\1\yon^\1$ consists of a function $\epsilon_1\colon S\to\1$, of which there is only one possible, and a function $\epsilon^\sharp\colon S\times\1\to S$. While $S\times\1$ is not literally equal to $S$, and hence $\epsilon^\sharp$ can't literally be the identity, there is a canonical isomorphism $S\times\1\cong S$ given by the functions $(s,1)\mapsto s$ and $s\mapsto (s,1)$. It is this canonical isomorphism that is meant here.
\end{solution}
\end{exercise}

\begin{example}[Repeater]
Suppose given a dependent dynamical system $\varphi$ that sometimes outputs an element of $A$ and sometimes outputs only silence. That is, its interface is $A\yon+\yon$. But what if some other system inputs only $A$'s; we will combine $\varphi$ with another system---a repeater---to get an $A$-emitter, i.e.\ a system with interface $A\yon$. How do we construct the repeater, and how do the two systems fit together?

For the repeater, we will dispense with any interesting interface and just use $A\yon^A$: it listens for an $A$ and outputs an $A$. We enclose the two systems in an $A$-emitter using a map
\[\psi\colon (A\yon+\yon)\otimes A\yon^A\to A\yon\]
which we now describe. Since $\otimes$ distributes over $+$, it suffices to give maps $A\yon\otimes A\yon^A\to A\yon$ and $\yon\otimes A\yon^A\to A\yon$. The former corresponds to the case that the system is currently outputting, and the second corresponds to when the system is silent. For the former we use the map
\[
A\yon\otimes A\yon^A\cong (A\yon\otimes\yon^A)\otimes A\yon\To{\epsilon\otimes\id}\yon\otimes A\yon
\]
In other words, we output the repeater's position as the current $A$, and we update the repeater's position to the system's current state.

For the latter it suffices---by the universal property of products---to give $A\yon^A\to\yon$ and $A\yon^A\to A$. For the first we use $\epsilon$, which means that during silence, the repeater holds the $A$ constant; for the second we use the product projection, which means that we output the current value of the repeater.
\end{example}

\begin{example}[Inputting a start state]
Suppose you have a closed system $f^\sharp\colon S\yon^S\to\yon$. The modeler can choose a start state $\yon\to S\yon^S$, but what if we want some other system to choose the start state? We haven't gotten to wiring diagrams yet, but the idea is to create a system that starts as not-closed---accepting as input a state $s\in S$---and then dives into its closed loop with that start state.

Let $S'\coloneqq S+\1$, so that the start state $\yon\to S'\yon^{S'}$ now is canonical: it's the new $\1$. We also have a canonical inclusion $S\To{i}S'$. We will give a morphism
\[
S'\yon^{S'}\to\yon+\yon^S
\]
that starts out with its outer box in the mode $\yon^S$ of accepting an $S$-input, and then moves to the mode $\yon$ so that it is a closed system forever after.

To give a morphism $S'\yon^{S'}\to\yon+\yon^S$, it is sufficient to give two morphisms: $S\yon^{S'}\to\yon$ and $\yon^{S'}\to\yon^S$. The first is equivalent to a function $S\to S'$ and we take the map $S\To{f^\sharp}S\To{i}S'$; this means that whenever we want to update the state from a state in $S$ we'll just do whatever our original closed system did. The second is also equivalent to a function $S\to S'$ and we use $i$; this means that whatever state is input at the beginning will be what we take as our first noncanonical state.
\end{example}

\begin{exercise}
Find what you think is an interesting generalization of deterministic finite state automata that you can model using generalized Moore machines.
\begin{solution}
**
\end{solution}
\end{exercise}

\begin{example}\label{ex.movement_options}
Choose $n\in\nn$, a \emph{grid size}, and for each $i\in\ord{n}=\{1,\ldots,n\}$ let $d(i)$ be the set
\[
	d(i)\coloneqq
	\begin{cases}
		\{0,1\}&\tn{ if }i=1\\
		\{-1,0\}&\tn{ if }i=n\\
		\{-1,0,1\}&\tn{ if } 1<i<n
	\end{cases}
\]
So $d(i)$ is the set of directions someone could move (or not move) if at position $i$.
\[
\begin{tikzpicture}[scale=.5]
  \draw[step=1cm,gray,very thin] (-3,-3) grid (4,4);
	\draw[->, red ] (-2.5,3.5) -- (-1.5, 3.5);
	\draw[->, red ] (-2.5,3.5) -- (-1.5, 2.5);
	\draw[->, red ] (-2.5,3.5) -- (-2.5, 2.5);
	\draw[->, blue] (1.5, 0.5) -- (0.5, 0.5);
	\draw[->, blue] (1.5, 0.5) -- (2.5, 0.5);
	\draw[->, blue] (1.5, 0.5) -- (1.5, 1.5);
	\draw[->, blue] (1.5, 0.5) -- (1.5,-0.5);
	\draw[->, blue] (1.5, 0.5) -- (0.5, 1.5);
	\draw[->, blue] (1.5, 0.5) -- (0.5,-0.5);
	\draw[->, blue] (1.5, 0.5) -- (2.5, 1.5);
	\draw[->, blue] (1.5, 0.5) -- (2.5,-0.5);
\end{tikzpicture}
\]
Then a generalized Moore machine of the form $S\yon^S\to p$, where
\[
p= \sum_{(i,j)\in\ord{n}\times\ord{n}}\yon^{d(i)\times d(j)},
\]
is one that has more movement options when it is in the center of the grid than when it is on the sides or corners.
\end{example}

\begin{exercise}
Add to \cref{ex.movement_options} as follows.
\begin{enumerate}
	\item Redefine $p$ so that at each grid value, the robot can receive not only the set of directions it can move in but also a ``reward value'' $r\in\rr$. 
	\item Define the set $S$ of robot states so that an element $s\in S$ includes both the robot's position and a list of all reward values so far.
	\item Define a morphism of polynomials $S\yon^S\to p$ in a way that respects positions and properly updates the robots list of rewards, but otherwise does anything you want.
\qedhere
\end{enumerate}
\begin{solution}
**
\end{solution}
\end{exercise}

\begin{example}\label{ex.generalized_file_reader}
In \cref{exc.file_reader} one is tasked with making a file reader $\Sys{FileReader}$, where a file is a function $f\colon\ord{n}\to\Set{ascii}$. Now we take that same idea but give the robot a different interface when it is in read-mode: namely, one where it cannot take in signals.

Let $\State{FileReader} \coloneqq \{(s,t)\mid 1\leq s\leq t\leq n\}$ consist of a
current position $s$ and a terminal position $t$. For our interface, we'll have
two modes, each of which yields an ascii character:
\[\Out{FileReader} = \present{ \fun{Accepting}(c),\, \fun{Busy}(c) \mid c \in \Set{ascii} }
\]
%%
%% Jaz: In Haskell, this would read
%%      data \Out{FileReader} = Accepting ascii | Busy ascii
%%
For our input, we need a family $\In{FileReader} \colon \Out{FileReader} \to \smset$.
We'll define this by cases:
\begin{align*}
  \In{FileReader}(\fun{Accepting}(c)) &= \State{FileReader}, \\
  \In{FileReader}(\fun{Busy}(c)) &= \ord{1}.
\end{align*}

Our file reader will be $\fun{Accepting}$ if its current position is the
terminal position; otherwise, it will be $\fun{Busy}$. In either case, it will
yield the ascii character at the current position.
\begin{align*}
  \yield{FileReader}(s, t) = \begin{cases} \fun{Accepting}(f(s)) &\mbox{if $s = t$}\\ \fun{Busy}((f(s)))  \end{cases}
\end{align*}

While the file reader is $\fun{Busy}$, it will step forward through the file.
When it is $\fun{Accepting}$, it will set its new current and terminal position
to be the input. 
\begin{align*}
  \update{FileReader}(s, t) = \begin{cases} \_ \mapsto (s + 1, t) &\mbox{if $\yield{FileReader}(s, t)$ is $\fun{Busy}$} \\ (s', t') \mapsto (s', t') &\mbox{if $\yield{FileReader}(s, t)$ is $\fun{Accepting}$}  \end{cases}
\end{align*}

Let $A\coloneqq\{(s,t)\mid 1\leq s\leq t\leq n\}$, and let $p\coloneqq \Set{ascii}\mdot\yon^A+\Set{ascii}\mdot\yon^\1$; we construct a morphism in $\poly$
\[
(r_1,r^\sharp)\colon A\yon^A\to \Set{ascii}\mdot\yon^A+\Set{ascii}\mdot\yon^\1
\]
as follows.

\[A + B = \langle \fun{inl}(a), \fun{inr}(b) \mid a \in A, b \in B \rangle\]

\[A +_C B = \langle \fun{inl}(a), \fun{inr}(b) \mid a \in A, b \in B, \forall c
\in C.
\fun{inl}(f(c)) = \fun{inr}(g(c)) \rangle\]


On positions define
\[
	r_1(s,t)\coloneqq
	\begin{cases}
		\const{inl}\ f(s)&\tn{ if } s=t\\
		\const{inr}\ f(s)&\tn{ if } s<t
	\end{cases}
\]

\[
	r^\sharp_{(t,t)}(s',t')\coloneqq (s',t')
	\qquad
	r^\sharp_{(s,t)}(1)\coloneqq (s+1,t)
\]
\end{example}

\begin{exercise}
Make a file reader that acts like that in \cref{ex.generalized_file_reader}, except that it only emits output $o\in\Set{ascii}$ when $o=100$.
\begin{solution}
**
\end{solution}
\end{exercise}

\begin{example}[Picking up the chalk]\label{ex.pickup_chalk}
Imagine you are at a table; you see some chalk and you pinch it between your thumb and forefinger. An amazing thing about reality is that you will then have the chalk, in the sense that you can move it around. How might we model this in $\poly$?

Let's say that your hand can be at one of two heights, down or up, and that you can either press (apply pressure between your thumb and forefinger) or not press. Let's also say that you take in information about the chalk's height. Here are the two sets we'll be using:
\[
	H\coloneqq\{\text{down, up}\}
	\qqand
	P\coloneqq\{\text{press, no press}\}.
\]
Your interface is $HP\yon^H$: producing your own height and pressure, and receiving the chalk's height.

As for the chalk, it is in one of two modes: in your possession or not. Either way, it reveals its height, which is either down on the table or up in the air. The chalk always takes in information about whether pressure is being applied or not. When it's out of your possession (mode ``out''), that's the whole story, but when it is in your possession (mode ``in'') it also receives your hand's height. All together, here are the two interfaces:
\[
	\textit{you}\coloneqq HP\yon^H
	\qqand
	\textit{chalk}\coloneqq \{\text{out}\}H\yon^P + \{\text{in}\}H\yon^{HP}.
\]

Now we want to give the interaction between you and the chalk.
As we said before, you see the chalk's height.
If your hand is not at the height of the chalk, the chalk receives no pressure.
Otherwise, your hand is at the height of the chalk, so the chalk receives your pressure (or lack thereof).
Furthermore, if the chalk is in your possession, it also receives your hand's height. 

To provide a map $\textit{you}\otimes\textit{chalk}\To{\varphi}\yon$, we use the fact that $\textit{chalk}$ is a sum and that $\otimes$ distributes over $+$. Thus we need to give two maps
\[
	HP\yon^H\otimes H\yon^P\To{\psi}\yon
	\qqand
	HP\yon^H\otimes H\yon^{HP}\To{\psi'}\yon
\]
The map $\psi'$, corresponding to when the chalk is in your possession, is quite easy to describe; it can be unfolded to a function
$HPH\to HHP$, and we take it to be the obvious map sending your height and pressure to the chalk and the chalk's height to you; see \cref{exc.pickup_chalk}. But $\psi$ is more semantically interesting: it is given by the map
\[
  ((h_\textit{you},p_\textit{you}),h_\textit{chalk})\mapsto
  \begin{cases}
  	(h_\textit{chalk},\text{no press}) & \tn{ if } h_\textit{you} \neq h_\textit{chalk} \\
  	(h_\textit{chalk},p_\textit{you}) & \tn{ if } h_\textit{you} = h_\textit{chalk}.
  \end{cases}
\]

So now we've got you and the chalk in a closed system together, given by $\varphi=\psi+\psi'$, so we are ready to add some dynamics. Your dynamics can be whatever you want, so let's just add some dynamics to the chalk and call it a day. The chalk has only four states $C\coloneqq \{\text{out}, \text{in}\} \times H \cong\4$: the $H$ coordinate is its current height, and the other coordinate is whether or not it is ``in your possession.'' We will give a dynamical system $C\yon^C\to\textit{chalk}$ with states $C$ and chalk-interface, i.e.\ a map
\begin{equation}\label{eqn.chalk_dynamics}
	\{\text{out}, \text{in}\} \times H\yon^{\{\text{out}, \text{in}\} \times H}\to \{\text{out}\}H\yon^P + \{\text{in}\}H\yon^{HP}.
\end{equation}
As you might guess, the chalk reveals its height and whether it is in your possession directly. If it's not in your possession, it falls down unless you catch it (i.e.\ apply pressure to it so that it enters your possession); if it is in your possession, it takes whatever height you give it. This is all expressed by the following dynamics, which define \eqref{eqn.chalk_dynamics}:
\begin{align*}
  (\text{out}, h_\textit{chalk})&\mapsto
  	\left(\text{out}, h_\textit{chalk},
		\begin{aligned}
			\text{no press} &\mapsto (\text{out, down}) \\
			\text{press} &\mapsto (\text{in}, h_\textit{chalk})
		\end{aligned}\right)\\
	(\text{in}, h_\textit{chalk})&\mapsto
		\left(\text{in},h_\textit{chalk},
		\begin{aligned}
			(h_\textit{you}, \text{no press}) &\mapsto (\text{out}, h_\textit{you}) \\
			(h_\textit{you}, \text{press}) &\mapsto (\text{in}, h_\textit{you})
		\end{aligned}\right)
\end{align*}
Obviously, this is all quite complicated, intricate, and contrived. Our goal here is only to show that you can define interactions in which one system can engage with or disengage from another, and that when the two systems are engaged, the first controls the behavior of the second.
\end{example}

\begin{exercise}\label{exc.pickup_chalk}
\begin{enumerate}
	\item In \cref{ex.pickup_chalk}, we said that $\psi'\colon HP\yon^H\otimes H\yon^{HP}\to\yon$ was easy to describe and given by a function $HPH\to HHP$. Explain what's being said, and provide the function.
	\item Provide dynamics to the \textit{you} character so that you repeatedly reach down and grab the chalk, lift it with your hand, and drop it. 
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item A morphism $HP\yon^H \otimes H\yon^{HP} \to \yon$ consists of an on-positions function $HPH \to \1$ and an on-directions function $HPH \times \1 \to HHP$.
    This amounts to a function $HPH \to HHP$.
    We can easily define this function to be the isomorphism that sends $(h,p,h') \in HPH$ to $(h,h',p)$.
    \item To model the way in which you cycle through three possible actions---reaching down and grabbing the chalk, lifting it with your hand, and dropping it---it is simplest to work with a set of $\3$ possible states.
    So we will give your dynamics as a polynomial morphism $\3\yon^\3 \to HP\yon^H$, where the yield function $\3 \to HP$ indicates what happens at each state, sending $1 \mapsto (\text{down, press}), 2 \mapsto (\text{up, press})$, and $3 \mapsto (\text{up, no press})$.
    Then the update function $\3H \to \3$ always goes to the next state, regardless of input: it ignores the $H$ coordinate and sends $1$ to $2$, $2$ to $3$, and $3$ to $1$.
\end{enumerate}
\end{solution}
\end{exercise}

%---- Section ----%
\section{Wiring diagrams}

We want our dynamical systems to interact with each other.
\begin{equation}\label{eqn.control_diag}
\begin{tikzpicture}[oriented WD, baseline=(B)]
	\node[bb={2}{1}, fill=blue!10] (plant) {\texttt{Plant}};
	\node[bb={1}{1}, below left=-1 and 1 of plant, fill=blue!10]  (cont) {\texttt{Controller}};
	\node[circle, inner sep=1.5pt, fill=black, right=.1] at (plant_out1) (pdot) {};
	\node[bb={0}{0}, inner ysep=25pt, inner xsep=1cm, fit=(plant) (pdot) (cont)] (outer) {};
	\coordinate (outer_out1) at (outer.east|-plant_out1);
	\coordinate (outer_in1) at (outer.west|-plant_in1);
	\begin{scope}[above, font=\footnotesize]
  	\draw (outer_in1) -- node {$A$} (plant_in1);
  	\draw (cont_out1) to node (B) {$B$} (plant_in2);
  	\draw (plant_out1) to node {$C$} (outer_out1);
  	\draw
  		let 
  			\p1 = (cont.south west-| pdot),
  			\p2 = (cont.south west),
  			\n1 = \bby,
  			\n2 = \bbportlen
  		in
  			(pdot) to[out=0, in=0]
  			(\x1+\n2, \y1-\n1) --
  			(\x2-\n2, \y2-\n1) to[out=180, in=180]
  			(cont_in1);
		\end{scope}
	\node[below=0of outer.north] {\texttt{System}};
\end{tikzpicture}
\end{equation}
In this picture the plant is receiving information from the world outside the system, as well as from the controller. It's also producing information for the outside world which is being monitored by the controller.

There are three boxes shown in \eqref{eqn.control_diag}: the controller, the plant, and the system. Each has inputs and outputs, and so we can consider the interface as a monomial.
\begin{equation}\label{eqn.basic_diagram}
	\const{Plant}=C\yon^{AB}
	\qquad\quad
	\const{Controller} = B\yon^C
	\qquad\quad
	\const{System} = C\yon^A.
\end{equation}

The wiring diagram itself is a morphism in $\poly$ of the form
\[
	w\colon\const{Plant}\otimes\const{Controller}\to\const{System}
\]
Since everything involved is a monomial---the parallel product of monomials is a monomial---the whole wiring diagram $w$ is a lens $CB\yon^{ABC}\to C\yon^A$. This morphism says how wires are feeding from outputs to inputs. Like all lenses, it consists of two functions
\[
  \text{get}\colon CB\to C
  \qqand
  \text{set}\colon CBA\to ABC
\]
The first says ``inside the system you have boxes outputting values of type $C$ and $B$. The system needs to produce an output of type $C$; how shall I obtain it?'' The second says ``the system is providing an input value of type $A$, and inside the system you have boxes outputting values of type $C$ and $B$. These boxes need input values of type $A$, $B$, and $C$; how shall I obtain them?'' The answer of course is that \emph{get} is given by projection $(c,b)\mapsto c$ and \emph{set} is given by a permutation $(c,b,a)\mapsto (a,b,c)$. The wiring diagram is a picture that tells us which maps to use.

\begin{exercise}
\begin{enumerate}
	\item Make a new wiring diagram like \eqref{eqn.control_diag} except where the controller also receives information of type $A'$ from the outside world.
	\item What are the monomials in your diagram (replacing \eqref{eqn.basic_diagram})?
	\item What is the morphism of polynomials corresponding to this diagram?
\qedhere
\end{enumerate}
\begin{solution}
**
\end{solution}
\end{exercise}

Now suppose given a dynamical system in each inner box:
\[
S\yon^S\To{f}\const{Plant}
\qqand
T\yon^T\To{g}\const{Controller}
\]
Then since $\otimes$ is a monoidal product on $\poly$ (see \cref{prop.dirichlet_monoidal}), we get a map
\[
S\yon^S\otimes T\yon^T\To{f\otimes g}\const{Plant}\otimes\const{Controller}
\]
In other words we have a morphism of polynomials $ST\yon^{ST}\to \const{Plant}\otimes\const{Controller}$; that's a new dynamical system with state space $ST=(S\times T)$; a state in it is just a pair of states, one in $S$ and one in $T$. Furthermore our wiring diagram already gave us a map $\const{Plant}\otimes\const{Controller}\to\const{System}$, so combining, we have a new system
\[
ST\yon^{ST}\to\const{System}.
\]

\begin{exercise}
Consider the following wiring diagram.
\[
\begin{tikzpicture}[oriented WD, font=\footnotesize, bb port sep=1, bb port length=2.5pt, bb min width=.4cm, bby=.2cm, inner xsep=.2cm, x=.5cm, y=.3cm, text height=1.5ex, text depth=.5ex]
   	\node[bb={2}{1}, fill=blue!10] (Trf) {$\const{Alice}$};
  	\node[bb={1}{2}, fill=blue!10, below=1 of Trf] (Trg) {$\const{Bob}$};
		\node[bb={2}{2}, fill=blue!10] at ($(Trf)!.5!(Trg)+(1.5,0)$) (Trh) {$\const{Carl}$}; 
  	\node[bb={0}{0}, fit={($(Trf.north west)+(-.25,4)$) (Trg) ($(Trh.north east)+(.25,0)$)}] (Tr) {};
		\node[below] at (Tr.north) {$\const{Team}$};
  	\node[coordinate] at (Tr.west|-Trf_in2) (Tr_in1) {};
  	\node[coordinate] at (Tr.west|-Trg_in1) (Tr_in2) {};
  	\node[coordinate] at (Tr.east|-Trh_out2) (Tr_out1) {};
  	\node at ($(Trg_out2)+(5pt,0)$) (dot) {$\bullet$};
\begin{scope}[font=\tiny]
  	\draw[shorten <=-2pt] (Tr_in1) -- node[below=-3pt] {$A$} (Trf_in2);
  	\draw[shorten <=-2pt] (Tr_in2) -- node[below=-3pt] {$B$} (Trg_in1);
		\draw (Trf_out1) to node[above=-3pt] {$D$} (Trh_in1);
		\draw (Trg_out1) to node[above=-3pt] {$E$} (Trh_in2);
  	\draw (Trg_out2) -- node[below=-3pt] {$F$} (dot.center);
  	\draw[shorten >=-2pt] (Trh_out2) -- node[below=-3pt] {$G$} (Tr_out1);
  	\draw let \p1=(Trh.east), \p2=(Trf.north west), \n1=\bbportlen, \n2=\bby in
  		(Trh_out1) to[in=0] (\x1+\n1,\y2+\n2) -- node[pos=.3, below=-3pt] {$H$} (\x2-\n1,\y2+\n2) to[out=180] (Trf_in1);
	\end{scope}
\end{tikzpicture}
\]
\begin{enumerate}
	\item Write out the polynomials for each of Alice, Bob, and Carl.
	\item Write out the polynomial for the outer box, Team.
	\item The wiring diagram constitutes a morphism $f$ in $\poly$; what is its type $f\colon ?\to ?$
	\item What morphism is it?
	\item Suppose we are given dynamical systems $A\yon^A\to\const{Alice}$, $B\yon^B\to\const{Bob}$, and $C\yon^C\to\const{Carl}$. What is the induced dynamical system on $\const{Team}$?
\qedhere
\end{enumerate}
\begin{solution}
**
\end{solution}
\end{exercise}

\begin{exercise}[Long division]
\begin{enumerate}
	\item Come up with a function ``divmod'' of type $\nn\times\nn_{\geq1}\to\nn\times\nn$ and which, for example, sends $(10,7)$ to $(1,3)$ and $(30,7)$ to $(4,2)$.
	\item Use \cref{exc.funs_to_moore} to turn it into a dynamical system.
	\item Interpret the following wiring diagram:
\[
\begin{tikzpicture}[oriented WD, bb small]
	\node[bb port sep=3, fill=blue!10, bb={2}{2}] (divmod) {divmod};
	\node[bb={0}{1}, fill=blue!10, left=of divmod_in2] (7) {$7$};
	\node[bb port sep=2, bb={2}{1}, fill=blue!10, below right=-1 and 3 of divmod_out2] (times) {$*$};
	\node[bb={0}{1}, fill=blue!10, below left=-1 and 1 of times_in2] (10) {$10$};
	\node[bb={0}{0}, inner xsep=\bbx, fit=(divmod) (times)(7) (10)] (outer) {};
	\coordinate (outer_in1) at (outer.west|-divmod_in1);
	\coordinate (outer_out1) at (outer.east|-divmod_out1);
	\coordinate (outer_out2) at (outer.east|-times_out1);
	\draw (outer_in1) -- (divmod_in1);
	\draw (7_out1) -- (divmod_in2);
	\draw (10_out1) -- (times_in2);
	\draw (divmod_out1) -- (outer_out1);
	\draw (divmod_out2) to (times_in1);
	\draw (times_out1) -- (outer_out2);
\end{tikzpicture}
\]
	\item Use the above and a diagram of the following form to create a function that spits out the base-10 digits of $1/7$.
\[
\begin{tikzpicture}[oriented WD]
	\node[bb={1}{2}, fill=blue!10] (inner) {};
	\node[bb={0}{0}, inner xsep=1cm, inner ysep=1cm] (outer) {};
	\coordinate (outer_out1) at (outer.east|-inner_out1);
	\draw[shorten >=-3pt] (inner_out1) -- (outer_out1);
	\draw 
		let \p1=(inner.south east), \p2=(inner.south west), \n1=\bbportlen, \n2=\bby in
		(inner_out2) to[in=0] (\x1+\n1,\y1-\n2) -- (\x2-\n1,\y1-\n2) to[out=180] (inner_in1);
		\node[right, font=\footnotesize] at (outer_out1) {$0.142857142857142857\cdots$};
\end{tikzpicture}
\]
\end{enumerate}
\begin{solution}
**
\end{solution}
\end{exercise}

\begin{example}[Graphs as interaction diagrams]\label{ex.graph_interaction}
Suppose given a graph $G$ and a set $\tau(v)$ for every vertex
\[
\begin{tikzcd}
	A\ar[r, shift left=3pt, "\src"]\ar[r, shift right=3pt, "\tgt"']&
	V\ar[r, "\tau"]&
	\smset
\end{tikzcd}
\]
For each vertex $v\in V$, let $A_v\coloneqq\src\inv(v)\ss A$ denote the arrows emanating from $v$, and define the monomial
\[
	p_v\coloneqq\tau(v)\yon^{\prod_{a\in A_v}\tau(\tgt\ a)}
\]
From this data we can give a morphism
\[
\bigotimes_{v\in V}p_v\Too{\varphi}\yon.
\]
To do so, we just need a function of the form
\[
\prod_{v\in V}\tau(v)\too\prod_{v\in V}\prod_{a\in A_v}\tau(\tgt\ a)
\]
We use $f\mapsto v\mapsto a\mapsto f(\tgt\ a)$.

The map $\varphi$ wires together the interfaces $p_v$, over all vertices $v\in V$, inside a closed outer box. That way, once we instantiate each $p_v$ with a dynamical system $S\yon^S\to p_v$---one that outputs $\tau(v)$ and inputs $\prod_{a\in A_v}\tau(\tgt\ a)$---they will all interact together as specified by the graph.

For example, a common graph found in cellular automata is a 2-dimensional integer lattice, with vertices $V\coloneqq\zz\times\zz$. The \emph{stencil} indicates which vertices ``hear'' which other vertices. One might use
\[A\coloneqq\{-1,0,1\}\times\{-1,0,1\}\times V\]
with $\src(i,j,m,n)=(m,n)$ and $\tgt(i,j,m,n)=(m+i, n+j)$.
\end{example}

\begin{exercise}[Conway's Game of Life]\label{exc.conway}
This exercise is for those who are familiar with Conway's Game of Life. We will use \cref{ex.graph_interaction}.
\begin{enumerate}
	\item What is the appropriate graph $A\tto V$?
	\item What is the appropriate function $\tau\colon V\to\smset$?
	\item What are the polynomials $p_v$ from \cref{ex.graph_interaction}?
	\item What is the appropriate state set $S_v$ for each interface $p_v$?
	\item What is the appropriate dynamical system map $S_v\yon^{S_v}\to p_v$?
\qedhere
\end{enumerate}
\begin{solution}
**
\end{solution}
\end{exercise}

% ** was commented out for some reason, not sure why
\begin{exercise}[Cellular automata]\label{exc.cellular_automata}
Let $G=(V,E)$ be a simple graph, i.e.\ $V,E\in\smset$ and $E\ss V\times V$. You can imagine it as a grid 
\[
\begin{tikzcd}[shift left=2pt]
	\LMO{(1,1)}\ar[r]      \ar[d]&
	\LMO{(1,2)}\ar[r]\ar[l]\ar[d]&
	\LMO{(1,3)}\ar[r]\ar[l]\ar[d]&
	\LMO{(1,4)}\ar[r]\ar[l]\ar[d]&
	\LMO{(1,5)}      \ar[l]\ar[d]\\
	\LMO{(2,1)}\ar[r]      \ar[d]\ar[u]&
	\LMO{(2,2)}\ar[r]\ar[l]\ar[d]\ar[u]&
	\LMO{(2,3)}\ar[r]\ar[l]\ar[d]\ar[u]&
	\LMO{(2,4)}\ar[r]\ar[l]\ar[d]\ar[u]&
	\LMO{(2,5)}      \ar[l]\ar[d]\ar[u]\\
	\LMO{(3,1)}\ar[r]      \ar[d]\ar[u]&
	\LMO{(3,2)}\ar[r]\ar[l]\ar[d]\ar[u]&
	\LMO{(3,3)}\ar[r]\ar[l]\ar[d]\ar[u]&
	\LMO{(3,4)}\ar[r]\ar[l]\ar[d]\ar[u]&
	\LMO{(3,5)}      \ar[l]\ar[d]\ar[u]\\
	\LMO{(4,1)}\ar[r]      \ar[d]\ar[u]&
	\LMO{(4,2)}\ar[r]\ar[l]\ar[d]\ar[u]&
	\LMO{(4,3)}\ar[r]\ar[l]\ar[d]\ar[u]&
	\LMO{(4,4)}\ar[r]\ar[l]\ar[d]\ar[u]&
	\LMO{(4,5)}      \ar[l]\ar[d]\ar[u]\\
	\LMO{(5,1)}\ar[r]            \ar[u]&
	\LMO{(5,2)}\ar[r]\ar[l]      \ar[u]&
	\LMO{(5,3)}\ar[r]\ar[l]      \ar[u]&
	\LMO{(5,4)}\ar[r]\ar[l]      \ar[u]&
	\LMO{(5,5)}      \ar[l]      \ar[u]
\end{tikzcd}
\]
finite or infinite, or just an arbitrary graph. For every vertex $v$, the set of vertices $v'$ for which $(v',v)\in E$, i.e.\ for which there is an edge $v'\to v$, is denoted
\[I(v)\coloneq\{v'\mid (v',v)\in E\}.\]
For each $v\in V$, let $p_v\coloneqq \2\yon^{\2^{I(v)}}$; it ``outputs'' a color $\2\cong\{\const{black},\const{white}\}$ and inputs a function $I(v)\to\2$, specifying what all the neighbors are outputting.
\begin{enumerate}
	\item In the drawn grid, what is $I(1,1)$? What is $I(2,2)$?
	\item Specify a morphism $g\colon \bigotimes_{v\in V}p_v\to\yon$ that passes to each vertex $v$ the colors of its neighbors in $I(v)$.
	\item Suppose that for each vertex $v\in V$ you are given a function $f_v\colon 2^{I(v)}\to\2$. Use it to construct a dynamical system $f'_v\colon\2\yon^\2\to p_v$ that updates its state in keeping with $f_v$ and outputs its state directly.
	\item Briefly look up cellular automata in a reference of your choice. Would you say that the dynamical system $\bigotimes_{v\in V}\2\yon^\2\To{\bigotimes f'_v}\bigotimes_{v\in V}p_v\To{g}\yon$ we obtain by wiring together the dynamical systems in the specified way does the same thing as the cellular automata in your reference?
\qedhere
\end{enumerate}
\end{exercise}

%---- Section ----%
\section{General interaction}

In general, we want systems that can change their interface---remove a port, add a port, change the type of a port, etc.---based on their internal states. But when such systems interact with others, the interaction pattern must be able to accommodate all of the various combinations of interfaces.

\begin{example}
Suppose given two interfaces $p$ and $p'$, having mode sets $M$ and $M'$ respectively
\[
	p\coloneqq \sum_{m\in M} B_m\yon^{A_m}
	\qqand
	p'\coloneqq \sum_{m'\in M'} B'_m\yon^{A'_m}
\]
The parallel product of these is:
\[
p\otimes p'\cong \sum_{(m,m')\in MM'}B_mB'_m\yon^{A_mA'_m}
\]
so the interface $B_mB'_{m'}\yon^{A_mA'_{m'}}$ at each of these $(M\times M')$-many modes $(m,m')$ must be accommodated in any morphism $w\colon p\otimes p'\to q$. For example if $M=\2$ and $M'=\3$ then $w$ can be specified by six maps.
\end{example}

But in fact the possibilities for interaction are much more general than we have led the reader to believe. They may not be broken down into modes at all.

\begin{example}
Let $p\coloneqq B\yon^A$ and $p'=B'\yon^{A'}$. To give a morphism $f\colon p\otimes p'\to \yon$, one specifies a map $B\times B'\to \1$, which is no data, as well as a map $BB'\to AA'$. In other words, for every pair of outputs $(b,b')$ one specifies a pair of inputs $(a,a')$. 

Let's think of elements of $B$ and $B'$ not as outputs, but as positions. 
\[
\begin{tikzpicture}[oriented WD, bb port length=0]
	\node[bb={1}{0}, fill=blue!10, dotted] (p) {$b$};
	\node[bb={1}{0}, fill=blue!10, dotted, below right=-0.5 and 0.5 of p] (q) {$b'$};
	\node[bb={0}{0}, inner sep=10pt, fit=(p) (q)] {};
	\node at (p_in1) {\faEye};
	\node at (q_in1) {\faEye};
\end{tikzpicture}
\hspace{.5in}
\begin{tikzpicture}[oriented WD, bb port length=0]
	\node[bb={1}{0}, fill=blue!10, dotted] (p) {$b$};
	\node[bb={1}{0}, fill=blue!10, dotted, below left=-0.5 and 0.5 of p] (q) {$b'$};
	\node[bb={0}{0}, inner sep=10pt, fit=(p) (q)] {};
	\node at (p_in1) {\faEye};
	\node at (q_in1) {\faEye};
\end{tikzpicture}
\]
Then given both positions $(b,b')$, the interaction pattern $f$ tells us what the two eyes see, i.e.\ what values of $(a,a')$ we get.
\end{example}

\begin{example}
Suppose you have two systems $p,q$, both having the same type $p=q\coloneqq\rr^2\yon^{\rr^2-(0,0)}$. 
\[
\begin{tikzpicture}
	\node (m1) {\faMotorcycle};
	\node[above=-.15 of m1] (e1) {\faEye};
	\node[draw, thick, blue!10, fit = (m1) (e1)] {};
	\node[below right=0 and 1 of m1] (m2) {\scalebox{-1}[1]{\faMotorcycle}};
	\node[above=-.15 of m2] (e2) {\faEye};
	\node[draw, thick, blue!10, fit = (m2) (e2)] {};
\end{tikzpicture}
\]
Taking all pairs of reals except $(0,0)$ corresponds to the fact that the eye cannot see that which is at the same position as the eye.

Let's have the two systems constantly approaching each other with a force equal to the reciprocal of the squared distance between them. If they finally collide, let's have the whole thing come to a halt.

To do this, we want the outer system to be of type $\{\const{go}\}\yon+\{\const{stop}\}$. The morphism $\rr^2\yon^{\rr^2-(0,0)}\otimes\rr^2\yon^{\rr^2-(0,0)}\to\{\const{go}\}\yon+\{\const{stop}\}$ is given on positions by
\[
  \big((x_1,y_1),(x_2,y_2)\big)\mapsto
	\begin{cases}
		\const{stop}&\mbox{ if $x_1=x_2$ and $y_1=y_2$}\\
		\const{go}&\mbox{ otherwise}.
	\end{cases}
\]
On directions, we use the function
\[
  \big((x_1,y_1),(x_2,y_2)\big)\mapsto \big((x_2-x_1,y_2-y_1),(x_1-x_2,y_1-y_2)\big),
\]
so that each system is able to see the vector pointing from it to the other system (unless that vector is zero, in which case the whole thing has halted). 

Let's use these vectors to define the internal dynamics of each system. Each system will hold as its internal state its current position and velocity, i.e.\ $S=\rr^2\times\rr^2$. To define a map of polynomials $S\yon^S\to\rr^2\yon^{\rr^2-(0,0)}$ we simply output the current position and update the current velocity by adding a vector pointing to the other system and having appropriate magnitude:
\begin{align*}
	\rr^2\times\rr^2&\To{\text{get}}\rr^2\\
	\big((x,y),(x',y')\big)&\Mapsto{\text{get}}(x,y)
\end{align*}
\begin{align*}
	\rr^2\times\rr^2\times(\rr^2-(0,0))&\To{\text{set}}\rr^2\times\rr^2\\
	\big((x,y),(x',y'),(a,b)\big)&\Mapsto{\text{set}}\left(x+x',y+y',x'+\frac{a}{(a^2+b^2)^{3/2}},y'+\frac{b}{(a^2+b^2)^{3/2}}\right)
\end{align*}
\end{example}

\begin{exercise}
Let $p,q\coloneqq\nn\yon^\nn$.
\begin{enumerate}
	\item Write a polynomial morphism $p\otimes q\to\yon$ that corresponds to the function $(a,b)\mapsto (b,a+b)$.
	\item Write dynamical systems $\nn\yon^\nn\to p$ and $\nn\yon^\nn\to q$, each of which simply outputs the previous input.
	\item Suppose each system starts in state $1\in\nn$. What is the trajectory of the $p$-system?
\qedhere
\end{enumerate}
\begin{solution}
**
\end{solution}
\end{exercise}

\begin{exercise}
Suppose $(X,d)$ is a metric space, i.e.\ $X$ is a set and $d\colon X\times X\to\rr$ is a function satisfying the usual laws. Let's have robots interact in this space.

Let $A,A'$ be sets, each thought of as a set of signals, and let $a_0\in A$ and $a_0'\in A'$ be elements, each thought of as a default value. Let $p\coloneqq AX\yon^{A'X}$ and $p'\coloneqq A'X\yon^{AX}$, and imagine there are two robots, one with interface $p$ and one with interface $p'$.
\begin{enumerate}
	\item Write down a morphism $p\otimes p'\to\yon$ such that each robot receives the other's location, but that it only receives the other's signal when the locations $x,x'$ are sufficiently close, $d(x,x')<1$. Otherwise it receives the default signal.
	\item Write down a morphism $p\otimes p'\to\yon^{[0,5]}$ where the value $s\in [0,5]$ is a scalar, allowing the signal to travel $s$ times further.
	\item Suppose that each robot has a set $S,S'$ of private states. What functions are involved in providing a dynamical system $f\colon SX\yon^{SX}\to AX\yon^{A'X}$?
	\item Change the setup in any way so that the robots only extend a port to hear the other's signal when the distance between them is less than 1. Otherwise, they can only detect the position (element of $X$) that the other currently inhabits.
\qedhere
\end{enumerate}
\begin{solution}
**
\end{solution}
\end{exercise}

So what is a map $p_1\otimes\cdots\otimes p_k\to q$ in general? It's a protocol by which he $k$-many participants $p_i$ together decide what decision $q$ must make, as well as how $q$'s choice among its options (the decision once made) is passed back and distributed as an option at each $p_i$.

\begin{example}[Cellular automata who vote on their interaction pattern]\label{ex.cell_auto_vote_interaction}
Recall from \cref{exc.cellular_automata} how we constructed cellular automata on a graph $G=(V,E)$. Here $E\ss V\times V$, or equivalently what we might call an \emph{interaction pattern} $I\colon V\to \2^V$, specifies the incoming neighbors $I(v)$ of each $v\in V$.

Suppose now that we are given a function $i\colon V\to\nn$ that we think of as specifying the number $i(v)$ of neighbors each $v\in V$ accepts. Let $\ord{i}(v)=\{1,2,\ldots, i(v)\}$. We will be interested in the polynomial $p_v\coloneqq\2\yon^{\2^{\ord{i}(v)}}$ for each $v$; it represents an interface that outputs a color $\2\cong\{\const{black},\const{white}\}$ and that inputs a function $\ord{i}(v)\to\2$, meant to give the colors of the neighboring vertices.

Say that an interaction pattern $I\colon V\to\2^V$ \emph{respects} $i$ if we have an isomorphism $I(v)\cong\ord{i}(v)$ for each $v\in V$. Suppose given a function $I\colon \2^V\to (\2^V)^V$ such that for each element $s\in\2^V$, the interaction pattern $I_s\colon V\to \2^V$ respects $i$. In the case of \cref{exc.cellular_automata}, $I$ was a constant function. Now we can think of it like all the vertices are voting, via $I$, on the connection pattern. 

We can put this all together by giving a morphism in $\poly$ of the form
\begin{equation}\label{eqn.polymap_misc9237}
\bigotimes_{v\in V}p_v\cong\2^V\yon^{\2^{\sum_{v\in V}\ord{i}(v)}}\too\yon.
\end{equation}
We can such a morphism with a function $g\colon \2^V\to\2^{\sum_{v\in V}\ord{i}(v)}$. Suppose given $s\in\2^V$, so that we have an isomorphism $I_s(v)\cong\ord{i}(v)$ for each $v\in V$; we want a function $g(s)\colon\sum_{v\in V}I_s(v)\to\2$. That is, for each $v$ we want a function
\[
g(s)_v\colon I_s(v)\to\2.
\]
But $I_s(v)\ss V$, so our function $s\colon V\to\2$ induces the desired function $I_s(v)\to\2$.

We have accomplished our goal: the automata vote on their connection pattern. Of course, we don't mean to imply that this vote needs to be democratic or fair in any way: it is an arbitrary function $I\colon \2^V\to(\2^V)^V$. It could be dictated by a given vertex $v_0\in V$ in the sense that its on/off state completely determines the connection pattern $V\times V\to \2$; this would be expressed by saying that $I$ factors as $\2^V\to\2^{v_0}\cong\2\To{I_0}(\2^V)^V$ for some $I_0$.
\end{example}

\begin{exercise}
Change \cref{ex.cell_auto_vote_interaction} slightly by changing the outer box.
\begin{enumerate}
	\item First change it to $A\yon$ for some set $A$ of your choice, and update \eqref{eqn.polymap_misc9237} so that the system outputs some aspect of the current state $\2^V$.
	\item What would it mean to change \eqref{eqn.polymap_misc9237} to a map $\bigotimes_{v\in V}p_v\to\yon^A$ for some $A$?
\qedhere
\end{enumerate}
\begin{solution}
**
\end{solution}
\end{exercise}

\begin{example}\label{ex.bonds_break}
Recall the picture from \cref{ex.changing_wiring_bonds_supplier_assemble}. We said that when too much force is applied to a material, bonds can break. Let's simplify the picture a bit.
\[
\begin{tikzpicture}[oriented WD, bb small, bb port length=0]
	\node[bb={1}{1}, fill=blue!10] (x1) {$\Phi_1$};
	\node[bb={1}{1}, fill=blue!10, right=of x1] (x2) {$\Phi_2$};
	\node[bb={1}{1}, fit= (x1) (x2)] (outer) {};
	\draw[->, shorten >= -4mm] (x1_in1) -- (outer_in1) node[left=4.5mm, font=\tiny] {Force};
	\draw (x1_out1) -- (x2_in1);
	\draw[->, shorten >= -4mm] (x2_out1) -- (outer_out1) node[right=4.5mm, font=\tiny] (L) {Force};
%
	\node[bb={1}{1}, fill=blue!10, right=2in of L] (y1) {$\Phi_1$};
	\node[bb={1}{1}, fill=blue!10, right=of y1] (y2) {$\Phi_2$};
	\node[bb={1}{1}, fit= (y1) (y2)] (outer) {};
	\draw[->, shorten >= -4mm] (y1_in1) -- (outer_in1) node[left=4.5mm, font=\tiny] (R){Force};
	\draw[->, shorten >= -4mm] (y2_out1) -- (outer_out1) node[right=4.5mm, font=\tiny] {Force};
	\node[starburst, draw, minimum width=2cm, minimum height=1.5cm,red,fill=orange,line width=1.5pt] at ($(L)!.5!(R)$)
{Snap!};
\end{tikzpicture}
\]
We will imagine systems $\Phi_1$ and $\Phi_2$ as initially connected in space, that they experience forces from the outside world, and that---for as long as they are connected---they experience forces from each other. More precisely, each internal arena is defined by
\[
	p_1=p_2\coloneqq F\yon^{FF}+\yon^F.
\]
Elements of $F$ will be called \emph{forces}. We need to be able to add and compare forces, i.e.\ we need $F$ to be an ordered monoid; let's say $F=\nn$ for simplicity. The idea is that the arena has two modes: the monomial $F\yon^{FF}$ consisting of two input forces (one from its left and one from its right) and an output force $f_i$, and the monomial $\yon^F$ consisting of one input force (just from the outside). Similarly, in the first mode the system $\Phi_i$ is outputting a force for the other---whether the other uses it or not---but in the second mode the system produces no force for the other.

The external arena is defined to be
\[
p\coloneqq\yon^{FF};
\]
it takes as input two forces $(f_L, f_R)$ and produces unchanging output.

Though the systems $\Phi_1$ and $\Phi_2$ may be initially connected, if the forces on either one surpass a threshold, that system stops sending and receiving forces from the other. The connection is broken and neither system ever receives forces from the other again. This is what we will implement explicitly below.

To do so, we need to create a contract $p_1\otimes p_2\to p$ of the external arena $p$ around (the arenas of) the internal systems. That is, we need to give a morphism of polynomials
\[
\kappa\colon (F\yon^{FF}+\yon^F)\otimes (F\yon^{FF}+\yon^F)\to\yon^{FF}.
\]
By distributivity and the universal property of coproducts, it suffices to give four maps:
\[\arraycolsep=1.4pt
\begin{array}{lll}
	\kappa_{11}\colon&~ FF\yon^{(FF)(FF)}&\to\yon^{FF}\\
	\kappa_{12}\colon&~ F\yon^{(FF)F}&\to\yon^{FF}\\
	\kappa_{21}\colon&~ F\yon^{F(FF)}&\to\yon^{FF}\\
	\kappa_{22}\colon&~ \yon^{FF}&\to\yon^{FF}
\end{array}
\]
The middle two maps $\kappa_{12}$ and $\kappa_{21}$ won't actually occur in our dynamics, so we take them to be arbitrary. We take the last map $\kappa_{22}$ to be an identity (the forces from outside are passed to the two internal boxes). The first map $\kappa_{11}$ is equivalent to a function $(FF)(FF)\to (FF)(FF)$ which we take to be $((f_1,f_2),(f_L,f_R))\mapsto((f_L, f_2),(f_1,f_R))$.

Now that we have the arenas wired together, it remains to give the dynamics on the internal boxes. The states in the two cases will be identical, namely $S\coloneqq F+1$, meaning that at any point the system will either be in the state of holding a force or not. The dynamics will be identical as well, up to a symmetry swapping left and right; let's work with the first. Its interface is $p_1=F\yon^{FF}+\yon^F$ and its dynamics are given by
\[\Phi_1\colon (F+1)\yon^{F+1}\to F\yon^{FF}+\yon^F\]
which splits up as the coproduct of $F\yon^{F+1}\to F\yon^{FF}$ and $\yon^{F+1}\to\yon^F$. The second map corresponds to when the connection is broken; it is given by projection, meaning it just updates the state to be the received force. The first map  $F\yon^{F+1}\to F\yon^{FF}$ corresponds to the case where the system is holding some force, is receiving two input forces and must update its state and produce one output force. For the passforward $F\to F$, let's use identity meaning it outputs the force it's holding. For the passback $F(FF)\to \{\const{Just}\}F+\{\const{Nothing}\}$, let's use the map $(f,(f_L,f_2))\mapsto t(f_L,f_2)$ defined here:
\[
t(f_L,f_2)\coloneqq
\begin{cases}
	\const{Just}~f_L&\tn{ if }f_1+f_2<100\\
	\const{Nothing}&\tn{ otherwise}
\end{cases}
\]
Thus when the sum of forces is high enough, the internal state is updated to the broken state; otherwise it is sent to the force it receives from outside.
\end{example}


\begin{example}\label{ex.supplier_change}
We want to consider the case of a company $C$ that may change its supplier based on its internal state. The company has no output wires, but has two modes of operation---two positions---corresponding to who it wants to receive widgets $W$ from:
\[
\begin{tikzpicture}[oriented WD, every node/.style={fill=blue!10}]
	\node[bb={0}{1}] (s1) {Supplier 1};
	\node[bb={0}{1}, below=of s1] (s2) {Supplier 2};
	\node[bb={1}{0}, right=0.5 of s1] (c) {Company};
	\draw (s1_out1) to node[above, fill=none, font=\tiny] {$W$} (c_in1);
	\draw (s2_out1) to +(5pt,0) node[fill=none] {$\bullet$};
\begin{scope}[xshift=3.5in]
	\node[bb={0}{1}] (s1') {Supplier 1};
	\node[bb={0}{1}, below=of s1'] (s2') {Supplier 2};
	\node[bb={1}{0}, right=0.5 of s2'] (c') {Company};
	\draw (s2'_out1) to node[above, fill=none, font=\tiny] {$W$} (c'_in1);
	\draw (s1'_out1) to +(5pt,0) node[fill=none] {$\bullet$};
\end{scope}
	\node[starburst, draw, minimum width=2cm, minimum height=2cm,align=center,fill=white, font=\small,line width=1.5pt] at ($(c.east)!.5!(s2'.west)$)
{Change\\supplier!};
\end{tikzpicture}
\]
The company has interface $2\yon^W$, and the each supplier has interface $W\yon$; let's take the total system interface (undrawn) to be the closed system $\yon$. Then this mode-dependent wiring diagram is just a map $2\yon^W\otimes W\yon\otimes W\yon\to\yon$. Its on-positions function $2W^2\to1$ is uniquely determined, and its on-directions function $2W^2\to W$ is the evaluation. In other words, the company's position determines which supplier from which it receives widgets.
\end{example}

\begin{example}\label{ex.assemble_machine}
When someone assembles a machine, their own outputs dictate the connection pattern of the machine's components.
\begin{equation}\label{eqn.someone2}
\begin{tikzpicture}[oriented WD, font=\ttfamily, bb port length=0, every node/.style={fill=blue!10}, baseline=(someone.north)]
	\node[bb port sep=.5, bb={0}{1}] (A) {unit A};
	\node[bb port sep=.5, bb={1}{0}, right=of A] (B) {unit B};
	\coordinate (helper) at ($(A)!.5!(B)$);
	\node[bb={1}{1}, below=2 of helper] (someone) {\tikzsymStrichmaxerl[3]};
	\draw[->, dashed, blue] (someone_in1) to[out=180, in=270] (A.270);
	\draw[->, dashed, blue] (someone_out1) to[out=0, in=270] (B.270);
	\draw[->] (A_out1) -- +(10pt,0);
	\draw (B_in1) -- +(-10pt,0);
%
\begin{scope}[xshift=3.5in]
	\node[bb port sep=.5, bb={0}{1}] (A') {unit A};
	\node[bb port sep=.5, bb={1}{0}, right=.5of A'] (B') {unit B};
	\coordinate (helper') at ($(A')!.5!(B')$);
	\node[bb={1}{1}, below=2 of helper'] (someone') {\tikzsymStrichmaxerl[3]};
	\draw[->, dashed, blue] (someone'_in1) to[out=180, in=270] (A'.270);
	\draw[->, dashed, blue] (someone'_out1) to[out=0, in=270] (B'.270);
	\draw[->] (A'_out1) -- (B'_in1);
\end{scope}
%
	\node[starburst, draw, minimum width=2cm, minimum height=2cm,fill=blue!50,line width=1.5pt, align=center, font=\upshape] at ($(B)!.5!(A')-(0,.6cm)$)
{Attach!};
\end{tikzpicture}
\end{equation}
In order for the above picture to make sense, $A$ has the same output that $B$ has as input, say $X$, and we need a default value $x_0\in X$ for $B$ to input when not connected to $A$.

We could say that the person in \eqref{eqn.someone2} has interface $2\yon$, the units have interfaces $X\yon$ and $\yon^X$ respectively, and the whole system is closed; that is, the diagram represents a morphism $2\yon\otimes X\yon\otimes \yon^X\to\yon$. The morphism $2X\yon^X\to\yon$ is uniquely determined on positions, and on directions it is given by cases $(1,x)\mapsto x_0$ and $(2,x)\mapsto x$.
\end{example}

We can easily generalize \cref{ex.assemble_machine}. Indeed, we will see in the next section that there is a polynomial $\ihom{q_1\otimes\cdots\otimes q_k\,,\,r}$ of all ways  $q_1,\ldots,q_k$ can interact in $r$, and that a map from some $p$ to it is just a bigger interaction pattern:
\[
\poly(p,[q_1\otimes\cdots\otimes q_k\,,\,r])\cong\poly(p\otimes q_1\otimes\cdots\otimes q_k\,,\,r).
\]
In other words, if $p$ thinks it's deciding how $q_1,\ldots,q_k$ are wired up in $r$, and gets feedback from that wiring pattern itself, then in actuality $p$ is just part of a wiring diagram with $q_1,\ldots,q_k$ inside of $r$.\

What it also means is that if you want, you can put a little dynamical system inside of $[q_1\otimes\cdots\otimes q_k,r]$ and have it be constantly choosing interaction patterns. Let's see how it works.


%---- Section ----%
\section{Closure of $\otimes$}%[-,-]

The parallel monoidal product is closed---we have a monoidal closed structure on $\poly$---meaning that there is a closure operation, which we denote $\ihom{-,-}\colon\poly\op\times\poly\to\poly$, such that there is an isomorphism
\begin{equation}\label{eqn.monoidal_closure}
  \poly(p\otimes q,r) \iso \poly(p,\ihom{q,r})
\end{equation}
natural in $p,q,r$.
The closure operation is defined on $q,r$ as follows:
\begin{equation}\label{eqn.dir_hom}
	\ihom{q,r} \coloneqq \prod_{j\in q(\1)}r\circ(q[j]\yon)
\end{equation}
Here $\circ$ denotes standard functor composition; informally, $r \circ (q[j]\yon)$ is the polynomial you get when you replace each appearance of $\yon$ in $r$ by $q[j]\yon$.
Composition, together with the unit $\yon$, is in fact yet another monoidal structure, as we will see in more depth in \cref{chapter.comon}.

Before we prove that the isomorphism \eqref{eqn.monoidal_closure} holds naturally, let us investigate the properties of the closure operation, starting with some simple examples.

\begin{exercise}
Calculate $\ihom{q,r}$ for $q,r\in\poly$ given as follows.
\begin{enumerate}
	\item $q\coloneqq \0$ and $r$ arbitrary.
	\item $q\coloneqq \1$ and $r$ arbitrary.
	\item $q\coloneqq\yon$ and $r$ arbitrary.
	\item $q\coloneqq A$ for $A\in\smset$ (constant) and $r$ arbitrary.
	\item $q\coloneqq A\yon$ for $A\in\smset$ (linear) and $r$ arbitrary.
	\item $q\coloneqq\yon^\2+\2\yon$ and $r\coloneqq\2\yon^\3+\3$.
\qedhere
\end{enumerate}
\begin{solution}
We compute $\ihom{q,r}$ for various values of $q, r \in \poly$ using \eqref{eqn.dir_hom}.
\begin{enumerate}
    \item If $q \coloneqq \0$, then $q(\1) \iso \0$, so $\ihom{q,r}$ is an empty product.
    Hence $\ihom{q,r} \iso \1$.
    \item If $q \coloneqq \1$, then $q(\1) \iso \1$ and $q[1] \iso \0$, so $\ihom{q,r} \iso r \circ (\0\yon) \iso r(\0)$.
    \item If $q \coloneqq \yon$, then $q(\1) \iso \1$ and $q[1] \iso \1$, so $\ihom{q,r} \iso r \circ (\1\yon) \iso r$.
	\item If $q \coloneqq A$ for $A \in \smset$, then $q(\1) \iso A$ and $q[j] \iso \0$ for every $j \in A$, so $\ihom{q,r} \iso \prod_{j \in A} (r \circ (\0\yon)) \iso r(\0)^A$.
	\item If $q \coloneqq A\yon$ for $A\in\smset$, then $q(\1) \iso A$ and $q[j] \iso \1$ for every $j \in A$, so $\ihom{q,r} \iso \prod_{j \in A} (r \circ (\1\yon)) \iso r^A$.
	\item If $q\coloneqq\yon^\2+\2\yon$ and $r\coloneqq\2\yon^\3+\3$, then
	\begin{align*}
	    \ihom{q,r} &\iso (r \circ (\2\yon))(r \circ (\1\yon))^\2 \\
	    &\iso \left(\2(\2\yon)^\3 + \3\right)\left(\2\yon^\3 + \3\right)^\2 \\
	    &\iso \6\4\yon^\9 + \2\0\4\yon^\6 + \1\8\0\yon^\3 + \2\7 \\
	\end{align*}
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.sum_times_closure}
Show that for any polynomials $p_1,p_2,q$, we have an isomorphism
\[
\ihom{p_1 + p_2, q} \iso \ihom{p_1, q} \times \ihom{p_2, q}.
\]
\begin{solution}
We wish to show that for all $p_1, p_2, q \in \poly$, we have $\ihom{p_1 + p_2, q} \iso \ihom{p_1, q} \times \ihom{p_2, q}$.
By \eqref{eqn.dir_hom},
\[
    \ihom{p_1 + p_2, q} \iso \left(\prod_{i \in p_1(\1)} q \circ (p_1[i]\yon)\right) \left(\prod_{i \in p_2(\1)} q \circ (p_2[i]\yon)\right) \iso \ihom{p_1, q} \times \ihom{p_2, q}.
\]
\end{solution}
\end{exercise}

\begin{exercise} \label{exc.dir_hom_sum}
Show that there is an isomorphism
\begin{equation} \label{eqn.dir_hom_sum}
\scalebox{1.3}{$\displaystyle
\ihom{q,r} \iso \sum_{f\colon q\to r}\yon^{\sum_{j\in q(\1)}r[f_1(j)]}
$}
\end{equation}
where the sum is indexed by $f\in\poly(q,r)$.
\begin{solution}
We may compute
\begin{align*}
    \ihom{q, r} &\iso \prod_{j \in q(\1)} r \circ (q[j]\yon) \tag*{\eqref{eqn.dir_hom}} \\
    &\iso \prod_{j \in q(\1)} \, \sum_{k \in r(\1)} (q[j]\yon)^{r[k]} \tag{Replacing each $\yon$ in $r$ by $q[j]\yon$} \\
    &\iso \sum_{f_1 \colon q(\1) \to r(\1)} \, \prod_{j \in q(\1)} (q[j]\yon)^{r[f_1(j)]} \tag*{\eqref{eqn.push_prod_sum_set_indep}} \\
    &\iso \sum_{f_1 \colon q(\1) \to r(\1)} \, \left(\prod_{j \in q(\1)} q[j]^{r[f_1(j)]} \right)\left(\prod_{j \in q(\1)} \yon^{r[f_1(j)]} \right) \\
    &\iso \sum_{f_1 \colon q(\1) \to r(\1)} \; \sum_{f^\sharp \in \prod_{j \in q(\1)} q[j]^{r[f_1(j)]}} \yon^{\sum_{j \in q(\1)} r[f_1(j)]} \\
    &\iso \sum_{f \colon q \to r} \yon^{\sum_{j \in q(\1)} r[f_1(j)]}. \tag*{\eqref{eqn.main_formula}}
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise} \label{exc.dir_hom_p_yon_dir_p}
Verify that \eqref{eqn.dir_hom_p_yon_dir_p} holds.
\begin{solution}
We verify \eqref{eqn.dir_hom_p_yon_dir_p} as follows:
\begin{align*}
    \ihom{p, \yon} \otimes p
    &\iso
    \left(\sum_{f \colon p \to \yon} \yon^{\sum_{i \in p(\1)} \yon[f_1(i)]}\right) \otimes p
    \tag*{\eqref{eqn.dir_hom_sum}} \\
    &\iso
    \sum_{f \in \Gamma(p)} \yon^{p(\1)} \otimes \sum_{i \in p(\1)} \yon^{p[i]} \\
    &\iso
    \sum_{f \in \Gamma(p)} \; \sum_{i \in p(\1)} \yon^{p(\1) \times p[i]}
    \tag*{\eqref{eqn.parallel_def}} \\
    &\iso
    \sum_{f \in \prod_{i \in p(\1)} p[i]} \; \sum_{i \in p(\1)} \yon^{p(\1) \times p[i]}.
    \tag*{\eqref{eqn.gamma_prod}}
\end{align*}
\end{solution}
\end{exercise}

\begin{example}\label{ex.dirichlet_dual}
For any $A\in\smset$ we have
\[
  \ihom{\yon^A,\yon} \iso A\yon
  \qqand
  \ihom{A\yon,\yon} \iso \yon^A.
\]
More generally, for any polynomial $p\in\poly$ we have
\begin{equation}\label{eqn.dir_dual}
  \ihom{p,\yon} \iso \Gamma(p)\yon^{p(\1)}.
\end{equation}
All these facts follow directly from \eqref{eqn.dir_hom}.
\end{example}

\begin{exercise}
Verify the three facts above.
\begin{solution}
We have that
\[
    \ihom{\yon^A, \yon} \iso \prod_{j \in \yon^A(\1)} \yon \circ (\yon^A[j]\yon) \iso \prod_{j \in \1} A\yon \iso A\yon,
\]
that
\[
    \ihom{A\yon, \yon} \iso \prod_{j \in A\yon(\1)} \yon \circ ((A\yon)[j]\yon) \iso \prod_{j \in A} \yon \iso \yon^A,
\]
and that
\begin{align*}
    \ihom{p, \yon} &\iso \sum_{f \colon p \to \yon} \yon^{\sum_{i \in p(\1)} \yon[f_1(i)]} \tag{\cref{exc.dir_hom_sum}} \\
    &\iso \sum_{f \in \Gamma(p)} \yon^{\sum_{i \in p(\1)} \1} \\
    &\iso \Gamma(p)\yon^{p(\1)}.
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}
Show that for any $p\in\poly$, if there is an isomorphism $\ihom{\ihom{p,\yon},\yon} \iso p$, then $p$ is either linear $A\yon$ or representable $\yon^A$ for some $A$. Hint: first show that $p$ must be a monomial.
\begin{solution}
Given $p \in \poly$ and an isomorphism $\ihom{\ihom{p,\yon},\yon} \iso p$, we wish to show that $p$ is either linear or representable.
Applying \eqref{eqn.dir_dual} twice, we have that
\[
    p \iso \ihom{\ihom{p,\yon},\yon} \iso \Gamma\left(\Gamma(p)\yon^{p(\1)}\right)\yon^{\Gamma(p)}.
\]
We can compute the coefficient of $p$ via \eqref{eqn.gamma_prod} to obtain
\[
    \Gamma\left(\Gamma(p)\yon^{p(\1)}\right) \iso \prod_{\gamma \in \Gamma(p)} p(\1) \iso p(\1)^{\Gamma(p)}.
\]
Hence
\begin{equation} \label{eqn.p_as_gamma_monomial}
    p \iso p(\1)^{\Gamma(p)}\yon^{\Gamma(p)}.
\end{equation}
In particular, $p$ is a monomial, so we can write $p \coloneqq B\yon^A$ for some $A,B \in \smset$.
Then $p(\1) \iso B$ and \eqref{eqn.gamma_prod} tells us that $\Gamma(p) \iso A^B$.
It follows from \eqref{eqn.p_as_gamma_monomial} that $A \iso A^B$ and that $B \iso B^A$.

We conclude with some elementary set theory.
If either one of $A$ or $B$ were $\1$, then $p$ would be either linear or representable, and we would be done.
Meanwhile, if either one of $A$ or $B$ were $\0$, then the other would be $\1$, and we would again be done.
Otherwise, $|A|,|B| \geq 2$.
But by Cantor's theorem,
\[
    |B| < \big|\2^B\big| \leq \big|A^B\big| = |A| \qqand |A| < \big|\2^A\big| \leq \big|B^A\big| = |B|,
\]
a contradiction.
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.dirichlet_closure}
With $\ihom{-,-}$ as defined in \eqref{eqn.dir_hom}, there is a natural isomorphism
\begin{equation}\label{eqn.poly_closure_brackets}
	\poly(p\otimes q,r)\cong\poly(p,\ihom{q,r}).
\end{equation}
\end{proposition}
\begin{proof}
We have the following chain of natural isomorphisms:
\begin{align*}
	\poly(p\otimes q,r)
	&\iso
	\poly\Big(\sum_{i\in p(\1)}\sum_{j\in q(\1)}\yon^{p[i]q[j]},r\Big) \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}\poly(\yon^{p[i]q[j]},r)
	\tag{Universal property of coproducts} \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}r(p[i]q[j])
	\tag{Yoneda lemma} \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}\poly(\yon^{p[i]},r\circ(q[j]\yon))
	\tag{Yoneda lemma} \\
	&\iso
	\poly\Big(\sum_{i\in p(\1)}\yon^{p[i]},\prod_{j\in q(\1)}r\circ(q[j]\yon)\Big)
	\tag{Universal property of (co)products} \\
	&\iso
	\poly(p,\ihom{q,r}).
\end{align*}
\end{proof}

\begin{exercise}\label{exc.poly_plug_1}
Show that for any $p,q$ we have an isomorphism of sets
\[
\poly(p,q) \iso \ihom{p,q}(\1).
\]
Hint: you can either use the formula \eqref{eqn.dir_hom}, or just use 
\eqref{eqn.poly_closure_brackets} with the Yoneda lemma and the fact that $\yon\otimes p \iso p$.
\begin{solution}
The isomorphism $\poly(p,q) \iso \ihom{p,q}(\1)$ follows directly from \cref{exc.dir_hom_sum} when both sides are applied to $\1$.
Alternatively, we can apply \eqref{eqn.poly_closure_brackets}.
Since $p \iso \yon \otimes p$, we have that
\begin{align*}
    \poly(p, q) &\iso \poly(\yon \otimes p, q) \\
    &\iso \poly(\yon, \ihom{p,q}) \tag*{\eqref{eqn.poly_closure_brackets}} \\
    &\iso \ihom{p,q}(\1). \tag{Yoneda lemma}
\end{align*}
\end{solution}
\end{exercise}

The closure of $\otimes$ implies that for any $p,q\in\poly$, there is a canonical \emph{evaluation} map
\begin{equation}\label{eqn.eval_dirichlet}
  \fun{eval}\colon \ihom{p,q}\otimes p\too q.
\end{equation}

\begin{exercise} \label{exc.eval_dirichlet}
\begin{enumerate}
	\item Obtain the evaluation map $\fun{eval}\colon \ihom{p,q}\otimes p\too q$ from \eqref{eqn.eval_dirichlet}. \label{exc.eval_dirichlet.explicit}
	\item Show that for any $p,q,r \in \poly$ and map $f\colon p\otimes q\to r$, there is a unique morphism $f'\colon p\to\ihom{q,r}$ such that the following diagram commutes:
	\[
	\begin{tikzcd}
		p\otimes q\ar[r, "f'\otimes q"]\ar[rr, bend right, "f"']&
		{\ihom{q,r}}\otimes q\ar[r, "\fun{eval}"]&
		r
	\end{tikzcd}
	\]
	\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item To obtain the evaluation map $\fun{eval}\colon \ihom{p,q}\otimes p\too q$, we consider the following special case of the isomorphism \eqref{eqn.poly_closure_brackets}:
    \[
        \poly(\ihom{p,q} \otimes p, q) \iso \poly(\ihom{p,q}, \ihom{p,q}).
    \]
    Then the evaluation map is the map corresponding to the identity morphism on $\ihom{p,q}$ under the above isomorphism.
    To recover this map, we can start from the identity morphism on $\ihom{p,q}$ and work our way along a chain of natural isomorphisms from $\poly(\ihom{p,q}, \ihom{p,q})$ until we get to $\poly(\ihom{p,q} \otimes p, q)$.
    To start, \cref{exc.dir_hom_sum} implies that
    \begin{align*}
        \poly(\ihom{p,q}, \ihom{p,q})
        &\iso
        \poly\left(\sum_{f \colon p \to q} \, \prod_{i' \in p(\1)} \yon^{q[f_1(i')]}, \prod_{i \in p(\1)} \, \sum_{j \in q(\1)} (p[i]\yon)^{q[j]}\right) \\
        &\iso
        \prod_{f \colon p \to q} \, \prod_{i \in p(\1)} \poly\left(\prod_{i' \in p(\1)} \yon^{q[f_1(i')]}, \sum_{j \in q(\1)} (p[i]\yon)^{q[j]}\right),
    \end{align*}
    where the second isomorphism follows from the universal properties of products and coproducts.
    In particular, under this isomorphism, the identity morphism on $\ihom{p,q}$ corresponds to a collection of morphisms, namely for each $f \colon p \to q$ and each $i \in p(\1)$ the composite
    \[
        \prod_{i' \in p(\1)} \yon^{q[f_1(i')]} \to \yon^{q[f_1(i)]} \to \sum_{g \colon q[f_1(i)] \to p[i]} \yon^{q[f_1(i)]} \iso (p[i]\yon)^{q[f_1(i)]} \to \sum_{j \in q(\1)} (p[i]\yon)^{q[j]}
    \]
    of the canonical projection with index $i' = i$, the canonical inclusion with index $g = f^\sharp_i$, and the canonical inclusion with index $j = f_1(i)$.
    On positions, this map picks out the position of $\sum_{j \in q(\1)} (p[i]\yon)^{q[j]}$ corresponding to $j = f_1(i) \in q(\1)$ and $f^\sharp_i \colon q[f_1(i)] \to p[i]$; on directions, the map is the canonical inclusion $q[f_1(i)] \to \sum_{i' \in p(\1)} q[f_1(i')]$ with index $i' = i$.
    
    We can reinterpret each of these maps as a map
    \[
        \yon^{p[i] \times \sum_{i' \in p(\1)} q[f_1(i')]} \to \sum_{j \in q(\1)} \yon^{q[j]} \iso q
    \]
    that, on positions, picks out the position $f_1(i) \in q(\1)$ of $q$ and, on directions, is the map $q[f_1(i)] \to p[i] \times \sum_{i' \in p(\1)} q[f_1(i')]$ induced by the universal property of products applied to the map $f^\sharp_i \colon q[f_1(i)] \to p[i]$ and the inclusion $q[f_1(i)] \to \sum_{i' \in p(\1)} q[f_1(i')]$.
    Then by the universal property of coproducts, this collection of maps induces a single map $\fun{eval} \colon \ihom{p, q} \otimes p \to q$ that sends each position $f \colon p \to q$ of $\ihom{p,q}$ and position $i \in p(\1)$ of $p$ to the position $f_1(i)$ of $q$, with the same behavior on directions as the corresponding map described previously.
    \item **
    % \item The fact that for any $p,q,r $
    % follows from the natural isomorphism given in \eqref{eqn.poly_closure_brackets}, but we can also construct this map explicitly. (Can we?)
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
\begin{enumerate}
	\item For any set $S$, obtain the map $S\yon^S\to\yon$ whose on-directions map is the identity on $S$ using eval and \cref{ex.dirichlet_dual}.
	\item Show that maps of the four types $\kappa_{11}$, $\kappa_{12}$, $\kappa_{21}$, $\kappa_{22}$ shown in \cref{ex.bonds_break} can be obtained by tensoring together identity maps and eval maps.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item Given a set $S$, we wish to obtain the map $S\yon^S \to \yon$ whose on-diretions map is the identity by using eval and \cref{ex.dirichlet_dual}.
    The example shows that
    \[
        \ihom{S\yon, \yon} \otimes (S\yon) \iso \yon^S \otimes (S\yon) \iso S\yon^S,
    \]
    so by setting $p \coloneqq S\yon$ and $q \coloneqq S$ in \eqref{eqn.eval_dirichlet}, we obtain an evaluation map $\fun{eval} \colon S\yon^S \to \yon$.
    By the solution to \cref{exc.eval_dirichlet} \cref{exc.eval_dirichlet.explicit}, given a position $s \in S$ of $S\yon^S$, the evaluation map on directions is the map $\1 \to S$ that picks out $s$.
    In other words, it is indeed the identity on directions.
    \item **
\end{enumerate}
\end{solution}
\end{exercise}

\begin{example}[Modeling your environment without knowing what it is]
Let's imagine a robot whose interface is an arbitrary polynomial $p$. Let's imagine it is living together in a closed system
\[
	f\colon (q_1\otimes\cdots\otimes q_n)\otimes p\to \yon
\]
with some other robots whose interfaces are $q_1,\ldots,q_n$; let $q\coloneqq(q_1\otimes\cdots\otimes q_n)$. The interaction pattern induces a morphism $f'\colon q\to \ihom{p,\yon}$ such that the original system $f$ factors through the evaluation $\ihom{p,\yon}\otimes p\to \yon$.

In other words $\ihom{p,\yon}$ holds within it all of the possible ways $p$ can interact with other systems in a closed box.%
\footnote{And if you want the generic way $p$ to interact with other systems in a box $r$, just use $\ihom{p,r}$.}
To investigate this just a bit, note that $\ihom{p,\yon}\cong\prod_{i\in p(\1)}p[i]\yon$. That is, for each position in $p$ it produces a direction there, which is just what $p$ needs as input.

Now suppose we were to populate the interface $p$ with dynamics, a map $S\yon^S\to p$. One could aim to choose a set $S$ along with an interesting map $g\colon S\to\poly(p,\yon)$. Then each state $s$ would include a guess $g(s)$ about what the state of the environment is in. This is not the real environment $q$, but just the environment as it affects $p$, namely $\ihom{p,\yon}$. The robot's states model environmental conditions.
\end{example}

\begin{example}[Chu $\&$]
Suppose given polynomials $p_1,p_2,q_1,q_2,r\in\poly$ and morphisms
\[
	\varphi_1\colon p_1\otimes q_1\to r
	\qqand
	\varphi_2\colon p_2\otimes q_2\to r
\]
One might call these ``$r$-Chu spaces.'' One operation you can do with these as Chu spaces is to return something denoted $\varphi_1\&\varphi_2$, or ``$\varphi_1$ \emph{with} $\varphi_2$'' of the following type:
\[
\varphi_1\&\varphi_2\colon (p_1\times p_2)\otimes (q_1+q_2)\to r
\]
Suppose we are given a position in $p_1$ and a position in $p_2$. Then given a position in either $q_1$ or $q_2$, one evaluates either $\varphi_1$ or $\varphi_2$ respectively to get a position in $r$; given a direction there, one returns the corresponding direction in $q_1$ or $q_2$ respectively, as well as a direction in $p_1\times p_2$ which is either a direction in $p_1$ or in $p_2$.

This sounds complicated, but it can be done formally, once we have monoidal closure. We first rearrange both $\varphi_1,\varphi_2$ to be $p$-centric, using monoidal currying:
\[
\psi_1\colon p_1\to \ihom{q_1,r}
\qqand
\psi_2\colon p_2\to \ihom{q_2,r}
\]
Now we multiply to get $\psi_1\times\psi_2\colon p_1\times p_2\to\ihom{q_1,r}\times\ihom{q_2,r}$. Then we apply \cref{exc.sum_times_closure} to see that $\ihom{q_1,r}\times\ihom{q_2,r}\cong\ihom{q_1+q_2,r}$, and finally monoidal-uncurry to obtain $(p_1\times p_2)\otimes(q_1+q_2)\to r$ as desired.
\end{example}

\Closesolutionfile{solutions}

%-------- Section --------%
\section{Exercise solutions}
{\footnotesize
\input{solution-file3}}

\Opensolutionfile{solutions}[solution-file4]

%------------ Chapter ------------%
\chapter{More categorical properties of polynomials} \label{sec.bonus_poly}

The category $\poly$ has very useful formal properties, including completion under colimits and limits, various adjunctions with $\smset$, factorization systems, and so on. Most of the following material is not necessary for the development of our main story, but we collect it here for reference. The reader can skip directly to \cref{chapter.comon} if so inclined. Better yet might be to just gently leaf through \cref{sec.bonus_poly}, to see how well-behaved and versatile the category $\poly$ really is.

%-------- Section --------%
\section{Special polynomials and adjunctions}

There are a few special classes of polynomials that are worth discussing: 
\begin{enumerate}[label=\alph*)]
	\item constant polynomials $\0,\1,\2,A$; 
	\item linear polynomials $\0,\yon, \2\yon, A\yon$;
	\item pure-power (or representable) polynomials $\1, \yon, \yon^\2, \yon^A$; and 
	\item monomials $\0, A, \yon, \2\yon^\3, A\yon^B$.
\end{enumerate}
The first two classes, constant and linear polynomials, are interesting because they both put a copy of $\smset$ inside $\poly$, as we'll see in \cref{prop.ff_const_set_to_poly,prop.ff_lin_set_to_poly}. 
The third puts a copy of $\smset\op$ inside $\poly$.
Finally, the fourth puts a copy of bimorphic lenses inside $\poly$, as we saw in \cref{subsec.poly.func_nat.prepare_dyn.bimorphic_lens}.

\begin{exercise}
Which of the four classes above are closed under
\begin{enumerate}
	\item the cocartesian monoidal structure $(\0,+)$ (i.e.\ addition)?
	\item the cartesian monoidal structure $(\1,\times)$ (i.e.\ multiplication)?
	\item the parallel monoidal structure $(\yon,\otimes)$ (i.e.\ taking the parallel product)?
	\item composition of polynomials $p\circ q$?
\qedhere
\end{enumerate}
\begin{solution}
Here $A, B, A', B' \in \smset$.
\begin{enumerate}
    \item We determine whether various classes of polynomials are closed under addition.
    \begin{enumerate}
        \item Constant polynomials are closed under addition: given constants $A, B$, their sum $A + B$ is also a constant polynomial.
        \item Linear polynomials are closed under addition: given linear polynomials $A\yon, B\yon$, their sum $A\yon + B\yon \iso (A + B)\yon$ is also a linear polynomial.
        \item Representable polynomials are \emph{not} closed under addition: for example, $\yon$ is a representable polynomial, but the sum of $\yon$ with itself, $\2\yon$, is not.
        \item Monomials are \emph{not} closed under addition: for example, $\yon$ and $\2\yon^\3$ are monomials, but their sum $\yon + \2\yon^\3$ is not.
    \end{enumerate}
    \item We determine whether various classes of polynomials are closed under multiplication.
    The results below follow from \cref{exc.general_poly_times} \cref{exc.general_poly_times.monomial}.
    \begin{enumerate}
        \item Constant polynomials are closed under multiplication: given constants $A, B$, their product $AB$ is also a constant polynomial.
        \item Linear polynomials are \emph{not} closed under multiplication: for example, $\yon$ and $\2\yon$ are linear polynomials, but their product $\2\yon^\2$ is not.
        \item Representable polynomials are closed under multiplication: given representables $\yon^A, \yon^B$, their product $\yon^{A+B}$ is also a representable polynomial.
        \item Monomials are closed under multiplication: given monomials $B\yon^A, B'\yon^{A'}$, their product $BB'\yon^{A+A'}$ is also a monomial.
    \end{enumerate}
    \item We determine whether various classes of polynomials are closed under taking parallel products.
    The results below follow from \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial}.
    \begin{enumerate}
        \item Constant polynomials are closed under taking parallel products: given constants $A, B$, their parallel product $AB$ is also a constant polynomial.
        \item Linear polynomials are closed under taking parallel products: given linear polynomials $A\yon, B\yon$, their parallel product $AB\yon$ is also a linear polynomial.
        \item Representable polynomials are closed under taking parallel products: given representables $\yon^A, \yon^B$, their parallel product $\yon^{AB}$ is also a representable polynomial.
        \item Monomials are closed under taking parallel products: given monomials $B\yon^A, B'\yon^{A'}$, their parallel product $BB'\yon^{AA'}$ is also a monomial.
    \end{enumerate}
    \item We determine whether various classes of polynomials are closed under composition. (Recall that we can think of computing the composite $p \circ q$ of $p, q \in \poly$ as replacing each appearance of $\yon$ in $p$ with $q$.)
    \begin{enumerate}
        \item Constant polynomials are closed under composition: given constants $A, B$, their composite $A \circ B \iso A$ is also a constant polynomial.
        \item Linear polynomials are closed under composition: given linear polynomials $A\yon, B\yon$, their composite $A\yon \circ B\yon \iso A(B\yon) \iso AB\yon$ is also a linear polynomial.
        \item Representable polynomials are closed under composition: given representables $\yon^A, \yon^B$, their composite $\yon^A \circ \yon^B \iso (\yon^B)^A \iso \yon^{BA}$ is also a representable polynomial.
        \item Monomials are closed under taking parallel products: given monomials $B\yon^A, B'\yon^{A'}$, their composite $B\yon^A \circ B'\yon^{A'} \iso B(B'\yon^{A'})^A \iso BB'^A\yon^{A'A}$ is also a monomial.
    \end{enumerate}
\end{enumerate}
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.ff_const_set_to_poly}
There is a fully faithful functor $\smset\to\poly$ sending $A\mapsto A\yon^\0=A$.
\end{proposition}
\begin{proof}
By \eqref{eqn.colax_poly_map}, a map $f\colon A\yon^\0\to B\yon^\0$ consists of a function $f\colon A\to B$ and, for each $a\in A$, a function $\0\to\0$. There is only one function $\0\to\0$, so $f$ can be identified with just a map of sets $A\to B$.
\end{proof}

\begin{proposition}\label{prop.ff_lin_set_to_poly}
There is a fully faithful functor $\smset\to\poly$ sending $A\mapsto A\yon$.
\end{proposition}
\begin{proof}
By \eqref{eqn.colax_poly_map}, a map $f\colon A\yon^\1\to B\yon^\1$ consists of a function $f\colon A\to B$ and for each $a\in A$ a function $\1\to\1$. There is only one function $\1\to\1$, so $f$ can be identified with just a map of sets $A\to B$.
\end{proof}

\begin{theorem}\label{thm.adjoint_quadruple}
$\poly$ has an adjoint quadruple with $\smset$:
\begin{equation}\label{eqn.adjoints_galore}
\begin{tikzcd}[column sep=60pt, background color=theoremcolor]
  \smset
  	\ar[r, shift left=7pt, "A" description]
		\ar[r, shift left=-21pt, "A\yon"']&
  \poly
  	\ar[l, shift right=21pt, "p(\0)"']
  	\ar[l, shift right=-7pt, "p(\1)" description]
	\ar[l, phantom, "\scriptstyle\Leftarrow"]
	\ar[l, phantom, shift left=14pt, "\scriptstyle\Rightarrow"]
	\ar[l, phantom, shift right=14pt, "\scriptstyle\Rightarrow"]
\end{tikzcd}
\end{equation}
where the functors have been labeled by where they send $A\in\smset$ and $p\in \poly$. 

Both rightward functors are fully faithful.
\end{theorem}
\begin{proof}
For any set $A$, there is a functor $\poly\to\smset$ given by sending $p$ to $p(A)$; by the Yoneda lemma, it is the functor $\poly(\yon^A,-)$. This, together with \cref{prop.ff_const_set_to_poly,prop.ff_lin_set_to_poly}, gives us the four functors and the fact that the two rightward functors are fully faithful. It remains to provide the following three natural isomorphisms:
\[
\poly(A,p)\iso\smset(A,p(\0))\qquad
\poly(p,A)\iso\smset(p(\1),A)\qquad
\poly(A\yon,p)\iso\smset(A,p(\1)).
\]
All three come from our formula \eqref{eqn.main_formula} for computing general hom-sets in $\poly$; we leave the details to the reader in \cref{exc.adjoint_quadruple}.
\end{proof}

\begin{exercise}\label{exc.adjoint_quadruple}
Here we prove the remainder of \cref{thm.adjoint_quadruple} using \eqref{eqn.main_formula}:
\begin{enumerate}
	\item Provide a natural isomorphism $\poly(A,p)\iso\smset(A,p(\0))$.
	\item \label{exc.adjoint_quadruple.pos_const} Provide a natural isomorphism $\poly(p,A)\iso\smset(p(\1),A)$.
	\item \label{exc.adjoint_quadruple.linear_pos} Provide a natural isomorphism $\poly(A\yon,p)\iso\smset(A,p(\1))$.
\qedhere
\end{enumerate}
\begin{solution}
We complete the proof of \cref{thm.adjoint_quadruple} by exhibiting three natural isomorphisms, all special cases of \eqref{eqn.main_formula}, as follows.
\begin{enumerate}
    \item By \eqref{eqn.main_formula}, we have the natural isomorphism
    \[
        \poly(A, p) % \iso \prod_{a \in A} \sum_{i \in p(\1)} A[a]^{p[i]}
        \iso \prod_{a \in A} \sum_{i \in p(\1)} \0^{p[i]}.
    \]
    As $\0^{p[i]}$ is $\1$ if $p[i] \iso \0$ and $\0$ otherwise, it follows that
    \[
        \poly(A, p) \iso \prod_{a \in A} \{ i \in p(\1) \ | \ p[i] \iso \0 \} \iso \prod_{a \in A} p(\0) \iso \smset(A, p(0)).
    \]
    \item By \eqref{eqn.main_formula}, we have the natural isomorphism
    \begin{align*}
        \poly(p, A) %&\iso \prod_{i \in p(\1)} \sum_{a \in A} p[i]^{A[a]} \\
        &\iso \prod_{i \in p(\1)} \sum_{a \in A} p[i]^\0 \\
        &\iso \prod_{i \in p(\1)} \sum_{a \in A} \1 \\
        &\iso \prod_{i \in p(\1)} A \\
        &\iso \smset(p(\1), A).
    \end{align*}
    \item By \eqref{eqn.main_formula}, we have the natural isomorphism
    \begin{align*}
        \poly(A\yon, p) %&\iso \prod_{a \in A} \sum_{i \in p(\1)} (A\yon)[a]^{p[i]} \\
        &\iso \prod_{a \in A} \sum_{i \in p(\1)} \1^{p[i]} \\
        &\iso \prod_{a \in A} \sum_{i \in p(\1)} \1 \\
        &\iso \prod_{a \in A} p(\1) \\
        &\iso \smset(A, p(\1)).
    \end{align*}
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.positions_maps_yon}
Show that for any polynomial $p$, its set $p(\1)$ of positions is in bijection with the set of functions $\yon\to p$.
\begin{solution}
Given $p \in \poly$, we wish to show that $p(\1)$ is in bijection with the set of functions $\yon \to p$.
In fact, this follows directly from the Yoneda lemma, but we can also invoke the isomorphism from \cref{exc.adjoint_quadruple} \cref{exc.adjoint_quadruple.linear_pos} with $A \coloneqq \1$ to observe that
\[
    p(1) \iso \smset(\1, p(\1)) \iso \poly(\yon, p).
\]
\end{solution}
\end{exercise}

In \cref{thm.adjoint_quadruple} we see that $p\mapsto p(\0)$ and $p\mapsto p(\1)$ have left adjoints. This is true more generally for any set $A$ in place of $\0$ and $\1$, as we show in \cref{cor.substituting_adj}. However, the fact that $p\mapsto p(\1)$ is itself the left adjoint of the left adjoint of $p\mapsto p(\0)$---and hence that we have the \emph{quadruple} of adjunctions in \eqref{eqn.adjoints_galore}---is special to $A=\0,\1$.

Next we note that the set of polynomial morphisms $p\to q$ % todo: what was the rest of this sentence??
% copower-hom-power two-variable adjunction

\begin{proposition}\label{prop.two_var_adj}
There is a two-variable adjunction between $\poly$, $\smset$, and $\poly$:%
% \footnote{The first set $\poly(Ap,q)$ involves the $A$-fold coproduct of $p$ and the middle set $\poly(p,q^A)$ involves the $A$-fold product of $q$, neither of which we have proven exists. For organizational purposes, we put that off until \cref{prop.coprod_prod_poly}.
% }
\begin{equation}\label{eqn.two_var_adj}
\poly(Ap,q) \iso \smset(A,\poly(p,q)) \iso \poly(p,q^A).
\end{equation}
\end{proposition}
\begin{proof}
Since $Ap$ is the $A$-fold coproduct of $p$ and $q^A$ is the $A$-fold product of $q$, the universal properties of coproducts and products give natural isomorphisms
\[\poly(Ap,q)\cong\prod_{a\in A}\poly(p,q)\cong\poly(p,q^A).\]
The middle set is naturally isomorphic to $\smset(A,\poly(p,q))$, completing the proof.
\end{proof}

Replacing $p$ with $\yon^B$ in \eqref{eqn.two_var_adj}, we obtain the following using the Yoneda lemma.

\begin{corollary}\label{cor.substituting_adj}
For any set $B$ there is an adjunction
\[
\adj{\smset}{A\yon^B}{q(B)}{\poly}
\]
where the functors are labeled by where they send $q\in\poly$ and $A\in\smset$.
\end{corollary}

\begin{exercise}
Prove \cref{cor.substituting_adj} from \cref{prop.two_var_adj}.
\begin{solution}
To prove \cref{cor.substituting_adj}, it suffices to exhibit a natural isomorphism
\[
    \poly(A\yon^B, q) \iso \smset(A, q(B)).
\]
Replacing $p$ with $\yon^B$ in \eqref{eqn.two_var_adj} from \cref{prop.two_var_adj}, we obtain the natural isomorphism
\[
    \poly(A\yon^B, q) \iso \smset(A, \poly(\yon^B, q)).
\]
By the Yoneda lemma, $\poly(\yon^B, q)$ is naturally isomorphic to $q(B)$, yielding the desired result.
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.yoneda_left_adjoint}
The Yoneda embedding $A\mapsto \yon^A$ has a left adjoint
\[
\adjr{\smset\op}{\yon^-}{\Gamma}{\poly}
\]
where $\Gamma(p) \coloneqq \poly(p, \yon) \iso \prod_{i\in p(\1)}p[i]$, as in \eqref{eqn.gamma_def} and \eqref{eqn.gamma_prod}.
That is, there is a natural isomorphism
\begin{equation} \label{eqn.yoneda_left_adjoint}
    \poly(p, \yon^A) \iso \smset(A, \Gamma(p)).
\end{equation}
\end{proposition}
\begin{proof}
By \eqref{eqn.main_formula}, we have the natural isomorphism
\[
    \poly(p, \yon^A) \iso \prod_{i \in p(\1)} p[i]^A,
\]
which in turn is naturally isomorphic to $\smset(A, \Gamma(p))$ by \eqref{eqn.gamma_prod}.
\end{proof}

% \begin{exercise} % Already basically done
% Show that $\Gamma(p)\cong\ihom{p,\yon}(\1)$ where $\ihom{-,-}$ is as in \cref{prop.dirichlet_closure}.
% \end{exercise}

\begin{corollary}[Principle monomial]\label{cor.principle_monomial}
There is an adjunction
\[
    \adj{\poly}{{(p(\1),\Gamma(p))}}{A\yon^B}{\smset\times\smset\op}
\]
where the functors are labeled by where they send $p\in\poly$ and $(A,B)\in\smset\times\smset\op$.
That is, there is a natural isomorphism
\begin{equation} \label{eqn.principle_monomial}
    \poly(p, A\yon^B) \iso \smset(p(1), A) \times \smset(B, \Gamma(p)).
\end{equation}
\end{corollary}
\begin{proof}
By the universal property of the product of $A$ and $\yon^B$, we have a natural isomorphism
\[
    \poly(p, A\yon^B) \iso \poly(p, A) \times \poly(p, \yon^B).
\]
Then the desired natural isomorphism follows from \cref{exc.adjoint_quadruple} \cref{exc.adjoint_quadruple.pos_const} and \eqref{eqn.yoneda_left_adjoint}.
\end{proof}

\begin{exercise}
Use \eqref{eqn.principle_monomial} together with \eqref{eqn.dir_dual} and \eqref{eqn.poly_closure_brackets} to find an alternative proof for \cref{prop.situations2}, i.e.\ that there is an isomorphism
\[
    \Gamma(p\otimes q) \iso \smset\big(q(\1),\Gamma(p)\big) \times \smset\big(p(\1),\Gamma(q)\big).
\]
for any $p,q\in\poly$.
\begin{solution}
% The universal properties of the adjunctions
% \[
%       \adj[40pt]{\poly}{{(-(\1),\Gamma(-))}}{-\yon^-}{\smset\times\smset\op}
%       \qqand
%       \adj{\poly}{-\otimes q}{{\ihom{q,-}}}{\poly}
% \]
% from \cref{cor.principle_monomial,prop.dirichlet_closure} together with the isomorphism $\ihom{p,\yon} \iso \Gamma(p)\yon^{p(\1)}$ from \eqref{eqn.dir_dual} give
We have the following chain of natural isomorphisms:
\begin{align*}
	\Gamma(p \otimes q) &=
	\poly(p \otimes q,\yon) 
	\tag*{\eqref{eqn.gamma_def}} \\
	&\iso
	\poly(p, \ihom{q,\yon}) 
	\tag*{\eqref{eqn.poly_closure_brackets}} \\
	&\iso
	\poly(p, \Gamma(q)\yon^{q(\1)})
	\tag*{\eqref{eqn.dir_dual}} \\
% 	&\iso
% 	(\smset\times\smset\op)\Big(\big((p(\1),\Gamma(p)\big),\big(\Gamma(q),q(\1)\big)\Big) \\
	&\iso
	\smset\big(p(\1),\Gamma(q)\big) \times \smset\big(q(\1),\Gamma(p)\big).
	\tag*{\eqref{eqn.principle_monomial}}
\qedhere
\end{align*}
\end{solution}
\end{exercise}

%-------- Section --------%
\section{Epi-mono factorization}

\begin{proposition}\label{prop.monics_in_poly}
Let $f \colon p \to q$ be a morphism in $\poly$. It is a monomorphism if and only if the on-positions function $f_1 \colon p(\1) \to q(\1)$ is a monomorphism in $\smset$ and, for each $i \in p(\1)$, the on-directions function $f_i^\sharp \colon q[f_1(i)]\to p[i]$ is an epimorphism in $\smset$.
\end{proposition}
\begin{proof}
To prove the forward direction, suppose that $f$ is a monomorphism.
Since $p\mapsto p(\1)$ is a right adjoint (\cref{thm.adjoint_quadruple}), it preserves monomorphisms, so the on-positions function $f_1$ is also a monomorphism.

We now need to show that for any $i\in p(\1)$, the on-directions function $f_i^\sharp \colon q[f_1(i)] \to p[i]$ is an epimorphism.
Suppose we are given a set $A$ and a pair of functions $g^\sharp,h^\sharp\colon p[i]\tto A$ with $f^\sharp_i \then g^\sharp = f^\sharp_i \then h^\sharp$.
Then there exist morphisms $g,h \colon \yon^A \tto p$ whose on-positions functions both pick out $i$ and whose on-directions functions are $g^\sharp$ and $h^\sharp$, so that $g \then f = h \then f$.
As $f$ is a monomorphism, $g = h$; in particular, their on-directions functions $g^\sharp$ and $h^\sharp$ are equal, as desired.

Conversely, suppose that $f_1$ is a monomorphism and that, for each $i\in p(\1)$, the function $f^\sharp_i$ is an epimorphism.
Let $r$ be a polynomial and $g,h\colon r\tto p$ be two morphisms such that $g \then f = h \then f$.
Then $g_1 \then f_1 = h_1 \then f_1$, which implies $g_1 = h_1$; we'll consider $g_1$ the default representation.
We also have that $f^\sharp_{g_1(k)} \then g^\sharp_k = f^\sharp_{g_1(k)} \then h^\sharp_k$ for any $k \in r(\1)$. But $f^\sharp_{g_1(k)}$ is an epimorphism, so in fact $g^\sharp_k = h^\sharp_k$, as desired.
\end{proof}

\begin{example}\label{ex.clock_in_N}
Choose a finite nonempty set $\ord{k}$ for $1\leq k\in\nn$, e.g.\ $\ord{k}=\1\2$. There is a monomorphism
\[
f \colon\ord{k}\yon^{\ord{k}}\to\nn\yon^\nn
\]
such that the trajectory ``going around and around the $k$-clock'' comes from the usual counting trajectory \cref{ex.counting_trajectory} $\nn\yon^\nn\to\yon$.

On positions, we have $f_1(i)=i$ for all $i \in \ord{k}$. On directions, for any $i \in \ord{k}$, we have $f^\sharp_i(n) = n \mod k$ for all $n \in \nn$.
\end{example}

\begin{exercise}
In \cref{ex.clock_in_N}, we gave a map $\1\2\yon^{\1\2}\to\nn\yon^\nn$. This allows us to turn any dynamical system with $\nn$-many states into a dynamical system with 12 states, while keeping the same interface---say, $p$. 

Explain how the behavior of the new system $\1\2\yon^{\1\2}\to p$ would be seen to relate to the behavior of the old system $\nn\yon^\nn\to p$.
\begin{solution}
We are given a monomorphism $f \colon \1\2\yon^{\1\2} \to \nn\yon^\nn$ from \cref{ex.clock_in_N}.
Let $g \colon \nn\yon^\nn \to p$ be a dynamical system with yield function $g_1 \colon \nn \to p(\1)$ and update functions $g^\sharp_n \colon p[g_1(n)] \to \nn$ for each state $n \in \nn$.
Then the new composite dynamical system $h \coloneqq f \then g$ has a yield function $h_1 \colon \1\2 \to p(\1)$ which sends each state $i \in \1\2$ to the output $h_1(i) = g_1(f_1(i)) = g_1(i)$, the same output that the original system yielded in the state $i \in \nn$.
Meanwhile, the update function for each state $i \in \1\2$ is a function $h^\sharp_i \colon p[g_1(i)] \to \1\2$ which, given an input $d \in p[g_1(i)]$, updates the state from $i$ to $h^\sharp_i(d) = f^\sharp_{g_1(i)}(g^\sharp_i(d)) = g^\sharp_i(d) \mod 12$, which is where the original system would have taken the same state to, but reduced modulo 12.
In other words, the new system behaves like the old system but with only the states in $\1\2 \ss \nn$ retained, and on any input that would have caused the old system to move to a state outside of $\1\2$, the new system moves to the equivalent state (modulo 12) within $\1\2$ instead. 
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.epis_in_poly}
Let $f \colon p \to q$ be a morphism in $\poly$. It is an epimorphism if and only if the function $f_1 \colon p(\1) \to q(\1)$ is an epimorphism in $\smset$ and, for each $j\in q(\1)$, the induced function
\[
    f^\flat_j \colon q[j] \to \prod_{\substack{i\in p(\1), \\ f_1(i)=j}} p[i]
\]
from \eqref{eqn.useful_misc472} is a monomorphism.
\end{proposition}
\begin{proof}
To prove the forward direction, suppose that $f$ is an epimorphism. Since $p \mapsto p(\1)$ is a left adjoint (\cref{thm.adjoint_quadruple}), it preserves epimorphisms, so the on-positions function $f_1$ is also a epimorphism.

We now need to show that for any $j\in q(\1)$, the induced function $f^\flat_j$ is a monomorphism.
Suppose we are given a set $A$ and a pair of functions $g',h'\colon A\tto q[j]$ with $g' \then f^\flat_j = h' \then f^\flat_j$.
They can be identified with morphisms $g,h\colon q\tto \yon^A+\1$, which send the $j$-component to the first component, $\yon^A$, and send all other component to the second component, $\1$. It is easy to check that $fg=fh$, hence $g=h$, and hence $g^\sharp=h^\sharp$ as desired.

Then we can construct morphisms $g,h \colon q \tto \yon^A+\1$ whose on-positions functions both send $j$ to the first position, corresponding to $\yon^A$, and all other positions to the second position, corresponding to $\1$.
In addition, we let the on-directions functions be $g^\sharp_j \coloneqq g'$ and $h^\sharp_j \coloneqq h'$.
Then $f \then g = f \then h$.
As $f$ is an epimorphism, $g = h$; in particular, their on-directions functions are equal, so $g' = h'$, as desired.

Conversely, suppose that $f_1$ is an epimorphism and that, for each $j\in q(\1)$, the function $f^\flat_j$ is a monomorphism. 
Let $r$ be a polynomial and $g,h\colon q\tto r$ be two morphisms such that $f \then g = f \then h$.
Then $f_1 \then g_1 = f_1 \then h_1$, which implies $g_1=h_1$; we'll consider $g_1$ the default representation.
We also have that $g^\sharp_{f_1(i)} \then f^\sharp_i = h^\sharp_{f_1(i)} \then f^\sharp_i$ for any $i\in p(\1)$.
It follows that, for any $j \in q(\1)$, the two composites
\[
\begin{tikzcd}
	r[g_1(j)] \ar[r, shift left, "g^\sharp_j"] \ar[r, shift right, "h^\sharp_j"'] & q[j] \ar[r, "f^\flat_j"] & \displaystyle\prod_{\substack{i\in p(\1), \\ f_1(i)=j}} p[i]
\end{tikzcd}
\]
are equal, which implies that $g^\sharp_j=h^\sharp_j$ as desired.
\end{proof}

% Insert exercise exploring the difference between the epi proposition and the one about monos

\begin{exercise}
Show that the only way for a map $p\to\yon$ to \emph{not} be an epimorphism is when $p=0$.
\begin{solution}
Given $p \in \poly$ and a map $f \colon p \to \yon$, we will use \cref{prop.epis_in_poly} to show that either $f$ is an epimorphism or $p = \0$.
First, note that $f_1 \colon p(\1) \to \1$ must be an epimorphism unless $p(\1) \iso \0$, in which case $p = \0$.
Next, note that the induced function
\[
    f^\flat \colon \1 \to \prod_{i\in p(\1)} p[i]
\]
from \eqref{eqn.useful_misc472} must be a monomorphism.
So it follows from \cref{prop.epis_in_poly} that either $f$ is an epimorphism or $p = \0$.
\end{solution}
\end{exercise}

\begin{exercise}
Let $A$ and $B$ be sets and $AB$ their product. Find an epimorphism $\yon^A+\yon^B\surj\yon^{AB}$.
\begin{solution}
Given sets $A$ and $B$, by \cref{prop.epis_in_poly}, a morphism $f \colon \yon^A + \yon^B \to \yon^{AB}$ is an epimorphism if its on-positions function $f_1 \colon \2 \to \1$ is an epimorphism (which must be true) and if the induced function
\[
    f^\flat \colon AB \to \prod_{i \in \2} (\yon^A + \yon^B)[i] \iso AB
\]
is a monomorphism.
If we take the on-directions functions $AB \to A$ and $AB \to B$ of $f$ to be the canonical projections, then the induced function $f^\flat \colon AB \to AB$ would be the identity, which is indeed a monomorphism.
So $f$ would be an epimorphism.
\end{solution}
\end{exercise}

\begin{exercise}
Suppose a polynomial morphism is both a monomorphism and an epimorphism; it is then an isomorphism? (That is, is $\poly$ \emph{balanced}?)

Hint: You may use the following facts.
\begin{enumerate}
    \item A function that is both a monomorphism and an epimorphism in $\smset$ is an isomorphism.
    \item A polynomial morphism is an isomorphism if and only if the on-positions function is an isomorphism and every on-directions function is an isomorphism.
\end{enumerate}
\begin{solution}
Let $f \colon p \to q$ be a morphism in $\poly$ that is both a monomorphism and an epimorphism.
We claim that $f$ is an isomorphism.
By \cref{prop.monics_in_poly} and \cref{prop.epis_in_poly}, the on-positions function $f_1 \colon p(\1) \to q(\1)$ is both a monomorphism and an epimorphism, so it is an isomorphism.
Meanwhile, \cref{prop.epis_in_poly} says that, for each $j \in q(\1)$, the induced function
\[
    f^\flat_j \colon q[j] \to \prod_{\substack{i \in p(\1), \\ f_1(i) = j}} p[i]
\]
is a monomorphism.
As $f_1$ is an isomorphism, it follows that for each $i \in p(\1)$, the function
\[
    f^\flat_{f_1(i)} \colon q[f_1(i)] \to p[i]
\]
is a monomorphism.
But this is just the on-directions function $f^\sharp_i$ of $f$.
From \cref{prop.monics_in_poly}, we also know that $f^\sharp_i$ is an epimorphism.
It follows that every on-directions function of $f$ is an isomorphism.
Hence $f$ itself is an isomorphism.
\end{solution}
\end{exercise}

\begin{exercise}[Epi-mono factorization]\label{exc.mono_epi_poly}
% Suppose $p,q\in\poly$ are polynomials and $f \colon p \to q$ is a morphism.
\begin{enumerate}
	\item Can every morphism in $\poly$ be factored as an epic followed by a monic?
	\item Is your factorization unique up to isomorphism?
\qedhere
\end{enumerate}
\begin{solution}
**
\end{solution}
\end{exercise}

%-------- Section --------%
\section{Limits, colimits, and cartesian closure}

We have already seen that $\poly$ has all coproducts (\cref{prop.poly_coprods}) and products (\cref{prop.poly_prods}).
We will now see that $\poly$ has all limits and colimits, and moreover it is cartesian closed.

% \begin{proposition}\label{prop.completely_distributive}
% The category $\poly$ is completely distributive, i.e.\ for any set $A$, sets $(B_a)_{a\in A}$, and polynomials $(p_{(a,b),})_{a\in A, b\in B_a}$, there is an isomorphism
% \[
%   \sum_{b\colon\prod_{a\in A}B(a)}\;\prod_{a\in A}\;p_{(a,b(a)),}
%   \cong
% 	\prod_{a\in A}\;\sum_{b\in B(a)}\;p_{(a,b),}
% \]
% \end{proposition}
% \begin{proof}
% By the universal property of coproducts and products, to provide a morphism $\sum_{b\colon\prod_{a\in A}B(a)}\prod_{a\in A}p_{(a,b),}
%   \to	\prod_{a\in A}\sum_{b\in B(a)}p_{(a,b),}$, it suffices to fix an arbitrary $b_0\in\prod_{a\in A}B(a)$ and $a_0\in A$ and provide a morphism $\prod_{a\in A}p_{(a,b_0(a)),}\to\sum_{b\in B(a_0)}p_{(a_0,b),}$. We do so by first projecting onto the $a_0$-factor $\prod_{a\in A}p_{(a,b_0(a)),}\to p_{(a_0, b_0(a_0)),}$ and then including into the $b_0(a_0)$-summand $p_{(a_0,b_0(a_0)),}\to\sum_{b\in B(a_0)}p_{(a_0,b),}$. This establishes the morphism.
  
%   To see that it is an isomorphism in $\poly$, i.e.\ a natural isomorphism of functors, it suffices to check it on components. So fix $X\in\smset$. The induced map
%   \[
%   \sum_{b\colon\prod_{a\in A}B(a)}\;\prod_{a\in A}\;p_{(a,b(a)),}
%   \cong
% 	\prod_{a\in A}\;\sum_{b\in B(a)}\;p_{(a,b),}
%   \]
%   is exactly as in \eqref{eqn.set_completely_distributive}, which we proved is an isomorphism. This completes the proof.
% \end{proof}

% \begin{exercise}
% How is the usual distributive law,
% \[
% p(q+r)\cong pq+pr
% \]
% for $p,q,r\in\poly$, a special case of \cref{prop.completely_distributive}?
% \end{exercise}

%---- Subection ----%
\subsection{Cartesian closure}

For any two polynomials $q,r$, define $r^q\in\poly$ by the formula
\begin{equation}\label{eqn.exponential}
  r^q\coloneqq\prod_{j\in q(\1)}r\circ(\yon+q[j])
\end{equation}
where $\circ$ denotes composition.

Before proving that this really is an exponential in $\poly$, which we do in \cref{thm.poly_cart_closed}, we first get some practice with it.

\begin{example}
Let $A$ be a set. We've been writing the polynomial $A\yon^\0$ simply as $A$, so it better be true that the there is an isomorphism 
\[
    \yon^A \iso \yon^{A\yon^\0}
\]
in order for the notation to be consistent. 
Luckily, this is true.
By \eqref{eqn.exponential}, we have
\[
    \yon^{A\yon^\0} = \prod_{a\in A} \yon \circ (\yon+\0) \iso \yon^A
\]
\end{example}

\begin{exercise}
Compute the following exponentials in $\poly$ using \eqref{eqn.exponential}:
\begin{enumerate}
	\item $p^\0$ for an arbitrary $p\in\poly$.
	\item $p^\1$ for an arbitrary $p\in\poly$.
	\item $\1^p$ for an arbitrary $p\in\poly$.
	\item $A^p$ for an arbitrary $p\in\poly$ and $A\in\smset$.
	\item $\yon^\yon$.
	\item $\yon^{\4\yon}$.
	\item $(\yon^A)^{\yon^B}$ for arbitrary sets $A,B\in\smset$.
\qedhere
\end{enumerate}
\begin{solution}
We use \eqref{eqn.exponential} to compute various exponentials.
Here $p \in \poly$ and $A, B \in \smset$.
\begin{enumerate}
    \item We have that $p^\0$ is an empty product, so $p^\0 \iso \1$ as expected.
	\item We have that $p^\1 \iso p \circ (\yon + \0) \iso p$, as expected.
	\item We have that $\1^p \iso \prod_{i \in p(\1)} \1 \circ (\yon + p[i]) \iso \1$, as expected.
	\item We have that $A^p \iso \prod_{i \in p(\1)} A \circ (\yon + p[i]) \iso A^{p(\1)}$.
	\item We have that $\yon^\yon \iso \yon \circ (\yon + \1) \iso \yon + \1$.
	\item We have that $\yon^{\4\yon} \iso \prod_{j \in \4} \yon \circ (\yon + \1) \iso (\yon + \1)^\4 \iso \yon^\4 + \4\yon^\3 + \6\yon^\2 + \4\yon + \1$.
	\item We have that $(\yon^A)^{\yon^B} \iso (\yon^A) \circ (\yon + B) \iso (\yon + B)^A \iso \sum_{f \colon A \to \2} B^{f\inv(1)} \yon^{f\inv(2)}$.
\end{enumerate}
\end{solution}
\end{exercise}

% \begin{exercise} % Immediate from previous exercise
% Using \eqref{eqn.exponential}, show that the functor $\smset\to\poly$ that sends each set $A$ to the constant polynomial $A$ preserves exponentials.
% That is, given sets $A, B \in \smset$, the set $B^A$ as a constant polynomial coincides with the exponential in $\poly$ that is the constant polynomial $B$ raised to the constant polynomial $A$.
% \begin{solution}
% By \eqref{eqn.exponential}, the exponential that is the constant polynomial $B$ raised to the constant polynomial $A$ can be written as
% \[
%     \prod_{a \in A} B \circ (\yon + A[a]) \iso \prod_{a \in A} B \iso B^A.
% \]
% \end{solution}
% \end{exercise}

\begin{theorem}\label{thm.poly_cart_closed}
The category $\poly$ is Cartesian closed. That is, we have a natural isomorphism
\[
    \poly(p,r^q) \iso \poly(p\times q,r),
\]
where $r^q$ is the polynomial defined in \eqref{eqn.exponential}.
\end{theorem}
\begin{proof}
We have the following chain of natural isomorphisms:
\begin{align*}
	\poly(p, r^q) &\iso
	\poly\Big(p, \prod_{j \in q(\1)} r \circ (\yon+q[j])\Big)
	\tag*{\eqref{eqn.exponential}} \\
% 	&\iso
% 	\prod_{j\in q(\1)}\poly(p,r\circ(\yon+q[j]))
% 	\tag{Universal property of products} \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}\poly\big(\yon^{p[i]},r\circ(\yon+q[j])\big)
	\tag{Universal property of (co)products} \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}r\circ(p[i]+q[j])
	\tag{Yoneda lemma} \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}\sum_{k\in r(\1)}(p[i]+q[j])^{r[k]}
	\\
	&\iso
	\prod_{(i,j) \in (p \times q)(\1)} \; \sum_{k\in r(\1)}(p \times q)[(i, j)]^{r[k]}
	\tag*{\eqref{eqn.poly_times}} \\
	&\iso
	\poly(p \times q,r).
	\tag*{\eqref{eqn.main_formula}}
\end{align*}
\end{proof}

\begin{exercise}
Use \cref{thm.poly_cart_closed} to show that for any polynomials $p,q$, there is a canonical evaluation map
\begin{equation*}%\label{eqn.eval_times}
	\text{eval}\colon p^q \times q \to p.
\end{equation*}
\begin{solution}
By \cref{thm.poly_cart_closed}, there is a natural isomorphism
\[
    \poly(p^q, p^q) \iso \poly(p^q \times q, p).
\]
Under this isomorphism, there exists a map $\text{eval} \colon p^q \times q \to p$ corresponding to the identity map on $p^q$.
The map $\text{eval}$ is the canonical evaluation map.
\end{solution}
\end{exercise}

%---- Subection ----%
\subsection{Limits and colimits}

\begin{theorem}\label{thm.poly_limits}
The category $\poly$ has all limits.
\end{theorem}
\begin{proof}
A category has all limits if and only if it has products and equalizers, so by \cref{prop.poly_prods}, it suffices to show that $\poly$ has equalizers. 

We claim that equalizers in $\poly$ are simply equalizers on positions and coequalizers on directions.
More precisely, let $f,g \colon p \tto q$ be two maps of polynomials.
We construct the equalizer $p'$ of $f$ and $g$ as follows.\footnote{If we're being precise, a ``(co)equalizer'' is an object equipped with a map, but we will use the term to refer to either just the object or just the map when the context is clear.}
We define its set of positions $p'(\1)$ to be the equalizer of $f_1,g_1 \colon p(\1) \tto q(\1)$ in $\smset$; that is,
\[
    p'(\1) \coloneqq \{i \in p(\1) \ | \ f_1(i) = g_1(i)\}.
\]
Then for each $i \in p'(\1)$, we can define the set of directions $p'[i]$ to be the coequalizer of $f^\sharp_i, g^\sharp_i \colon q[f_1(i)] \tto p[i]$.
In this way, we obtain a polynomial $p'$ that comes equipped with a morphism $e \colon p' \to p$.
One can check that $p'$ together with $e$ satisfies the universal property of the equalizer of $f$ and $g$; see \cref{exc.poly_limits}.
\end{proof}

\begin{exercise}\label{exc.poly_limits}
Complete the proof of \cref{thm.poly_limits} as follows:
\begin{enumerate}
	\item We said that $p'$ comes equipped with a morphism $e \colon p' \to p$; what is it?
	\item Show that $e \then f = e \then g$.
	\item Show that $e$ is the equalizer of the pair $f,g$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item The morphism $e \colon p' \to p$ can be characterized as follows.
    The on-positions function $e_1 \colon p'(\1) \to p(\1)$ is the equalizer of $f_1, g_1 \colon p(\1) \tto q(\1)$ in $\smset$.
    In particular, $e_1$ is the canonical inclusion that sends each element of $p'(\1)$ to the same element in $p(\1)$.
    Then for each $i \in p'(\1)$, the on-directions function $e^\sharp_i \colon p[i] \to p'[i]$ is the coequalizer of $f^\sharp_i, g^\sharp_i \colon q[f_1(i)] \tto p[i]$ in $\smset$.
    
    \item To show that $e \then f = e \then g$, it suffices to show that both sides are equal on positions and on directions.
    On positions, $e_1$ is defined to be the equalizer of $f_1$ and $g_1$, so $e_1 \then f_1 = e_1 \then g_1$.
    Then for each $i \in p'(\1)$, the on-directions function $e^\sharp_i$ is defined to be the coequalizer of $f^\sharp_i$ and $g^\sharp_i$, so $f^\sharp_i \then e^\sharp_i = g^\sharp_i \then e^\sharp_i$.
    
    \item To show that $e$ is the equalizer of $f$ and $g$, it suffices to show that for any $r \in \poly$ and map $a \colon r \to p$ satisfying $a \then f = a \then g$, there exists a unique map $h \colon r \to p'$ for which $a = h \then e$, so that the following diagram commutes.
    \begin{equation*} %\label{eqn.eq_univ_prop}
    \begin{tikzcd}
        p' \ar[r, "e"] & p \ar[r, "f", shift left] \ar[r, "g"', shift right] & q \\
        r \ar[u, "h", dashed] \ar[ur, "a"']
    \end{tikzcd}
    \end{equation*}
    In order for $a = h \then e$ to hold, we must have $a_1 = h_1 \then e_1$ on positions.
    But we have that $a_1 \then f_1 = a_1 \then g_1$, so by the universal property of $p'(\1)$ and the map $e_1$ as the equalizer of $f_1$ and $g_1$ in $\smset$, there exists a unique $h_1$ for which $a_1 = h_1 \then e_1$.
    Hence $h$ is uniquely characterized on positions.
    In particular, it must send each $k \in r(\1)$ to $a_1(k) \in p'(\1)$.
    
    Then for $a = h \then e$ to hold on directions, we must have that $a^\sharp_k = e^\sharp_{a_1(k)} \then h^\sharp_k$ for each $k \in r(\1)$.
    But we have that $f^\sharp_{a_1(k)} \then a^\sharp_{a_1(k)} = g^\sharp_{a_1(k)} \then a^\sharp_{a_1(k)}$, so by the universal property of $p'[a_1(k)]$ and the map $e^\sharp_{a_1(k)}$ as the coequalizer of $f^\sharp_{a_1(k)}$ and $g^\sharp_{a_1(k)}$ in $\smset$, there exists a unique $h^\sharp_k$ for which $a^\sharp_k = e^\sharp_{a_1(k)} \then h^\sharp_k$, so that the diagram below commutes.
    \begin{equation*} %\label{eqn.eq_univ_prop_dir}
    \begin{tikzcd}[sep=large]
        p'[a_1(k)] \ar[d, "h^\sharp_k"', dashed] & p[a_1(k)] \ar[l, "e^\sharp_{a_1(k)}"'] \ar[dl, "a^\sharp_k"] & q[f_1(a_1(k))] \ar[l, "f^\sharp_{a_1(k)}"', shift right] \ar[l, "g^\sharp_{a_1(k)}", shift left] \\
        r[k]
    \end{tikzcd}
    \end{equation*}
    Hence $h$ is also uniquely characterized on directions, so it is unique overall.
    Moreover, we have shown that we can define $h$ on positions so that $a_1 = h_1 \then e_1$, and that we can define $h$ on directions such that $a^\sharp_k = e^\sharp_{a_1(k)} \then h^\sharp_k$ for all $k \in r(\1)$.
    It follows that there exists $h$ for which $a = h \then e$.
\end{enumerate}
\end{solution}
\end{exercise}

% **Talk about how limits are just limits on positions and colimits on directions

\begin{example}[Pullbacks in $\poly$]\label{ex.pullbacks_in_poly}
Given $q,q',r \in \poly$ and morphisms $q\To{f} r\From{f'} q'$, the pullback 
\[
\begin{tikzcd}
	p\ar[r, "g'"]\ar[d, "g"']&
	q'\ar[d, "f'"]\\
	q\ar[r, "f"']&
	r\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
 is given as follows.
 The set of positions of $p$ is the pullback of the positions of $q$ and $q'$ over those of $r$ in $\smset$.
 Then at each position $(i, i') \in p(\1) \ss q(\1) \times q'(\1)$ with $f_1(i)=f'_1(i')$, we take the set of directions $p[(i, i')]$ to be the pushout of the directions $q[i]$ and $q'[i']$ over $r[f_1(i)]=r[f_1'(i')]$ in $\smset$.
 These pullback and pushout squares also given the morphisms $g$ and $g'$ on positions and on directions:
\begin{equation}\label{eqn.pullback_poly}
\begin{tikzcd}
	p(\1)\ar[r, "g'_1"]\ar[d, "g_1"']&
	q'(\1)\ar[d, "f_1'"]\\
	q(\1)\ar[r, "f_1"']&
	r(\1)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\qqand
\begin{tikzcd}
	p[(i,i')]\ar[from=r, "(g')^\sharp_{(i,i')}"']\ar[from=d, "g^\sharp_{(i,i')}"]&
	q'[i']\ar[from=d, "(f')^\sharp_{i'}"']\\
	q[i]\ar[from=r, "f^\sharp_i"]&
	r[f_1(i)]\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\end{equation}
\end{example}

\begin{exercise}
Let $p$ be any polynomial.
\begin{enumerate}
	\item There is a canonical choice of morphism $\eta\colon p\to p(\1)$; what is it?
	\item Given an element $i\in p(\1)$, i.e.\ a function (or morphism between constant polynomials) $i\colon\1\to p(\1)$, let $p_i$ be the pullback
	\[
	\begin{tikzcd}
	p_i\ar[r, "g"]\ar[d, "f"']&
	p\ar[d, "\eta"]\\
	\1\ar[r, "i"']&
	p(\1)\ar[ul, phantom, very near end, "\lrcorner"]
	\end{tikzcd}
	\]
	What is $p_i$? What are the maps $f \colon p_i \to \1$ and $g \colon p_i \to p$? \qedhere
\end{enumerate}
\begin{solution}
Here $p \in \poly$.
\begin{enumerate}
    \item The canonical morphism $\eta \colon p \to p(\1)$ is the identity $\eta_1 \colon p(\1) \to p(\1)$ on positions and the empty function on directions.
    
    \item On positions, we have that $p_i(\1)$ along with $f_1$ and $g_1$ form the following pullback square in $\smset$:
    \[
	\begin{tikzcd}
    	p_i(\1) \ar[r, "g_1"] \ar[d, "f_1"'] &
    	p(\1) \ar[d, equals] \\
    	\1 \ar[r, "i"'] &
    	p(\1) \ar[ul, phantom, very near end, "\lrcorner"]
	\end{tikzcd}
	\]
	So $p_i(\1) \coloneqq \{(a, i') \in \1 \times p(\1) \ | \ i = i' \} = \{(1, i)\}$, with $f_1$ uniquely determined and $g_1$ picking out $i \in p(\1)$.
	Then on directions, we have that $p_i[(1,i)]$ along with $f^\sharp_{(1,i)}$ and $g^\sharp_{(1,i)}$ form the following pushout square in $\smset$:
	\[
	\begin{tikzcd}
    	p_i[(1,i)] \ar[from=r, "g^\sharp_{(1,i)}"'] \ar[from=d, "f^\sharp_{(1,i)}"] &
    	p[i] \ar[from=d, "!"'] \\
    	\0 \ar[from=r, "!"] &
    	\0 \ar[ul, phantom, very near end, "\lrcorner"]
    \end{tikzcd}
    \]
    So $p_i[(1,i)] \coloneqq p[i]$, with $f^\sharp_{(1,i)}$ uniquely determined and $g^\sharp_{(1,i)}$ as the identity.
    It follows that $p_i \coloneqq \{(1,i)\}\yon^{p[i]} \iso \yon^{p[i]}$, where $f \colon p_i \to \1$ is uniquely determined and $g \colon p_i \to p$ picks out $i \in p(\1)$ on positions and is the identity on $p[i]$ on directions.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Let $q\coloneqq \yon^\2+\yon$, $q'\coloneqq\2\yon^\3+\yon^\2$, and $r\coloneq\yon+\1$.
\begin{enumerate}
	\item Choose morphisms $f\colon q\to r$ and $f'\colon q'\to r$ and write them down.
	\item Find the pullback of $q\To{f} r\From{f'} q'$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item There are many possible answers, but one morphism $f \colon q \to r$, on positions, sends $1 \in q(\1)$ (corresponding to $\yon^\2$) to $2 \in r(\1)$ (corresponding to $\1$) and $2 \in q(\1)$ (corresponding to $\yon$) to $1 \in r(\1)$ (corresponding to $\yon$).
    Then the on-directions functions $f^\sharp_1 \colon \0 \to \2$ and $f^\sharp_2 \colon \1 \to \1$ are uniquely determined.
    Another morphism $f' \colon q' \to r$, on positions, sends $1 \in q'(\1)$ (corresponding to one of the $\yon^\3$ terms) to $2 \in r(\1)$ and both $2 \in q'(\1)$ (corresponding to the other $\yon^\3$ term) and $3 \in q'(\1)$ (corresponding to the $\yon^\2$ term) to $1 \in r(\1)$.
    Then the on-directions function $(f')^\sharp_1 \colon \0 \to \3$ is uniquely determined, while we can let $(f')^\sharp_2 \colon \1 \to \3$ pick out $3$ and $(f')^\sharp_3 \colon \1 \to \2$ pick out $1$.
    
    \item We compute the pullback $p$ along with the morphisms $g \colon p \to q$ and $g' \colon p \to q'$ of $q\To{f} r\From{f'} q'$ by following \cref{ex.pullbacks_in_poly}.
    We can compute $p(\1)$ by taking the pullback in $\smset$:
    \[
        p(\1) \coloneqq \{(i, i') \in \2 \times \3 \ | \ f_1(i) = f'_1(i)\} = \{(1,1), (2,2), (2,3)\}.
    \]
    Moreover, the on-positions functions $g_1$ and $g'_1$ send each pair in $p(\1)$ to its left component and its right component, respectively.
    
    To compute the set of directions at each position of $p$, we must compute a pushout.
    At $(1,1)$, we have $r[f_1(1)] = r[f'_1(1)] = r[2] = \0$, so the pushout $p[(1,1)]$ is just the sum $q[1] + q'[1] = \2 + \3 \iso \5$.
    Moreover, the on-directions functions $g^\sharp_{(1,1)}$ and $(g')^\sharp_{(1,1)}$ are the canonical inclusions $\2 \to \2 + \3$ and $\3 \to \2 + \3$.
    
    At $(2,2)$, we have $r[f_1(2)] = r[f'_1(2)] = r[1] = \1$, with $f^\sharp_2$ picking out $1 \in \1 = q[2]$ and $(f')^\sharp_2$ picking out $3 \in \3 = q'[2]$.
    So the pushout $p[(2,2)]$ is the set $\1 + \3 = \{(1,1), (2,1), (2,2), (2,3)\}$ but with $(1,1)$ identified with $(2,3)$; we can think of it as the set of equivalence classes $p[(2,2)] \iso \{\{(1,1), (2,3)\}, \{(2,1)\}, \{(2,2)\}\} \iso \3$.
    Moreover, the on-directions function $g^\sharp_{(2,2)}$ maps $1 \mapsto \{(1,1), (2,3)\}$, while the on-directions function $(g')^\sharp_{(2,2)}$ maps $1 \mapsto \{(2,1)\}, 2 \mapsto \{(2,2)\},$ and $3 \mapsto \{(1,1), (2,3)\}$.
    
    Finally, at $(2,3)$, we have $r[f_1(2)] = r[f'_1(3)] = r[1] = \1$, with $f^\sharp_2$ still picking out $1 \in \1 = q[2]$ and $(f')^\sharp_3$ picking out $1 \in \2 = q'[3]$.
    So the pushout $p[(2,3)]$ is the set $\1 + \2 = \{(1,1), (2,1), (2,2)\}$ but with $(1,1)$ identified with $(2,1)$; we can think of it as the set of equivalence classes $p[(2,3)] \iso \{\{(1,1), (2,1)\}, \{(2,2)\}\} \iso \2$.
    Moreover, the on-directions function $g^\sharp_{(2,3)}$ maps $1 \mapsto \{(1,1), (2,1)\}$, while the on-directions function $(g')^\sharp_{(2,3)}$ maps $1 \mapsto \{(1,1), (2,1)\}$ and $2 \mapsto \{(2,2)\}$.

    It follows that $p \iso \yon^\5 + \yon^\3 + \yon^\2$, with $g$ and $g'$ as described.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{theorem}\label{thm.poly_colimits}
The category $\poly$ has all colimits.
\end{theorem}
\begin{proof}
A category has all colimits if and only if it has coproducts and coequalizers, so by \cref{prop.poly_coprods}, it suffices to show that $\poly$ has coequalizers.

Let $s,t \colon p \tto q$ be two maps of polynomials.
We construct the coequalizer $q'$ of $s$ and $t$ as follows.
The pair of functions $s_1, t_1 \colon p(\1) \tto q(\1)$ define a graph $G \colon \fbox{$\bullet\tto\bullet$} \to \smset$ with vertices in $q(\1)$, edges in $p(\1)$, sources indicated by $s_1$, and targets indicated by $t_1$.
Then the set $C$ of connected components of $G$ is given by the coequalizer $g_1 \colon q(\1) \to C$ of $s_1$ and $t_1$.
We define the set of positions of $q'$ to be $C$.
Each set of directions of $q'$ will be a limit of a diagram of sets of directions of $p$ and $q$, but expressing this limit, as we proceed to do, is a bit involved.

For each connected component $c \in C$, we have a connected subgraph $G_c \ss G$ with vertices $V_c \coloneqq g_1\inv(c)$ and edges $E_c \coloneqq s_1\inv(g_1\inv(c)) = t_1\inv(g_1\inv(c))$.
Note that $E_c\ss p(\1)$ and $V_c\ss q(\1)$, so to each $e\in E_c$ (resp.\ to each $v\in V_c$) we have an associated set of directions $p[e]$ (resp.\ $q[v]$).

The category of elements $\int G_c$ has objects $E_c+V_c$ and two kinds of (non-identity) morphisms, $e \to s_1(e)$ and $e \to t_1(e)$, associated to each $e \in E_c$, all pointing from an object in $E_c$ to an object in $V_c$. %**This is a cofunctor into graph category, right?
There is a functor $F \colon (\int G_c)\op \to \smset$ sending every $v \mapsto q[v]$, every $e \mapsto p[e]$, and every morphism to a function between them, namely either $s^\sharp_e \colon q[s_1(e)] \to p[e]$ or $t^\sharp_e \colon q[t_1(e)] \to p[e]$.
So we can define $q'[c]$ to be the limit of $F$ in $\smset$.

We claim that $q'\coloneqq\sum_{c\in C}\yon^{q'[c]}$ is the coequalizer of $s$ and $t$. We leave the complete proof to the interested reader in \cref{exc.poly_colimits}.
\end{proof}

\begin{exercise}\label{exc.poly_colimits}
Complete the proof of \cref{thm.poly_colimits} as follows:
\begin{enumerate}
	\item Provide a map $g \colon q \to q'$.
	\item Show that $s \then g = t \then g$.
	\item Show that $g$ is a coequalizer of the pair $s, t$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We define a map $g \colon q \to q'$ as follows.
    The on-positions function $g_1 \colon q(\1) \to q'(\1)$ is the coequalizer of $s_1, t_1 \colon p(\1) \tto q(\1)$.
    In particular, $g_1$ sends each vertex in $q(\1)$ to its corresponding connected component in $q'(\1) = C$.
    Then for each $v \in q(\1)$, if we let its corresponding connected component be $c \coloneqq g_1(v)$, we can define the on-directions function $g^\sharp_v \colon q'[c] \to q[v]$ to be the projection from the limit $q'[c]$ to its component $q[v]$.
    
    \item To show that $s \then g = t \then g$, we must show that both sides are equal on positions and on directions.
    The on-positions function $g_1$ is defined to be the coequalizer of $s_1$ and $t_1$, so $s_1 \then g_1 = t_1 \then g_1$.
    So it suffices to show that for all $e \in p(\1)$, if we let its corresponding connected component be $c \coloneqq g_1(s_1(e)) = g_1(t_1(e))$, then the following diagram of on-directions functions commutes:
    \[
    \begin{tikzcd}[sep=small]
        & q[s_1(e)] \ar[dl, "s^\sharp_e"'] \\
        p[e] & & q'[c] \ar[ul, "g^\sharp_{s_1(e)}"'] \ar[dl, "g^\sharp_{t_1(e)}"] \\
        & q[t_1(e)] \ar[ul, "t^\sharp_e"]
    \end{tikzcd}
    \]
    But this is automatically true by the definition of $q'[c]$ as a limit---specifically the limit of a functor with $s^\sharp_e$ and $t^\sharp_e$ in its image---and the definitions of $g^\sharp_{s_1(e)}$ and $g^\sharp_{t_1(e)}$ as projections from this limit.
    
    \item To show that $g$ is the coequalizer of $s$ and $t$, it suffices to show that for any $r \in \poly$ and map $f \colon q \to r$ satisfying $s \then f = t \then f$, there exists a unique map $h \colon q' \to r$ for which $f = g \then h$, so that the following diagram commutes.
    \begin{equation*} %\label{eqn.eq_univ_prop}
    \begin{tikzcd}
        p \ar[r, "s", shift left] \ar[r, "t"', shift right] & q \ar[r, "g"] \ar[dr, "f"'] & q' \ar[d, "h", dashed] \\
        & & r
    \end{tikzcd}
    \end{equation*}
    In order for $f = g \then h$ to hold, we must have $f_1 = g_1 \then h_1$ on positions.
    But we have that $s_1 \then f_1 = t_1 \then f_1$, so by the universal property of $q'(\1)$ and the map $g_1$ as the coequalizer of $s_1$ and $t_1$ in $\smset$, there exists a unique $h_1$ for which $f_1 = g_1 \then h_1$.
    Hence $h$ is uniquely characterized on positions.
    In particular, it must send each connected component $c \in q'(\1)$ to the element in $r(\1)$ to which $f_1$ sends every vertex $v \in V_c = g_1\inv(c)$ that lies in the connected component $c$.
    
    Then for $f = g \then h$ to hold on directions, we must have that $f^\sharp_v = h^\sharp_{g_1(v)} \then g^\sharp_v$ for each $v \in q(\1)$.
    Put another way, given $c \in q'(\1)$, we must have that $f^\sharp_v = h^\sharp_c \then g^\sharp_v$ for every $v \in V_c$.
    But $s \then f = t \then f$ implies that for each $e \in E_c = s_1\inv(g_1\inv(c)) = t_1\inv(g_1\inv(c)) \ss p(\1)$, the following diagram of on-directions functions commutes:
    \[
    \begin{tikzcd}[sep=small]
        & q[s_1(e)] \ar[dl, "s^\sharp_e"'] \\
        p[e] & & r[f_1(v)] \ar[ul, "f^\sharp_{s_1(e)}"'] \ar[dl, "f^\sharp_{t_1(e)}"] \\
        & q[t_1(e)] \ar[ul, "t^\sharp_e"]
    \end{tikzcd}
    \]
    It follows that $r[f_1(v)]$ together with the maps $\{f^\sharp_v\}_{v \in V_c}$ form a cone over the functor $F$.
    So by the universal property of the limit $q'[c]$ of $F$ with projection maps $\{g^\sharp_v\}_{v \in V_c}$, there exists a unique $h^\sharp_c \colon r[f_1(v)] \to q'[c]$ for which $f^\sharp_v = h^\sharp_c \then g^\sharp_v$ for every $v \in V_c$.
    Hence $h$ is also uniquely characterized on directions, so it is unique overall.
    Moreover, we have shown that we can define $h$ on positions so that $f_1 = g_1 \then h_1$, and that we can define $h$ on directions such that $f^\sharp_v = h^\sharp_c \then g^\sharp_v$ for all $c \in q'(\1)$ and $v \in V_c$.
    It follows that there exists $h$ for which $f = g \then h$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{example}
Given a diagram in $\poly$, one could either take its colimit as a diagram of \emph{polynomial} functors (i.e.\ its colimit in $\poly)$ or its colimit simply as a diagram of functors (i.e.\ its colimit in $\smset^\smset$).
These can yield different results, as evidenced by the fact that, per the co-Yoneda lemma, \emph{every} functor $\smset \to \smset$---even those that are not polynomials---can be written as the colimit of representable functors in $\smset^\smset$, yet the colimit of the same representables in $\poly$ can only be another polynomial.

As a concrete example, consider the two projections $\yon^\2\to\yon$, which form the diagram
\begin{equation} \label{eqn.2_projs}
    \yon^\2\tto\yon.
\end{equation}
According to \cref{thm.poly_colimits}, the colimit of \eqref{eqn.2_projs} in $\poly$ has the coequalizer of $\1 \tto \1$, namely $\1$, as its set of positions, and the limit of the diagram $\1 \tto \2$ consisting of the two inclusions as its sole set of directions.
But this latter limit is just $\0$, so in fact the colimit of \eqref{eqn.2_projs} in $\poly$ is $\1$.

But as functors, the colimit of \eqref{eqn.2_projs} is the functor
\[
  X\mapsto
  \begin{cases}
  	\0&\tn{ if }X=\0\\
  	\1&\tn{ if }X\neq\0
  \end{cases}
\]
% (its universal property can be explicitly verified). **Why?
\end{example}

\begin{exercise}
By \cref{prop.adjoint_quadruple}, for any polynomial $p$, there are canonical maps
\[
	\epsilon \colon p(\1)\yon\to p
	\qqand
	\eta \colon p\to \yon^{\Gamma(p)}.
\]
\begin{enumerate}
	\item Characterize the behavior of the canonical map $\epsilon \colon p(\1)\yon\to p$.
	\item Characterize the behavior of the canonical map $\eta \colon p\to \yon^{\Gamma(p)}$.
	\item Show that the following is a pushout in $\poly$:
    \begin{equation} \label{eqn.pushout_adjoint}
    \begin{tikzcd}
    	p(\1)\yon\ar[r, "!"]\ar[d, "\epsilon"']&
    	\yon\ar[d, "!"]\\
    	p\ar[r, "\eta"']&
    	\yon^{\Gamma(p)}\ar[ul, phantom, very near start, "\ulcorner"]
    \end{tikzcd}
    \end{equation}
% 	\item Is the pushout of a Cartesian map always Cartesian? % **We haven't intro'd Cartesian maps yet
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We characterize the map $\epsilon \colon p(\1)\yon \to p$ as follows.
    On positions, it is the identity on $p(\1)$.
    Then for each $i \in p(\1)$, on directions, it is the unique map $p[i] \to \1$.
    
    \item We characterize the map $\eta \colon p \to \yon^{\Gamma(p)}$ as follows.
    On positions, it is the unique map $p(\1) \to \1$.
    Then for each $i \in p(\1)$, on directions, it is the canonical projection $\Gamma(p) \iso \prod_{i' \in p(\1)} p[i'] \to p[i]$.
    
    \item Showing that \eqref{eqn.pushout_adjoint} is a pushout square is equivalent to showing that, in the diagram
    \begin{equation} \label{eqn.coeq_adjoint}
    \begin{tikzcd}[sep=large]
        & \yon \ar[d, "\iota"] \ar[dr, "!"] \\
        p(\1)\yon \ar[ur, "!"] \ar[dr, "\epsilon"'] \ar[r, "s", shift left] \ar[r, "t"', shift right] & \yon + p \ar[r, "g"] & \yon^{\Gamma(p)} \\
        & p \ar[u, "\iota'"'] \ar[ur, "\eta"']
    \end{tikzcd}
    \end{equation}
    in which $\iota, \iota'$ are the canonical inclusions and the four triangles commute, $\yon^{\Gamma(p)}$ equipped with the morphism $g$ is the coequalizer of $s$ and $t$.
    To do so, we apply \cref{thm.poly_colimits} to compute the coequalizer $q'$ of $s$ and $t$.
    The set of positions of $q'$ is the coequalizer of $s_1 = (! \then \iota)_1$, which sends every $i \in p(\1)$ to the position of $\yon + p$ corresponding to the summand $\yon$, and $t_1 = (\epsilon \then \iota')_1$, which sends each $i \in p(\1)$ to the corresponding position in the summand $p$ of $\yon + p$.
    It follows that the coequalizer of $s_1$ and $t_1$ is $\1$, so $q'(\1) \iso \1$.
    
    Then the set of directions of $q'$ at its sole position is the limit of the functor $F$ whose image consists of morphisms of the form $\1 \to \1$ or $p[i] \to \1$ for every $i \in p(\1)$.
    It follows that the limit of $F$ is just a product, namely $\prod_{i \in p(\1)} p[i] \iso \Gamma(p)$.
    Hence $q' \iso \yon^{\Gamma(p)}$, as desired.
    
    It remains to check that the upper right and lower right triangles in \eqref{eqn.coeq_adjoint} commute.
    The upper right triangle must commute by the uniqueness of morphisms $\yon \to \yon^{\Gamma(p)}$; and the lower right triangle must commute on positions.
    Moreover, the on-directions function of the coequalizer morphism $g$ at each position $i \in p(\1) \ss (\yon+p)(\1)$ must be the canonical projection $\Gamma(p) \to p[i]$, which matches the behavior of the corresponding on-directions function of $\eta$; hence the lower right triangle also commutes on directions.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.tensor_as_pushout}
For polynomials $p,q$, the following is a pushout:
\[
\begin{tikzcd}
	p(\1)\yon\otimes q(\1)\yon\ar[r]\ar[d]&
	p(\1)\yon\otimes q\ar[d]\\
	p\otimes q(\1)\yon\ar[r]&
	p\otimes q\ar[ul, phantom,very near start, "\ulcorner"]
\end{tikzcd}
\]
\end{proposition}
\begin{proof}
All the maps shown are identities on positions, so the displayed diagram is the coproduct over all $(i,j)\in p(\1)\times q(\1)$ of the diagram shown left
\[
\begin{tikzcd}
	\yon\ar[r]\ar[d]&
	\yon^{q[j]}\ar[d]\\
	\yon^{p[i]}\ar[r]&
	\yon^{p[i]\times q[j]}\ar[ul, phantom, very near start, "\ulcorner"]
\end{tikzcd}
\begin{tikzcd}
	\1\ar[dr, phantom, very near end, "\ulcorner"]&
	q[j]\ar[l]\\
	p[i]\ar[u]&
	p[i]\times q[j]\ar[l]\ar[u]
\end{tikzcd}
\]
where we used $(p\otimes q)[(i,j)]\cong p[i]\times q[j]$. This is the image under the Yoneda embedding of the diagram of sets shown right, which is clearly a pullback. The result follows by \cref{prop.yoneda_left_adjoint}.
\end{proof}

This means that to give a map $\varphi\colon p\otimes q\to r$, it suffices to give two maps $\varphi_p\colon p\otimes q(\1)\yon\to r$ and $\varphi_q\colon p(\1)\yon\otimes q\to r$ that agree on positions. The map $\varphi_p$ says how information about $q$'s position is transferred to $p$, and the map $\varphi_q$ says how information about $p$'s position is transferred to $q$.

\begin{corollary}\label{cor.tensor_as_pushout}
Suppose given polynomials $p_1,\ldots,p_n\in\poly$. Then $p_1\otimes\cdots\otimes p_n$ is isomorphic to the wide pushout
\[
  \colim
  \left(
  \begin{tikzcd}[column sep=-10pt]
  	&
		p_1(\1)\yon\otimes\cdots\otimes  p_n(\1)\yon\ar[dl]\ar[dr]\\
		p_1\otimes p_2(\1)\yon\otimes\cdots\otimes p_n(\1)\yon&
		\cdots&
		p_1(\1)\yon\otimes\cdots\otimes p_{n-1}(\1)\yon \otimes p_n
  \end{tikzcd}
  \right)
\]
\end{corollary}
\begin{proof}
We proceed by induction on $n\in\nn$.
When $n=0$, the wide pushout has no legs and the empty tensor product is $\yon$, so the result holds.
If the result holds for $n$, then it holds for $n+1$ by \cref{prop.tensor_as_pushout}.
\end{proof}

We can use \cref{cor.tensor_as_pushout} to characterize interaction patterns. Given polynomials $p_1,\ldots, p_n,$ and $q$, and a map
\[\varphi\colon p_1\otimes\cdots\otimes p_n\to q\]
we may want to know which $p_i$'s can ``hear'' which $p_j$'s. To make this precise, we will associate to $\varphi$ a simple directed graph that describes the information flow. Given a set $V$, define
\[
\Cat{SimpleGraph}(V)\coloneqq\{A\ss V\times V\mid \forall(v\in V). (v,v)\not\in A\}.
\]
In fact, we will start with a more general result that explains how maps such as $\varphi$ can be \emph{mode-dependent}. But before we do, let's explain the idea in an example.

\begin{example}[Introducing random process]\label{ex.random_process}
What is it about flipping a coin or rolling a die that is random? This is a different question than whether the coin or die is fair or even consistent. What's important is that we don't have any influence over the result of the outcome.

Suppose that we have a system $\varphi\colon p\otimes c\to r$, where we think of $p$ as a player and $c$ as a coin. By \cref{prop.tensor_as_pushout} we know that $\varphi$ can be identified with a commuting diagram
\[
\begin{tikzcd}
	p(\1)\yon\otimes c(\1)\yon\ar[r]\ar[d]&
	p(\1)\yon\otimes c\ar[d]\\
	p\otimes c(\1)\yon\ar[r]&
	r
\end{tikzcd}
\]
Assuming for simplicity that the player and coin form a closed system, i.e.\ that$r=\yon$, the map $\varphi$ can be identified with two maps: $\varphi_c\colon p(\1)\yon\otimes c\to\yon$ and $\varphi_p\colon p\otimes c(\1)\yon\to\yon$. To say that $p$'s position does not influence $c$ is to say that the former map factors through the projection $\pi\colon p(\1)\yon\otimes c\to c$
\[
	p(\1)\yon\otimes c\To{\pi} c\to \yon.
\]
In general, we will be interested in the interaction pattern between a number of systems, in terms of who influences who.
\end{example}

\begin{example}
Carrying on from \cref{ex.random_process}, suppose given polynomials $p_1,\ldots,p_n,r$ and a map
\[\varphi\colon p_1\otimes\cdots\otimes p_n\to r.\]
By \cref{cor.tensor_as_pushout}, $\varphi$ can be identified with a tuple of maps 
\[\varphi_i\colon p_i\otimes\bigotimes_{j\neq i}p_j(\1)\yon\too r\]
that agree on positions $\varphi(\1)\colon p(\1)\otimes\cdots\otimes p_n(\1)\to r(\1)$.

Now for each $1\leq i\leq n$, we can ask for the smallest subset $A_i\ss\{j\mid 1\leq j\leq n, j\neq i\}$ such that $\varphi_i$ factors through the projection
\[
	p_i\otimes\bigotimes_{j\neq i}p_j(\1)\yon\To{\pi_A}
	p_i\otimes\bigotimes_{j\in A_i}p_j(\1)\yon\to
	r.
\]
The result is a simple directed graph: its vertices are the numbers $V\coloneqq\{i\mid 1\leq i\leq n\}$ and for each vertex we have a subset $A_i\ss V\setminus \{i\}$, which we consider as the arrows with target $i$. The set of all arrows in this graph is $A\coloneqq\sum_{i}A_i$.

The simple directed graph associated to the player-and-coin model in \cref{ex.random_process} would be
\[
\LMO{c}\to\LMO{p}
\]
indicating that the coin's position can be noticed by the player but the player's position cannot be noticed by the coin. In general, we may have something like the following:
\[
\begin{tikzcd}
	&
	\LMO{p_1}\ar[dl]\ar[dr]\ar[r, shift left]&
	\LMO{p_2}\ar[l, shift left]\ar[dr]\\
	\LMO{p_3}&&
	\LMO{p_4}&
	\LMO{p_5}\ar[l]
\end{tikzcd}
\]
which says that $p_1$ is heard by $p_2,p_3,p_4$ but not by $p_5$, etc.
\end{example}

For any $p\in\poly$, let $p/\poly$ denote the coslice category, i.e.\ the category whose objects are maps $p\to q$ emanating from $p$, and whose morphisms are commutative triangles.

\begin{proposition}[Interaction graphs]
For any polynomials $p_1,\ldots,p_n\in\poly$ there is an adjunction
\[
	\adj
		{\Cat{SimpleGraph}(\ord{n})\op}
		{}{}
		{p_1\otimes\cdots\otimes p_n/\poly}
\]
Moreover, the left adjoint is fully faithful if and only if $n\leq 1$ or $|p_i(\1)|\geq 2$ for each $1\leq i\leq n$, i.e.\ if and only if each polynomial has at least two positions.
\end{proposition}
\begin{proof}
**
\end{proof}

%-------- Section --------%
\section{Monoidal $*$-bifibration over $\smset$}

We will see that the functor $p\mapsto p(\1)$ has special properties making it what
\cite{shulman2008framed} refers to as a \emph{monoidal $*$-bifibration}. This means that $\smset$ acts as a sort of remote controller on the category $\poly$, grabbing every polynomial by its positions and pushing or pulling it this way and that. 

For example, suppose one has a set $A$ and a function $f\colon A\to p(\1)$, which we can also think of as a morphism between constant polynomials.
Then we get a new polynomial $f^*p$ with positions $A$, as follows.
%The notation here agrees with that in \cref{thm.poly_lcc}:
It is given by a pullback
\begin{equation}\label{eqn.f^*_defined}
\begin{tikzcd}
	f^*p\ar[r, "\fun{cart}"]\ar[d]&
	p\ar[d, "\eta_p"]\\
	A\ar[r, "f"']&
	p(\1)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\end{equation}
Here $\eta_p$ is the unit of the adjunction $\adjr{\smset}{A}{p(\1)}{\poly}$.
By \cref{ex.pullbacks_in_poly}, this pullback yields an on-positions pullback
\begin{equation}\label{eqn.f^*_defined_pos}
\begin{tikzcd}
	(f^*p)(\1) \ar[r, "\fun{cart}_1"] \ar[d] &
	p(\1) \ar[d, "(\eta_p)_1"] \\
	A \ar[r, "f"'] &
	p(\1) \ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\end{equation}
in $\smset$.
Since $\eta_p$ is an isomorphism on positions, the map $f^*p\to A$ is also an isomorphism on positions and $\fun{cart}_1 = f$.

\cref{ex.pullbacks_in_poly} also implies that for each position $a \in A$, the pullback in \eqref{eqn.f^*_defined} yields an on-directions pushout
\begin{equation}
\begin{tikzcd}
	(f^*p)[a] \ar[from=r, "\fun{cart}^\sharp_a"'] \ar[from=d] &
	p[f(a)] \ar[from=d, "(\eta_p)^\sharp_i"'] \\
	\0 \ar[from=r, "f^\sharp_a"] &
	\0 \ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\end{equation}
in $\smset$.
The on-directions function $f^\sharp_a \colon \0 \to \0$ is trivially an isomorphism, so each on-directions function $\fun{cart}^\sharp_a$ is an isomorphism, too.
Hence
\[
    f^*p \iso \sum_{a \in A} \yon^{p[f(a)]}.
\]
We'll see this as part of a bigger picture in \cref{prop.basechange,thm.triple_adjoint_basechange}, but first, we introduce vertical and cartesian morphisms in $\poly$ (thus explaining why we named our morphism ``$\fun{cart}$'').

\begin{definition}[Vertical, cartesian]
Let $f\colon p\to q$ be a morphism of polynomials.
It is called \emph{vertical} if $f_1\colon p(\1)\to q(\1)$ is an isomorphism.
It is called \emph{cartesian} if, for each $i\in p(\1)$, the function $f^\sharp_i\colon q[f(i)]\to p[i]$ is an isomorphism.
\end{definition}

\begin{proposition}\label{prop.vert_cart_factorization}
Every morphism in $\poly$ can be uniquely factored as a vertical morphism followed by a cartesian morphism.
\end{proposition}
\begin{proof}
Recall from \eqref{eqn.colax_poly_map} that a morphism in $\poly$ can be written as to the left; we can thus rewrite it as to the right:
\[
\begin{tikzcd}[column sep=small]
	p(\1)\ar[dr, bend right, "{p[-]}"']\ar[rr, "f_1"]&~&
	q(\1)\ar[dl, bend left, "{q[-]}"]\\&
	\smset\ar[u, phantom, near end, "\overset{f^\sharp}{\Leftarrow}"]
\end{tikzcd}
\hspace{1in}
\begin{tikzcd}
	p(\1)\ar[dr, bend right, "{p[-]}"']\ar[r, equal, ""' name=equal]&
	p(\1)\ar[d, "{q[f_1(-)]}" description]\ar[r, "f_1"]&
	q(\1)\ar[dl, bend left, "{q[-]}"]\\&
	|[alias=set]|\smset\ar[from=equal, to=set, pos=.3, phantom, "\overset{f^\sharp}{\Leftarrow}"]
\end{tikzcd}
\]
The intermediary object $\sum_{i\in p(\1)} \yon^{q[f_1(i)]}$ is clearly unique up to isomorphism.
\end{proof}

\begin{proposition}
Vertical morphisms satisfy 2-out-of-3: given $p\To{f}q\To{g}r$ with $h = f \then g$, if any two of $f,g,h$ are vertical, then so is the third.

If $g$ is cartesian, then $h$ is cartesian if and only if $f$ is cartesian.
\end{proposition}
\begin{proof}
Given $h = f \then g$, we have that $h_1 = f_1 \then g_1$.
Since isomorphisms satisfy 2-out-of-3, it follows that vertical morphisms satisfy 2-out-of-3 as well.

Now assume $g$ is cartesian.
On directions, $h = f \then g$ implies that for every $i \in p(\1)$, we have $h^\sharp_i = g^\sharp_{f_1(i)} \then f^\sharp_i$.
Since $g^\sharp_{f_1(i)}$ is an isomorphism, it follows that every $h^\sharp_i$ is an isomorphism if and only if every $f^\sharp_i$ is an isomorphism, so $h$ is cartesian if and only if $f$ is cartesian.
\end{proof}

\begin{exercise}
Give an example of polynomials $p,q,r$ and maps $p\To{f}q\To{g}r$ such that $f$ and $f \then g$ are cartesian but $g$ is not.
\begin{solution}
Consider the maps $\yon \To{f} \yon^\2 + \yon \To{g} \yon$ where $f$ is the canonical inclusion and $g$ is uniquely determined on positions and picks out $1 \in \2$ and $1 \in \1$ on directions.
Then the only on-directions function of $f$ is a function $\1 \to \1$, an isomorphism, so $f$ is cartesian.
Meanwhile, one of the on-directions functions of $g$ is a function $\1 \to \2$, which is not an isomorphism, so $g$ is not cartesian.
Finally, $f \then g$ can only be the unique morphism $\yon \to \yon$, namely the identity, which is cartesian.
\end{solution}
\end{exercise}

% ** Usual def of cartesian morph?


Recall from \cref{exc.deriv_directions} that for any polynomial $p$, there is a corresponding function $\pi_p\colon\dot{p}(\1)\to p(\1)$, i.e.\ the set of directions mapping to the set of positions.
A map of polynomials $(f_1,f^\sharp)\colon p\to q$ can then be described as a function $f_1\colon p(\1)\to q(\1)$ along with a function $f^\sharp$ that makes the following diagram in $\smset$ commute:
\begin{equation}\label{eqn.poly_map_usu}
\begin{tikzcd}
	\dot{p}(\1)\ar[d, "\pi_p"']&
	\bullet\ar[l, "f^\sharp"']\ar[r]\ar[d]&
	\dot{q}(\1)\ar[d, "\pi_q"]\\
	p(\1)\ar[r, equal]&
	p(\1)\ar[r, "f_1"']&
	q(\1)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\end{equation}

\begin{proposition}\label{prop.cart_as_nt}
Let $f\colon p\to q$ be a morphism of polynomials. The following are equivalent:
	\begin{enumerate}
		\item for each $i\in p(\1)$, the induced map $f^\sharp_i$ on directions is a bijection;
		\item the diagram in \eqref{eqn.poly_map_usu} can be simplified to a pullback:
\[
\begin{tikzcd}
	\dot{p}(\1)\ar[d, "\pi_p"']\ar[r]&
	\dot{q}(\1)\ar[d, "\pi_q"]\\
	p(\1)\ar[r, "f_1"']&
	q(\1)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]		
		\item Viewed as a natural transformation, $f$ is cartesian, i.e.\ for any sets $A,B$ and function $g\colon A\to B$, the naturality square
\[
\begin{tikzcd}
	p(A)\ar[r, "f_A"]\ar[d, "p(g)"']&
	q(A)\ar[d, "q(g)"]\\
	p(B)\ar[r, "g_A"']&
	q(B)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
is a pullback.
  \end{enumerate}
\end{proposition}

\begin{exercise}
Prove \cref{prop.cart_as_nt}.
\begin{solution}
**
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.monoidal_pres_carts}
The monoidal structures $+$, $\times$, and $\otimes$ preserve cartesian morphisms.
\end{proposition}
\begin{proof}
Suppose that  $f\colon p\to p'$ and $g\colon q\to q'$ are cartesian. 

A position of $p+q$ is a position $i\in p(\1)$ or a position $j\in q(\1)$, and the map $(f+g)^\sharp$ at that position is either $f^\sharp_i$ or $g^\sharp_j$; either way it is an isomorphism, so $f+g$ is cartesian.

A position of $p\times q$ (resp.\ of $p\otimes q$) is a pair $(i,j)\in p(\1)\times q(\1)$. The morphism $(f\times g)^\sharp_{i,j}$ is $f^\sharp_i+g^\sharp_j$ (resp.\ $f^\sharp_i\times g^\sharp_j$) which is again an isomorphism if $f^\sharp_i$ and $g^\sharp_j$ are. Hence $f\times g$ (resp.\ $f\otimes g$) is cartesian, completing the proof.
\end{proof}

\begin{proposition}\label{prop.pullback_cartesian}
Let $f\colon p\to q$ be a morphism and $g\colon q'\to q$ a cartesian morphism. Then the pullback $g'$ of $g$ along $p$
\[
\begin{tikzcd}
	p\times_qq'\ar[r]\ar[d]&
	q'\ar[d, "g"]\\
	p\ar[r, "f"']&
	q\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
is cartesian.
\end{proposition}
\begin{proof}
This follows from \cref{ex.pullbacks_in_poly}, since the pushout of an isomorphism is an isomorphism.
\end{proof}

For any $p\in\poly$, let $\poly/p$ denote the slice category, i.e.\ the category whose objects are maps to $p$ and whose morphisms are commutative triangles.

\begin{definition}
Given a category $\cat{C}$ with objects $c, d$ and morphism $f \colon c \to d$ such that all pullbacks along $f$ exist in $\cat{C}$, we say that $f$ is \emph{exponentiable} if the functor $f^* \colon \cat{C}/d \to \cat{C}/c$ given by pulling back along $f$ is a left adjoint.
\end{definition}

\begin{theorem}\label{thm.cart_exponentiable}
Cartesian morphisms in $\poly$ are exponentiable.
That is, if $f\colon p\to q$ is cartesian, then the functor $f^*\colon\poly/q\to\poly/p$ given by pulling back along $f$ is a left adjoint:
\[
\begin{tikzcd}[column sep=50pt, background color=theoremcolor]
	\poly/p\ar[r, shift right=7pt, "f_*"']&
	\poly/q\ar[l, shift right=7pt, "f^*"']\ar[l, phantom, "\Leftarrow"]
\end{tikzcd}
\]
\end{theorem}
\begin{proof}
Fix $e\colon p'\to p$ and $g\colon q'\to q$.
\[
\begin{tikzcd}
	p'\ar[d, "e"']&q'\ar[d, "g"]\\
	p\ar[r, "f"']&q
\end{tikzcd}
\]
We need to define a functor $f_*\colon\poly/p\to\poly/q$ and prove the analogous isomorphism establishing it as right adjoint to $f^*$. We first establish some notation. Given a set $Q$ and sets $(P'_i)_{i\in I}$, each equipped with a map $Q\to P'_i$, let $Q/\sum_{i\in I}P'_i$ denote the coproduct in $Q/\smset$, or equivalently the wide pushout of sets $P'_i$ with apex $Q$. Then we give the following formula for $f_*p'$, which we write in larger font for clarity:
\begin{equation}\label{eqn.cart_exp}
f_*p'\coloneqq
\scalebox{1.3}{$\displaystyle
\sum_{j\in q(\1)}\;\sum_{i'\in\prod\limits_{i\in f_1\inv(j)}e_1\inv(i)}\;\yon^{q[j]/\sum_{i\in f_1\inv(j)}p'[i'(i)]}
$}
\end{equation}
Again, $q[j]/\sum_{i\in f_1\inv(j)}p'[i'(i)]$ is the coproduct of the $p'[i'(i)]$, taken in $q[j]/\smset$. Since $p[i]\cong q[f(i)]$ for any $i\in p(\1)$ by the cartesian assumption on $f$, we have the following chain of natural isomorphisms
\begin{align*}
	(\poly/p)(f^*q', p')&\cong
	\prod_{i\in p(\1)}\;\prod_{\{j'\in q'(\1)\,\mid\,g_1(j')=f_1(i)\}}\;\sum_{\{i'\in p'(\1)\,\mid\,e_1(i')=i\}}\;(p[i]/\smset)(p'[i'],p[i]+_{q[f(i)]}q'[j'])\\&\cong
	\prod_{i\in p(\1)}\;\prod_{\{j'\in q'(\1)\,\mid\,g_1(j')=f_1(i)\}}\;\sum_{\{i'\in p'(\1)\,\mid\,e_1(i')=i\}}\;(q[f(i)]/\smset)(p'[i'],q'[j'])\\&\cong
	\prod_{j\in q(\1)}\;\prod_{\{j'\in q'(\1)\,\mid\, g_1(j')=j\}}\;\prod_{\{i\in p(\1)\,\mid\,f_1(i)=j\}}\;\sum_{\{i'\in p'(\1)\,\mid\,e_1(i')=i\}}\;(q[j]/\smset)(p'[i'],q'[j'])\\&\cong
	\prod_{j\in q(\1)}\;\prod_{\{j'\in q'(\1)\,\mid\, g_1(j')=j\}}\;\sum_{i'\in\prod_{i\in f_1\inv(j)}e_1\inv(i)}\;\prod_{i\in f_1\inv(j)}\;(q[j]/\smset)(p'[i'(i)],q'[j'])\\&\cong
	\prod_{j\in q(\1)}\;\prod_{\{j'\in q'(\1)\,\mid\, g_1(j')=j\}}\;\sum_{i'\in\prod_{i\in f_1\inv(j)}e_1\inv(i)}\;(q[j]/\smset)\Big(\sum_{i\in f_1\inv(j)}p'[i'(i)],q'[j']\Big)\\&\cong
	(\poly/q)(q',f_*p')
\end{align*}
\end{proof}

\begin{example}
Let $p\coloneqq\2\yon^\2$, $q\coloneqq\yon^\2+\yon^\0$, and $f\colon p\to q$ the unique cartesian morphism between them.
Then for any $e\colon p'\to p$ over $p$, \eqref{eqn.cart_exp} provides the following description for the pushforward $f_*p'$.
%We use the isomorphisms $p(\1)\cong\2$ and $q(\1)\cong\2$ to talk about the positions of $p$ and $q$.

Over the $j=2$ position, $f_1\inv(2)=\0$ and $q[2]=\0$, so $\prod_{i \in f_1\inv(2)} e_1\inv(i)$ is an empty product and $q[2]/\sum_{i\in f_1\inv(2)} p'[i'(i)]$ is an empty pushout.
Hence the corresponding summand of \eqref{eqn.cart_exp} is simply $\yon^\0\cong\1$.

Over the $j=1$ position, $f_1\inv(1)=\2$ and $q[1]=p[1]=p[2]=\2$, so $\prod_{i'\in f_1\inv(1)} e_1\inv(i) \iso e_1\inv(1)\times e_1\inv(2)$.
For $i' \in e_1\inv(1) \times e_1\inv(2)$, we have that $q[1]/\sum_{i\in f_1\inv(2)} p'[i'(i)] \iso X_{i'}$ in the following pushout square:
\[
\begin{tikzcd}
	X_{i'} \ar[from=r] \ar[from=d] &
	p'[i'(2)] \ar[from=d, "e^\sharp_{i'(2)}"'] \\
	p'[i'(1)] \ar[from=r, "e^\sharp_{i'(1)}"] &
	\2 \ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
Then in sum we have
\[
    f_*p' \iso \left(\sum_{i' \in e_1\inv(1) \times e_2\inv(2)} \yon^{X_{i'}}\right) + \1.
\]
\end{example}

\begin{exercise}
Prove that the unique map $f\colon\yon\to\1$ is exponentiable.
\begin{solution}
Choose $p\in\poly$ and $q'\in\poly/\yon$. Then there is $q\in\poly$ such that $q'\cong q\yon$, equipped with the projection $q\yon\to\yon$. The pushforward is given by the exponential
\[f_*(q\yon)\coloneqq q^\yon\]
from the cartesian closure; see \eqref{eqn.exponential}. Indeed, we have
\begin{align*}
	\poly/\yon(f^*p,q\yon)&\cong
	\poly/\yon(p\yon,q\yon)\\&\cong
	\poly(p\yon,q)\\&\cong
	\poly(p,q^\yon).
\end{align*}
\end{solution}
\end{exercise}

For any set $A$, let $\poly[A.]$ denote the category whose objects are polynomials $p$ equipped with an isomorphism $A\cong p(\1)$, and whose morphisms are polynomial maps respecting the isomorphisms with $A$.

\begin{proposition}[Base change]\label{prop.basechange}
For any function $f\colon A\to B$, pullback $f^*$ along $f$ induces a functor $\poly[B.]\to\poly[A.]$, which we also denote $f^*$.
\end{proposition}
\begin{proof}
This follows from \eqref{eqn.pullback_poly} with $q\coloneqq A$ and $r\coloneqq B$, since pullback of an iso is an iso.
\end{proof}

\begin{theorem}\label{thm.triple_adjoint_basechange}
For any function $f\colon A\to B$, the pullback functor $f^*$ has both a left and a right adjoint
\begin{equation}\label{eqn.adjoint_triple_monoidal_fib}
\begin{tikzcd}[column sep=large, background color=theoremcolor]
	\poly[A.]\ar[r, shift left=16pt, "f_!"]\ar[r, shift right=16pt, "f_*"']
	\ar[r, phantom, shift left=9pt, "\Rightarrow"]\ar[r, phantom, shift right=9pt, "\Leftarrow"]
&
	\poly[B.]\ar[l, "f^*" description]
\end{tikzcd}
\end{equation}
Moreover $\otimes$ preserves the op-Cartesian arrows, making this a monoidal $*$-bifibration in the sense of \cite[Definition 12.1]{shulman2008framed}.
\end{theorem}
\begin{proof}
Let $p$ be a polynomial with $p(\1)\cong A$. Then the formula for $f_!p$ and $f_*p$ are given as follows:
\begin{equation}\label{eqn.f_!andf_*}
f_!p\cong\scalebox{1.3}{$\displaystyle\sum_{b\in B}\yon^{\big(\prod\limits_{a\mapsto b}p[a]\big)}$}
\qqand
f_*p\cong\scalebox{1.3}{$\displaystyle\sum_{b\in B}\yon^{\big(\sum\limits_{a\mapsto b}p[a]\big)}$}
\end{equation}
It may at first be counterintuitive that the left adjoint $f_!$ involves a product and the right adjoint $f_*$ involves a sum. The reason for this comes from \cref{prop.dlens_self_indexing}, namely that $\poly$ is equivalent to the Grothendieck construction applied to the functor $\smset\op\to\smcat$ sending each set $A$ to the category $(\smset/A)\op$. The fact that functions $f\colon A\to B$ induces an adjoint triple between $\smset/A$ and $\smset/B$, and hence between $(\smset/A)\op$ and $(\smset/B)\op$ explains the variance in \eqref{eqn.f_!andf_*} and simultaneously establishes the adjoint triple \eqref{eqn.adjoint_triple_monoidal_fib}.

The functor $p\mapsto p(\1)$ is strong monoidal with respect to $\otimes$ and strict monoidal if we choose the lens construction as our model of $\poly$. By \cref{prop.monoidal_pres_carts}, the monoidal product $\otimes$ preserves cartesian morphisms; thus we will have established the desired monoidal $*$-bifibration in the sense of \cite[Definition 12.1]{shulman2008framed} as soon as we know that $\otimes$ preserves op-cartesian morphisms.

Given $f$ and $p$ as above, the op-cartesian morphism is the morphism $p\to f_!p$ obtained as the composite $p\to f^*f_!p\to f_!p$ where the first map is the unit of the $(f_!,f^*)$ adjunction and the second is the cartesian morphism for $f_!p$. On positions $p\to f_!p$ acts as $f$, and on directions it is given by projection. 

If $f\colon p(\1)\to B$ and $f'\colon p'(\1)\to B'$ are functions then we have
\begin{align*}
	f_!(p)\otimes f'_!(p')&\cong
	\sum_{b\in B}\sum_{b'\in B'}\yon^{\big(\prod_{a\mapsto b}p[a]\big)\times\big(\prod_{a'\mapsto b'}p'[a']\big)}\\&\cong
	\sum_{(b,b')\in B\times B'}\yon^{\big(\prod_{(a,a')\mapsto(b,b')}p[a]\times p[b]\big)}\\&
	\cong (f_!\otimes f'_!)(p\otimes p')
\end{align*}
and the op-cartesian morphisms are clearly preserved since projections in the second line match with projections in the first.
\end{proof}

%-------- Section --------%
\section{Summary and further reading}

...

See work by Gambino and Kock, Joyal, Paul Taylor, Michael Abbott and Neil Ghani (containers), ...

\Closesolutionfile{solutions}

%-------- Section --------%
\section{Exercise solutions}
{\footnotesize
\input{solution-file4}}

\end{document}
