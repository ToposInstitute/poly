% !TeX root = P1-Polynomials.tex
\documentclass[Book-Poly]{subfiles}
\begin{document}


\setcounter{chapter}{0}%Just finished 0.

%---------------- Part ----------------%
\part{The category of polynomial functors}\label{part.poly}

\Opensolutionfile{solutions}[solution-file1]

%------------ Chapter ------------%
\chapter{Introduction}\label{ch.poly.intro}

\begin{quote}
It is a treasury box!\\
Full of unexpected connections!\\
It is fascinating!\\
I will think about it.\\
\mbox{}\hfill --Andr\'e Joyal, Summer 2020,\\
\mbox{}\hfill personal communication.
\end{quote}

\section{Perspectives on polynomials}

In this book we will investigate a remarkable category called $\poly$. We will see its intimate relationships with dynamic processes, decision-making, and the storage and transformation of data. But our story begins with something quite humble---high school algebra:
\begin{align}\label{eqn.poly_example}
\yon^\2+\2\yon+\1 \quad&\quad
\textit{polynomial}
\intertext{
Such polynomials will form the objects of $\poly$. All our polynomials will involve one variable, $\yon$, chosen for reasons we'll explain soon. Polynomials in one variable can be drawn as a forest of mini-trees:
}
\label{eqn.forest_example}
\begin{tikzpicture}[trees]
  \node (1) {$\bullet$} 
    child {}
    child {};
  \node[right=.5 of 1] (2) {$\bullet$} 
    child {};
  \node[right=.5 of 2] (3) {$\bullet$} 
    child {};
  \node[right=.5 of 3] (4) {$\bullet$};
\end{tikzpicture}
\quad&\quad\textit{forest}
\end{align}
More technically, each mini-tree---a rooted tree whose \emph{leaves} (the arrows) are all children of the \emph{root} (the solid dot)---is called a \emph{corolla}.
A union of trees is called a \emph{forest}, so our polynomials can be viewed as forests of corollas, which we will call \emph{corolla forests}.
Each corolla in \eqref{eqn.forest_example} corresponds to a \emph{pure-power summand} of the form $\yon^A$ in the polynomial given in \eqref{eqn.poly_example}: the corolla with $2$ leaves corresponds to $\yon^\2$; the two corollas with $1$ leaf each correspond to the two copies of $\yon = \yon^\1$; and the corolla with no leaves corresponds to $\1 = \yon^0$.

We can label the roots and leaves of our forest, like so:
\[
%\label{eqn.labeled_forest}
\begin{tikzpicture}[trees]
  \node["$1$" below] (1) {$\bullet$} 
    child {node {$1$}}
    child {node {$2$}};
  \node["$2$" below, right=.5 of 1] (2) {$\bullet$} 
    child {node {$1$}};
  \node["$3$" below, right=.5 of 2] (3) {$\bullet$} 
    child {node {$1$}};
  \node["$4$" below, right=.5 of 3] (4) {$\bullet$};
\end{tikzpicture}
\]
This suggests yet another depiction of the polynomial \eqref{eqn.poly_example}: as a \emph{dependent set}\footnote{Perhaps better known as an \emph{indexed collection} (or \emph{family}) \emph{of sets}. But we refer to these as \emph{dependent sets} to compare them to the usual sets we are familiar with from set theory, the same way dependent types compare to the usual types we are familiar with from type theory.} $(p[i])_{i \in I}$.
Here $I = \4 = \{1, 2, 3, 4\}$\footnote{
In standard font, 4 represents the usual natural number. In sans serif font, \4 represents the set $\4=\{1,2,3,4\}$ with 4 elements.
}, the set of roots, and
\begin{align} \label{eqn.arena_example}
p[1] = \2 = \{1, 2\}, \quad p[2] = \1 = \{1\}, \quad p[3] = \1 = \{1\}, \quad p[4] = \0 = \varnothing, \quad&\quad\textit{arena}
\end{align}
so that for each root $i \in I$, the set $p[i]$ consists of the leaves at that root.
We call the entire dependent set $(p[i])_{i \in I}$ an \emph{arena}.
Each element $i \in I$ is a \emph{position} in the arena, and each element $d \in p[i]$ is a \emph{direction} at position $i$.
So the relationship between the arena perspective and the forest picture is that positions are roots and directions are leaves.

We will sometimes refer to the index set $I$ as the \emph{position-set} of $p$, each element $i \in I$ as a \emph{$p$-position}, each set $p[i]$ as the \emph{direction-set} of $p$ at $i$, and each element $d \in p[i]$ as a \emph{$p[i]$-direction}.

Throughout this book, we will see several other perspectives from which we can view polynomials.
Here is a table of terminology, capturing five different perspectives from which we may view our objects of study.
The first row shows the algebraic notation, as in \eqref{eqn.poly_example};
the second row shows the dependent set terminology, as in \eqref{eqn.arena_example};
the third shows the pictorial terminology of trees, as in \eqref{eqn.forest_example};
the fourth shows dynamical systems terminology, which we will explore in \cref{ch.poly.dyn_sys};
and the fifth row shows decision-making terminology, which we will introduce in \cref{sec.poly.intro.dec}.

\begin{equation}%\label{eqn.table_terminology}
\footnotesize
\begin{tabular}{l|l|l|l}
\multicolumn{4}{c}{\normalsize Polynomial Terminology}\\[3pt]
\textbf{algebra} & $p \coloneqq \sum_{i \in p(\1)} \yon^{p[i]}$ & $i \in p(\1)$ & $d \in p[i]$ \\
\textbf{dependent sets} & arena & position & direction \\
\textbf{tree pictures} & corolla forest & root $\bullet$ & leaf $\uparrow$ \\
\textbf{dynamics} & (mode-dependent) interface & output & input \\
\textbf{decisions} & menu & decision & option \\
\end{tabular}
\end{equation}

\begin{remark}
Though a polynomial will turn out to be a \emph{functor}, while an arena is a \emph{dependent set}, they are so closely related that we often do not make a distinction between a polynomial $p$ and its arena $(p[i])_{i \in I}$; they are essentially two different syntaxes for the same object.
For example, we may often directly refer to the positions and directions of a polynomial, when we mean the positions and directions of its associated arena.

On the other hand, while \emph{arenas} and \emph{dependent sets} are exactly the same mathematical object, with the same syntax, we will stick to using the term ``arena'' when referring to them as objects in our categories of interest that coincide with polynomials, while we will use the term ``dependent set'' for more general set-theoretic purposes.
For example, we will refer to the positions and directions of arenas, but not the positions and directions of dependent sets; on the other hand, we will refer to the sum or product of a single dependent set when we mean the set-theoretic disjoint union or cartesian product of a collection of sets, but not the sum or product of a single arena in this way.
\end{remark}

\begin{exercise}%\label{exc.forest}
Consider the polynomial $p\coloneqq\2\yon^\3+\2\yon+\1$ and the associated corolla forest and arena.
\begin{enumerate}
	\item Draw the polynomial $p$ as a corolla forest.
	\item How many roots does this forest have?
	\item How many positions in the arena does this represent?
	\item For each corolla in the forest, say how many leaves it has.
	\item For each position in the arena, how many directions does it have? \qedhere
\end{enumerate}
\begin{solution}
We consider the polynomial $p\coloneqq\2\yon^\3+\2\yon+\1$.
\begin{enumerate}
	\item Here is $p$ drawn as a forest of corollas (note that the order in which the corollas are drawn does not matter):
	\[
	\begin{tikzpicture}[trees, sibling distance=3mm]
    \node (1) {$\bullet$} 
      child {}
      child {}
      child {};
    \node[right=.7 of 1] (2) {$\bullet$} 
      child {}
      child {}
      child {};
    \node[right=.5 of 2] (3) {$\bullet$} 
      child {};
    \node[right=.3 of 3] (4) {$\bullet$} 
      child {};
    \node[right=.3 of 4] (5) {$\bullet$};
  \end{tikzpicture}
  \]
	\item It has five (5) roots.
	\item It represents five positions, one per root.
	\item \label{sol.forest.leaves} The first and second corollas have three leaves, the third and fourth corollas have one leaf, and the fifth corolla has no leaves.
	\item The direction-set at each position is the same as the set of leaves of each corolla, so just copy the answer from \cref{sol.forest.leaves}, replacing ``corolla'' with ``position'' and ``leaf'' with ``direction.'' Sheesh! Who wrote this.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Consider the polynomial $q\coloneqq\yon^\8+\4\yon$.
\begin{enumerate}
	\item Does the polynomial $q$ have a pure-power summand $\yon^\2$?
	\item Does the polynomial $q$ have a pure-power summand $\yon$?
	\item Does the polynomial $q$ have a pure-power summand $\4\yon$?
	\qedhere
\end{enumerate}
\begin{solution}
We refer to the polynomial $q\coloneqq\yon^\8+4\yon$.
\begin{enumerate}
	\item No, $q$ does not have $\yon^\2$ as a pure-power summand.
	\item Yes, $q$ does have $\yon$ as a pure-power summand.
	\item No, $q$ does not have $\4\yon$ as a pure-power summand, because $\4\yon$ is not a pure-power! But to make amends, we could say that $\4\yon$ is a summand; this means that there is some $q'$ such that $q=q'+\4\yon$. So $\3\yon$ is also a summand, but $\5\yon$ and $\yon^\2$ are not.
\end{enumerate}
\end{solution}
\end{exercise}

One feature that sets our polynomials apart from the polynomials we are familiar with from high school algebra is that the coefficients and exponents are not, strictly speaking, numbers; rather, they are sets, like $\1 = \{1\}$ and $\2 = \{1, 2\}$.
In fact, they can be arbitrary sets, as in $B\yon^A + D\yon^C$ for sets $A, B, C, D$, including infinite ones, as in $\rr\yon^\nn + \2^\rr\yon^\rr$, leading to an infinite number of roots or leaves per root.
Any finite or infinite sum of pure-power summands, each with a finite or infinite set as an exponent, is still a polynomial.
Of course, this makes their forests rather unwieldy to draw, but they can be approximated.
We sketch the polynomial $\yon^\3 + \nn\yon^{[0,1]}$ as a forest below.
\[%\label{eqn.represented_interval}
\begin{tikzpicture}[trees, sibling distance=0.0625mm]
  \node (1) {$\bullet$} 
    child[sibling distance=3mm] foreach \i in {1,2,3}
    ;
  \node[right=1 of 1] (2) {$\bullet$} 
    child foreach \i in {1,...,160}
    ;
  \node[right=1 of 2] (3) {$\bullet$} 
    child foreach \i in {1,...,160}
    ;
  \node[right=1 of 3] (4) {$\bullet$} 
    child foreach \i in {1,...,160}
    ;
  \node[right=.7 of 4] (5) {$\cdots$};
\end{tikzpicture}
\]

\begin{exercise}%\label{exc.suitor_love}
If you were a suitor choosing the corolla forest you love, aesthetically speaking, which would strike your interest? Answer by circling the associated polynomial:
\begin{enumerate}
	\item $\yon^\2+\yon+\1$
	\item $\yon^\2+\3\yon^\2+\3\yon+\1$
	\item $\yon^\2$
	\item $\yon+\1$
	\item $(\nn\yon)^\nn$
	\item $S\yon^S$
	\item $\yon^{\1\0\0}+\yon^\2+\3\yon$
	\item $\yon + \2\yon^\4 + \3\yon^\9 + \4\yon^{\1\6} + \cdots$
	\item Your polynomial's name $p$ here.
\end{enumerate}
Any reason for your choice? Draw a sketch of your forest.
\begin{solution}
Aesthetically speaking, here's the associated polynomial of a beautiful corolla forest:
\[
\yon^\0+\yon^\1+\yon^\2+\yon^\3+\cdots
\]
It's reminiscent (and formally related) to the notion of lists: if $A$ is any set, then $A^\0+A^\1+A^\2+\cdots$ is the set $\lst(A)$ of lists (i.e.\ finite ordered sequences) with entries in $A$. 

Here's a picture of this lovely forest:
\[
	\begin{tikzpicture}[trees, sibling distance=3mm]
    \node (1) {$\bullet$};
    \node[right=.3 of 1] (2) {$\bullet$}
      child {};
    \node[right=.4 of 2] (3) {$\bullet$} 
      child {}
      child {};
    \node[right=.6 of 3] (4) {$\bullet$} 
      child {}
      child {}
      child {};
    \node[right=.6 of 4] {$\cdots$};
  \end{tikzpicture}
\]
\end{solution}
\end{exercise}

Before we can really get into this story, let's summarize where we're going: polynomials are going to have really surprising applications to dynamics, decisions, and data. We speak superlatively of $\poly$:
\slogan{
The category of polynomials is a jackpot. Its beauty flows without bound}
but we have not yet begun to deliver. So let's introduce some of the applications and mathematics to come.

%-------- Section --------%
\section{Dynamical systems} \label{sec.poly.intro.dyn_sys}

When we say ``dynamical system,'' we are referring to a concept that may be familiar: a machine that stores an internal state.
The machine may return output according to its current state, while it may also receive input that updates this state.

For example, the internal state of a digital clock may consist of the time and the display format (``12-hour'' vs. ``24-hour'').
If the time is six minutes past noon and the display format is ``12-hour,'' the clock will display ``12:06pm'' as its output.
If the time is twenty minutes till midnight and the display format is ``24-hour,'' the clock will instead display ``23:40.''

Meanwhile, the clock may receive input via one button that increments its time by one minute and another button that toggles between the two display formats.
At the press of a button, the internal state will change depending on which button was pressed and what the previous internal state was.

So our digital clock is a very simple dynamical system.
We can think of its input buttons and its output display as the way in which the clock interacts with the outside world---its \emph{interface}.

Here's one way the clock might interact with the outside world: my finger is on the minute button, while your finger is on the format button, and the clock is facing you so that only you can read its display.
In fact, we could think of ourselves as a couple of (particularly complex) dynamical systems as well, with each of our interfaces connected with the interface of the clock.
We could model this scenario with the following picture, called a \emph{wiring diagram}, showing how each system can receive input from and send output to other systems in a particular interaction pattern:
\begin{equation*}
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bb min width =.5cm, bbx=.5cm, bb port sep =1,bb port length=0, bby=.15cm]
	\node[bb={3}{3}, green!25!black] (X12) {\tiny me};
	\node[bb={2}{2}, green!25!black, below right = -1 and 1.5 of X12] (X21) {\tiny clock};
	\node[bb={1}{2}, green!25!black, above right=-1 and 1 of X21] (X22) {\tiny you};
	\draw (X21_out1) to (X22_in1);
	\draw let \p1=(X22.north east), \p2=(X21.north west), \n1={\y1+\bby}, \n2=\bbportlen in
          (X22_out1) to[in=0] (\x1+\n2,\n1) -- (\x2-\n2,\n1) to[out=180] (X21_in1);
	\draw (X12_out2) to (X21_in2);
\end{tikzpicture}
\end{equation*}

Of course, there is a lot going on in the world around us that we haven't drawn.
We each have some input ports: our eyes, our ears, etc., and some output ports: our speech, our gestures, etc. We can connect with other systems: our family, our colleagues, etc. And we can think of all of these systems as subsystems of one larger system interacting with the world.
\begin{equation} \label{eqn.wired_forever}
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bb min width =.5cm, bbx=.5cm, bb port sep =1,bb port length=0, bby=.15cm]
	\node[bb={2}{2}, green!25!black] (X11) {\tiny Alice};
	\node[bb={3}{3}, green!25!black, below right=of X11] (X12) {\tiny me};
	\node[bb={2}{1}, green!25!black, above right=of X12] (X13) {\tiny Bob};
	\node[bb={2}{2}, green!25!black, below right = -1 and 1.5 of X12] (X21) {\tiny clock};
	\node[bb={1}{2}, green!25!black, above right=-1 and 1 of X21] (X22) {\tiny you};
  \node[bb={2}{2}, fit = {($(X11.north east)+(-1,4)$) (X11) (X12) (X13) ($(X21.south)$) ($(X22.east)+(.5,0)$)}, bb name = {\small Wired together like this forever?}] (Z) {};
	\draw (X21_out1) to (X22_in1);
	\draw let \p1=(X22.north east), \p2=(X21.north west), \n1={\y1+\bby}, \n2=\bbportlen in
          (X22_out1) to[in=0] (\x1+\n2,\n1) -- (\x2-\n2,\n1) to[out=180] (X21_in1);
	\draw (X11_out1) to (X13_in1);
	\draw (X11_out2) to (X12_in1);
	\draw (X12_out1) to (X13_in2);
	\draw (Z_in1'|-X11_in2) to (X11_in2);	
	\draw (Z_in2'|-X12_in2) to (X12_in2);
	\draw (X12_out2) to (X21_in2);
% 	\draw (X21_out2) to (Z_out2'|-X21_out2);
	 \draw let \p1=(X12.south east), \p2=(X12.south west), \n1={\y1-\bby}, \n2=\bbportlen in
	  (X12_out3) to[in=0] (\x1+\n2,\n1) -- (\x2-\n2,\n1) to[out=180] (X12_in3);
	\draw let \p1=(X22.north east), \p2=(X11.north west), \n1={\y2+\bby}, \n2=\bbportlen in
          (X22_out2) to[in=0] (\x1+\n2,\n1) -- (\x2-\n2,\n1) to[out=180] (X11_in1);
	\draw (X13_out1) to (Z_out1'|-X13_out1);
\end{tikzpicture}
\end{equation}

We wrote a little question for you at the top of the diagram.
Isn't there something a little funny about the way we've connected these systems?
Maybe for very simple machines, you would wire things together once and they'd stay like that for the life of the machine.
But you could turn your eyes away from the clock to look at Bob, Alice drops her connection to me for weeks at a time, and I would really like to be able to lift my finger off of the clock to do something else.
So
\slogan{the way systems connect can change over time.}
In fact, $\poly$ will let us express this.

% In this book, we'll be looking at dynamical systems with a lot of interesting new options:
% \begin{enumerate}
% 	\item The interface of the system---the way in which it can be interacted with---can change shape through time.
% 	\item The wiring diagram connecting a bunch of systems can change through time.
% % 	\item One can speed up the dynamics of a system.
% % 	\item One can introduce ``effects,'' i.e.\ as defined by monads on $\smset$.
% % 	\item The dynamical systems on any interface form a topos.
% \end{enumerate}
% To give some intuition for the first two, imagine yourself as a system, wired up to other systems. 


\begin{example}\label{ex.changing_wiring_bonds_supplier_assemble}
Here are some familiar circumstances where we see interaction patterns changing over time.
\begin{enumerate}[itemsep=0pt]
%	\item Airplanes only communicate when they get near enough;
%	\item A person can choose when to open (receive input through) their eyes and when to speak (produce output);\goodbreak
	\item When too much force is applied to a material, bonds can break:
\end{enumerate}
\[
\begin{tikzpicture}[oriented WD, bb small, bb port length=0]
	\foreach \i in {0,...,4} {
		\node[bb={1}{1}, fill=blue!10] at (1.7*\i,0) (X\i) {};
	}
%	\node[bb={1}{1}, fit=(X0) (X4)] (X) {};
	\foreach \i in {0,...,3} {
		\draw[thick] (X\i_out1) -- (X\the\numexpr\i+1\relax_in1);
	};
	\draw[thick, ->] (X0_in1) -- node[above, font=\tiny] {Force} +(-2.5,0);
	\draw[thick, ->] (X4_out1) -- node[above, font=\tiny] {Force} +(2.5,0) node (R) {};
%
\def\x{21};
	\foreach \i in {0,...,2} {
		\node[bb={1}{1}, fill=blue!10] at (\x+1.7*\i,0) (Y\i) {};
	}
	\foreach \i in {3,...,4} {
		\node[bb={1}{1}, fill=blue!10] at (\x+1.3+1.7*\i,0) (Y\i) {};
	}
%	\node[bb={1}{1}, fit=(Y0) (Y4)] (Y) {};
	\foreach \i in {0,1,3} {
		\draw[thick] (Y\i_out1) -- (Y\the\numexpr\i+1\relax_in1);
	};
	\draw[thick, ->] (Y0_in1) -- node[above, font=\tiny] {Force} +(-2.5,0) node (L) {};
	\draw[thick, ->] (Y4_out1) -- node[above, font=\tiny] {Force} +(2.5,0);
	\node[starburst, draw, minimum width=2cm, minimum height=1.5cm,red,fill=orange,line width=1.5pt] at ($(L)!.5!(R)$)
{Snap!};
\end{tikzpicture}
\]
\begin{quote}
In materials science, the Young's modulus accounts for how much force can be transferred across a material as its endpoints are pulled apart. When the material breaks, the two sides can no longer feel evidence of each other. Thinking of pulling as sending a signal (a signal of force), we might say that the ability of internal entities to send signals to each other---the connectivity of the wiring diagram---is being measured by the Young's modulus. It will also be visible within $\poly$.
\end{quote}
\begin{enumerate}[resume]
	\item A company may change its supplier at any time:
\end{enumerate}
\begin{equation*}%\label{eqn.supplier}
\begin{tikzpicture}[oriented WD, font=\ttfamily, every node/.style={fill=blue!10}, baseline=(c)]
	\node[bb={0}{1}] (s1) {Supplier 1};
	\node[bb={0}{1}, below=of s1] (s2) {Supplier 2};
	\coordinate (helper) at ($(s1)!.5!(s2)$);
	\node[bb={1}{0}, right=1.5 of helper] (c) {Company};
	\draw (s1_out1) to (c_in1);
	\draw (s2_out1) to +(5pt,0) node[fill=none] {$\bullet$};
\begin{scope}[xshift=3.5in]
	\node[bb={0}{1}] (s1') {Supplier 1};
	\node[bb={0}{1}, below=of s1'] (s2') {Supplier 2};
	\coordinate (helper') at ($(s1')!.5!(s2')$);
	\node[bb={1}{0}, right=1.5 of helper'] (c') {Company};
	\draw (s2'_out1) to (c'_in1);
	\draw (s1'_out1) to +(5pt,0) node[fill=none] {$\bullet$};
\end{scope}
	\node[starburst, draw, minimum width=2cm, minimum height=2cm,align=center,fill=green!10, font=\small, fill=white, line width=1.5pt] at ($(c)!.5!(helper')$)
{Change\\supplier!};
\end{tikzpicture}
\end{equation*}
\begin{quote}
The company can get widgets either from supplier 1 or supplier 2; we could imagine this choice is completely up to the company. The company can decide based on the quality of widgets it has received in the past: when the company gets a bad widget, it updates an internal variable, and sometimes that variable passes a threshold making the company switch states. Whatever its strategy for deciding, we should be able to encode it in $\poly$.
\end{quote}
\begin{enumerate}[resume]
	\item When someone assembles a machine, their own outputs dictate the connection pattern of the machine's components.
\end{enumerate}
\begin{equation*}%\label{eqn.someone}
\begin{tikzpicture}[oriented WD, font=\ttfamily, bb port length=0, every node/.style={fill=blue!10}, baseline=(someone.north)]
	\node[bb port sep=.5, bb={0}{1}] (A) {unit A};
	\node[bb port sep=.5, bb={1}{0}, right=of A] (B) {unit B};
	\coordinate (helper) at ($(A)!.5!(B)$);
	\node[bb={1}{1}, below=2 of helper] (someone) {\tikzsymStrichmaxerl[3]};
	\draw[->, dashed, blue] (someone_in1) to[out=180, in=270] (A.270);
	\draw[->, dashed, blue] (someone_out1) to[out=0, in=270] (B.270);
	\draw (A_out1) -- +(10pt,0);
	\draw (B_in1) -- +(-10pt,0);
%
\begin{scope}[xshift=3.5in]
	\node[bb port sep=.5, bb={0}{1}] (A') {unit A};
	\node[bb port sep=.5, bb={1}{0}, right=.5of A'] (B') {unit B};
	\coordinate (helper') at ($(A')!.5!(B')$);
	\node[bb={1}{1}, below=2 of helper'] (someone') {\tikzsymStrichmaxerl[3]};
	\draw[->, dashed, blue] (someone'_in1) to[out=180, in=270] (A'.270);
	\draw[->, dashed, blue] (someone'_out1) to[out=0, in=270] (B'.270);
	\draw (A'_out1) -- (B'_in1);
\end{scope}
%
	\node[starburst, draw, minimum width=2cm, minimum height=2cm,fill=blue!50,line width=1.5pt, align=center, font=\upshape] at ($(B)!.5!(A')-(0,.6cm)$)
{Attach!};
\end{tikzpicture}
\end{equation*}
\begin{quote}
Have you ever assembled something? Your internal states dictate the interaction pattern of some other things. We can say this in $\poly$.
\end{quote}

All of the examples discussed here will be presented in some detail once we have the requisite mathematical theory (\cref{ex.bonds_break,ex.supplier_change,ex.assemble_machine}).
\end{example}

\begin{exercise}%\label{exc.changing_types}
Think of another example where systems are sending each other information, but where who the information is being sent to or received from can change based on the states of the systems involved. You might have more than two, say $\rr$-many, different interaction patterns in your setting.
\begin{solution}
When I am carrying my phone in my house, my phone will connect to my Wi-Fi router. But my phone may send me a reminder that tells me to leave my house. 
Once I carry my phone far enough away, it will disconnect from my router and connect to a cellular network instead.
While I am outside, I might then press a button on my phone to disconnect it from the cellular network to reduce my data usage, so that it is no longer connectd to any network.
\end{solution}
\end{exercise}

But there's more that's intuitively wrong or limiting about the picture in \eqref{eqn.wired_forever}. Ever notice how you can change how you interface with the world? Sometimes I close my eyes, which makes that particular way of sending me information inaccessible: that port vanishes, and you need to use your words. Sometimes I'm in a tunnel and my car can't receive a radio signal. Sometimes I extend my hand to give or receive an object from another person, but sometimes I don't. So
\slogan{a system's interface itself can change over time.} % with the setting?
We will be able to say all this using $\poly$ as well.

And there's even more that's wrong with the above description.
Namely, when I use my muscles or mouth to express things, my very position changes: my tongue moves, my body moves.
The display of an analog clock is literally the positions of its hands.
So
\slogan{the output of a system is essentially the position it takes.}
Moreover, when I move my eyes, that's something you can actually see---you can tell if I'm looking at you.
When I turn around, I see different things, and \emph{you can notice I'm turned around}!
In other words,
\slogan{the range of inputs a system can receive depends on the position it currently outputs.}
This is integral to our model of dynamical systems in $\poly$, and why we say that outputs correspond to positions and inputs to directions---with an entire interface represented by a polynomial.

\begin{example}\label{ex.pond_eyeballs}
Imagine a million little eyeballs, each of which has a tiny brain inside it, all together in a pond of eyeballs. All that an individual eyeball $e$ can do is open and close. When $e$ is open, it can make some distinction about all the rest of the eyeballs in view: maybe it can count how many are open, or maybe it can see whether just one certain eyeball $e'$ is open or closed. But when $e$ is closed, it can't see anything; whatever else is happening, it's all the same to $e$. All it can do in that state is process previous information.

Each eyeball in this system will correspond to the polynomial $\yon^\ord{n}+\yon$, which consists of two positions: an ``open'' position with $n$-many possible inputs it may perceive, and a ``closed'' position with only one. For simplicity, we could assume $n=2$, so that each eyeball makes a single yes-no distinction whenever it's open.

The point, however, is that any other eyeball may be capable of noticing if $e$ is open or closed. We can imagine some interesting dynamics in this system, e.g.\ waves of openings or closings sweeping over the group, a ripple of openings expanding through the pond.

Talk about real-world applications! 
\end{example}

\begin{exercise}
Give another example of a system where the range of possible inputs the system can receive is dependent on what output the system is currently providing.
\begin{solution}
Consider a computer application whose output is an image displayed on the screen.
This image consists of buttons I can click, and each button-click is a possible input that the application can receive.
Clicking a button will alter the display, including which buttons are now available to be clicked.
For example, when I click the ``File'' button, the ``File'' menu will appear, allowing me to click other buttons like the ``New Window'' button.
So the set of inputs I can send to the system (i.e.\ buttons I can click) is directly dependent on the output the system sends to me (i.e.\ the image displayed on the screen).
\end{solution}
\end{exercise}

Hopefully you now have an idea of what we mean by \emph{mode-dependence}: interfaces and interaction patterns changing over time, based on the states of all the systems involved. We'll see that $\poly$ speaks about mode-dependent systems and interaction patterns in this sense. 

\begin{remark}
We ended \cref{ex.pond_eyeballs} by joking about ``real-world applications,'' because a pond of eyeballs is about the most bizarre thing one can imagine. But recall Nobel physicist Frank Wilczek's quote from the preface:
\begin{quote}
For me, though, it is difficult to resist the idea that space-time is not essentially different from matter, which we understand more deeply. If so, it will consist of vast numbers of identical units---``particles of space''---each in contact with a few neighbors, exchanging messages, joining and breaking apart, giving birth and passing away.
\end{quote}
Suppose the world was made out of a vast number of identical units, each with its own behavior, able to connect and disconnect with neighbors, and even disappear from the world of cause and effect. We may not even be interested in what our world is actually made of---just what these units are able to do. Is there such an elementary unit that could produce all other dynamical systems? The $\yon^\2+\yon$ eyeballs give a sense of a very simple interface---open and perceiving a single distinction about the world, or closed and making no distinctions---that we could imagine building an entire world from.
\end{remark}

It turns out $\poly$ is very versatile in its applications. In the next section, we'll show how it relates to decision-making.

%-------- Section --------%
\section{Decisions} \label{sec.poly.intro.dec}
We return to the example polynomial $\yon^\2 + \2\yon + \1$ from \eqref{eqn.poly_example} and its corresponding corolla forest, in which positions are expressed as roots and directions are represented as leaves:
\begin{equation} \label{eqn.forest2110}
\begin{tikzpicture}[trees]
  \node (1) {$\bullet$} 
    child {}
    child {};
  \node[right=.5 of 1] (2) {$\bullet$} 
    child {};
  \node[right=.5 of 2] (3) {$\bullet$} 
    child {};
  \node[right=.5 of 3] (4) {$\bullet$};
\end{tikzpicture}
\end{equation}
Concretely, we might think of each position as representing a \emph{decision}. Associated to every decision is a set of \emph{options} (directions). The four decisions we exhibit in \eqref{eqn.forest2110} are particularly interesting: they respectively have two options, one option, one option, and no options. Having two options is familiar from life---it's the classic yes/no decision---as well as from Claude Shannon's Information Theory. Having one option is also familiar theoretically and in life: ``sorry, ya just gotta go through it.'' Having no options is when you actually don't get through it: an impossible decision, a sort of ``dead end.'' %While the corollas $\1,\yon,$ and $\yon^\2$ are each interesting as decisions, the sum $\yon^\2+\2\yon+\1$ has very little theoretical interest; it's just an example.

This perspective will prove insightful in \cref{sec.poly.func_nat.morph}, when we start to discuss the \emph{morphisms} between two polynomials.
As a sneak peek, it turns out that a morphism is just a way of \emph{delegating} decisions from one menu to another.
We'll cover this in more detail in \cref{sec.poly.func_nat.morph}.

Now consider the following three trees.
The first two are infinite, but that's hard to draw, so we've included just the first five levels:
\[
\begin{tikzpicture}[trees]
\begin{scope}[
  level 1/.style={sibling distance=20mm},
  level 2/.style={sibling distance=10mm},
  level 3/.style={sibling distance=5mm},
  level 4/.style={sibling distance=2.5mm},
  level 5/.style={sibling distance=1.25mm}]
  \node[dgreen] (a) {$\bullet$}
    child {node[dgreen] {$\bullet$}
    	child {node[dgreen] {$\bullet$}
    		child {node[dgreen] {$\bullet$}
  				child {node[dgreen] {$\bullet$}
    				child {}
    				child {}
    			}
  				child {node[dyellow] {$\bullet$}
    				child {}
    				child {}
    			}
  			}
    		child {node[dyellow] {$\bullet$}
					child {node[dgreen] {$\bullet$}
      			child {}
      			child {}
     			}
    			child  {node[red] {$\bullet$}}
  			}
    	}
    	child {node[dyellow] {$\bullet$}
    		child {node[dgreen] {$\bullet$}
  				child {node[dgreen] {$\bullet$}
    				child {}
    				child {}
    			}
  				child {node[dyellow] {$\bullet$}
    				child {}
    				child {}
    			}
  			}
    		child  {node[red] {$\bullet$}}
    	}
    }
    child {node[dyellow] {$\bullet$}
    	child {node[dgreen] {$\bullet$}
    		child {node[dgreen] {$\bullet$}
  				child {node[dgreen] {$\bullet$}
    				child {}
    				child {}
    			}
  				child {node[dyellow] {$\bullet$}
    				child {}
    				child {}
    			}
  			}
    		child {node[dyellow] {$\bullet$}
					child {node[dgreen] {$\bullet$}
      			child {}
      			child {}
     			}
    			child  {node[red] {$\bullet$}}
  			}
  		}
  		child {node[red] {$\bullet$}
  		}
  	}
  ;
\end{scope}
\begin{scope}[
  level 1/.style={sibling distance=13mm},
  level 2/.style={sibling distance=10mm},
  level 3/.style={sibling distance=5mm},
  level 4/.style={sibling distance=2.5mm},
  level 5/.style={sibling distance=1.25mm}]
\node (b) [right=4 of a, dyellow] {$\bullet$}
  child {node[dgreen] {$\bullet$}
  	child {node[dgreen] {$\bullet$}
  		child {node[dgreen] {$\bullet$}
  			child {node[dgreen] {$\bullet$}
    			child {}
    			child {}
   			}
 				child {node[dyellow] {$\bullet$}
   				child {}
   				child {}
   			}
			}
    		child {node[dyellow] {$\bullet$}
					child {node[dgreen] {$\bullet$}
      			child {}
      			child {}
     			}
    			child  {node[red] {$\bullet$}}
  			}
		}
  	child {node[dyellow] {$\bullet$}
  		child {node[dgreen] {$\bullet$}
  			child {node[dgreen] {$\bullet$}
    			child {}
    			child {}
   			}
 				child {node[dyellow] {$\bullet$}
   				child {}
   				child {}
   			}
			}
  		child  {node[red] {$\bullet$}}
  	}
	}
	child {node[red] {$\bullet$}}	
;
\end{scope}
\node (c) [red, right=2 of b] {$\bullet$};
\end{tikzpicture}
\]
These are patterned examples---and we'll understand what this pattern is more clearly in \cref{**}---of what we will call \emph{decision streams}.
% Decision streams form the objects in a category that also includes the following level-3 abbreviations of binary decision streams (the third of which is a level-3 abbreviation of a finite stream):
% \[
% \begin{tikzpicture}[trees]
% \begin{scope}[
%   level 1/.style={sibling distance=20mm},
%   level 2/.style={sibling distance=10mm},
%   level 3/.style={sibling distance=5mm},
%   level 4/.style={sibling distance=2.5mm}]
%   \node (a) {$\bullet$}
%     child {node {$\bullet$}
%     	child {node {$\bullet$}
%     		child {node {$\bullet$}
%   			}
%     		child {node {$\bullet$}
%   				child 
%   				child
%   			}
%     	}
%     	child {node {$\bullet$}
%   			}
%     }
%     child {node {$\bullet$}
%     	child {node {$\bullet$}
%     		child {node {$\bullet$}
%   				child
%   				child
%   			}
%     		child {node {$\bullet$}
%   				child
%   				child
%   			}
%   		}
%   		child {node {$\bullet$}
%     		child {node {$\bullet$}
%   			}
%     		child {node {$\bullet$}
%   			}
%   		}
%   	}
%   ;
% \end{scope}
% \begin{scope}[
%   level 1/.style={sibling distance=13mm},
%   level 2/.style={sibling distance=10mm},
%   level 3/.style={sibling distance=5mm},
%   level 4/.style={sibling distance=2.5mm}]
%   \node (b) [right=4 of a] {$\bullet$}
%     child {node {$\bullet$}
%     }
%     child {node {$\bullet$}
%     	child {node {$\bullet$}
%     		child {node {$\bullet$}
%   			}
%     		child {node {$\bullet$}
%   				child
%   				child
%   			}
%   		}
%   		child {node {$\bullet$}
%     		child {node {$\bullet$}
%   				child
%   				child
%   			}
%     		child {node {$\bullet$}
%   				child
%   				child
%   			}
%   		}
%   	}
%   ;
% \end{scope}
% \begin{scope}[
%   level 1/.style={sibling distance=13mm},
%   level 2/.style={sibling distance=8mm},
%   level 3/.style={sibling distance=5mm},
%   level 4/.style={sibling distance=2.5mm}]
%   \node (c) [right=4 of b] {$\bullet$}
%     child {node {$\bullet$}
%     	child {node {$\bullet$}
%   		}
%   		child {node {$\bullet$}
%     		child {node {$\bullet$}
%   			}
%     		child {node {$\bullet$}
%   			}
%   		}
%     }
%     child {node {$\bullet$}
%   	}
%   ;
% \end{scope}
% \end{tikzpicture}
% \]
Decision streams form the objects of a category with very nice properties (it's a topos), which we call $\sys(\yon^\2+\1)$. The idea is that these trees are built up of smaller corollas, each of which has either two options, corresponding to $\yon^\2$, or no options, corresponding to $\1\iso\yon^\0$.
We say that each such decision stream has type $\yon^\2+\yon^\0$.

\begin{exercise}\label{exc.decision_streams}
\begin{enumerate}
	\item Draw the first three levels of a decision stream of type $\yon^\2+\yon^\0$.
	\item Draw the first four levels of a decision stream of type $\yon$.
	\item Draw a the first three levels of a decision stream of type $\nn\yon^\2$ by labeling every node with a natural number.
	\qedhere
\end{enumerate}

\begin{solution}
\begin{enumerate}
	\item Here are the first three levels of a decision stream of type $\yon^\2+\yon^\0$.
\[
\begin{tikzpicture}[trees,
  level 1/.style={sibling distance=20mm},
  level 2/.style={sibling distance=10mm},
  level 3/.style={sibling distance=5mm},
  level 4/.style={sibling distance=2.5mm}]
  \node (a) {$\bullet$}
    child {node {$\bullet$}
    	child {node {$\bullet$}
    		child
    		child
    	}
    	child {node {$\bullet$}
  			}
    }
    child {node {$\bullet$}
    	child {node {$\bullet$}}
    	child {node {$\bullet$}
  		}
  	}
  ;
\end{tikzpicture}
\]
	\item Here are the first four levels of a decision stream of type $\yon$.
\[
\begin{tikzpicture}[trees]
	\node (a) {$\bullet$}
		child {node {$\bullet$}
			child {node {$\bullet$}
				child {node {$\bullet$}
  				child
			}}};
\end{tikzpicture}
\]
	\item Here are the first three levels of a decision stream of type $\nn\yon^\2$ where we indicate the position of each node by labeling it with a natural number.
\[
\begin{tikzpicture}[trees,
  level 1/.style={sibling distance=20mm},
  level 2/.style={sibling distance=10mm},
  level 3/.style={sibling distance=5mm},
  level 4/.style={sibling distance=2.5mm}]
  \node (a) {$27$}
    child {node {$5040$}
    	child {node {$192$}
    		child 
    		child
			}
    	child {node {$0$}
    		child 
    		child
  			}
    }
    child {node {$314159$}
    	child {node {$1000$}
  				child
  				child
  		}
			child {node {$1296$}
				child
				child
			}
  	}
  ;
\end{tikzpicture}
\]
\end{enumerate}
\end{solution}
\end{exercise}

But decisions aren't just about choosing; they're also about trying to accomplish something. The logic of accomplishment is exceptionally rich in this setting. We will concentrate on what we call a \emph{win condition}, which is an induced subgraph of the decision stream with the property that if $n$ is a winning node, then any child of $n$ is also a winning node. 
\[\begin{tikzpicture}[trees,
  level 1/.style={sibling distance=20mm},
  level 2/.style={sibling distance=10mm},
  level 3/.style={sibling distance=5mm},
  level 4/.style={sibling distance=2.5mm}]
  \node (root) {$\bullet$}
    child {node {$\bullet$}
    	child {node {$\bullet$}
    		child {node {$\bullet$}
  			}
    		child {node {$\bullet$}
  				child
  				child
  			}
    	}
    	child {node {$\bullet$}
  			}
    }
    child {node {$\bullet$}
    	child {node {$\bullet$}
    		child {node {$\bullet$}
  				child
  				child
  			}
    		child {node {$\bullet$}
  				child
  				child
  			}
  		}
  		child {node {$\bullet$}
    		child {node {$\bullet$}
  				child
  				child
  			}
    		child {node {$\bullet$}
  			}
  		}
  	}
  ;
 \begin{scope}[every node/.style={circle, inner sep=3pt, blue, draw}]
  \node at (root-1-2)     {};
  \node at (root-2-1-1-1) {};
  \node at (root-2-1-1-2) {};
  \node at (root-2-1-2-1) {};
	\node at (root-2-2) 		{};
  \node at (root-2-2-1) 	{};
  \node at (root-2-2-2) 	{};
  \node at (root-2-2-1-1) {};
  \node at (root-2-2-1-2) {};
 \end{scope}
\end{tikzpicture}
\]
More formally, these are called \emph{sieves}. They form the elements of a logical system called a Heyting algebra: you can take any two sieves and form the intersection or union (which correspond to AND and OR), or even things like implication, negation, and existential and universal quantification. This will give us a calculus of win-conditions for any type $p$ decision stream.

%-------- Section --------%
\section{Data} \label{sec.poly.intro.data}

Data is information, maybe thought of as quantized into atomic pieces, but where these atomic pieces are somehow linked together according to a conceptual structure. When a person or organization uses certain data repeatedly, they often find it useful to put their data in a database. This requires organizing the little pieces into a conceptual structure. So when you hear ``database,'' just think of it as a conceptual structure filled with examples.

To fix a mental image, let's say that you need to constantly look up employees, what department they're in, who the admin person is for that department, who their manager is, etc. Here's an associated database
\begin{equation}\label{eqn.mytables}
\begin{tabular}{ c | c  c  c}
  \textbf{Employee}&\textbf{FirstName}&\textbf{WorksIn}&\textbf{Mngr}\\\hline
  1&Alan&101&2\\
  2&Ruth&101&2\\
  3&Carla&102&3
\end{tabular}
\hspace{.3in}
\begin{tabular}{ c | c  c}
  \textbf{Department}&\textbf{Name}&\textbf{Admin}\\\hline
  101&Sales&1\\
  102&IT&3\\~
\end{tabular}
%\hspace{.3in}
%\begin{tabular}{ c |}
%	\textbf{String}\\\hline
%	Alan\\
%	IT\\[-3pt]
%	\resizebox{!}{10pt}{$\vdots$}
%\end{tabular}
\end{equation}
We can see it as being associated to the following conceptual scheme, also called a \emph{schema}:
\begin{equation}\label{eqn.myschema}
\cat{C}\coloneqq
\boxCD{white}{
\begin{tikzcd}[row sep=large, ampersand replacement = \&]
 	\LTO{Employee}\ar[rr, shift left, "\text{WorksIn}"]\ar[dr, bend right, "\text{FirstName}"']\ar[loop left, "\text{Mngr}"]\&\&
  \LTO{Department}\ar[ll, shift left, "\text{Admin}"]\ar[dl, bend left, "\text{Name}"]\\
  \&\LTO[\circ]{String}
\end{tikzcd}
\leavevmode\\\bigskip
\text{Department.Admin.WorksIn = Department}
}
\end{equation}
The equation at the bottom says that for any department $d$, if you ask for the admin person and see which department they work in, it's required to be $d$.

What's called $\cat{C}$ in \eqref{eqn.myschema} is a \emph{finitely presented category}.
The objects of the category are the points, while each arrow corresponds to a morphism of $\cat{C}$ (there are additional morphisms, including identity morphisms, that are not depicted).
The equation at the bottom indicates that composing the morphism Admin with the morphism WorksIn yields the identity morphism on the object Department.

The database instance presented in \eqref{eqn.mytables} then corresponds to a functor $I\colon\cat{C}\to\smset$.
For example, $I$ sends the object $\text{Employee}\in\cat{C}$ to the set $I(\text{Employee})\coloneqq\{1,2,3\}$, the entries in the Employee column of the left table.

The functor also sends the morphism $\text{Mngr}\colon\text{Employee}\to\text{Employee}$ to the function $I(\text{Mngr})\colon\{1,2,3\}\to\{1,2,3\}$ that sends each entry in the Employee column to its corresponding entry in the Mngr column.
So $I(\text{Mngr})(1)=2$, $I(\text{Mngr})(2)=2$, and $I(\text{Mngr})(3)=3$.

A functor $\cat{C}\to\smset$ is called a \emph{copresheaf on $\cat{C}$}. So the story of database schemas and their data can be based on the story of categories and their copresheaves.

\begin{exercise} %\label{exc.my_schema_and_tables}
As above, we define the finitely presented category $\cat{C}$ according to \eqref{eqn.myschema} and the copresheaf $I$ on $\cat{C}$ according to \eqref{eqn.mytables}.
\begin{enumerate}
    \item What is $I(\text{Department})$?
    \item What is $I(\text{Admin})$?
    \item Composing Admin with FirstName yields a morphism from Department to String that we denote by Admin.FirstName.
    What is $I(\text{Admin.FirstName})$?
    \item Say we require that managers work in the same department as the employees they oversee.
    Write down an equation in $\cat{C}$ (like the one at the bottom of \eqref{eqn.myschema} that expresses this condition.
    \item How might we define $I(\text{String})$? \qedhere
\end{enumerate}

\begin{solution}
We refer to \eqref{eqn.myschema} and \eqref{eqn.mytables} to characterize the category $\cat{C}$ and the functor $I \colon \cat{C} \to \smset$.
\begin{enumerate}
    \item Here Department is an object of $\cat{C}$, so $I(\text{Department})$ is a set. From the Department column in the table on the right of \eqref{eqn.mytables}, we observe that $I(\text{Department}) = \{101, 102\}$.
    \item Here Admin is an morphism of $\cat{C}$ from Department to Employee, so $I(\text{Admin})$ is a function from $I(\text{Department}) = \{101, 102\}$ to $I(\text{Employee}) = \{1, 2, 3\}$. From the Admin column in the table on the right of \eqref{eqn.mytables}, we observe that $I(\text{Admin})(101) = 1$ and $I(\text{Admin})(102) = 3$.
    \item By functoriality, $I(\text{Admin.FirstName})$ is the function $I(\text{Admin})$ composed with $I(\text{FirstName})$.
    We have that $I(\text{Admin})$ sends $101$ to $1$ and $102$ to $3$, while the FirstName column tells us that $I(\text{FirstName})$ sends $1$ to Alan and $3$ to Carla.
    So $I(\text{Admin.FirstName})$ sends $101$ to Alan and $102$ to Carla.
    \item If every employee works in the same department as their manager, then Employee.WorksIn = Employee.Mngr.WorksIn.
    \item The name suggests that $I(\text{String})$ is the set of all possible strings of characters.
    Perhaps this could be defined as $\bigcup_{n \in \nn} A^n$, where $A$ is our alphabet of allowed characters, which may include the English letters, spaces, digits, and whatever other characters we allow.
    At the very least, since FirstName and Name are both morphisms to String, every entry in the FirstName and Name columns must be in $I(\text{String})$.
    So all we know for sure is that $\{\text{Alan, Ruth, Carla, Sales, IT}\} \ss I(\text{String})$.
\end{enumerate}
\end{solution}
\end{exercise}

There's a very important thing that we do with databases: we query them. We ask them questions like ``tell me the First Name of every Employee that's either the Admin of the Sales department or their Manager.''
\begin{minted}{mysql}
 FOR    d: Department, e: Employee
 WHERE  Name(d)="Sales" AND
        (e=Admin(d) OR e=Mngr(Admin(d)))
 RETURN FirstName(e)
\end{minted}
This sort of question is formally called a ``union of conjunctive queries.'' We will see this sort of query is intimately connected with $\poly$.
We will also see how databases can be conceived in terms of dynamical systems.

%-------- Section --------%
\section{Implementation} \label{sec.poly.intro.code}

Everything we talk about can actually be implemented in a computer without much difficulty, at least if you have access to a language that supports dependent types, such as Agda or Idris.

What we have been calling polynomials---things like $\yon^\2+\2\yon+\1$---are often called \emph{containers} in the computer science literature. A container consists of a type $S$, usually called the type of \emph{shapes}, and a type $P(s)$ for each term $s:S$, called the type of \emph{positions} in shape $s$. It's mildly unfortunate that the names clash with our own: for us a container-shape is a position and a container-position is a direction.

Luckily, the Agda code is pretty easy to understand.
\begin{agda}
record Arena : Set where  -- an arena consists of 
   field                      -- two fields
     pos : Set                -- one called pos, a set; and
     dir : pos -> Set         -- one called dir,
                              -- a set for each element of the set pos
\end{agda}

%-------- Section --------%
\section{Mathematical theory} \label{sec.poly.intro.math_theory}

The applications of $\poly$ are quite diverse and interesting, encapsulating dynamics, data, and decisions. However it is how the mathematics of $\poly$ supports these applications that is so fantastic. For experts, here are some reasons for the excitement.

\begin{proposition}
$\poly$ has all products and coproducts and is completely distributive.
$\poly$ also has exponential objects, making it a bicartesian closed category.
It therefore supports the simply typed lambda calculus.
\end{proposition}
\begin{proof}
We will prove that $\poly$ has coproducts in \cref{prop.poly_coprods}, that it has products in \cref{prop.poly_prods}, that it is completely distributive in \cref{prop.poly_completely_distributive}, and that it has exponential objects in \cref{thm.poly_cart_closed}.
\end{proof}

\begin{proposition}
Beyond the cocartesian and cartesian monoidal structures $(\0,+)$ and $(\1,\times)$, the category $\poly$ has two additional monoidal structures, denoted $(\yon,\otimes)$ and $(\yon,\circ)$, which are together duoidal.\footnote{We will follow the convention of writing the tensor unit before the tensor product when specifying a monoidal structure.} Moreover $\otimes$ is a closed monoidal structure that distributes over coproducts, while $\circ$ is a left coclosed monoidal structure that preserves connected limits.
\end{proposition}
\begin{proof}
We will define $\otimes$ in \cref{def.dirichlet} and prove that $(\yon, \otimes)$ is a monoidal structure on $\poly$ in \cref{prop.dirichlet_monoidal}, and we will define $\circ$ in \cref{def.comp} and prove that $(\yon, \circ)$ is a monoidal structure on $\poly$ in \cref{cor.comp_monoidal}.
Then we will show that $\circ$ is duoidal over $\otimes$ in \cref{**}.

In \cref{prop.day}, we will show that $\otimes$ distributes over coproducts.
Then in \cref{prop.dirichlet_closure}, we will prove that $\otimes$ is closed.
In \cref{prop.comp_left_coclosed}, we will show that $\circ$ is left coclosed, and in \cref{thm.connected_limits}, we will show that $\circ$ preserves connected limits.
\end{proof}

\begin{proposition}\label{prop.adjoint_quadruple}
$\poly$ has an adjoint quadruple with $\smset$ and an adjoint pair with $\smset\op$:
\begin{equation*}%\label{eqn.adjoints_galore}
\begin{tikzcd}[column sep=60pt]
  \smset
  	\ar[r, shift left=7pt, "A" description]
		\ar[r, shift left=-21pt, "A\yon"']&
  \poly
  	\ar[l, shift right=21pt, "p(\0)"']
  	\ar[l, shift right=-7pt, "p(\1)" description]
	\ar[l, phantom, "\scriptstyle\Leftarrow"]
	\ar[l, phantom, shift left=14pt, "\scriptstyle\Rightarrow"]
	\ar[l, phantom, shift right=14pt, "\scriptstyle\Rightarrow"]
\end{tikzcd}
\hspace{.6in}
\adjr[50pt]{\smset\op}{\yon^A}{\Gamma(p)}{\poly}.\footnote{
	We use the notation 
	$\begin{tikzcd}[ampersand replacement=\&]
		\cat{C}\ar[r, shift left=4pt, "L"]\&
		\cat{D}\ar[l, shift left=4pt, "R"]\ar[l, phantom, "\scriptstyle\Rightarrow"]
	\end{tikzcd}$
	to denote an adjunction $L \dashv R$. The double arrow, always pointing in the same direction as the left adjoint, indicates both the unit $\cat{C}\Rightarrow R \circ L$ and the counit $L \circ R\Rightarrow \cat{D}$ of the adjunction.
}
\end{equation*}
Each functor is labeled by where it sends $p\in\poly$ or $A\in\smset$; in particular, $\Gamma(p) \coloneqq \poly(p, \yon)$.
\end{proposition}
\begin{proof}
We will prove that $\poly$ has an adjoint quadruple with $\smset$ in \cref{thm.adjoint_quadruple}, and that it has an adjoint pair with $\smset\op$ in \cref{prop.yoneda_left_adjoint}.
\end{proof}

There's a lot we're leaving out of this summary, just so we can hit the highlights.

\begin{proposition}
The functor $\poly\to\smset$ given by $p\mapsto p(\1)$ is a monoidal fibration.
\end{proposition}

In fact it's a monoidal $*$-bifibration in the sense of \cite{shulman2008framed}. But here's where things get really interesting.

\begin{proposition}[Ahman-Uustalu]\label{prop.ahman_uustalu1}
Comonoids in $(\poly,\yon,\circ)$ are categories (up to isomorphism).
\end{proposition}

\begin{proposition}
The category $\comon$ has finite coproducts and products, and coproducts in $\comon$ agree with those in $\smcat$.
\end{proposition}

\begin{proposition}
The forgetful functor $U\colon\comon\to\poly$ has a right adjoint
\[
\adjr{\poly}{\cofree{-}}{U}{\comon}
\]
called the \emph{cofree comonoid} construction. It is lax monoidal with respect to $\otimes$.
\end{proposition}

\begin{proposition}
The category $\comon$ has a third symmetric monoidal structure $(\yon,\otimes)$, and the forgetful functor $U\colon(\comon,\yon,\otimes)\to(\poly,\yon,\otimes)$ is strong monoidal.
\end{proposition}

\begin{proposition}\label{prop.sysp[i]s_topos}
For any polynomial $p$, the category
\[\sys(p)\cong[\cofree{p},\smset]\]
of dynamical systems on $p$ forms a topos.\footnote{We use the notation $[\cat{C}, \cat{D}]$ to denote the category of functors $\cat{C}\to\cat{D}$.}
\end{proposition}

The proposition above indicates that there is a full-fledged logic of dynamical systems inhabiting any interface, while the following proposition implies that these logics can be combined and compared.

\begin{proposition}\label{prop.poly_map_pregeo_topos}
A morphism $p\to q$ of polynomials induces a pre-geometric morphism between their respective toposes:
\[
\adj{\sys(p)}{}{}{\sys(q)}.
\]
\end{proposition}

The following propositions suggest that the whole story of dynamics carries a strong connection to database theory.

\begin{proposition}
Suppose that the category $\cat{C}$ corresponds under \cref{prop.ahman_uustalu1} to the comonoid $\com{C}$. Then there is an equivalence of categories
\[
\Cat{Bimod}(\com{C},0)\cong[\cat{C},\smset]
\]
between $(\com{C},0)$-bimodules\footnote{Technically, what we are calling bimodules over a pair of comonoids are really bi\emph{co}modules, but we will use the simpler term throughout.} and $\cat{C}$-copresheaves.
\end{proposition}
We will use the convention that the comonoid $\com{C}$ corresponds to category $\cat{C}$ under \cref{prop.ahman_uustalu1}, and similarly for $\com{D}$ and $\cat{D}$, etc. 

\begin{proposition}
For any category $\cat{C}$, the category of left $\com{C}$-modules is equivalent to the category of functors $\cat{C}\to\poly$:
\[\Cat{Bimod}(\com{C},\yon)\cong\Cat{Fun}(\cat{C},\poly).\]
\end{proposition}

\begin{proposition}[Garner]
For any categories $\cat{C}$ and $\cat{D}$, there is an equivalence of categories
\[
\Cat{Bimod}(\com{C},\com{D})\cong\Cat{pra}([\cat{D},\smset],[\cat{C},\smset])
\]
between that of bimodules between comonoids in $\poly$ and parametric right adjoints between copresheaf categories.
\end{proposition}

If you skipped over any of that---or all of that---it'll be no problem whatsoever! We will cover each of the above results in detail over the course of this book. There are many avenues for study, but we need to push forward.

We'll begin in the next chapter.

%-------- Section --------%
\section{Exercise solutions}
\Closesolutionfile{solutions}
{\footnotesize
\input{solution-file1}}

\Opensolutionfile{solutions}[solution-file2]

%------------ Chapter ------------%
\chapter{Polynomial functors and natural~transformations} \label{ch.poly.func_nat}

In this chapter, we will set down the basic category-theoretic story of $\poly$, so that we can have a firm foundation from which to speak about dynamical systems, decisions, and data. 
We will formally introduce the objects of $\poly$: polynomial functors.
We will also study the natural transformations between these functors, which are $\poly$'s morphisms.
Finally, we'll present some of $\poly$'s most versatile categorical properties.

But we'll begin by examining a specific kind of polynomial functor that you may already be familiar with---representable functors on the category $\smset$ of sets and functions.
We'll highlight how these functors are closely related to what is arguably the fundamental theorem of category theory, the Yoneda lemma.

%-------- Section --------%
\section{Representable functors and the Yoneda lemma} \label{sec.poly.func_nat.yon}

Representable functors form the basis of the category $\poly$.
Much of the following theory is applicable for general functors to $\smset$; but for now, we are only interested in functors $\smset\to\smset$.

\begin{definition}[Representable functor] \label{def.representable}
Given any set $S$, we denote by $\yon^S\colon\smset\to\smset$ the functor that sends any set $X$ to the set $X^S=\smset(S,X)$, and sends any function $h\colon X\to Y$ to the function $h^S\colon X^S\to Y^S$, the one that sends $g\colon S\to X$ to $g\then h\colon S\to Y$.\footnote{Throughout this text, in any category, given objects $A, B, C$ and morphisms $f \colon A \to B$ and $g \colon B \to C$, we will denote their composite morphism $A \to C$ interchangeably as $f \then g$ or $g \circ f$, depending on which is more natural to think about.}

We refer to functors of this form as \emph{representable functors}, or simply as \emph{representables}.
In particular, we call $\yon^S$ the functor \emph{represented by} $S$, and we call $S$ the \emph{representing set} of $\yon^S$.
\end{definition}

The symbol $\yon$ stands for Yoneda, for reasons we will get to in \cref{lemma.yoneda}. For now, here are some pictures for your eyes to gaze at; they are the polynomials corresponding to various representables, namely the pure powers:
\begin{equation}\label{eqn.trees_for_gazing}
\begin{tikzpicture}[trees, sibling distance=2mm]
  \node["$\yon^{\5}$" below] (1) {$\bullet$} 
    child foreach \i in {1,...,5}
    ;
\end{tikzpicture}
\qquad
\begin{tikzpicture}[trees, sibling distance=1mm]
  \node["$\yon^{\1\0}$" below] (1) {$\bullet$} 
    child foreach \i in {1,...,10}
    ;
\end{tikzpicture}
\qquad
\begin{tikzpicture}[trees, sibling distance=0.5mm]
  \node["$\yon^{\2\0}$" below] (1) {$\bullet$} 
    child foreach \i in {1,...,20}
    ;
\end{tikzpicture}
\qquad
\begin{tikzpicture}[trees, sibling distance=0.25mm]
  \node["$\yon^{\4\0}$" below] (1) {$\bullet$} 
    child foreach \i in {1,...,40}
    ;
\end{tikzpicture}
\qquad
\begin{tikzpicture}[trees, sibling distance=0.0625mm]
  \node["$\yon^{[0,1]}$" below] (1) {$\bullet$} 
    child foreach \i in {1,...,160}
    ;
\end{tikzpicture}
\end{equation}

\begin{example}
The functor that sends every set $X$ to $X\times X$, and sends $h\colon X\to Y$ to $(h\times h)\colon (X\times X)\to(Y\times Y)$, is representable. After all, $X \times X \iso X^\2$, so this functor is just a pure power we are already familiar with: $\yon^\2$.
\end{example}

\begin{exercise}\label{exc.representable_fun}
For each of the following functors $\smset\to\smset$, say if it is representable or not; if it is, say what the representing set is.
\begin{enumerate}
	\item The identity functor $X\mapsto X$, which sends each function to itself.
	\item The constant functor $X\mapsto\2$, which sends every function to the identity on $\2$.
	\item The constant functor $X\mapsto\1$, which sends every function to the identity on $\1$.
	\item The constant functor $X\mapsto\0$, which sends every function to the identity on $\0$.
	\item A functor $X\mapsto X^\nn$.
	If it were representable, where would it send each function?
	\item A functor $X\mapsto 2^X$.
	If it were representable, where would it send each function?
\qedhere
\end{enumerate}

\begin{solution}
Our goal is to say whether various functors are representable and, for those that are, what the representing set is.
\begin{enumerate}
	\item The identity functor $X\mapsto X$ is represented by $S=\1$: a function $\1 \to X$ is just an element of $X$, so $\smset(\1,X) \iso X$.
	Alternatively, note that $X^1 \iso X$.
	\item \label{sol.representable_fun.2} The constant functor $X\mapsto\2$ is not representable: the functor sends $\1$ to $\2$, but $\1^S \iso \1 \not\iso \2$ for any set $S$.
	\item The constant functor $X\mapsto\1$ is representable by $S=\0$: there is exactly one function $\0 \to X$, so $\smset(\0,X) \iso \1$.
	Alternatively, note that $X^0 \iso \1$.
	\item The constant functor $X\mapsto\0$ is not representable, for the same reason as in \cref{sol.representable_fun.2}.
	\item The functor $\yon^\nn$ that sends $X\mapsto X^\nn$ is represented by $S=\nn$, by definition.
	It sends each function $h \colon X \to Y$ to the function $h^\nn \colon X^\nn \to Y^\nn$ that sends each $g \colon \nn \to X$ to $g \then h \colon \nn \to Y$.
	\item No $\smset \to \smset$ functor $X\mapsto \2^X$ is representable, for the same reason as in \cref{sol.representable_fun.2}.
	(There \emph{is}, however, a functor $\smset\op \to \smset$ sending $X \mapsto 2^X$ that is understood to be representable in a more general sense.)
\end{enumerate}
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.representable_nt}
For any function $f\colon R\to S$, there is an induced natural transformation $\yon^f\colon\yon^S\to \yon^R$; on any set $X$ the $X$-component $X^f\colon X^S\to X^R$ is given by sending $g\colon S\to X$ to $f\then g\colon R\to X$.
\end{proposition}

\begin{exercise} \label{exc.representable_nt}
Prove that for any function $f\colon R\to S$, what we said was a natural transformation in \cref{prop.representable_nt} really is natural. That is, for any function $h\colon X\to Y$, show that the following diagram commutes:
\[
\begin{tikzcd}[bottom base]
	X^S\ar[r, "h^S"]\ar[d, "X^f"']&
	Y^S\ar[d, "Y^f"]\\
	X^R\ar[r, "h^R"']&
	Y^R\ar[ul, phantom, "?"]
\end{tikzcd}
\qedhere
\]

\begin{solution}
To show that
\[
\begin{tikzcd}[ampersand replacement=\&]
	X^S\ar[r, "h^S"]\ar[d, "X^f"']\&
	Y^S\ar[d, "Y^f"]\\
	X^R\ar[r, "h^R"']\&
	Y^R\ar[ul, phantom, "?"]\&
	\qedhere
\end{tikzcd}
\]
commutes, we note that by \cref{prop.representable_nt}, both vertical maps compose functions from $S$ with $f \colon R \to S$ from the left, and by \cref{def.representable}, both horizontal maps compose functions to $X$ with $h \colon X \to Y$ on the right.
So by the associativity of composition, the diagram commutes.
\end{solution}
\end{exercise}

\begin{exercise} \label{exc.representable_nt_components}
Let $X$ be an arbitrary set. For each of the following sets $R,S$ and functions $f\colon R\to S$, describe the $X$-component of, i.e.\ the function $X^S\to X^R$ coming from, the natural transformation $\yon^f\colon\yon^S\to\yon^R$.
\begin{enumerate}
	\item \label{exc.representable_nt_components.id} $R=\5$, $S=\5$, $f=\id$.  (Here you're supposed to give a function called $X^{\id_\5}\colon X^\5\to X^\5$.)
	\item $R=\2$, $S=\1$, $f$ is the unique function.
	\item $R=\1$, $S=\2$, $f(1)=1$.
	\item $R=\1$, $S=\2$, $f(1)=2$.
	\item $R=\0$, $S=\5$, $f$ is the unique function.
	\item $R=\nn$, $S=\nn$, $f(n)=n+1$.
\qedhere
\end{enumerate}

\begin{solution}
In each case, given $f \colon R \to S$, we can find the $X$-component $X^f \colon X^S \to X^R$ of the natural transformation $\yon^f\colon\yon^S\to\yon^R$ by applying \cref{prop.representable_nt}, which says that $X^f$ sends each $g \colon S \to X$ to $f \then g \colon R \to X$.
\begin{enumerate}
    \item If $R=\5$, $S=\5$, and $f=\id$, then $X^f$ is the identity function on $X^\5$.
    \item If $R=\2$, $S=\1$, and $f$ is the unique function, then $X^f$ sends each $g \in X$ (i.e.\ function $g \colon \1 \to X$) to the function that maps both elements of $\2$ to $g$.
    We can think of $X^f$ as the diagonal $X \to X \times X$.
	\item If $R=\1$, $S=\2$, and $f(1)=1$, then $X^f$ sends each $g \colon \2 \to X$ to $g(1)$, viewed as a function $\1 \to X$.
	We can think of $X^f$ as the left projection $X \times X \to X$.
	\item If $R=\1$, $S=\2$, and $f(1)=2$, then $X^f$ sends each $g \colon \2 \to X$ to $g(2)$, viewed as a function $\1 \to X$.
	We can think of $X^f$ as the right projection $X \times X \to X$.
	\item If $R=\0$, $S=\5$, and $f$ is the unique function, then $X^f$ is the unique function $X^\5 \to X^\0 \iso \1$.
	\item If $R=\nn$, $S=\nn$, and $f(n)=n+1$, then $X^f$ sends each $g \colon \nn \to X$ to the function $h \colon \nn \to X$ satisfying $h(n) = g(n+1)$ for all $n \in \nn$.
	We can think of $X^f$ as removing the first term of an infinite sequence of elements of $X$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise} \label{exc.representable_nt_functorial}
Show that the construction in \cref{prop.representable_nt} is functorial
\begin{equation} \label{eqn.yoneda_embedding}
\yon^-\colon\smset\op\to[\smset,\smset],
\end{equation}
as follows.
\begin{enumerate}
	\item Show that for any set $S$, we have that $\yon^{\id_S}\colon\yon^S\to\yon^S$ is the identity.
	\item Show that for any functions $f\colon R\to S$ and $g\colon S\to T$, we have $\yon^g\then\yon^f=\yon^{f\then g}$.
\end{enumerate}

\begin{solution}
\begin{enumerate}
    \item The fact that $\yon^{\id_S}\colon\yon^S\to\yon^S$ is the identity is just a generalization of \cref{exc.representable_nt_components} \cref{exc.representable_nt_components.id}.
    For any set $X$, the $X$-component $X^{\id_S} \colon X^S \to X^S$ of $\yon^{\id_S}$ sends each $h \colon S \to X$ to $\id_S \then h = h$, so $X^{\id_S}$ is the identity on $X^S$.
    Hence $\yon^{\id_S}$ is the identity on $\yon^S$.
    \item Fix $f \colon R \to S$ and $g \colon S \to T$; we wish to show that $\yon^g \then \yon^f = \yon^{f \then g}$.
    It suffices to show componentwise that $X^g \then X^f = X^{f \then g}$ for every set $X$.
    Indeed, $X^g$ sends each $h \colon T \to X$ to $g \then h$; then $X^f$ sends $g \then h$ to $f \then g \then h = X^{f \then g}(h)$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{lemma}[Yoneda lemma]\label{lemma.yoneda}
Given a functor $F\colon\smset\to\smset$ and a set $S$, there is an isomorphism
\begin{equation}\label{eqn.yoneda}
F(S)\iso\nat(\yon^S,F)
\end{equation}
where $\nat$ denotes the set of natural transformations. Moreover, \eqref{eqn.yoneda} is natural in both $S$ and $F$.
\end{lemma}
\begin{proof}[Sketch of proof]
For any natural transformation $m\colon\yon^S\to F$, consider the component $m_S\colon S^S\to F(S)$. Applying it to the identity on $S$ as an element of $S^S$, we get an element $m_S(\id_S)\in F(S)$.

Conversely, for any element $a\in F(S)$, there is a natural transformation $m^a\colon\yon^S\to F$ whose $X$-component is the function $X^S\to F(X)$ given by sending $g\colon S\to X$ to $F(g)(a)$. In \cref{exc.finish_proof_yoneda} we ask you to show that this is indeed natural in $X$, and that these two constructions, $m\mapsto m_S$ and $a\mapsto m^a$, are mutually inverse.
\end{proof}

\begin{exercise}\label{exc.finish_proof_yoneda}
Whoever solves this exercise can say they've proved the Yoneda lemma.
\begin{enumerate}
	\item Show that for any $a\in F(S)$, the maps $X^S\to F(X)$ given as in the proof sketch of \cref{lemma.yoneda} are natural in $X$.
	\item Show that the two mappings from the proof sketch of \cref{lemma.yoneda} are mutually inverse.
	\item Show that \eqref{eqn.yoneda} is natural in $F$.
	\item Show that \eqref{eqn.yoneda} is natural in $S$.
	\item As a corollary of \cref{lemma.yoneda}, show that the functor $\yon^-\colon\smset\op\to[\smset,\smset]$ from \eqref{eqn.yoneda_embedding} is fully faithful---in particular, that there is an isomorphism $S^T\iso \nat(\yon^S,\yon^T)$.
	For this reason, we call this functor the \emph{Yoneda embedding}.
\qedhere
\end{enumerate}

\begin{solution}
\begin{enumerate}
    \item Given $a \in F(S)$, naturality of the maps $X^S \to F(X)$ that send $g \colon S \to X$ to $F(g)(a)$ amounts to the commutativity of
    \[
    \begin{tikzcd}[ampersand replacement=\&]
    	X^S\ar[r, "h^S"]\ar[d, "F(-)(a)"']\&
    	Y^S\ar[d, "F(-)(a)"]\\
    	F(X)\ar[r, "F(h)"']\&
    	F(Y)
    \end{tikzcd}
    \]
    for all $h \colon X \to Y$.
    The top map $h^S$ sends any $g \colon X \to S$ to $g \then h$ (\cref{def.representable}), which is then sent to $F(g \then h)(a)$ by the right map.
    Meanwhile, the left map sends $g$ to $F(g)(a)$, which is then sent to $F(h)(F(g)(a))$ by the bottom map.
    So by the functoriality of $F$, the square commutes.
    
    \item We seek to show that the two maps from the proof sketch of \cref{lemma.yoneda} are mutually inverse. First, we show that for any natural transformation $m \colon \yon^S \to F$, we have that $m^{m_S(\id_S)} = m$.
    Given a set $X$, the $X$-component of $m^{m_S(\id_S)}$ sends each $g \colon S \to X$ to $F(g)(m_S(\id_S))$; it suffices to show that this is also where the $X$-component of $m$ sends $g$.
    Indeed, by the naturality of $m$, the square
    \[
    \begin{tikzcd}[ampersand replacement=\&]
    	S^S\ar[r, "g^S"]\ar[d, "m_S"']\&
    	X^S\ar[d, "m_X"]\\
    	F(S)\ar[r, "F(g)"']\&
    	F(X)
    \end{tikzcd}
    \]
    commutes, so in particular
    \begin{equation} \label{eqn.finish_proof_yoneda}
        F(g)(m_S(\id_S)) = m_X(g^S(\id_S)) = m_X(\id_S \then g) = m_X(g).
    \end{equation}
    In the other direction, we show that for any $a \in F(S)$, we have $m^a_S(\id_S) = a$: by construction, $m^a_S \colon S^S \to F(S)$ sends $\id_S$ to $F(\id_S)(a) = a$.
    
    \item Given functors $F, G \colon [\smset,\smset]$ and a natural transformation $\alpha \colon F \to G$, we wish to show that the naturality square
    \[
    \begin{tikzcd}[ampersand replacement=\&]
    	\nat(\yon^S,F)\ar[d, "- \then \alpha"']\ar[r, "\sim"]\&
    	F(S)\ar[d, "\alpha_S"]\\
    	\nat(\yon^S,G)\ar[r, "\sim"]\&
    	G(S)
    \end{tikzcd}
    \]
    commutes.
    The top map sends any $m \colon \yon^S \to F$ to $m_S(\id_S)$, which in turn is sent by the right map to $\alpha_S(m_S(\id_S)) = (m \then \alpha)_S(\id_S)$.
    This is also where the bottom map sends $m \then \alpha$, so the square commutes.
    
    \item Given a function $g \colon S \to X$, we wish to show that the naturality square on the left side of the diagram
    \[
    \begin{tikzcd}[ampersand replacement=\&]
    	\nat(\yon^S,F)\ar[d, "\yon^g \then -"']\ar[r, "\sim"]\&
    	F(S)\ar[d, "F(f)"]\\
    	\nat(\yon^X,F)\ar[r, "\sim"]\&
    	F(X)
    \end{tikzcd}
    \]
    commutes.
    The left map sends any $m \colon \yon^S \to F$ to $\yon^g \then m$, which is sent by the bottom map to $(\yon^g \then m)_X(\id_X) = m_X(X^g(\id_X)) = m_X(f \then \id_X) = m_X(g)$.
    Meanwhile, the top map sends $m$ to $m_S(\id_S)$, which is sent by the right map to $F(g)(m_S(\id_S))$.
    So the square commutes by \eqref{eqn.finish_proof_yoneda}.
    
    \item To show that $\nat(\yon^S, \yon^T) \iso S^T$, just take $F = \yon^T$ in \cref{lemma.yoneda}.
\end{enumerate}
\end{solution}
\end{exercise}

%-------- Section --------%
\section{Polynomials: sums of representables} \label{sec.poly.func_nat.repr_sum}

We've seen that for any set $A$, the symbol $\yon^A$ denotes a functor $\smset\to\smset$. We will generalize this by adding representable functors together to form polynomials. In some sense, the name ``polynomial'' doesn't quite fit: in algebra, polynomials are generally taken to be finite sums, whereas we will use sums that may be infinite. However, we are not the first by far to use the term ``polynomial'' in this way.

So here's the deal. All of our polynomials will be polynomials in one variable, namely $\yon$.
Every other letter or number that shows up will represent a set.%
\footnote{For those who clamor for polynomials in many variables, we will see in \cref{**} that the multivariable story falls out of the one-variable story.}
For example, in the following bizarre polynomial
\begin{equation}\label{eqn.biz_poly}
p\coloneqq\rr\yon^\zz+\3\yon^{\3}+A\yon+\sum_{i\in I}Q_i\yon^{R_i+Q_i^\2},
\end{equation}
$\rr$ denotes the set of real numbers, $\zz$ denotes the set of integers, $\3$ denotes the set $\{1,2,3\}$, and $A$, $I$, $Q_i$, and $R_i$ denote some arbitrary sets that should have already been defined in order for \eqref{eqn.biz_poly} to make sense.

The polynomials we already understand at this point are the pure power polynomials $\yon^A$ for some set $A$: they are representable functors $\yon^A\colon\smset\to\smset.$
So to understand general polynomials like $p$, we just need to understand what the sums of functors are. It will be useful to discuss products of functors at the same time.

To undertand the sums and products of set-valued functors, we will first need to understand the sums and products of sets.

%---- Subsection ----%
\subsection{Dependent sums and products of sets} \label{subsec.poly.func_nat.repr_sum.dep_sum_prod_set}

Let $I$ be a set, and let $X_i$ be a set for each $i\in I$. We denote this
\emph{$I$-indexed collection of sets} --- a \emph{dependent set} --- in the form of a functor as $X\colon I\to\smset$ (where we view the set $I$ as a discrete category) or more classically as $(X_i)_{i\in I}$.
Indeed, when a functor $X \colon I \to \smset$ is understood to be a dependent set, we will denote $X(i)$ as $X_i$ for $i \in I$.
A single element from one set in the collection would be denoted $(i,x)$ where $x\in X_i$. Choosing an element from
every set in the collection would give us a function $i \mapsto x_i$, where each
$x_i\in X_i$. This is a sort of function we haven't seen before, at least in
this form --- its codomain \emph{depends} on the element we are applying it to.
Think of a vector field $v$: to each point $p$ it assignes a tangent vector
$v_p$ in the tangent space \emph{at $p$}. We can write the signature of such a
function as
\[f \colon (i \in I) \to X_i.\]
We call this a \emph{dependent function}, since its codomain depends on the element of its domain that we are applying it to.

\begin{definition}[Dependent sum and product of sets] \label{def.dep_sums_prods_sets}
Let $I$ be a set and $X\colon I\to\smset$ be an $I$-indexed collection of sets. The \emph{(dependent) sum} $\sum_{i\in I}X_i$ and \emph{(dependent) product} $\prod_{i\in I}X_i$ of this collection are the sets
\[
\sum_{i\in I}X_i:=\{(i,x)\mid i\in I\text{ and }x\in X_i\}
\qqand
\prod_{i\in I}X_i:=\{f \colon (i \in I) \to X_i\}.
\]
\end{definition}

\begin{example}\label{ex.two_sums_and_prods}
If $I=\2=\{1,2\}$ then a collection $X\colon I\to \smset$ is just two sets, say $X_1=\{a,b,c\}$ and $X_2=\{c,d\}$. Their sum is the disjoint union
\[\sum_{i\in \2}X_i=X_1+X_2=\{(1,a),(1,b),(1,c),(2,c),(2,d)\}.\]
Its cardinality (i.e.\ the number of elements it contains) will always be the sum of the cardinalities of $X_1$ and $X_2$.

Meanwhile, their product is the usual cartesian product
\[\prod_{i\in \2}X_i \cong X_1\times X_2=\{(a,c),(a,d),(b,c),(b,d),(c,c),(c,d)\}.\]
Its cardinality will always be the product of the cardinalities of $X_1$ and $X_2$.
\end{example}

\begin{exercise}\label{exc.on_sums_prods_sets}
Let $I$ be a set and let $X_i\coloneqq\1$ be a one-element set for each $i\in I$. 
\begin{enumerate}
	\item \label{exc.on_sums_prods_sets.sum} Show that there is an isomorphism of sets $I\cong\sum_{i\in I}\1$.
	\item \label{exc.on_sums_prods_sets.prod} Show that there is an isomorphism of sets $\1\cong\prod_{i\in I}\1$.
\end{enumerate}
As a special case, suppose $I\coloneqq\varnothing$ and $X\colon \varnothing\to\smset$ is the unique empty collection of sets.
\begin{enumerate}[resume]
	\item Is it true that $X_i=\1$ for each $i\in I$?
	\item Show that there is an isomorphism of sets $\0\cong\sum_{i\in\varnothing}X_i$, to justify the statement ``the empty sum is $\0$.''
	\item Show that there is an isomorphism of sets $\1\cong\prod_{i\in\varnothing}X_i$, to justify the statement ``the empty product is $\1$.''
\qedhere
\end{enumerate}

\begin{solution}
We are given a set $I$ and a dependent set $(X_i)_{i \in I}$ for which $X_i \coloneqq \1$ for every $i \in I$.
\begin{enumerate}
    \item \label{sol.on_sums_prods_sets.sum}
    To show that $I \iso \sum_{i \in I} \1$, we note that $x \in \1 = \{1\}$ if and only if $x = 1$, so $\sum_{i \in I} \1 = \{(i, 1) \mid i \in I\}$.
    Then function $I \to \sum_{i \in I} \1$ that sends each $i \in I$ to $(i, 1)$ is clearly an isomorphism.
    
    \item \label{sol.on_sums_prods_sets.prod}
    To show that $\1 \iso \prod_{i \in I} \1$, it suffices to demonstrate that there is a unique dependent function $f \colon (i \in I) \to \1$.
    As $\1 = \{1\}$, such a function $f$ must always send $i \in I$ to $1$.
    This uniquely characterizes $f$, so there is only one such dependent function.
    
\end{enumerate}
Now $I \coloneqq \varnothing$ and $X \colon \varnothing \to \smset$ is the unique empty collection of sets.
\begin{enumerate}[resume]
    \item \label{sol.on_sums_prods_sets.vac} Yes: since $I$ is empty, there are no $i \in I$.
    So it is true that $X_i = 1$ holds whenever $i \in I$ holds, because $i \in I$ never holds. 
    We say that this sort of statement is ``vacuously true.''
    
    \item As $I = \varnothing = \0$, we have $\0 = I \iso \sum_{i \in I} \1 = \sum_{i \in \varnothing} X_i$, where the middle isomorphism follows from \cref{sol.on_sums_prods_sets.sum} and the last equation follows from \cref{sol.on_sums_prods_sets.vac}.
    
    \item As $I = \varnothing = \0$, we have $\1 \iso \prod_{i \in I} \1 = \prod_{i \in \varnothing} X_i$, where the isomorphism on the left follows from \cref{sol.on_sums_prods_sets.prod} and the equation on the right follows from \cref{sol.on_sums_prods_sets.vac}.
\end{enumerate}
\end{solution}
\end{exercise}

We'll assume you're already familiar with the following fact, which justifies why we call the constructions in \cref{def.dep_sums_prods_sets} sums and products.

\begin{proposition}
Let $I$ be a set and $X\colon I\to\smset$ be an $I$-indexed collection of sets. Then the sum $\sum_{i\in I}X_i$ is the categorical coproduct of these sets in $\smset$ (i.e.\ the colimit of the functor $X\colon I\to\smset$), and the product $\prod_{i\in I}X_i$ is the categorical product of these sets in $\smset$ (i.e.\ the limit of the functor $X\colon I\to\smset$).
\end{proposition}

\begin{exercise}\label{exc.dependent_product_as_sections}
  Let $X \colon I \to \smset$ be a set depending on an $i \in I$. There is a
  projection function
  $\pi_1 \colon \sum_{i \in I} X_i \to I$
  defined by $\pi_1(i, x) = i$.
  \begin{enumerate}
    \item What is the signature of the second projection $\pi_2(i, x) = x$?
    (Hint: it's a dependent function.)
    \item A \emph{section} of a function $r \colon A \to B$ is a function $s \colon B \to A$ such that $s \then r = \id_B$.
    Show that the dependent product is isomorphic to the set of sections of $\pi_1$:
    \[\prod_{i \in I} X_i \cong \left\{s \colon I \to \sum_{i \in I} X_i \,\middle|\, s \then \pi_1 = \id_I\right\}.\]
    \qedhere
  \end{enumerate}
\begin{solution}
We have a dependent set $X \colon I \to \smset$ and a projection function $\pi_1 \colon \sum_{i \in I} X_i \to I$ defined by $\pi_1(i, x) = i$.
\begin{enumerate}
    \item The second projection $\pi_2(i, x) = x$ sends each pair $p = (i, x) \in \sum_{i \in I} X_i$ to $x$, an element of $X_i$.
    Note that we can write $i$ in terms of $p$ as $\pi_1(p)$.
    This allows us to write the signature of $\pi_2$ as $\pi_2 \colon (p \in \sum_{i \in I} X_i) \to X_{\pi_1(p)}$.
    
    \item Let $S := \{s \colon I \to \sum_{i \in I} X_i \mid s \then \pi_1 = \id_I\}$ be the set of sections of $\pi_1$. To show that $\prod_{i \in I} X_i \cong S$, we will exhibit maps in either direction and show that they are mutually inverse.
    For each $f \colon (i \in I) \to X_i$ in $\prod_{i \in I} X_i$, we have that $f(i) \in X_i$ for all $i \in I$, so we can define a function $s_f \colon I \to \sum_{i \in I} X_i$ that sends each $i \in I$ to $(i, f(i))$.
    Then $\pi_1(s_f(i)) = \pi_1(i, f(i)) = i$, so $s_f$ is indeed a section of $\pi_1$.
    Hence $f \mapsto s_f$ is a map $\prod_{i \in I} X_i \to S$.
    
    In the other direction, for each section $s \colon I \to \sum_{i \in I} X_i$ we have $\pi_1(s(i)) = i$ for all $i \in I$, so we can write $s(i)$ as an ordered pair $(i, \pi_2(s(i)))$ with $\pi_2(s(i)) \in X_i$.
    It follows that we can define a function $f_s \colon (i \in I) \to X_i$ that sends each $i \in I$ to  $\pi_2(s(i))$.
    Then $s \mapsto f_s$ is a map $S \to \prod_{i \in I} X_i$.
    By construction $s_{f_s}(i) = (i, f_s(i)) = (\pi_1(s(i)), \pi_2(s(i))) = s(i)$ and $f_{s_f}(i) = \pi_2(s_f(i)) = \pi_2(i, f(i)) = f(i)$, so these maps are mutually inverse.
\end{enumerate}
\end{solution}
\end{exercise}

A helpful way to think about sum or product sets is by considering what choices must be made to specify an element of such a set.
In the following examples, say that we have a dependent set $X \colon I \to \smset$.

Below, we give the instructions for choosing an element of $\sum_{i \in I} X_i$.

\begin{quote}
To choose an element of $\sum_{i \in I} X_i$: 
\begin{enumerate}
    \item choose an element $i \in I$;
    \item choose an element of $X_i$.
\end{enumerate}
\end{quote}

Then the projection $\pi_1$ from \cref{exc.dependent_product_as_sections} sends each element of $\sum_{i \in I} X_i$ to the element of $i \in I$ chosen in step 1, while the projection $\pi_2$ sends each element of $\sum_{i \in I} X_i$ to the element of $X_i$ chosen in step 2.

Next, we give the instructions for choosing an element of $\prod_{i \in I} X_i$.

\begin{quote}
To choose an element of $\prod_{i \in I} X_i$: 
\begin{enumerate}
    \item for each element $i \in I$:
    \begin{enumerate}[label*=\arabic*.]
        \item choose an element of $X_i$.
    \end{enumerate}
\end{enumerate}
\end{quote}

Armed with these interpretations, we can tackle more complicated expressions, including those with nested $\sum$'s and $\prod$'s, such as
\begin{equation}\label{eqn.misc98339}
A \coloneqq \sum_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k).
\end{equation}
The instructions for choosing an element of $A$ form a nested list, as follows.

\begin{quote}
To choose an element of $A$:
\begin{enumerate}
    \item choose an element $i \in I$;
    \item for each element $j \in J(i)$:
    \begin{enumerate}[label*=\arabic*.]
        \item choose an element $k \in K(i,j)$;
        \item choose an element of $X(i,j,k)$.
    \end{enumerate}
\end{enumerate}
\end{quote}

Note that the choice of $k\in K(i,j)$ can depend on $i$ and $j$; it must be able to, because different values of $i$ and $j$ may lead to different sets $K(i,j)$.

By describing $A$ like this, it is clear that each $a \in A$ can be projected to an element $\pi_1(a) \in I$ from step 1 and a dependent function $\pi_2(a)$ from step 2.
This dependent function in turn sends each $j \in J(i)$ to a pair that can be projected to an element $\pi_1(\pi_2(a)(j)) \in K(i, j)$ from step 2.1 and an element $\pi_2(\pi_2(a)(j)) \in X(i,j,k)$ from step 2.2.

\begin{example}%[Notation for $\sum\prod$ stuff]
\label{ex.notation_sum_prod}
% Here we give notation for the elements of a set involving $\sum$'s and $\prod$'s such as that in \eqref{eqn.misc98339}.

Let $I=\{1,2\}$; let $J(1)=\{j\}$ and $J(2)\coloneqq\{j,j'\}$; let $K(1,j)\coloneqq\{k_1,k_2\}$, $K(2,j)\coloneqq\{k_1\}$, and $K(2,j')\coloneqq\{k'\}$; and let $X(i,j,k)=\{x,y\}$ for all $i,j,k$. Now the formula 
\[\sum_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k)\]
from \eqref{eqn.misc98339} has been given meaning as an actual set. Here is a list of all eight of its elements:
\[
\left\{
\begin{gathered}
	\big(1, j\mapsto(k_1,x)\big),\qquad
	\big(1, j\mapsto(k_1,y)\big),\qquad
	\big(1, j\mapsto(k_2,x)\big),\qquad
	\big(1, j\mapsto(k_2,y)\big),\\
	\big(2, j\mapsto(k_1,x), j'\mapsto(k',x)\big),\qquad
	\big(2, j\mapsto(k_1,x), j'\mapsto(k',y)\big),\\
	\big(2, j\mapsto(k_1,y), j'\mapsto(k',x)\big),\qquad
	\big(2, j\mapsto(k_1,y), j'\mapsto(k',y)\big)
\end{gathered}
\right\}
\]
In each case, we first chose an element $i\in I$, either 1 or 2. Then for each $j\in J(i)$ we chose an element $k\in K(i,j)$; then we concluded by choosing an element of $X(i,j,k)$.
\end{example}

\begin{exercise}
Consider the set
\begin{equation}\label{eqn.prod_sum_prod}B \coloneqq \prod_{i\in I}\sum_{j\in J(i)}\prod_{k\in K(i,j)}X(i,j,k).\end{equation}
\begin{enumerate}
	\item Give the instructions for choosing an element of $B$ as a nested list, like we did for $A$ just below \eqref{eqn.misc98339}.
	\item With $I$, $J$, $K$, and $X$ as in \cref{ex.notation_sum_prod}, how many elements are in $B$?
	\item Write out three of these elements in the style of \cref{ex.notation_sum_prod}.
\qedhere
\end{enumerate}
\begin{solution}
We are given the set
\[
    B \coloneqq \prod_{i\in I}\sum_{j\in J(i)}\prod_{k\in K(i,j)}X(i,j,k).
\]
\begin{enumerate}
	\item Here are the instructions for choosing an element of $B$ as a nested list.
	\begin{quote}
	    To choose an element of $B$:
	    \begin{enumerate}[label=\arabic*.]
	        \item for each element $i \in I$:
	        \begin{enumerate}[label*=\arabic*.]
	            \item choose an element $j \in J(i)$;
	            \item for each element $k \in K(i, j)$:
	            \begin{enumerate}[label*=\arabic*.]
	                \item choose an element of $X(i,j,k)$.
	            \end{enumerate}
	        \end{enumerate}
	    \end{enumerate}
	\end{quote}
	\item Given $I\coloneqq\{1,2\}$, $J(1)\coloneqq\{j\}$, $J(2)\coloneqq\{j,j'\}$, $K(1,j)\coloneqq\{k_1,k_2\}$, $K(2,j)\coloneqq\{k_1\}$, $K(2,j')\coloneqq\{k'\}$, and $X(i,j,k)=\{x,y\}$ for all $i,j,k$, our goal is to count the number of elements in $B$.
	To compute the cardinality of $B$, we can use the fact that the cardinality of a sum (resp.\ product) is the sum (resp.\ product) of the cardinalities.
	So
	\begin{align*}
	    |B| &= \prod_{i\in I}\sum_{j\in J(i)}\prod_{k\in K(i,j)}|X(i,j,k)| \\
	    &= \prod_{i\in \{1,2\}}\sum_{j\in J(i)}\prod_{k\in K(i,j)}2 \\
	    &= \left(\sum_{j\in J(1)} 2^{|K(1,j)|}\right)\left(\sum_{j\in J(2)} 2^{|K(2,j)|}\right) \\
	    &= \left(2^2\right)\left(2^1 + 2^1\right) = 16.
	\end{align*}
	\item Here are three of the elements of $B$ (you may have written down others):
	\begin{itemize}
	    \item $(1 \mapsto (j, k_1 \mapsto x, k_2 \mapsto y), 2 \mapsto (j', k' \mapsto x))$
	    \item $(1 \mapsto (j, k_1 \mapsto y, k_2 \mapsto y), 2 \mapsto (j, k_1 \mapsto y))$
	    \item $(1 \mapsto (j, k_1 \mapsto y, k_2 \mapsto x), 2 \mapsto (j', k' \mapsto y))$
	\end{itemize}
\qedhere
\end{enumerate}
\end{solution}
\end{exercise}

From here on out, we won't write out the full sequence of nested instructions corresponding to every dependent sum or product; we'll assume you're able to read them for yourself.

%---- Subsection ----%
\subsection{Expanding products of sums} \label{subsec.poly.func_nat.repr_sum.expand}

We will often encounter sums of dependent sets nested within products, as in \eqref{eqn.prod_sum_prod}.
The following proposition helps us work with these; it is sometimes called the \emph{type-theoretic axiom of choice}. Though we will take the time to walk you through its proof, it is almost trivial, once you understand what it's saying. For one thing, it is exactly how you would expect the distributive property of multiplication over addition to work, following the same sort of process that you would use to multiply multi-digit numbers from grade school or polynomials from high school algebra; for another, once the statement is written in Agda, its proof is one short line of Agda code. 

\begin{proposition}[Pushing $\prod$ past $\sum$]\label{prop.push_prod_sum_set}
For any set $I$, sets $(J(i))_{i\in I}$, and sets $(X(i,j))_{i\in I, j\in J(i)}$, we have a natural isomorphism
\begin{equation}\label{eqn.set_completely_distributive}
\prod_{i\in I}\sum_{j\in J(i)}X(i,j)
\iso
\sum_{\bar{j}\in \prod_{i\in I}J(i)}\;\prod_{i\in I}X(i,\bar{j}(i)).\footnote{We draw a bar over $j$ in $\bar{j}$ to remind ourselves that $\bar{j}$ is no longer just an index but a (dependent) function.}
\end{equation}
\end{proposition}
\begin{proof}
We'll do this the old-fashioned way: by giving a map from left to right, a
map from right to left, and a proof that the two maps are mutually inverse. 

First, let's go from left to right. An element of the set on the left is a dependent function $f \colon (i \in I) \to \sum_{j \in J(i)} X(i, j)$, which we can compose with projections from its codomain to yield $\pi_1(f(i)) \in J(i)$ and $\pi_2(f(i)) \in X(i, \pi_1(f(i)))$ for every $i \in I$.
We can then form the following pair in the right hand set:
\[
    (i \mapsto \pi_1(f(i)), i \mapsto \pi_2(f(i))).
\]

Now let's go from right to left. An element of the set on the right is a pair of dependent functions, $\bar{j} \colon (i \in I) \to J(i)$ and $g \colon
(i \in I) \to X(i, \bar{j}(i))$. We then get an element of the set on the left as follows:
\[i \mapsto (\bar{j}(i), g(i)).\]

Now, we just need to check that a round trip takes us back where we were. If we
start on the right from $(\bar{j}, g)$, our round trip gives us the pair
\[(i \mapsto \pi_1(\bar{j}(i), g(i)), i \mapsto \pi_2(\bar{j}(i), g(i))).\]
But $\pi_1(\bar{j}(i), g(i)) = \bar{j}(i)$ and $\pi_2(\bar{j}(i), g(i)) = g(i)$ by definition, so
we're back where we started. On the other hand, starting on the left from $f$ gives us the function
\[i \mapsto (\pi_1(f(i)), \pi_2(f(i))).\]
But again, since $f(i)$ is a pair whose components are $\pi_1(f(i))$ and $\pi_2(f(i))$, we're back where we started.
\end{proof}

When $J(i)=J$ does not depend on $i\in I$, the formula in \eqref{eqn.set_completely_distributive} becomes much easier.

\begin{corollary} \label{cor.push_prod_sum_set_indep}
For any set $I$, set $J$, and sets $(X(i, j))_{i \in I, j \in J}$, we have a natural isomorphism
\begin{equation} \label{eqn.push_prod_sum_set_indep}
    \prod_{i\in I}\sum_{j\in J}X(i,j)\cong\sum_{\bar{j}\colon I\to J}\prod_{i\in I}X(i,\bar{j}(i)).
\end{equation}
\end{corollary}
\begin{proof}
Just take $J(i) = J$ for all $i \in I$ in \eqref{eqn.set_completely_distributive}.
Note that dependent functions $\bar{j}$ in $\prod_{i \in I} J(i)$ then become standard functions $\bar{j} \colon I \to J$.
\end{proof}

It turns out that being able to push $\prod$ past $\sum$ as in \eqref{eqn.set_completely_distributive} is not a property that is unique to sets.
In general, we refer to a category having this property as follows.

\begin{definition}[Completely distributive category]
A category $\cat{C}$ with all small products and coproducts is \emph{completely distributive} if products distribute over coproducts as in \eqref{eqn.set_completely_distributive}; that is, for any set $I$, sets $(J(i))_{i\in I}$, and objects $(X(i,j))_{i\in I,j\in J(i)}$ from $\cat{C}$, we have a natural isomorphism
\begin{equation}\label{eqn.cat_completely_distributive}
\prod_{i\in I}\sum_{j\in J(i)}X(i,j)
\iso
\sum_{\bar{j}\in \prod_{i\in I}J(i)}\;\prod_{i\in I}X(i,\bar{j}(i)).
\end{equation}
\end{definition}

So \cref{prop.push_prod_sum_set} states that $\smset$ is completely distributive.
We'll see shortly that $\poly$ is, too.
Of course, \cref{cor.push_prod_sum_set_indep} generalizes to all completely distributive categories as well.

\begin{corollary} \label{cor.push_prod_sum_obj_indep}
Let $\cat{C}$ be a completely distributive category.
For any set $I$, set $J$, and objects $(X(i, j))_{i \in I, j \in J}$ from $\cat{C}$, we have a natural isomorphism
\begin{equation} \label{eqn.push_prod_sum_obj_indep}
    \prod_{i\in I}\sum_{j\in J}X(i,j)\iso\sum_{\bar{j}\colon I\to J}\prod_{i\in I}X(i,\bar{j}(i)).
\end{equation}
\end{corollary}
\begin{proof}
Again, just take $J(i) = J$ for all $i \in I$ in \eqref{eqn.cat_completely_distributive}.
\end{proof}

\begin{exercise}
Let $\cat{C}$ be a completely distributive category.
How is the usual distributive law,
\[
    X(Y+Z)\iso XY+XZ
\]
for $X,Y,Z\in\cat{C}$, a special case of \eqref{eqn.cat_completely_distributive}?
\begin{solution}
Given objects $X,Y,Z$ in a completely distributive category $\cat{C}$, we wish to show that $X(Y+Z)\iso XY+XZ$ using \eqref{eqn.cat_completely_distributive}.
On the left hand side, we are taking a $2$-fold product: a single object times a $2$-fold sum.
So we should let $I\coloneqq\2$ and let $J(1)\coloneqq\1$, with $X(1,1)\coloneqq X$, and $J(2)\coloneqq\2$, with $X(2,1)\coloneqq Y$ and $X(2,2)\coloneqq Z$.
Then \eqref{eqn.cat_completely_distributive} says that
\[
    X(Y+Z) \iso \sum_{\bar{j}\in\prod_{i\in\2}J(i)}\;\prod_{i\in \2}X(i,\bar{j}(i)) \iso \sum_{\bar{j}\in\prod_{i\in\2}J(i)}X(1,\bar{j}(1))\times X(2,\bar{j}(2)).
\]
The set $\prod_{i\in\2}J(i)$ contains two functions: $(1\mapsto1,2\mapsto1)$ and $(1\mapsto1,2\mapsto2)$.
So we can rewrite the right hand side as
\[
    X(1,1)\times X(2,1)+X(1,1)\times X(2,2)\iso XY+XZ,
\]
as desired.
\end{solution}
\end{exercise}

Throughout this book, e.g.\ in \cref{exc.practice_sum_prod}, you'll often see an expression consisting of alternating products and sums.
Using \eqref{eqn.cat_completely_distributive}, you can always write such an expression as a sum of products, in which every $\sum$ appears before every $\prod$ (i.e.\ in ``disjunctive normal form'').
This is analogous to how products of sums in high school algebra can always be expanded into sums of products via the distributive property.

\begin{exercise} \label{exc.push_prod_sum_set}
Let $I, (J(i))_{i\in I}, (K(i,j))_{(i,j)\in IJ}$ be sets, and for each $(i,j,k)\in IJK$, let $X(i,j,k)$ be an object in a completely distributive category.
\begin{enumerate}
    \item Rewrite
    \[
        \sum_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k)
    \]
    so that every $\sum$ appears before every $\prod$.
    \item Rewrite
    \[
        \prod_{i\in I}\sum_{j\in J(i)}\prod_{k\in K(i,j)}X(i,j,k)
    \]
    so that every $\sum$ appears before every $\prod$.
    \item Rewrite
    \[
        \prod_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k)
    \]
    so that every $\sum$ appears before every $\prod$.\qedhere
\end{enumerate}
\begin{solution}
Our goal is to rewrite each expression so that every $\sum$ appears before every $\prod$.
\begin{enumerate}
    \item By applying \eqref{eqn.cat_completely_distributive}, we can rewrite
    \[
        \sum_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k)
    \]
    as
    \[
        \sum_{i\in I}\sum_{\bar{k}\in \prod_{j\in J}K(i,j)}\prod_{j\in J(i)}X(i,j,\bar{k}(j)).
    \]
    \item By applying \eqref{eqn.cat_completely_distributive}, we can rewrite
    \[
        \prod_{i\in I}\sum_{j\in J(i)}\prod_{k\in K(i,j)}X(i,j,k)
    \]
    as
    \[
        \sum_{\bar{j}\in \prod_{i\in I}J(i)}\;\prod_{i\in I}X(i,\bar{j}(i))\prod_{k\in K(i,\bar{j}(i))}X(i,\bar{j}(i),k).
    \]
    \item By applying \eqref{eqn.cat_completely_distributive}, we can rewrite
    \[
        \prod_{i\in I}\prod_{j\in J(i)}\sum_{k\in K(i,j)}X(i,j,k)
    \]
    once as
    \[
        \prod_{i\in I}\sum_{\bar{k}\in\prod_{j\in J}K(i,j)}\prod_{j\in J(i)}X(i,j,\bar{k}(j))
    \]
    and then again as
    \[
        \sum_{\bar{\bar{k}}\in\prod_{i\in I}\prod_{j\in J}K(i,j)}\prod_{i\in I}\prod_{j\in J(i)}X(i,j,\bar{\bar{k}}(i,j)).
    \]
\end{enumerate}
\end{solution}
\end{exercise}

Now that we have defined dependent sums and products of sets, we are ready to tackle dependent sums and products of set-valued functors.

%---- Subsection ----%
\subsection{Dependent sums and products of functors $\smset\to\smset$} \label{subsec.poly.func_nat.repr_sum.dep_sum_prod_func}

Remember: our goal is to define polynomial functors, e.g.\ $\yon^\2+\2\yon+\1$, and the maps between them. Since $\yon^\2$, $\yon$, and $\1$ are functors, we just need to interpret sums of functors $\smset\to\smset$.
But we might as well examine products of functors at the same time, because they'll very much come in handy.
And both these concepts generalize to limits and colimits in $[\smset,\smset]$.

\begin{proposition} \label{prop.presheaf_lim_ptwise}
The category $[\smset,\smset]$ has all small limits and colimits, and they are computed pointwise.
That is, given a small category $\cat{J}$ and a functor $F\colon \cat{J}\to[\smset,\smset]$, for all $X\in\smset$, the limit and colimit of $F$ satisfy isomorphisms
\[
    \left(\lim_{j\in\cat{J}} F(j)\right)(X) \iso \lim_{j\in\cat{J}} \left(F(j)(X)\right) \qqand \left(\colim_{j\in\cat{J}} F(j)\right)(X) \iso \colim_{j\in\cat{J}} \left(F(j)(X)\right)
\]
natural in $X$.
\end{proposition}
\begin{proof}
This is a special case of a more general fact, where $[\smset,\smset]$ is replaced by an arbitrary functor category $[\cat{D},\cat{C}]$, where $\cat{C}$ is a category that (like $\smset$) has limits and colimits; see \cite[pages 22 -- 23, displays (24) and (25)]{macLane1992sheaves}.
\end{proof}

\begin{example}[Dependent sums and products of functors $\smset\to\smset$]\label{ex.sum_prod_set_endofuncs}
For any two functors $F,G\colon\smset\to\smset$, let
\[
  (F+G)\colon\smset\to\smset
  \qqand
  (F\times G)\colon\smset\to\smset
\]
respectively denote the categorical \emph{sum} (i.e.\ \emph{coproduct}) and \emph{product} of $F$ and $G$ in $[\smset,\smset]$.\footnote{We may also denote the product functor $F \times G$ by $FG$.}
Then by \cref{prop.presheaf_lim_ptwise}, for each $X\in\smset$,
\[
  (F+G)(X)\iso F(X)+G(X)
  \qqand
	(F\times G)(X)\iso F(X)\times G(X).
\]

More generally, for any set $I$ and functors $(F_i)_{i\in I}$, let
\[
\sum_{i\in I}F_i\colon\smset\to\smset
\qqand
\prod_{i\in I}F_i\colon\smset\to\smset
\]
respectively denote the categorical \emph{sum} (i.e.\ \emph{coproduct}) and \emph{product} of the functors $(F_i)_{i\in I}$.
Then by \cref{prop.presheaf_lim_ptwise}, for each $X\in\smset$,
\[
	\left(\sum_{i\in I}F_i\right)(X)\iso\sum_{i\in I} F_i(X)
	\qqand
	\left(\prod_{i\in I}F_i\right)(X)\iso\prod_{i\in I} F_i(X).
\]
Given a set $I\in\smset$, we will also use $I$ to denote the constant functor that assigns $I$ to each $X\in\smset$.
In particular, we denote by $\0,\1\colon\smset\to\smset$ the constant functors that respectively assign $\0$ and $\1$ to each $X\in\smset$.
As the set $\0$ (resp.\ $\1$) is the initial (resp.\ terminal) object in $\smset$, \cref{prop.presheaf_lim_ptwise} tells us that the constant functor $\0$ (resp.\ $\1$) is the is the initial (resp.\ terminal) object in $[\smset,\smset]$.
\end{example}

\begin{proposition}\label{prop.set_endofunc_distrib}
The category $[\smset,\smset]$ is completely distributive.
\end{proposition}
\begin{proof}
This follows directly from the fact that $\smset$ itself is completely distributive (\cref{prop.push_prod_sum_set}) and the fact that sums and products in $[\smset,\smset]$ are computed pointwise (\cref{prop.presheaf_lim_ptwise} and, in particular, \cref{ex.sum_prod_set_endofuncs}).
\end{proof}

\begin{exercise} \label{exc.repeated_sum_is_product}
\begin{enumerate}
	\item Show that for a set $I \in \smset$ and a functor $F \colon \smset \to \smset$, the sum of $I$ copies of $F$ is isomorphic to the product of the constant functor $I$ and $F$:
    \[
        \sum_{i \in I}F\iso IF.
    \]
    (This is analogous to the fact from basic arithmetic that adding up $n \in \nn$ copies of number is equal to multiplying that same number by $n$.)
	\item So does $\2\yon$ denote $\2\times\yon$ or $\yon+\yon$, or does it not matter for some reason?\qedhere
	\end{enumerate}
\begin{solution}
\begin{enumerate}
	\item It suffices to show that for all $X\in\smset$, there is an isomorphism
    \[
      \sum_{i\in I} F(X) \iso (IF)(X)
    \]
    natural in $X$.
    The left hand side is naturally isomorphic to the set $\{(i,s)\mid i\in I\text{ and }s\in F(X)\} \iso I \times F(X)$, while the right hand side is also naturally isomorphic to the set $I(X) \times F(X) \iso I \times F(X)$. (Alternatively, since $[\smset,\smset]$ is completely distributive by \cref{prop.set_endofunc_distrib}, the result also follows from \eqref{eqn.cat_completely_distributive}.)
    \item It doesn't matter: $\2\yon$ can denote either one. Indeed, taking $I\coloneqq\2$, the above says that there is an isomorphism $\2\times\yon\cong\yon+\yon$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
\begin{enumerate}
    \item Show that for a set $I\in\smset$, the product of $I$ copies of the identity functor $\yon\colon\smset\to\smset$ is isomorphic to the functor $\yon^I\colon\smset\to\smset$ represented by $I$:
	\[
	    \prod_{i\in I}\yon\iso\yon^I.
	\]
	(This is analogous to the fact from basic arithmetic that multiplying $n$ copies of a number together is equal to raising that same number to the power of $n$.)
	\item Show that the product of $I$ copies of a representable functor $\yon^A\colon\smset\to\smset$ for some $A\in\smset$ is isomorphic to the functor $\yon^{IA}\colon\smset\to\smset$ represented by the product set $IA$:
	\[
	    \prod_{i\in I}\yon^A\iso\yon^{IA}.
	\]
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item It suffices to show that for all $X\in\smset$, there is an isomorphism
    \[
        \prod_{i\in I} \yon(X) \iso \yon^I(X).
    \]
    natural in $X$.
    We have that $\yon(X)\iso X$ and that $\yon^I(X)\iso X^I$.
    So both sides are naturally isomorphic to the set of functions $I\to X$.
    \item It suffices to show that for all $X\in\smset$, there is an isomorphism
    \[
        \prod_{i\in I} \yon^A(X) \iso \yon^{IA}(X).
    \]
    natural in $X$.
    We have that $\yon^A(X)\iso X^A$ and that $\yon^{IA}(X)\iso X^{IA}$.
    By currying, both sides are naturally isomorphic to the set of functions $I\times A\to X$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{remark}
From here on out, given $I\in\smset$ and a functor $F\colon\smset\to\smset$, we define
\[
    F^I\coloneqq\prod_{i\in I}F.
\]
The exercise above shows that this notation does not conflict with the way we write representable functors as powers of the identity functor $\yon$.
The exercise also shows how the power of a representable functor can be simplified to a single representable functor.
\end{remark}

We've finally arrived: we can define polynomial functors!

%---- Subsection ----%
\subsection{What is a polynomial functor?} \label{subsec.poly.func_nat.repr_sum.what}

\begin{definition}[Polynomial functor]
A \emph{polynomial functor} (or simply a \emph{polynomial}) is a functor $p\colon\smset\to\smset$ such that there exists a set $I$, sets $(p[i])_{i\in I}$, and an isomorphism
\[p\cong\sum_{i\in I}\yon^{p[i]}\]
to a sum of representables.
\end{definition}

So (up to isomorphism), a polynomial functor is just a sum of representables.

\begin{remark}
Given sets $I, A \in \smset$, it follows from \cref{exc.repeated_sum_is_product} that we have an isomorphism of polynomials
\[
    \sum_{i \in I} \yon^A \iso I\yon^A.
\]
So when we write down a polynomial, we will often combine identical representable summands $\yon^A$ by writing them in the form $I\yon^A$.
In particular, the constant functor $\1$ is a representable functor ($\1 \iso \yon^\0$), so every constant functor $I$ is a polynomial functor: $I \iso \sum_{i \in I} \1$.
\end{remark}

\begin{example}\label{ex.pedantic_poly_eval}
Consider the polynomial $p \coloneqq \yon^\2+\2\yon+\1$. It denotes a functor $\smset\to\smset$; what does this functor do to the set $X\coloneqq\{a,b\}$? 
To be very precise and pedantic, let's say
\[
I\coloneqq\4\qqand
  p[1]\coloneqq\2,\quad
  p[2]\coloneqq\1,\quad
  p[3]\coloneqq\1,\quad
  p[4]\coloneqq\0\
\]
so that $p\cong\sum_{i\in I}\yon^{p[i]}$. Now we have
\[
p(X)\cong
\{(1,a,a),(1,a,b),(1,b,a),(1,b,b),(2,a),(2,b),(3,a),(3,b),(4)\}.
\]
It has $(2^2+2+2+1)$-many, i.e.\ $9$, elements. The representable summand $\yon^A$ throws in all $A$-tuples from $X$, but it's indexed by the name of the summand. In particular if $A=\0$ then it just records the empty tuple at that summand.
\end{example}

In general, a polynomial $p\coloneqq\sum_{i\in I}\yon^{p[i]}$ applied to a set $X$, expanded as
\[
    \sum_{i\in I}X^p[i],
\]
can be thought of as the set of all pairs comprised of an element of $I$ and a $p[i]$-tuple of elements of $X$.
Alternatively, we can think of each pair as an element of $I$ and a function $p[i]\to XS$.

\begin{exercise}
In the pedantic style of \cref{ex.pedantic_poly_eval}, write out all the elements of $p(X)$ for $p$ and $X$ as follows:
\begin{enumerate}
	\item $p\coloneqq\yon^\3$ and $X\coloneqq\{4,9\}.$
	\item $p\coloneqq\3\yon^\2+\1$ and $X\coloneqq\{a\}$.
	\item $p\coloneqq\0$ and $X\coloneqq\nn$.
	\item $p\coloneqq\4$ and $X\coloneqq\nn$.
	\item $p\coloneqq\yon$ and $X\coloneqq\nn$.
\qedhere
\end{enumerate}
\begin{solution}
Our goal is to write out the elements of $p(X)$ for each polynomial $p$ and set $X$ that we are given.
\begin{enumerate}
    \item If $p\coloneqq\yon^\3$ and $X\coloneqq\{4,9\}$, then let $I \coloneqq \1$ and $p[1] \coloneqq \3$ so that $p \iso \sum_{i \in I} \yon^{p[i]}$.
    So
    \[
        p(X) \iso \{(1, 4, 4, 4), (1, 4, 4, 9), (1, 4, 9, 4), (1, 4, 9, 9), (1, 9, 4, 4), (1, 9, 4, 9), (1, 9, 9, 4), (1, 9, 9, 9)\}.
    \]
    
    \item If $p\coloneqq\3\yon^\2+\1$ and $X\coloneqq\{a\}$, then let $I \coloneqq \4$, $p[1] \coloneqq p[2] \coloneqq p[3] \coloneqq \2$, and $p[4] \coloneqq \1$ so that $p \iso \sum_{i \in I} \yon^{p[i]}$.
    So $p(X) \iso \{(1, a, a), (2, a, a), (3, a, a), (4)\}$.
	
	\item If $p\coloneqq\0$ and $X\coloneqq\nn$, then let $I \coloneqq \0$ so that $p \iso \sum_{i \in I} \yon^{p[i]}$.
	So $p(X) \iso \0$.
	Alternatively, we note that $\0$ is the constant functor that assigns $\0$ to every set. 
	
	\item If $p\coloneqq\4$ and $X\coloneqq\nn$, then let $I \coloneqq \4$ and $p[i] \coloneqq \0$ for every $i \in I$.
	So $p(X) \iso \{(1), (2), (3), (4)\} \iso \4$.
	
	\item If $p\coloneqq\yon$ and $X\coloneqq\nn$, then let $I \coloneqq \1$ and $p[1] \coloneqq \1$ so that $p \iso \sum_{i \in I} \yon^{p[i]}$.
    So $p(X) \iso \{(1, n) \mid n \in \nn\}$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.apply1}
Let $p\coloneqq\sum_{i\in I}\yon^{p[i]}$ be an arbitrary polynomial functor. Then $I\cong p(\1)$, so there is an isomorphism of functors
\begin{equation}\label{eqn.sum_p1}
p\iso\sum_{i\in p(\1)}\yon^{p[i]}.
\end{equation}
\end{proposition}
\begin{proof}
We need to show that $I\iso p(\1)$; the latter claim follows directly. In \cref{exc.on_sums_prods_sets} it was shown that $I\iso\sum_{i\in I}\1$, so we just need to show that $(\yon^{p[i]})(\1)\iso\1$ for every $i \in I$. But $\1^{p[i]}\iso \1$ because there is a unique function $p[i]\to \1$ for any $p[i]$.
\end{proof}
We can draw an analogy between \cref{prop.apply1} and evaluating $p(1)$ for a polynomial $p$ from high school algebra, which yields the sum of the coefficients of $p$.
The notation in \eqref{eqn.sum_p1} will be how we denote arbitrary polynomials from now on.

\begin{exercise}\label{exc.apply0}
We saw in \cref{prop.apply1} that for any polynomial $p$, e.g.\ $p\coloneqq\yon^\3+\3\yon^\2+\4$, the set $p(\1)$ gives back the set of summands, in this case $\8$. 

What does $p(\0)$ give you?
\begin{solution}
We consider $p(\0)$ for arbitrary polynomials $p$.
A representable functor $\yon^S$ for some $S \in \smset$ sends $\0 \mapsto \0$ if $S \neq \0$ (as there are then no functions $S \to \0$), but sends $\0 \mapsto \1$ if $S = \0$ (as there is a unique function $\0 \to \0$).
So
\[
    p(\0) \iso \sum_{i \in p(\1)} (\yon^{p[i]})(\0) \iso \sum_{\substack{i \in p(\1), \\ p[i] \neq \0}} \0 + \sum_{\substack{i \in p(\1), \\ p[i] = \0}} \1 \iso \{i \in p(\1) \mid p[i] = \0\}.
\]
In other words, $p(\0)$ is the set of \emph{constant} representable summands of $p$.
For example, if $p\coloneqq\yon^\3+\3\yon^\2+\4$, then $p(\0) = \4$.
In the language of high school algebra, we might call $p(\0)$ the ``constant term'' of $p$.
\end{solution}
\end{exercise}

As a functor $\smset\to\smset$, a polynomial should be able to act not only on sets but also on functions.
Below, we explain how.

\begin{proposition} \label{prop.poly_on_functions}
Let $p$ be an arbitrary polynomial functor (our notation allows us to write it as $p\iso\sum_{i\in p(\1)} \yon^{p[i]}$\,), and let $f\colon X\to Y$ be an arbitrary function.
Then $p(f)\colon p(X)\to p(Y)$ sends each $(i, g)\in p(X)$, with $i\in p(\1)$ and $g\colon p[i]\to X$, to $(i, g\then f)$ in $p(Y)$.
\end{proposition}
\begin{proof}
For each $i\in p(\1)$, by \cref{def.representable}, the functor $\yon^{p[i]}$ sends $f$ to the function $f^{p[i]}\colon X^{p[i]}\to Y^{p[i]}$, the one that sends every $g\colon p[i]\to X$ to $g\then f\colon p[i]\to Y$.
So the sum of these functors over $i\in p(\1)$ sends each $(i, g)\in p(X)$ to $(i, f^{p[i]}(g)) = (i, g\then f)\in p(Y)$.
\end{proof}

\begin{example}
Suppose $p\coloneqq\yon^\2+\2\yon+1$. Let $X\coloneqq\{a_1,a_2,b_1\}$ and $Y\coloneqq\{a,b,c\}$, and let $f\colon X\to Y$ be the function sending $a_1,a_2\mapsto a$ and $b_1\mapsto b$. The induced function $p(f)\colon p(X)\to p(Y)$, according to \cref{prop.poly_on_functions}, is shown below:
\[
\begin{tikzcd}[row sep=2pt, column sep=3pt, shorten <=-5pt, shorten >=-5pt, dashed]
\LMO{(1,a_1,a_1)}\ar[rrrr, blue, bend left=25pt]&\LMO{(1,a_1,a_2)}\ar[rrr, blue, bend left=25pt]&\LMO{(1,a_1,b_1)}\ar[rrr, blue, bend left=25pt]&[30pt]&
\LMO{(1,a,a)}&\LMO{(1,a,b)}&\LMO{(1,a,c)}&&
\\
\LMO{(1,a_2,a_1)}\ar[rrrru, blue, bend left=10pt]&\LMO{(1,a_2,a_2)}\ar[rrru, blue, bend left=10pt]&\LMO{(1,a_2,b_1)}\ar[rrru, blue, bend left=10pt]&&
\LMO{(1,b,a)}&\LMO{(1,b,b)}&\LMO{(1,b,c)}&&
\\
\LMO{(1,b_1,a_1)}\ar[rrrru, blue, bend left=10pt]&\LMO{(1,b_1,a_2)}\ar[rrru, blue, bend left=10pt]&\LMO{(1,b_1,b_1)}\ar[rrru, blue, bend left=10pt]&&
\LMO{(1,c,a)}&\LMO{(1,c,b)}&\LMO{(1,c,c)}&&
\\
\LMO{(2,a_1)}\ar[rrrr, blue, bend left=25pt]&\LMO{(2,a_2)}\ar[rrr, blue, bend left=25pt]&\LMO{(2,b_1)}\ar[rrr, blue, bend left=25pt]&&
\LMO{(2,a)}&\LMO{(2,b)}&\LMO{(2,c)}&&
\\
\LMO{(3,a_1)}\ar[rrrr, blue, bend left=25pt]&\LMO{(3,a_2)}\ar[rrr, blue, bend left=25pt]&\LMO{(3,b_1)}\ar[rrr, blue, bend left=25pt]&&
\LMO{(3,a)}&\LMO{(3,b)}&\LMO{(3,c)}&&
\\&
\LMO{(4)}\ar[rrrr, blue]&&&&\LMO{(4)}
\end{tikzcd}
\]
\end{example}

\begin{exercise}
Let $p\coloneqq\yon^\2+\yon$. Choose a function $f\colon\1\to\2$ and write out the induced function $p(f)\colon p(\1)\to p(\2)$.
\begin{solution}
Given $p\coloneqq\yon^\2+\yon$, we seek the induced function $p(f)\colon p(\1)\to p(\2)$ for a function $f\colon\1\to\2$ of our choice.
We will choose $1 \mapsto 2$.
We can evaluate
\[
    p(\1) \iso \{(1, 1, 1), (2, 1)\} \text{ and } p(\2) \iso \{(1, 1, 1), (1, 1, 2), (1, 2, 1), (1, 2, 2), (2, 1), (2, 2)\}.
\]
So $p(f)$ sends $(1, 1, 1) \mapsto (1, 2, 2)$ and $(2, 1) \mapsto (2, 2)$.
(If we had instead picked $1 \mapsto 1$ as our function $f$, then $p(f)$ would send $(1, 1, 1) \mapsto (1, 1, 1)$ and $(2, 1) \mapsto (2, 1)$.)
\end{solution}
\end{exercise}


%-------- Section --------%
\section{Morphisms between polynomial functors}
\label{sec.poly.func_nat.morph}

Before we define the category $\poly$ of polynomial functors, we notice that polynomial functors already live inside a category, namely the category $[\smset,\smset]$ of functors $\smset \to \smset$, whose morphisms are natural transformations.
This leads to a very natural (if you will) definition of morphisms between polynomial functors, from which we can derive a category of polynomial functors for free.

\begin{definition}[Polynomial morphism, $\poly$] \label{def.poly_cat}
Given polynomial functors $p$ and $q$, a \emph{morphism of polynomial functors} (or a \emph{polynomial morphism}) is a natural transformation $p\to q$.
Then $\poly$ is the category whose objects are polynomial functors and whose morphisms are polynomial morphisms.
\end{definition}

In other words, $\poly$ is the full subcategory of $[\smset,\smset]$ spanned by the polynomial functors: we take the category $[\smset,\smset]$, throw out all the objects that are not polynomials, but keep all the same morphisms between any two polynomial functors.

%---- Subsection ----%
\subsection{Coproducts of polynomials}
\label{subsec.poly.func_nat.morph.coprod}

Since polynomial functors are defined as arbitrary sums of representables, coproducts in $\poly$ are quite easy to understand.

\begin{proposition} \label{prop.poly_coprods}
The category $\poly$ has arbitrary coproducts, coinciding with coproducts in $[\smset,\smset]$ given by the operation $\sum_{i \in I}$.
\end{proposition}
\begin{proof}
By \cref{ex.sum_prod_set_endofuncs}, the category $[\smset,\smset]$ has arbitrary coproducts given by $\sum_{i \in I}$.
The full subcategory inclusion $\poly \to [\smset,\smset]$ reflects these coproducts, and by definition $\poly$ is closed under the operation $\sum_{i \in I}$.
\end{proof}

Explicitly, given polynomials $(p_i)_{i \in I}$, their coproduct is
\begin{equation} \label{eqn.poly_coprod}
    \sum_{i \in I} p_i \iso \sum_{i \in I} \sum_{j \in p_i(\1)} \yon^{p_i[j]} \iso \sum_{(i,j) \in \sum_{i \in I} p_i(\1)} \yon^{p_i[j]},
\end{equation}
which coincides with our notion of sums of functors $\smset \to \smset$ from \cref{ex.sum_prod_set_endofuncs}.
Of course, it also coincides with our notion of polynomial addition from high school algebra---just add all the terms together!
Binary coproducts are given by binary sums of functors, appropriately denoted by $+$, while the initial object of $\poly$ is the constant polynomial $\0$.

In particular, \eqref{eqn.poly_coprod} implies that for any polynomials $p$ and $q$, their coproduct $p+q$ is given as follows.
The position-set of $p+q$ is the coproduct of sets $p(\1) + q(\1)$.
At position $(1,i) \in p(\1) + q(\1)$ with $i \in p(\1)$, the directions of $p+q$ are just the $p[i]$-directions; at position $(2,j) \in p(\1) + q(\1)$ with $j \in q(\1)$, the directions of $p+q$ are just the $q[j]$-directions.

%---- Subsection ----%
\subsection{Polynomial morphisms, concretely} \label{subsec.poly.func_nat.morph.concrete}

A natural transformation between polynomials $p \to q$ consists of a function $p(X) \to q(X)$ for every set $X \in \smset$ such that the naturality squares commute.
That's a lot of data to keep track of!
Fortunately, there is a much simpler way to think about these polynomial morphisms, which we will discover with some help from our old friend, the Yoneda lemma.

\begin{exercise} \label{exc.poly_morph_yoneda}
Given a set $S$ and a polynomial $q$, show that a polynomial morphism $\yon^S \to q$ can be identified with an element of the set $q(S)$.
That is, there is an isomorphism
\[
    \poly(\yon^S, q) \iso q(S).
\]
Moreover, show that this isomorphism is natural in both $S$ and $q$.
Hint: Use the Yoneda lemma (\cref{lemma.yoneda}).
\begin{solution}
As $\poly(\yon^S, q)$ is defined to be $\nat(\yon^S, q)$\, the natural isomorphism $\poly(\yon^S, q) \iso q(S)$ follows directly from the Yoneda lemma (\cref{lemma.yoneda}) with $F = q$.
\end{solution}
\end{exercise}

Before we present our alternative characterization of polynomial morphisms, recall that every polynomial $p \coloneqq \sum_{i \in p(\1)} \yon^{p[i]}$ is uniquely associated to a dependent set, $(p[i])_{i \in p(\1)}$, which we call its \emph{arena}, as in \eqref{eqn.arena_example}.
Alternatively, we could write such a dependent set as a functor $p[-] \colon p(\1) \to \smset$, where we view the set $p(\1)$ as a discrete category.
Below, we make use of this functor notation to express the arenas of polynomials.

\begin{proposition}\label{prop.poly_maps_prod_sum}
Let $p\coloneqq\sum_{i\in p(\1)}\yon^{p[i]}$ and $q\coloneqq\sum_{j\in q(\1)}\yon^{q[j]}$ be polynomials.
Then we have an isomorphism
\begin{equation}\label{eqn.main_formula}
\poly(p,q)\cong\prod_{i\in p(\1)}\sum_{j\in q(\1)}{p[i]}^{q[j]}
\end{equation}
natural in $p$ and $q$.
In other words, a morphism $p\to q$ can be identified with a pair $(f_\1,f^\sharp)$
\begin{equation}\label{eqn.colax_poly_map}
\begin{tikzcd}[column sep=small]
	p(\1)\ar[dr, "p{[-]}"']\ar[rr, "f_\1"]&~&
	q(\1)\ar[dl, "q{[-]}"]\\&
	\smset\ar[u, phantom, near end, "\overset{f^\sharp}{\Leftarrow}"]
\end{tikzcd}
\end{equation}
where $f_\1 \colon p(\1) \to q(\1)$ is a function (or functor between discrete categories) and $f^\sharp \colon q[f_\1(-)] \to p[-]$ is a natural transformation: for each $i\in p(\1)$ with $j\coloneqq f_\1(i)$, there is a function $f^\sharp_i\colon q[j]\to p[i]$.
\end{proposition}
\begin{proof}
By the universal property of the coproduct, we have a natural isomorphism
\[
    \poly\left(\sum_{i \in p(\1)}\yon^{p[i]}, q\right) \iso \prod_{i \in p(\1)} \poly(\yon^{p[i]}, q),
\]
so applying \cref{exc.poly_morph_yoneda} (i.e.\ the Yoneda lemma) and unraveling the definitions of $p$ and $q$ yields \eqref{eqn.main_formula}.

The expression on the right hand side of \eqref{eqn.main_formula} is the set of dependent functions $f \colon (i \in p(\1)) \to \sum_{j \in q(\1)} p[i]^{q[j]}$, each of which is uniquely determined by its components: $\pi_1 \circ f$, which sends $i \in p(\1)$ to $\pi_1(f(i)) \in q(\1)$, and $\pi_2 \circ f$, which sends $i \in p(\1)$ with $j \coloneqq \pi_1(f(i))$ to an element of $p[i]^{q[j]}$, i.e.\ a function $q[j] \to p[i]$.
These can be identified respectively with a (non-dependent) function $f_\1 \coloneqq \pi_1 \circ f$ from $p(\1) \to q(\1)$ and a natural transformation $f^\sharp \colon q[f_\1(-)] \to p[-]$.
\end{proof}

We have now greatly simplified our characterization of polynomial morphisms $f \colon p \to q$: rather than infinitely many functions satisfying infinitely many naturality conditions, they can be specified simply as a function $f_\1 \colon p(\1) \to q(\1)$ and, for each $i \in p(\1)$, a function $f^\sharp_i \colon q[f_\1(i)] \to p[i]$, without any additional restrictions.

Here is where we begin to see the advantages of viewing polynomials as arenas.
As a reminder, from the arena perspective, we call the elements of $p(\1)$ the \emph{positions} of $p$, and for each position $i \in p(\1)$, we call the elements of $p[i]$ the \emph{directions} of $p$ at $i$.
We can see that our characterization of a polynomial morphism can be written entirely in the language of positions and directions: when polynomials $p$ and $q$ are viewed as arenas, a morphism $f \colon p \to q$ consists of a ``forwards'' \emph{on-positions} function $f_\1$ from the position-set of $p$ to the position-set of $q$, along with, for every position $i$ of $p$, a ``backwards'' \emph{on-directions} function $f^\sharp_i$ from the direction-set of $q$ at $f_\1(i)$ to the direction-set of $p$ at $i$.

When we wish to emphasize the arena perspective, we will call the data of $(f_\1, f^\sharp)$ a \emph{morphism of arenas}, an \emph{arena morphism}, or a \emph{lens} between $p$ and $q$ (see \cref{subsec.poly.func_nat.morph.bimorphic_lens}); but since \cref{prop.poly_maps_prod_sum} tells us that polynomial morphisms and arena morphisms carry the same data, we may use these terms interchangeably.

Here is how you would implement such a morphism in Agda:
\begin{agda}
record Lens (p : Arena) (q : Arena) : Set where
   field
     onPos : (pos p) -> (pos q)
     onDir : (i : pos p) -> dir q (onPos i) -> dir p i
\end{agda}

This forwards on-positions/backwards on-directions formulation may still seem a little complicated, so here is some decision-making intuition.
Recall from \cref{sec.poly.intro.dec} that we may view each position of an arena as a decision and the directions at that position as the options available for that decision.
The morphisms $f\colon p\to q$ are then the ways to \emph{delegate} $p$'s decisions to $q$. Every one of $p$'s decisions, say $i\in p(\1)$, is passed forward to a decision $j\in q(\1)$ for $q$ to make, and every choice $d\in q[j]$ that $q$ could make among its options is passed back as some choice $c\in p[i]$ among $p$'s options.

\begin{example}\label{ex.practice_with_poly_morphisms}
Let $p\coloneqq \yon^\3+\2\yon$ and $q\coloneqq\yon^\4+\yon^\2+\2$. Here they are, depicted as corolla forests:
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, blue!50!black, "$p$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$} 
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$} 
      child {};
    \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$}
      child {};
  \end{tikzpicture}
  };
%
	\node (p2) [draw, red!75!black, right=2 of p1, "$q$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$} 
      child {}
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$} 
      child {}
      child {};
    \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$}
    ;
    \node[right=.5 of 3,"\tiny 4" below] (4) {$\bullet$}
    ;
  \end{tikzpicture}
  };
\end{tikzpicture}
\]
To give a map of polynomials $p\to q$, one sends each position $i\in p(\1)$ of $p$ to a position $j\in q(\1)$ of $q$, then sends each direction in $q[j]$ back to one in $p[i]$.

How many ways are there to do this? Before answering this, let's just pick one.
\[
\begin{tikzpicture}
	\node (p1) {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "{\color{blue!50!black}\tiny 1}" below] (1) {$\bullet$} 
      child[blue!50!black] {coordinate (11)}
      child[blue!50!black] {coordinate (12)}
      child[blue!50!black] {coordinate (13)};
    \node[right=1.5 of 1, red!75!black, "{\color{red!75!black}\tiny 1}" below] (2) {$\bullet$} 
      child[red!75!black] {coordinate (21)}
      child[red!75!black] {coordinate (22)}
      child[red!75!black] {coordinate (23)}
      child[red!75!black] {coordinate (24)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (13);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (13);
      \draw[postaction={decorate}] (24) to (13);
    \end{scope}
  \end{tikzpicture}	
	};	
%
	\node (p2) [right=1 of p1] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "{\color{blue!50!black}\tiny 2}" below] (1) {$\bullet$} 
      child[blue!50!black] {coordinate (11)};
    \node[right=of 1, red!75!black, "{\color{red!75!black}\tiny 1}" below] (2) {$\bullet$} 
      child[red!75!black] {coordinate (21)}
      child[red!75!black] {coordinate (22)}
      child[red!75!black] {coordinate (23)}
      child[red!75!black] {coordinate (24)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (11);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (11);
      \draw[postaction={decorate}] (24) to (11);
    \end{scope}
  \end{tikzpicture}	
	};	
%
	\node (p3) [below right=-1.05cm and 1 of p2] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "{\color{blue!50!black}\tiny 3}" below] (1) {$\bullet$} 
      child[blue!50!black] {};
    \node[right=of 1, red!75!black, "{\color{red!75!black}\tiny 4}" below] (2) {$\bullet$} 
		;
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
  \end{tikzpicture}	
	};	
\end{tikzpicture}
\]
This represents one morphism $p\to q$.

So how many different morphisms are there from $p$ to $q$? The first $p$-position can be sent to any $q$-position: 1, 2, 3, or 4. Sending it to $1$ requires choosing how each of the four options $(q[1]=\4)$ are to be assigned one of $p[1]=\3$ options; there are $3^4$ ways to do this. Similarly, we can calculate the remaining ways to handle the first $p$-position, then add them up: there are $3^4+3^2+3^0+3^0=92$ ways total. 

The second $p$-position can also be sent to 1, 2, 3, or 4, before sending back directions; there are $1^4+1^2+1^0+1^0=4$ ways to do this. 
Similarly there are four ways to send the third $p$-position to a $q$-position and send back directions.

In total, there are $92 \cdot 4 \cdot 4=1472$ morphisms $p\to q$.

Unsurprisingly, this is exactly what is given by \eqref{eqn.main_formula}:
\begin{align*}
    |\poly(p, q)| &= \prod_{i \in p(\1)} |q(p[i])| \\
    &= \prod_{i \in p(\1)} |p[i]|^4 + |p[i]|^2 + 2 \\
    &= (3^4 + 3^2 + 2)(1^4 + 1^2 + 2)^2 \\
    &= 92 \cdot 4^2 = 1472.
\end{align*}
\end{example}

\begin{exercise}\label{exc.practice_poly_maps}
\begin{enumerate}
	\item Draw the corolla forests associated to $p\coloneqq\yon^\3+\yon+\1$, $q\coloneqq \yon^\2+\yon^\2+\2$, and $r\coloneqq\yon^\3$.
	\item Give an example of a morphism $p\to q$ and draw it as we did in \cref{ex.practice_with_poly_morphisms}.
	\item Explain your morphism intuitively as a delegation of decisions.
	\item Explain in those terms why there can't be any morphisms $p\to r$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item Here are the corolla forests associated to $p\coloneqq\yon^\3+\yon+\1$, $q\coloneqq \yon^\2+\yon^\2+\2$, and $r\coloneqq\yon^\3$ (with each root labeled for convenience).
    \[
    \begin{tikzpicture}[rounded corners]
    	\node (p) [draw, blue!50!black, "$p$" above] {
    	\begin{tikzpicture}[trees, sibling distance=2.5mm]
            \node["\tiny 1" below] (1) {$\bullet$} 
              child {}
              child {}
              child {};
            \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$} 
              child {};
            \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$};
        \end{tikzpicture}
        };
    %
    	\node (q) [draw, red!75!black, right=2 of p, "$q$" above] {
    	\begin{tikzpicture}[trees, sibling distance=2.5mm]
            \node["\tiny 1" below] (1) {$\bullet$} 
              child {}
              child {};
            \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$} 
              child {}
              child {};
            \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$};
            \node[right=.5 of 3,"\tiny 4" below] (4) {$\bullet$};
        \end{tikzpicture}
        };
    %
    	\node (r) [draw, green!50!black, right=2 of q, "$r$" above] {
    	\begin{tikzpicture}[trees, sibling distance=2.5mm]
        \node["\tiny 1" below] (1) {$\bullet$} 
          child {}
          child {}
          child {};
        \end{tikzpicture}
      };
    \end{tikzpicture}
    \]
	\item Here is one possible morphism $p \to q$ (you may have drawn others).
	\[
    \begin{tikzpicture}
    	\node (p1) {
        	\begin{tikzpicture}[trees, sibling distance=2.5mm]
                \node[blue!50!black, "\tiny 1" below] (1) {$\bullet$} 
                  child[blue!50!black] {coordinate (11)}
                  child[blue!50!black] {coordinate (12)}
                  child[blue!50!black] {coordinate (13)};
                \node[right=1.5 of 1, red!75!black, "\tiny 2" below] (2) {$\bullet$}
                  child[red!75!black] {coordinate (21)}
                  child[red!75!black] {coordinate (22)};
                \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
                \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
                  \draw[postaction={decorate}] (21) to (13);
                  \draw[postaction={decorate}] (22) to (11);
                \end{scope}
            \end{tikzpicture}	
    	};	
        %
    	\node (p2) [right=1 of p1] {
        	\begin{tikzpicture}[trees, sibling distance=2.5mm]
                \node[blue!50!black, "\tiny 2" below] (1) {$\bullet$} 
                  child[blue!50!black] {coordinate (11)};
                \node[right=of 1, red!75!black, "\tiny 4" below] (2) {$\bullet$};
                \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
            \end{tikzpicture}	
    	};	
        %
    	\node (p3) [below right=-1.05cm and 1 of p2] {
        	\begin{tikzpicture}[trees, sibling distance=2.5mm]
                \node[blue!50!black, "\tiny 3" below] (1) {$\bullet$};
                \node[right=of 1, red!75!black, "\tiny 3" below] (2) {$\bullet$};
                \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
            \end{tikzpicture}	
    	};	
    \end{tikzpicture}
    \]
    \item As depicted, our morphism delegates the first decision of $p$ to the second decision of $q$, whose first and second options are passed back to the third and first options, respectively, of the first decision of $p$.
    Then the second decision of $p$ is delegated to the fourth decision of $q$, which has no options; effectively, the second decision of $p$ has been canceled.
    Finally, the third decision of $p$ is delegated to the third decision of $q$, neither of which has any options.
    \item There cannot be a morphism $p \to r$ for the following reason: if we delegate the third decision of $p$, which has no options, to the sole decision of $r$, which has $3$ options, then there is no way to pass a choice of one of the $3$ options back to any of the options associated with the third decision of $p$, as there are no such options.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
For any polynomial $p$ and set $A$, e.g.\ $A=\2$, the Yoneda lemma gives an isomorphism $p(A)\cong \poly(\yon^A,p)$.
\begin{enumerate}
	\item Choose a polynomial $p$ and draw both $\yon^\2$ and $p$ as corolla forests.
	\item Count all the maps $\yon^\2\to p$. How many are there?
	\item Is the previous answer equal to $p(\2)$?
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We let $p \coloneqq \yon^\3 + 1$ (you could have selected others) and draw both $p$ and $\yon^\2$ as corolla forests, labeling each root for convenience.
    \[
    \begin{tikzpicture}[rounded corners]
    	\node (y2) [draw, blue!50!black, "$\yon^\2$" above] {
    	\begin{tikzpicture}[trees, sibling distance=2.5mm]
            \node["\tiny 1" below] (1) {$\bullet$} 
              child {}
              child {};
        \end{tikzpicture}
        };
    %
    	\node (p) [draw, red!75!black, right=2 of y2, "$p$" above] {
    	\begin{tikzpicture}[trees, sibling distance=2.5mm]
            \node["\tiny 1" below] (1) {$\bullet$} 
              child {}
              child {}
              child {};
            \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$};
        \end{tikzpicture}
        };
    \end{tikzpicture}
    \]
    \item We count all the maps from $\yon^\2$ to $p$.
    The unique position of $\yon^\2$ can be sent to either $p$-position.
    If it is sent to the first $p$-position, then there are $2$ directions of $\yon^\2$ for each of the $3$ $p[1]$-directions to be sent to, for a total of $2^3 = 8$ maps.
    Otherwise, the unique position of $\yon^\2$ is sent to the second $p$-position---at which there are no directions, so there is exactly $1$ way to do this.
    So we have $8 + 1 = 9$ maps from $\yon^\2$ to $p$.
    \item Yes, the previous answer is equal to $p(\2) = 2^3 + 1 = 9$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
For each of the following polynomials $p,q$, compute the number of morphisms $p\to q$.
\begin{enumerate}
	\item $p=\yon^\3$,\quad $q=\yon^\4$.
	\item $p=\yon^\3+\1$,\quad $q=\yon^\4$.
	\item $p=\yon^\3+\1$,\quad $q=\yon^\4+\1$.
	\item $p=\4\yon^\3+\3\yon^\2+\yon$,\quad $q=\yon$.
	\item $p=\4\yon^\3$,\quad $q=\3\yon$.
\qedhere
\end{enumerate}
\begin{solution}
Our goal is to compute the number of natural transformations $p\to q$ for each of the following polynomials $p,q$.
By \eqref{eqn.main_formula}, we always have
\[
    |\poly(p, q)| = \prod_{i \in p(\1)} |q(p[i])|.
\]
\begin{enumerate}
	\item If $p=\yon^\3$ and $q=\yon^\4$, then
	\[
	    |\poly(p, q)| = \prod_{i \in \1} |p[i]|^4 = 3^4 = 81.
	\]
	\item If $p=\yon^\3+\1$ and $q=\yon^\4$, then
	\[
	    |\poly(p, q)| = \prod_{i \in \2} |p[i]|^4 = 3^4 \cdot 0^4 = 0.
	\]
	\item If $p=\yon^\3+\1$ and $q=\yon^\4+\1$, then
	\[
	    |\poly(p, q)| = \prod_{i \in \2} |p[i]|^4 + 1 = (3^4 + 1)(0^4 + 1) = 82.
	\]
	\item If $p=\4\yon^\3+\3\yon^\2+\yon$ and $q=\yon$, then
	\[
	    |\poly(p, q)| = \prod_{i \in \8} |p[i]| = 3^4 \cdot 2^3 \cdot 1 = 648.
	\]
	\item If $p=\4\yon^\3$ and $q=\3\yon$, then
	\[
	    |\poly(p, q)| = \prod_{i \in \4} 3|p[i]| = (3 \cdot 3)^4 = 6561.
	\]
\qedhere
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.practice_sum_prod}
\begin{enumerate}
\item Show that the following are isomorphic:
\begin{equation}\label{eqn.poly_p_q}
  \poly(p,q)
  \cong^?
  \prod_{i\in p(\1)}\sum_{j\in q(\1)}\prod_{d\in q[j]}\sum_{c\in p[i]}\1
\end{equation}
\item \label{exc.practice_sum_prod.useful} Show that the following are isomorphic:
	\begin{equation}\label{eqn.useful_misc472}
	\poly(p,q)\cong^?\sum_{f_\1\colon p(\1)\to q(\1)}\prod_{j\in q(\1)}\smset\Bigg(q[j],\prod_{\substack{i \in p(\1), \\ f_\1(i) = j}}p[i]\Bigg)
	\end{equation}
\item Describe in the language of decision-making how any element of the right-hand side of \eqref{eqn.useful_misc472} gives a way of delegating decisions from $p$ to $q$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{longenum}
\item We will show that the following isomorphism holds:
\[
  \poly(p,q)
  \cong
  \prod_{i\in p(\1)}\sum_{j\in q(\1)}\prod_{d\in q[j]}\sum_{c\in p[i]}\1.
\]
By \eqref{eqn.main_formula}, it suffices to show that for all $i \in p(\1)$ and $j \in q(\1)$, we have
\[
    p[i]^{q[j]} \iso \prod_{d \in q[j]} \sum_{c \in p[i]} \1.
\]
Indeed, by \eqref{eqn.push_prod_sum_set_indep} and \cref{exc.on_sums_prods_sets}, we have
\begin{align*}
    \prod_{d \in q[j]} \sum_{c \in p[i]} \1 &\iso \sum_{\bar{c} \colon q[j] \to p[i]} \, \prod_{d \in q[j]} \1 \tag*{\eqref{eqn.push_prod_sum_set_indep}} \\
    &\iso \sum_{\bar{c} \colon q[j] \to p[i]} \1 \tag{\cref{exc.on_sums_prods_sets} \cref{exc.on_sums_prods_sets.prod}} \\
    &\iso \smset(q[j], p[i]) \tag{\cref{exc.on_sums_prods_sets} \cref{exc.on_sums_prods_sets.sum}} \\
    &\iso p[i]^{q[j]}.
\end{align*}

\item We will show that the following isomorphism holds:
\[
	\poly(p,q) \iso \sum_{f_\1\colon p(\1)\to q(\1)} \; \prod_{j\in q(\1)}\smset\Bigg(q[j],\prod_{\substack{i \in p(\1), \\ f_\1(i) = j}}p[i]\Bigg).
\]
By \eqref{eqn.main_formula} and \eqref{eqn.push_prod_sum_set_indep}, we have
\begin{align*}
    \poly(p, q) &\iso \prod_{i \in p(\1)} \sum_{j \in q(\1)} p[i]^{q[j]} \tag*{\eqref{eqn.main_formula}} \\
    &\iso \sum_{f_\1 \colon p(\1) \to q(\1)} \; \prod_{i \in p(\1)} p[i]^{q[f_\1(i)]} \tag*{\eqref{eqn.push_prod_sum_set_indep}} \\
    &\iso \sum_{f_\1 \colon p(\1) \to q(\1)} \; \prod_{j \in q(\1)} \; \prod_{\substack{i \in p(\1), \\ f_\1(i) = j}} p[i]^{q[j]} \tag{$\ast$} \\
    &\iso \sum_{f_\1\colon p(\1)\to q(\1)} \; \prod_{j\in q(\1)}\smset\Bigg(q[j],\prod_{\substack{i \in p(\1), \\ f_\1(i) = j}}p[i]\Bigg) \tag{Universal property of products}
\end{align*}
where ($\ast$) follows from the fact that for any function $f_\1 \colon p(\1) \to q(\1)$, the set $p(\1)$ can be written as the disjoint union of sets of the form $f_\1\inv(j) = \{i \in p(1) \mid f_\1(i) = j\}$ for each $j \in q(\1)$.

\item To explain how the set
\[
	D_{p,q} \coloneqq \sum_{f_\1\colon p(\1)\to q(\1)}\prod_{j\in q(\1)}\smset\Bigg(q[j],\prod_{\substack{i \in p(\1), \\ f_\1(i) = j}}p[i]\Bigg)
\]
specifies a way of delegating decisions from $p$ to $q$, we first give the instructions for choosing an element of $D_{p,q}$ as a nested list:
\begin{quote}
To choose an element of $D_{p,q}$:
\begin{longenum}
    \item choose a function $f_\1 \colon p(\1) \to q(\1)$;
    \item for each element $j \in q(\1)$:
    \begin{longenum}
        \item for each element of $q[j]$:
        \begin{longenum}
            \item for each element $i \in p(\1)$ satisfying $f_\1(i) = j$:
            \begin{longenum}
                \item choose an element of $p[i]$.
            \end{longenum}
        \end{longenum}
    \end{longenum}
\end{longenum}
\end{quote}
So $f_\1$ delegates each of $p$'s decisions to one of $q$'s decisions.
Then for every option of every decision $j$ of $q$, we choose an option of each of $p$'s decisions that has been delegated to $j$ by $f_\1$.
\end{longenum}
\end{solution}
\end{exercise}

\begin{exercise}%\label{exc.poly_coprod}
Use \eqref{eqn.poly_coprod} and \eqref{eqn.main_formula} to verify that
\[
    \poly\left(\sum_{i \in I} p_i, q\right) \iso \prod_{i \in I} \poly(p_i, q)
\]
for all polynomials $(p_i)_{i \in I}$ and $q$, as expected from the universal property of coproducts.
\begin{solution}
Given $q \in \poly$ and $p_i \in \poly$ for each $i \in I$ for some set $I$, we use \eqref{eqn.poly_coprod} and \eqref{eqn.main_formula} to verify that
\begin{align*}
    \poly\left(\sum_{i \in I} p_i, q\right)
    &\iso \poly\left(\sum_{(i,j) \in \sum_{i \in I} p_i(\1)} \yon^{p_i[j]}, q\right)
    \tag*{\eqref{eqn.poly_coprod}} \\
    &\iso \prod_{(i,j) \in \sum_{i \in I} p_i(\1)} q(p_i[j])
    \tag*{\eqref{eqn.main_formula}} \\
    &\iso \prod_{i \in I} \prod_{j \in p_i(\1)} q(p_i[j]) \\
    &\iso \prod_{i \in I} \poly(p_i, q).
    \tag*{\eqref{eqn.main_formula}}
\end{align*}
\end{solution}
\end{exercise}

\begin{example}[Derivatives]\label{ex.derivatives}
The \emph{derivative} of a polynomial $p$, denoted $\dot{p}$, is defined as follows:
\[
\dot{p}\coloneqq\sum_{i\in p(\1)}\sum_{d\in p[i]}\yon^{p[i]-\{d\}}.
\]
For example, if $p\coloneq\yon^{\{U,V,W\}}+\{A,B\}\yon^{\{X\}}$ then 
\[\dot{p}=\{U\}\yon^{\{V,W\}}+\{V\}\yon^{\{U,W\}}+\{W\}\yon^{\{U,V\}}+\{(A,X),(B,X)\}\yon^\0.\]
Up to isomorphism $p\cong\yon^\3+\2\yon$ and $\dot{p}\cong\3\yon^\2+\2$.
Unsurprisingly, this coincides with the familiar notion of derivatives of polynomials from calculus.

Thus we get a canonical map $\dot{p}\yon\to p$, because we have an isomorphism
\[
\dot{p}\yon\cong\sum_{i\in p(\1)}\sum_{d\in p[i]}\yon^{p[i]}.
\]
This natural transformation comes up in computer science in the context of ``plugging in to one-hole contexts''; we will not explore that here, but see \cite{mcbride} and \cite{abbot2003derivatives} for more info.%The Derivative of a Regular Type is its Type of One-Hole Contexts. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.22.8611&rep=rep1&type=pdf.
%https://www.cs.nott.ac.uk/~psztxa/publ/tlca03.pdf

A morphism $f\colon p\to \dot{q}$ can be interpreted as something like an arena morphism from $p$ to $q$, except that each $p$-position explicitly selects a direction of $q$ to remain unassigned. More precisely, for each $i\in p(\1)$ we have $f_\1(i)=(j,d)\in \sum_{j\in q(\1)}q[j]$, i.e.\ a choice of position $j$ of $q$, as usual, together with a chosen direction $d\in q[j]$. Then every $q[j]$-direction \emph{other than $d$} is sent back to a $p[i]$-direction.
\end{example}

\begin{exercise} \label{exc.deriv_directions}
Show that $\dot{p}(\1)$ is isomorphic to the set of all directions of $p$ (i.e.\ the union of all direction-sets of $p$), so there is a canonical function $\pi_p \colon \dot{p}(\1) \to p(\1)$ that sends each direction $d$ of $p$ to the position $i$ of $p$ for which $d \in p[i]$.
\begin{solution}
We can evaluate $\dot{p}(\1)$ directly from the definition of $\dot{p}$ to obtain
\[
    \dot{p}(\1) = \sum_{i \in p(\1)} \sum_{d \in p[i]} \1^{p[i]-\{d\}} \iso \sum_{i \in p(\1)} p[i],
\]
which is isomorphic to the set of all directions of $p$.
Then $\pi_p \colon \dot{p}(\1) \to p(\1)$ is the canonical projection, sending each direction $d$ of $p$ to the position $i$ of $p$ for which $d \in p[i]$.
\end{solution}
\end{exercise}

\begin{exercise}
The derivative is not very well-behaved category-theoretically.
However, it is intriguing.
Below $p, q \in \poly$.
\begin{enumerate}
	\item Explain the canonical map $\dot{p}\yon\to p$ from \cref{ex.derivatives} in more detail.
	\item Is there always a canonical map $p\to \dot{p}$?
	\item Is there always a canonical map $\dot{p}\to p$?
	\item If given a map $p\to q$, does one get a map $\dot{p}\to\dot{q}$?
	\item We will define the binary operations $\otimes$ and $\ihom{-,-}$ on $\poly$ later on in \eqref{eqn.parallel_def} and \eqref{eqn.dir_hom}, and in \cref{exc.dir_hom_p_yon_dir_p}, you will be able to use \cref{exc.dir_hom_sum} to deduce that
	\begin{equation} \label{eqn.dir_hom_p_yon_dir_p}
	    \ihom{p, \yon} \otimes p \iso \sum_{f \in \prod_{i \in p(\1)} p[i]} \; \sum_{i \in p(\1)} \yon^{p(\1) \times p[i]},
	\end{equation}
% 	Using this, find a formula for a map $p\otimes\ihom{p,\yon}\to \dot{p}$ that works for any $p\in\poly$.
	Is there always a canonical map $\ihom{p,\yon}\otimes p\to \dot{p}$?
	\item When talking to someone who explains maps $p\to\dot{q}$ in terms of ``unassigned directions,'' how might you describe what is modeled by a map $p\yon\to q$?
	\qedhere
\end{enumerate}
\begin{solution}
Here $p, q \in \poly$.
\begin{enumerate}
	\item Our goal is to characterize the canonical map $\dot{p}\yon\to p$.
	If we unravel the definitions, this is a map
	\[
	    \sum_{i \in p(\1)} \sum_{d \in p[i]} \yon^{p[i]} \to \sum_{i \in p(\1)} \yon^{p[i]}.
	\]
	We observe that there is always such a map sending every position $(i, d) \in \sum_{i \in p(\1)} p[i]$ of $\dot{p}\yon$ to its first projection $i \in p(\1)$ and is the identity on directions.
	This is the canonical map.

	\item There cannot always be a canonical map $p\to \dot{p}$, for if $p \coloneqq \1$, then $\dot{p} \coloneqq \0$, and there is no map $\1 \to \0$.
	
	\item We show that there cannot always be a canonical map $\dot{p}\to p$.
	Take $p \coloneqq \yon$, so $\dot{p} \coloneqq \1$.
	A map $\1 \to \yon$ must have an on-directions function $\1 \to \0$, but such a function does not exist.
	
	\item We show that even when there is a map $p \to q$, there is not necessarily a map $\dot{p}\to\dot{q}$.
	Take $p \coloneqq \yon$ and $q \coloneqq \1$.
	Then there is a map $p \to q$ that sends the unique position of $\yon$ to the unique position of $\1$ and is the empty function on directions.
	But $\dot{p} = \1$ and $\dot{q} = \0$, and there is no map $\1 \to \0$.
	
	\item We show that there is a canonical map $g \colon \ihom{p,\yon} \otimes p \to \dot{p}$, where $\ihom{p,\yon} \otimes p$ is given by \eqref{eqn.dir_hom_p_yon_dir_p}.
	The on-positions function $g_\1$ takes $f \in \prod_{i \in p(\1)} p[i]$ and $i \in p(\1)$ and sends the pair of them to the $\dot{p}$-position corresponding to $i \in p(\1)$ and $f(i) \in p[i]$.
	We then have $\dot{p}[(i, f(i))] \iso p[i]$ and $(\ihom{p,\yon} \otimes p)[(f, i)] \iso p(\1) \times p[i]$, so the on-directions function $g^\sharp_{(f,i)}$ can send each $d \in p[i]$ to $(i, d) \in p(\1) \times p[i]$.

	\item We wish to describe a map $p\yon \to q$ in terms of ``unassigned to directions.''
	Observe that as an arena, $p\yon$ has the same positions as $p$ but has one more direction than $p$ does at each position.
	We denote this extra direction at each position $i \in p(\1)$ of $p\yon$ by $\ast_i$.
	So a map $f \colon p\yon\to q$ sends each position $i$ of $p$ to a position $j$ of $q$, but every $q[j]$-direction could either be sent back to a $p[i]$-direction or the extra direction $\ast_i$.
	We can say that an arena morphism $f \colon p\yon \to q$ is like an arena morphism $p \to q$, except that any of the directions of $q$ may remain unassigned---i.e.\ we may have partial on-directions functions.
\end{enumerate}
\end{solution}
\end{exercise}

%---- Subsection ----%
\subsection{Arena morphisms as lenses}\label{subsec.poly.func_nat.morph.bimorphic_lens}

Monomials are special polynomials: those of the form $A\yon^B$ for sets $A,B$. Here's a picture of $\5\yon^{\1\2}$:
\[
\begin{tikzpicture}
\node[draw, rounded corners, "$\5\yon^{\1\2}$"] {
	\begin{tikzpicture}[trees, sibling distance=1mm]
	\foreach \i in {1,...,5}
	{
    \node["\tiny \i" below] at (1.8*\i,0) {$\bullet$} 
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
    ;
	};
	\end{tikzpicture}
};
\end{tikzpicture}
\]

The formula for morphisms between these is particularly simple:
\begin{align*}
  \poly\left(A_1\yon^{B_1},\,A_2\yon^{B_2}\right) &\iso \prod_{a \in A_1} \sum_{a' \in A_2} B_1^{B_2} \tag*{\eqref{eqn.main_formula}} \\
  &\iso \smset(A_1, A_2 \times B_1^{B_2}) \\
  &\iso \smset(A_1,A_2)\times\smset(A_1\times B_2,B_1).
\end{align*}
It says that to give a morphism from one monomial to another, you just need to give two (non-dependent!) functions. Let's rewrite it to make those two functions explicit---they are the familiar on-positions and on-directions functions:
\[
  \poly\left(A_1\yon^{B_1},\,A_2\yon^{B_2}\right)
  \iso
  \left\{
    (f_\1,f^\sharp)
  \;\middle|\;
  	\parbox{1.2in}{
    $
    \begin{aligned}
  	  f_\1&\colon A_1\to A_2\\
  	  f^\sharp&\colon A_1\times B_2\to B_1
    \end{aligned}
    $
  }.
  \right\}
\]
Ordinarily, $f^\sharp$ is more involved: its type depends on the directions at each position of the domain and its image position via $f_\1$.
But for monomials, every position has the same set of directions, so $f^\sharp$ is just a standard function.

The monomials in $\poly$ and the morphisms between them form a full subcategory of $\poly$, and it has been called the \emph{the category of bimorphic lenses} \cite{hedges2018limits}. It comes up in functional programming. The functions $f_\1, f^\sharp$ corresponding to a morphism $f \colon A_1\yon^{B_1}\to A_2\yon^{B_2}$ are given special names:
\begin{equation}\label{eqn.bimorphic_lens}
\begin{aligned}
	\text{get} \coloneqq f_\1 &\colon A_1\to A_2\\
	\text{set} \coloneqq f^\sharp &\colon A_1\times B_2\to B_1
\end{aligned}
\end{equation}
The idea is that each position $a \in A_1$ of $A_1\yon^{B_1}$ ``gets'' a position $f_\1(a) \in A_2$ of $A_2\yon^{B_2}$, and given $a \in A_1$, every direction at $f_\1(a)$ in $B_2$ ``sets'' a direction back at $a$ in $B_1$.

So an arena morphism between two monomials is just a bimorphic lens, or a lens for short.
Then an arena morphism between any two arenas is a kind of generalized lens.
In fact, henceforth we will refer to arena morphisms simply as \emph{lenses}.

\begin{example}
Consider the monomial $S\yon^S$. As an arena, its position-set is $S$, and its direction-set at each position $s\in S$ is again just $S$.
In the language of decision-making, each $s \in S$ is a decision where the options you have to choose from are always just the decisions in $S$ again.
Notice that there is a natural way to string together a series of such decisions into a cycle: at each step, you start at some element of $S$, and the option you select is the element of $S$ that you will move to next.
We will formalize this idea in \cref{**}.

A lens $(\text{get, set})\colon S\yon^S\to T\yon^T$ is as usual a way to delegate decisions of $S\yon^S$ to decisions of $T\yon^T$.
When you need to make a decision at $s \in S$, you ask your friend at $\text{get}(s) \in T$ for help.
If your friend selects option $t \in T$, then you know to select option $\text{set}(s, t) \in S$.

But what happens when we string together these decisions into cycles?
Now you are moving between elements of $S$, looking to your friend for help at each step as they move between elements of $T$.
In this scenario, there are a few conditions that a lens $S\yon^S \to T\yon^T$ should satisfy to ensure that the associated delgation behaves well with respect to the movements of both you and your friend:
\begin{enumerate}
    \item If your friend chooses to stay put, then you should stay put, too.
    This is reflected by the equation
    \[
        \text{set}(s,\text{get}(s))=s.
    \]
    
    \item After your friend moves, and you move accordingly, you should delegate the decision at your new location to the decision at your friend's new location.
    This is reflected by the equation
    \[
        \text{get}(\text{set}(s,t))=t.
    \]

    \item If your friend moves to $t$, then to $t'$, the place where you end up should be where you would have ended up if your friend had moved directly to $t'$ in the first place.
    This is reflected by the equation
    \[
        \text{set}(\text{set}(s,t),t')=\text{set}(s,t')
    \]
\end{enumerate}
We will see these three conditions emerge from more general theory in \cref{**}.
\end{example}

%---- Subsection ----%
\subsection{Translating between natural transformations and lenses} \label{subsec.poly.func_nat.morph.translate}
We now know that we can specify a morphism of polynomials $p \to q$ in two ways:
\begin{itemize}
    \item in the language of functors, by specifying a natural transformation $p \to q$, i.e.\ for each $X\in\smset$, a function $p(X)\to q(X)$ such that naturality squares commute; or
    \item in the language of arenas, by specifying a lens $p\to q$, i.e.\ a function $f_\1 \colon p(\1) \to q(\1)$ and, for each $i \in p(\1)$, a function $f^\sharp_i \colon q[f_\1(i)] \to p[i]$.
\end{itemize}
But what is the relationship between these two formulations?
If you told me a lens between arenas, and I told you a natural transformation between polynomial functors, how could we tell if we were talking about the same morphism or not?
We want to be able to translate between these two languages.

Our Rosetta Stone turns out to be the proof of the Yoneda lemma.
The lemma itself forms the crux of the proof of \cref{prop.poly_maps_prod_sum}, that these two formulations of polynomial morphisms are equivalent; so unraveling this proof reveals the translation we seek.

\begin{proposition} \label{prop.morph_arena_to_func}
Let $p$ and $q$ be polynomial functors, and let $(f_\1, f^\sharp)$ be a morphism between their associated arenas.
Then the isomorphism in \eqref{eqn.main_formula} sends $(f_\1, f^\sharp)$ to the natural transformation $f \colon p \to q$ whose $X$-component $f_X \colon p(X) \to q(X)$ for each $X \in \smset$ sends every
\[
    (i, g) \in \sum_{i \in p(\1)} X^{p[i]} \iso p(X),
\]
with $i \in p(\1)$ and $g \colon p[i] \to X$, to
\[
    (f_\1(i), f^\sharp_i \then g) \in \sum_{j \in q(\1)} X^{q[j]} \iso q(X).
\]
\end{proposition}
\begin{proof}
As an element of the product over $I$ on the right hand side of \eqref{eqn.main_formula}, the pair $(f_\1, f^\sharp)$ can equivalently be thought of as multiple pairs $((f_\1(i), f^\sharp_i))_{i \in I}$.
Fixing $i \in I$, the pair $(f_\1(i), f^\sharp_i)$ is an element of
\[
    \sum_{j \in q(\1)} p[i]^{q[j]} = q(p[i])
\]
(so $f_\1(i) \in q(\1)$ and $f^\sharp_i \colon q[f_\1(i)] \to p[i]$).
By the Yoneda lemma (\cref{lemma.yoneda}), we have an isomorphism $q(p[i]) \iso \poly(\yon^{p[i]}, q)$, and by the proof of the Yoneda lemma, this isomorphism sends $(f_\1(i), f^\sharp_i)$ to the natural transformation $f^i \colon \yon^{p[i]} \to q$ whose $X$-component is the function $f^i_X \colon X^{p[i]} \to q(X)$ given by sending $g \colon p[i] \to X$ to
\[
    q(g)(f_\1(i), f^\sharp_i) = \left(\sum_{j \in q(\1)} g^{q[j]}\right)(f_\1(i), f^\sharp_i) = \left(f_\1(i), g^{q[f_\1(i)]}(f^\sharp_i)\right) = (f_\1(i), f^\sharp_i \then g).
\]
Taken together, the natural transformations $(f^i)_{i \in I}$ form an element of $\prod_{i \in I} \poly(\yon^{p[i]}, q)$.
Applying the universal property of coproducts, as in the proof of \cref{prop.poly_maps_prod_sum}, we find that $(f^i)_{i \in I}$ corresponds to the natural transformation $f \colon p \to q$ we desire.
\end{proof}

\begin{example}
Let us return to the polynomials $p \coloneqq \yon^\3 + \2\yon$ and $q \coloneqq \yon^\4 + \yon^\2 + \2$ from \cref{ex.practice_with_poly_morphisms} and the morphism $f \colon p \to q$ depicted below:
\[
\begin{tikzpicture}
	\node (p1) {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 1" below] (1) {$\bullet$} 
      child[blue!50!black] {coordinate (11)}
      child[blue!50!black] {coordinate (12)}
      child[blue!50!black] {coordinate (13)};
    \node[right=1.5 of 1, red!75!black, "\tiny 1" below] (2) {$\bullet$} 
      child[red!75!black] {coordinate (21)}
      child[red!75!black] {coordinate (22)}
      child[red!75!black] {coordinate (23)}
      child[red!75!black] {coordinate (24)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (13);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (13);
      \draw[postaction={decorate}] (24) to (13);
    \end{scope}
  \end{tikzpicture}	
	};	
%
	\node (p2) [right=1 of p1] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 2" below] (1) {$\bullet$} 
      child[blue!50!black] {coordinate (11)};
    \node[right=of 1, red!75!black, "\tiny 1" below] (2) {$\bullet$} 
      child[red!75!black] {coordinate (21)}
      child[red!75!black] {coordinate (22)}
      child[red!75!black] {coordinate (23)}
      child[red!75!black] {coordinate (24)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (11);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (11);
      \draw[postaction={decorate}] (24) to (11);
    \end{scope}
  \end{tikzpicture}	
	};	
%
	\node (p3) [below right=-1.05cm and 1 of p2] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 3" below] (1) {$\bullet$} 
      child[blue!50!black] {};
    \node[right=of 1, red!75!black, "\tiny 4" below] (2) {$\bullet$} 
		;
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
  \end{tikzpicture}	
	};	
\end{tikzpicture}
\]
Fix a set $X \coloneqq \{a,b,c,d,e\}$.
When viewed as a natural transformation, the morphism $f$ has as its $X$-component a function $f_X \colon p(X) \to q(X)$.
In other words, for any element of $p(X)$, the morphism $f$ should be able to give us an element of $q(X)$.

What does an element of $p(X)$ look like?
Well, to specify such an element, we would need to choose a position $i$ of $p$ and a function $p[i] \to X$.
We can depict this by selecting one of the corollas in the forest of $p$ and labeling each leaf of that corolla with an element of $X$.
For example, here we depict an element $(1, g)$ of $p(X)$, where $g \colon p[1] \to X$ is given by $1 \mapsto c, 2 \mapsto e,$ and $3 \mapsto a$:
\[
\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 1" below] (1) {$\bullet$} 
      child[blue!50!black] {node {$c$}}
      child[blue!50!black] {node {$e$}}
      child[blue!50!black] {node {$a$}};
\end{tikzpicture}
\]
Similarly, an element of $q(X)$ can be drawn as a corolla in the forest of $q$, with each leaf labeled by an element of $X$.
So what element of $q(X)$ is $f_X(1, g)$?

\cref{prop.morph_arena_to_func} tells us that $f_X(1, g)$ can be read off of the forest depiction of $f$ at position $1$:
\[
\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[blue!50!black, "\tiny 1" below] (1) {$\bullet$} 
      child[blue!50!black] {coordinate (11)}
      child[blue!50!black] {coordinate (12)}
      child[blue!50!black] {coordinate (13)};
    \node[right=1.5 of 1, red!75!black, "\tiny 1" below] (2) {$\bullet$} 
      child[red!75!black] {coordinate (21)}
      child[red!75!black] {coordinate (22)}
      child[red!75!black] {coordinate (23)}
      child[red!75!black] {coordinate (24)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (13);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (13);
      \draw[postaction={decorate}] (24) to (13);
    \end{scope}
\end{tikzpicture}	
\]
To draw $f_X(1, g)$, we first draw the corolla in the forest of $q$ corresponding to $f_\1(1)$: the corolla on the right hand side above.
Then we label each leaf of that corolla by following the arrow from that leaf (as given by $f^\sharp_i$) to a leaf of $p$ at $1$, and use the label there that is given by $(1, g)$.
So $f_X(1, g)$ looks like
\[
\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node[red!75!black, "\tiny 1" below] (1) {$\bullet$} 
      child[red!75!black] {node {$a$}}
      child[red!75!black] {node {$c$}}
      child[red!75!black] {node {$a$}}
      child[red!75!black] {node {$a$}};
\end{tikzpicture}
\]
\end{example}

\cref{prop.morph_arena_to_func} lets us translate from lenses to natural transformations.
The following corollary tells us how to go in the other direction.
In particular, it justifies the notation $f_\1$ for the on-positions function of $f$.

\begin{corollary} \label{cor.morph_func_to_arena}
Let $p$ and $q$ be polynomial functors, and let $f \colon p \to q$ be a natural transformation between them.
Then the isomorphism in \eqref{eqn.main_formula} sends $f$ to the lens $(f_\1, f^\sharp)$ for which $f_\1 \colon p(\1) \to q(\1)$ is the $\1$-component of $f$ and, for each $i \in p(\1)$, we have
\[
    (f_\1(i), f^\sharp_i) = f_{p[i]}(i, \id_{p[i]}).
\]
\end{corollary}
\begin{proof}
By \cref{prop.morph_arena_to_func}, the $\1$-component of $f$ is a function $p(\1)\to q(\1)$ sending every $i \in p(\1)$ to $f_\1(i) \in q(\1)$, so the on-positions function $f_\1$ is indeed equal to the $\1$-component of $f$.
Also, the $p[i]$-component $f_{p[i]} \colon p(p[i]) \to q(p[i])$ sends every $(i, \id_{p[i]}) \in p(p[i])$, with $i \in p(\1)$, to $(f_\1(i), f^\sharp_i \then \id_{p[i]}) = (f_\1(i), f^\sharp_i)$.
\end{proof}

%---- Subsection ----%
\subsection{Identity and composition of lenses} \label{subsec.poly.func_nat.morph.id_comp}

Thus far, we have seen how the category $\poly$ of polynomial functors and natural transformations can just as easily be thought of as the category of arenas and lenses.
But in order to actually discuss the latter category, we need to be able to give identity lenses and describe how these lenses compose.
To do so, we can leverage our ability to translate back and forth between lenses and natural transformations.

For instance, given a polynomial $p$, the identity morphism of its associated arena should correspond to the identity natural transformation of $p$ as a functor.

\begin{exercise}[Identity lenses] \label{exc.arena_morph_id}
Let $p$ be a polynomial and let $\id_p \colon p \to p$ be its identity natural transformation, whose $X$-component $(\id_p)_X \colon p(X) \to p(X)$ for each $X \in \smset$ is the identity function on $p(X)$; that is, $(\id_p)_X = \id_{p(X)}$.

Use \cref{cor.morph_func_to_arena} to show that the lens $((\id_p)_\1, (\id_p)^\sharp)$ associated to $\id_p$ is such that $(\id_p)_\1 \colon p(\1) \to p(\1)$ and $(\id_p)^\sharp_i \colon p[(\id_p)_\1(i)] \to p[i]$ for $i \in p(\1)$ are all identity functions.
\begin{solution}
We wish to show that the lens $((\id_p)_\1, (\id_p)^\sharp)$ associated to the identity natural transformation $\id_p$ of a polynomial $p$ is such that $(\id_p)_\1$ and every $(\id_p)^\sharp_i$ are all identity functions.
Indeed, by \cref{cor.morph_func_to_arena}, for each $i \in p(\1)$, we have
\[
    ((\id_p)_\1(i), (\id_p)^\sharp_i) = (\id_p)_{p[i]}(i, \id_{p[i]}) = (i, \id_{p[i]}),
\]
as $(\id_p)_{p[i]}$ is the identity function on $p(p[i])$.
\end{solution}
\end{exercise}

Similarly, we should be able to deduce how two lenses compose by translating them to natural transformations, composing those, then translating back to lenses.

\begin{exercise}[Composing lenses] \label{exc.arena_morph_comp}
Let $p,q,$ and $r$ be polynomials, let $f \colon p \to q$ and $g \colon q \to r$ be natural transformations, and let $h \coloneqq f \then g$ be their composite, whose $X$-component $h_X \colon p(X) \to r(X)$ for each $X \in \smset$ is the composite of the $X$-components of $f$ and $g$; that is, $h_X = f_X \then g_X$.

Use \cref{cor.morph_func_to_arena} to show that the lens $(h_\1, h^\sharp)$ associated to $h$ satisfies $h_\1 = f_\1 \then g_\1$ and $h^\sharp_i = g^\sharp_{f_\1(i)} \then f^\sharp_i$ for all $i \in p(\1)$.
\begin{solution}
Given polynomial morphisms $f \colon p \to q, g \colon q \to r,$ and their composite $h \coloneqq f \then g$, we wish to show that the lens $(h_\1, h^\sharp)$ associated to $h$ satisfies $h_\1 = f_\1 \then g_\1$ and $h^\sharp_i = g^\sharp_{f_\1(i)} \then f^\sharp_i$ for all $i \in p(\1)$.
Indeed, by \cref{cor.morph_func_to_arena} and \cref{prop.morph_arena_to_func}, for each $i \in p(\1)$, we have
\begin{align*}
    (h_\1(i), h^\sharp_i) &= h_{p[i]}(i, \id_{p[i]}) \\
    &= g_{p[i]}(f_{p[i]}(i, \id_{p[i]})) \tag{$h = f \then g$} \\
    &= g_{p[i]}(f_\1(i), f^\sharp_i) \tag{\cref{cor.morph_func_to_arena}} \\
    &= (g_\1(f_\1(i)), g^\sharp_{f_\1(i)} \then f^\sharp_i). \tag{\cref{prop.morph_arena_to_func}}
\end{align*}
\end{solution}
\end{exercise}

\begin{example}[Commutative diagrams in $\poly$] \label{ex.comm_poly}
The above exercise tells us how to interpret commutative diagrams in $\poly$ as commutative diagrams in the more familiar setting of $\smset$.
Given polynomials $p, q, r$ and natural transformations $f \colon p \to q, g \colon q \to r,$ and $h \colon p \to r$, the diagram
\[
\begin{tikzcd}
    p \ar[r, "f"] \ar[dr, "h"'] & q \ar[d, "g"] \\
    & r
\end{tikzcd}
\]
commutes in $\poly$ if and only if the forwards on-positions diagram
\[
\begin{tikzcd}
    p(\1) \ar[r, "f_\1"] \ar[dr, "h_\1"'] & q(\1) \ar[d, "g_\1"] \\
    & r(\1)
\end{tikzcd}
\]
commutes in $\smset$ and, for each $i \in p(\1)$, the backwards on-directions diagram
\[
\begin{tikzcd}
    p[i] & q[f_\1(i)] \ar[l, "f^\sharp_i"'] \\
    & r[h_\1(i)] \ar[u, "g^\sharp_{f_\1(i)}"'] \ar[ul, "h^\sharp_i"]
\end{tikzcd}
\]
commutes in $\smset$.
We can use this fact to determine whether a given diagram in $\poly$ commutes.
\end{example}

\begin{exercise}
Verify that, for $p, q \in \poly$, the polynomial $p+q$ given by the binary sum of $p$ and $q$ satisfies the universal property of the coproduct of $p$ and $q$.
That is, provide morphisms $\iota \colon p \to p + q$ and $\kappa \colon q \to p + q$, then show that for any other polynomial $r$ with morphisms $f \colon p \to r$ and $g \colon q \to r$, there exists a unique morphism $h \colon p+q \to r$---shown dashed---making the following diagram commute:
\begin{equation} \label{eqn.coprod_univ_prop}
\begin{tikzcd}
	p \ar[r, "\iota"] \ar[dr, "f"'] &
	p + q \ar[d, "h", dashed] &
	q \ar[l, "\kappa"'] \ar[dl, "g"] \\
	& r
\end{tikzcd}
\end{equation}
Hint: Use \cref{ex.comm_poly} to determine whether a diagram commutes.
\begin{solution}
We provide $\iota \colon p \to p + q$ and $\kappa \colon q \to p + q$ as follows.
On positions, they are the canonical inclusions $\iota_\1 \colon p(\1) \to p(\1)+q(\1)$ and $\kappa_\1 \colon q(\1) \to p(\1)+q(\1)$; on directions, they are identities.
We wish to show that, for $p, q \in \poly$, the polynomial $p+q$ along with $\iota$ and $\kappa$ satisfy the universal property of the coproduct.
That is, we must show that for any $r \in \poly$ and maps $f \colon r \to p$ and $g \colon r \to q$, there exists a unique map $h \colon r \to p+q$ for which the diagram \eqref{eqn.coprod_univ_prop} commutes.
% \begin{equation} \label{eqn.coprod_univ_prop}
% \begin{tikzcd}
% 	r & q \ar[l, "g"'] \ar[d, "\kappa"] \\
% 	p \ar[u, "f"] \ar[r, "\iota"'] & p+q \ar[ul, "h", dashed].
% \end{tikzcd}
% \end{equation}

We apply \cref{ex.comm_poly}.
In order for \eqref{eqn.coprod_univ_prop} to commute, it must commute on positions---that is, the following diagram of sets must commute:
\begin{equation} \label{eqn.coprod_univ_prop_pos}
\begin{tikzcd}
	p(\1) \ar[r, "\iota_\1"] \ar[dr, "f_\1"'] &
	p(\1) + q(\1) \ar[d, "h_\1", dashed] &
	q(\1) \ar[l, "\kappa_\1"'] \ar[dl, "g_\1"] \\
	& r(\1)
\end{tikzcd}
\end{equation}
But since $p(1)+q(\1)$ along with the inclusions $\iota_\1$ and $\kappa_\1$ form the coproduct of $p(\1)$ and $q(\1)$ in $\smset$, there exists a unique $h_\1$ for which \eqref{eqn.coprod_univ_prop_pos} commutes.
Hence $h$ is uniquely characterized on positions.
In particular, it must send each $(1,i) \in p(\1)+q(\1)$ with $i \in p(\1)$ to $f_\1(i)$ and each $(2,j) \in p(\1)+q(\1)$ with $j \in q(\1)$ to $g_\1(j)$.

Moreover, if \eqref{eqn.coprod_univ_prop} is to commute on directions, then for every $i \in p(\1)$ and $j \in q(\1)$, the following diagrams of sets must commute:
\begin{equation} \label{eqn.coprod_univ_prop_dir}
\begin{tikzcd}[sep=large]
	p[i] & (p+q)[(1,i)] \ar[l, "\iota^\sharp_i"'] & (p+q)[(2,j)] \ar[r, "\kappa^\sharp_j"] & q[j] \\
	& r[f_\1(i)] \ar[ul, "f^\sharp_i"] \ar[u, "h^\sharp_{(1,i)}"', dashed] & r[g_\1(j)] \ar[u, "h^\sharp_{(2,j)}", dashed] \ar[ur, "g^\sharp_j"']
\end{tikzcd}
\end{equation}
But $(p+q)[(1,i)] \iso p[i]$ and $\iota^\sharp_i$ is the identity, so we must have $h^\sharp_{(1,i)} = f^\sharp_i$.
Similarly, $(p+q)[(2,j)] \iso q[j]$ and $\kappa^\sharp_j$ is the identity, so we must have $h^\sharp_{(2,j)} = g^\sharp_j$.
Hence $h$ is also uniquely characterized on directions, so it is unique overall.
Moreover, we have shown that we can define $h$ on positions so that \eqref{eqn.coprod_univ_prop_pos} commutes, and that we can define $h$ on directions such that the diagrams in \eqref{eqn.coprod_univ_prop_dir} commute.
As the commutativity of the diagrams in \eqref{eqn.coprod_univ_prop_pos} and \eqref{eqn.coprod_univ_prop_dir} together imply the commutativity of \eqref{eqn.coprod_univ_prop}, it follows that there exists $h$ for which \eqref{eqn.coprod_univ_prop} commutes.
\end{solution}
\end{exercise}

\begin{exercise}[A functor $\Cat{Top}\to\poly$] \label{exc.top_poly_func}
This exercise is for those who know what topological spaces and continuous maps are. It will not be used again in this book.
\begin{enumerate}
	\item Suppose that $X$ is a topological space. Organize its points and their neighborhoods into a polynomial $p_X$.
	\item Give a formula by which any continuous map $X\to Y$ induces a map of polynomials $p_X\to p_Y$.
	\item Show that your formula defines a functor.
	\item Is it full? Faithful?
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
	\item Given a topological space $X$, we can define a polynomial $p_X$ whose positions are the points in $X$ and whose directions at $x \in X$ are the open neighborhoods of $x$.
	In other words,
	\[
	    p_X \coloneqq \sum_{x \in X} \yon^{\{ U \ss X \mid x \in U, \, U \text{ open} \}}.
	\]
	\item \label{exc.top_poly_func.morphs} For every continuous map $f \colon X \to Y$, we give a lens $p_f \colon p_X \to p_Y$.
	The on-positions function is just $f$, while for each position $x \in X$ of $p_X$, the on-directions function $(p_f)^\sharp_x \colon p_Y[f(x)] \to p_X[x]$ sends each open neighborhood $U$ of $f(x)$ to the open neighborhood $f\inv(U)$ of $x$.
	
	\item To show that $p_X$ is functorial in $X$, it suffices to show that sending continuous maps $f \colon X \to Y$ to their induced lenses $p_f \colon p_X \to p_Y$ preserves identities and composition.
	First, we show that for any topological space $X$, the lens $p_{\id_X}$, where $\id_X$ is the identity map on $X$, is an identity morphism.
	By \cref{exc.top_poly_func.morphs}, the on-positions function of $p_{\id_X}$ is $\id_X$, and for each $x \in X$ the on-directions function $(p_f)^\sharp_x \colon p_X[x] \to p_X[x]$ sends $U \in p_X[x]$ to $(\id_X)\inv(U) = U$.
	Hence $p_{\id_X}$ is the identity on both positions and directions; it follows from \cref{exc.arena_morph_id} that $p_{\id_X}$ is an identity morphism.
	
	We now show that for topological spaces $X,Y,$ and $Z$ and continuous maps $f \colon X \to Y$ and $g \colon Y \to Z$, we have $p_f \then p_g = p_{f \then g}$.
	By \cref{exc.top_poly_func.morphs} and \cref{exc.arena_morph_comp}, the on-positions function of either side is equal to $f \then g$, so it suffices to show that for all $x \in X$,
	\[
	    (p_{f \then g})^\sharp_x = (p_g)^\sharp_{f(x)} \then (p_f)^\sharp_x.
	\]
	Again by \cref{exc.top_poly_func.morphs}, the left hand side sends each $U \in p_Z[g(f(x))]$ to $(f \then g)\inv(U)$, while the right hand side sends $U$ to $f\inv(g\inv(U))$, but by elementary set theory, these sets are equal.
	
	\item The functor is not full.
	Consider the spaces $X = \2$ with the indiscrete topology (i.e.\ the only open sets are the empty set and $X$) and $Y = \2$ with the discrete topology (i.e.\ all subsets are open).
	Then $p_X \iso \2\yon$ and $p_Y \iso \2\yon^\2$, so our functor induces a function from the set of continuous functions $X \to Y$ to the set of lenses $\2\yon \to \2\yon^\2$.
	We claim that this function is not surjective: in particular, consider the lens $h \colon \2\yon \to \2\yon^\2$ that is the identity on positions (and uniquely defined on directions).
	Then a continuous function $f \colon X \to Y$ that our functor sends to $h$ must also be the identity on the underlying sets of $X$ and $Y$.
	But such a function cannot be continuous, as the preimage under $f$ of a singleton set of $Y$, which is open, would be a singleton set of $X$, which would not be open. 
	So our functor sends no continuous function $X \to Y$ to $h$, and therefore is not full.
	
	The functor is, however, faithful: for any spaces $X$ and $Y$ and continuous function $f \colon X \to Y$, we can uniquely recover $f$ from $p_f$ by taking the on-positions function $(p_f)_\1$.
\end{enumerate}
\end{solution}
\end{exercise}

%-------- Section --------%
\section{Symmetric monoidal products of polynomial functors} \label{sec.poly.func_nat.monoidal}

One of the reasons $\poly$ is so versatile is that there is an abundance of monoidal structures on it.
Monoidal structures are the key ingredient to many applications of categories to real-world scenarios, and $\poly$ is no different in that regard.
But there is an added bonus about the monoidal products on $\poly$ that we'll introduce that isn't shared by some fancy monoidal products on other categories like ``the tensor product of modules over a commutative ring'': if you know how to add, multiply, and exponentiate expressions from high school algebra, then you already know how to compute these monoidal products!

We have already seen one monoidal structure on $\poly$: the cocartesian monoidal structure, which gives $\poly$ its finite coproducts.
In fact, we know from \cref{prop.poly_coprods} that $\poly$ has all coproducts---and they are given by an operation that looks just like addition.
We'll quickly see in \cref{subsec.poly.func_nat.monoidal.prod} that $\poly$ has all products as well, giving it a cartesian monoidal structure.
Can you guess what that operation looks like?

Much of \cref{part.comon} will focus on the astonishing features of another monoidal structure, an asymmetric one, which we will hold off on defining---we'll save its surprises for when we can better savor them.
But here in \cref{subsec.poly.func_nat.monoidal.par}, we will introduce a third symmetric monoidal structure, given by an operation you probably weren't allowed to do to polynomials back in high school.
This monoidal structure will really come in handy when we apply $\poly$ to dynamics in the next chapter.

%---- Subsection ----%
\subsection{The categorical product} \label{subsec.poly.func_nat.monoidal.prod}
The category $\poly$ has limits and colimits, is cartesian closed, has epi-mono factorizations, etc., etc. However, in order to tell a good story of dynamics, we only need products right now. These will be useful for letting many different interfaces control the same internal dynamics.

\begin{proposition}\label{prop.poly_prods}
The category $\poly$ has arbitrary products, coinciding with products in $[\smset,\smset]$ given by the operation $\prod_{i \in I}$.
\end{proposition}
\begin{proof}
Unsurprisingly, the proof is very similar to that of \cref{prop.poly_coprods}.

By \cref{ex.sum_prod_set_endofuncs}, the category $[\smset,\smset]$ has arbitrary products given by $\prod_{i \in I}$.
The full subcategory inclusion $\poly \to [\smset,\smset]$ reflects these products.
It remains to show that $\poly$ is closed under the operation $\prod_{i \in I}$.

By \cref{prop.set_endofunc_distrib}, $[\smset,\smset]$ is completely distributive.
Hence, given polynomials $(p_i)_{i \in I}$, we can use \eqref{eqn.cat_completely_distributive} to write their product in $[\smset,\smset]$ as
\begin{equation} \label{eqn.poly_prod}
    \prod_{i \in I} p_i \iso \prod_{i \in I} \sum_{j \in p_i(\1)} \yon^{p_i[j]} \iso \sum_{\bar{j} \in \prod_{i \in I} p_i(\1)} \prod_{i \in I} \yon^{p_i[\bar{j}(i)]} \iso \sum_{\bar{j} \in \prod_{i \in I} p_i(\1)} \yon^{\sum_{i \in I} p_i[\bar{j}(i)]},
\end{equation}
which, as a coproduct of representables, is in $\poly$.
% We will see that $\1$ is a terminal object and that the product of $p$ and $q$ in $\poly$ is the usual product of $p$ and $q$ as polynomials. That is, if $p\coloneqq\sum_{i\in p(\1)}\yon^{p[i]}$ and $q\coloneqq\sum_{j\in q(\1)}\yon^{q[j]}$ are in standard notation, then
% \begin{equation}\label{eqn.poly_times}
% p\times q\cong\sum_{i\in p(\1)}\sum_{j\in q(\1)}\yon^{p[i]+q[j]}.
% \end{equation}
% We leave the proof as an exercise; see \cref{exc.poly_times}.
\end{proof}

\begin{corollary} \label{prop.poly_completely_distributive}
The category $\poly$ is completely distributive.
\end{corollary}
\begin{proof}
This is a direct consequence of the fact that $\poly$ has arbitrary (co)products coinciding with (co)products in $[\smset,\smset]$ (\cref{prop.poly_coprods,prop.poly_prods}) and the fact that $[\smset,\smset]$ itself is completely distributive (\cref{prop.set_endofunc_distrib}).
\end{proof}

The result above will allow us to apply \eqref{eqn.cat_completely_distributive}, or sometimes specifically \eqref{eqn.push_prod_sum_obj_indep}, to push $\prod$'s past $\sum$'s of polynomials whenever we so desire.

\begin{exercise}%\label{exc.poly_prod}
% \begin{enumerate}
% 	\item Use \eqref{eqn.main_formula} to verify that $\1$ is terminal in $\poly$.
	
% 	\item Use \eqref{eqn.main_formula} and  \eqref{eqn.poly_times} to verify that
% 	\[
% 	    \poly(r, p \times q) \iso \poly(r, p) \times \poly(r, q)
% 	\]
% 	for all polynomials $p,q,r$.
	
% 	\item
	Use \eqref{eqn.main_formula}
% 	and \eqref{eqn.poly_prod}
	to verify that
	\[
	    \poly\left(q, \prod_{i \in I} p_i\right) \iso \prod_{i \in I} \poly(q, p_i)
	\]
	for all polynomials $(p_i)_{i \in I}$ and $q$, as one would expect from the universal property of products.
\qedhere
% \end{enumerate}
\begin{solution}
% \begin{enumerate}
    % \item To verify that $\1$ is terminal in $\poly$, we use \eqref{eqn.main_formula} to show that $\poly(p, \1) \iso \1$ for all $p \in \poly$:
    % \[
    %     \poly(p, \1) \iso \prod_{i \in p(\1)} \sum_{j \in \1} p[i]^\0 \iso \prod_{i \in p(\1)} \1 \iso \1.
    % \]
    
    % \item Given $p,q,r \in \poly$, we use \eqref{eqn.main_formula} and  \eqref{eqn.poly_times} to verify that
    % \begin{align*}
    %     \poly(r, p \times q) &\iso \poly\left(r, \sum_{i \in p(\1)} \sum_{j \in q(\1)} \yon^{p[i] + q[j]} \right)
    %     \tag*{\eqref{eqn.poly_times}} \\
    %     &\iso \prod_{k \in r(\1)} \sum_{i \in p(\1)} \sum_{j \in q(\1)} r[k]^{p[i] + q[j]} \tag*{\eqref{eqn.main_formula}} \\
    %     &\iso \prod_{k \in r(\1)} \left(\sum_{i \in p(\1)} r[k]^{p[i]}\right) \times \left(\sum_{j \in q(\1)} r[k]^{q[j]}\right)  \\
    %     &\iso \left(\prod_{k \in r(\1)} \sum_{i \in p(\1)} r[k]^{p[i]}\right) \times \left(\prod_{k \in r(\1)} \sum_{j \in q(\1)} r[k]^{q[j]}\right) \\
    %     &\iso \poly(r, p) \times \poly(r, q).
    %     \tag*{\eqref{eqn.main_formula}}
    % \end{align*}
    
    % \item
    Given $q \in \poly$ and $p_i \in \poly$ for each $i \in I$ for some set $I$, we use \eqref{eqn.main_formula} to verify that
    \begin{align*}
        \poly\left(q, \prod_{i \in I} p_i\right) &\iso \prod_{k \in q(\1)} \left(\prod_{i \in I} p_i\right)(q[k])
        \tag*{\eqref{eqn.main_formula}} \\
        &\iso \prod_{k \in q(\1)} \prod_{i \in I} p_i(q[k]) \\
        &\iso \prod_{i \in I} \prod_{k \in q(\1)} p_i(q[k]) \\
        &\iso \prod_{i \in I} \poly(q, p_i).
        \tag*{\eqref{eqn.main_formula}}
    \end{align*}
% \end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Let $p_1\coloneqq\yon+\1, p_2\coloneqq\yon+\2,$ and $p_3\coloneqq\yon^\2$.
What is $\prod_{i\in\3}p_i$ according to \eqref{eqn.poly_prod}? Is the answer what you would expect?
\begin{solution}
Given $p_1\coloneqq\yon+\1,p_2\coloneqq\yon+\2,$ and $p_3\coloneqq\yon^\2$, we compute $\prod_{i\in\3}p_i$ via \eqref{eqn.poly_prod} as follows:
\begin{align*}
    \prod_{i\in\3} p_i
    &\iso
    \sum_{\bar{j} \in \prod_{i\in\3} p_i(\1)} \yon^{\sum_{i\in\3} p_i[\bar{j}(i)]}
    \tag*{\eqref{eqn.poly_prod}} \\
    &\iso
    \sum_{\bar{j} \colon (i\in\3) \to p_i(\1)} \yon^{p_1[\bar{j}(1)] + p_2[\bar{j}(2)] + p_3[\bar{j}(3)]} \\
    &\iso
    \yon^{p_1[1] + p_2[1] + p_3[1]}
    + \yon^{p_1[1] + p_2[2] + p_3[1]}
    + \yon^{p_1[1] + p_2[3] + p_3[1]} \\
    &+ \yon^{p_1[2] + p_2[1] + p_3[1]}
    + \yon^{p_1[2] + p_2[2] + p_3[1]}
    + \yon^{p_1[2] + p_2[3] + p_3[1]} \\
    &\iso
    \yon^{\1 + \1 + \2}
    + \yon^{\1 + \0 + \2}
    + \yon^{\1 + \0 + \2} \\
    &+ \yon^{\0 + \1 + \2}
    + \yon^{\0 + \0 + \2}
    + \yon^{\0 + \0 + \2} \\
    % &\iso
    % \yon^\4 + \yon^\3 + \yon^\3 + \yon^\3 + \yon^\2 + \yon^\2 \\
    &\iso
    \yon^\4 + \3\yon^\3 + \2\yon^\2,
\end{align*}
as we might expect from standard polynomial multiplication.
\end{solution}
\end{exercise}

It follows from \eqref{eqn.poly_prod} that the terminal object of $\poly$ is $\1$, and that binary products are given by
\begin{equation}\label{eqn.poly_times}
    p \times q \iso \sum_{i \in p(\1)} \sum_{j \in q(\1)} \yon^{p[i] + q[j]}.
\end{equation}

We will sometimes write $pq$ rather than $p\times q$:
\begin{equation} \tag{Notation}
pq\coloneqq p\times q
\end{equation}

\begin{example}
We can draw the product of two polynomials in terms of their associated forests. Let $p\coloneqq\yon^\3+\yon$ and $q\coloneqq\yon^\4+\yon^\2+\1$.
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, blue!50!black, "$p$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$} 
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$} 
      child {};
  \end{tikzpicture}
  };
%
	\node (p2) [draw, red!75!black, right=2 of p1, "$q$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$} 
      child {}
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$} 
      child {}
      child {};
    \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$}
    ;
  \end{tikzpicture}
  };
\end{tikzpicture}
\]
Then $pq\cong\yon^\7+\2\yon^\5+\2\yon^\3+\yon$.
As arenas, we take all pairs of positions, and for each pair we take the disjoint union of the directions.
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, "$pq$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny {(1,1)}" below] (11) {$\bullet$} 
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 11, "\tiny {(1,2)}" below] (12) {$\bullet$} 
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 12, "\tiny {(1,3)}" below] (13) {$\bullet$} 
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[blue!50!black] {};
    \node[right=1.5 of 13, "\tiny {(2,1)}" below] (21) {$\bullet$} 
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 21, "\tiny {(2,2)}" below] (22) {$\bullet$} 
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 22, "\tiny {(2,3)}" below] (23) {$\bullet$} 
      child[blue!50!black] {};
	\end{tikzpicture}
	};
\end{tikzpicture}
\]
\end{example}

In practice, we can multiply polynomial functors the same way we would multiply two polynomials in high school algebra.

\begin{exercise} \label{exc.general_poly_times}
\begin{enumerate}
    \item \label{exc.general_poly_times.monomial} Show that for sets $A_1, B_1, A_2, B_2$, we have
    \[
        A_1\yon^{B_1} \times A_2\yon^{B_2} \iso A_1 A_2\yon^{B_1 + B_2}.
    \]
    \item \label{exc.general_poly_times.polynomial} Show that for sets $(A_i)_{i \in I}$ and $(B_j)_{j \in J}$, we have
    \[
        \left(\sum_{i \in I} A_i\yon^{B_i}\right) \times \left(\sum_{j \in J} A_j\yon^{B_j}\right) \iso \sum_{i \in I} \sum_{j \in J} A_i A_j \yon^{B_i + B_j}.
    \]
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We compute the product $A_1\yon^{B_1} \times A_2\yon^{B_2}$ using \eqref{eqn.poly_times}:
    \begin{align*}
        A_1\yon^{B_1} \times A_2\yon^{B_2} &\iso \left(\sum_{i \in A_1} \yon^{B_1}\right) \times \left(\sum_{j \in A_2} \yon^{B_2}\right) \\
        &\iso \sum_{i \in A_1} \sum_{j \in A_2} \yon^{B_1 + B_2} \\
        &\iso A_1 A_2\yon^{B_1 + B_2}.
    \end{align*}

    \item We expand the product $\left(\sum_{i \in I} A_i\yon^{B_i}\right) \times \left(\sum_{j \in J} A_j\yon^{B_j}\right)$ by applying \eqref{eqn.set_completely_distributive}, with $I_1 \coloneqq I$ and $I_2 \coloneqq J$:
    \begin{align*}
        \left(\sum_{i \in I} A_i\yon^{B_i}\right) \times \left(\sum_{j \in J} A_j\yon^{B_j}\right) &\iso \prod_{k \in \2} \sum_{i \in I_k} A_i\yon^{B_i} \\
        &\iso \sum_{\bar{i} \in \prod_{k \in \2} I_k} \prod_{k \in \2} A_{\bar{i}(k)}\yon^{B_{\bar{i}(k)}} \\
        &\iso \sum_{i \in I} \sum_{j \in J} A_i\yon^{B_i} \times A_j\yon^{B_j} \\
        &\iso \sum_{i \in I} \sum_{j \in J} A_i A_j \yon^{B_i + B_j}
    \end{align*}
    where the last isomorphism follows from \cref{exc.general_poly_times.monomial}.
\end{enumerate}
\end{solution}
\end{exercise}

As lenses, the canonical projections $\pi \colon pq \to p$ and $\phi \colon pq \to q$ behave as you might expect: on positions, they are the projections from $(pq)(\1) \iso p(\1) \times q(\1)$ to $p(\1)$ and $q(\1)$, respectively; on directions, they are the inclusions $p[i] \to p[i] + q[j]$ and $q[j] \to p[i] + q[j]$ for each position $(i, j)$ of $pq$.

\begin{exercise} \label{exc.poly_prod}
Verify that, for $p, q \in \poly$, the polynomial $pq$ given by \eqref{eqn.poly_times} along with the maps $\pi \colon pq \to p$ and $\phi \colon pq \to q$ described above satisfy the universal property of the product of $p$ and $q$.
\begin{solution}
We wish to show that, for $p, q \in \poly$, the polynomial $pq$ along with the maps $\pi \colon pq \to p$ and $\phi \colon pq \to q$ as described in the text satisfy the universal property of the product.
That is, we must show that for any $r \in \poly$ and maps $f \colon r \to p$ and $g \colon r \to q$, there exists a unique map $h \colon r \to pq$ for which the following diagram commutes:
\begin{equation} \label{eqn.prod_univ_prop}
\begin{tikzcd}
	r \ar[d, "f"'] \ar[r, "g"] \ar[dr, "h", dashed] & q \\
	p & pq. \ar[l, "\pi"] \ar[u, "\phi"'] 
\end{tikzcd}
\end{equation}
We apply \cref{ex.comm_poly}.
In order for \eqref{eqn.prod_univ_prop} to commute, it must commute on positions---that is, the following diagram of sets must commute:
\begin{equation} \label{eqn.prod_univ_prop_pos}
\begin{tikzcd}
	r(\1) \ar[d, "f_\1"'] \ar[r, "g_\1"] \ar[dr, "h_\1", dashed] & q(\1) \\
	p(\1) & (pq)(\1). \ar[l, "\pi_\1"] \ar[u, "\phi_\1"'] 
\end{tikzcd}
\end{equation}
But since $(pq)(\1) \iso p(1) \times q(\1)$ along with the projections $\pi_\1$ and $\phi_\1$ form the product of $p(\1)$ and $q(\1)$ in $\smset$, there exists a unique $h_\1$ for which \eqref{eqn.prod_univ_prop_pos} commutes.
Hence $h$ is uniquely characterized on positions.
In particular, it must send each $k \in r(\1)$ to the pair $(f_\1(k), g_\1(k)) \in (pq)(\1)$.

Moreover, if \eqref{eqn.coprod_univ_prop} is to commute on directions, then for every $k \in r(\1)$, the following diagram of sets must commute:
\begin{equation} \label{eqn.prod_univ_prop_dir}
\begin{tikzcd}[sep=large]
	r[k] & q[g_\1(k)] \ar[l, "g^\sharp_k"'] \ar[d, "\phi^\sharp_{(f_\1(k), g_\1(k))}"] \\
	p[f_\1(k)] \ar[u, "f^\sharp_k"] \ar[r, "\pi^\sharp_{(f_\1(k), g_\1(k))}"'] & (pq)[(f_\1(k), g_\1(k))]. \ar[ul, "h^\sharp_k"', dashed] 
\end{tikzcd}
\end{equation}
As $(pq)[(f_\1(k), g_\1(k))] \iso p[f_\1(k)] + q[g_\1(k)]$ along with the inclusions $\pi^\sharp_{(f_\1(k), g_\1(k))}$ and $\phi^\sharp_{(f_\1(k), g_\1(k))}$ form the coproduct of $p[f_\1(k)]$ and $q[g_\1(k)]$ in $\smset$, there exists a unique $h^\sharp_k$ for which \eqref{eqn.prod_univ_prop_dir} commutes.
Hence $h$ is also uniquely characterized on directions, so it is unique overall.
Moreover, we have shown that we can define $h$ on positions so that \eqref{eqn.prod_univ_prop_pos} commutes, and that we can define $h$ on directions such that \eqref{eqn.prod_univ_prop_dir} commutes.
As the commutativity of \eqref{eqn.prod_univ_prop_pos} and \eqref{eqn.prod_univ_prop_dir} together imply the commutativity of \eqref{eqn.prod_univ_prop}, it follows that there exists $h$ for which \eqref{eqn.prod_univ_prop} commutes.
\end{solution}
\end{exercise}

%---- Subsection ----%
\subsection{The parallel product} \label{subsec.poly.func_nat.monoidal.par}
There is a closely related monoidal structure on $\poly$ that will be useful for putting dynamical systems in parallel.

\begin{definition}[Parallel product of polynomials] \label{def.dirichlet}
Let $p$ and $q$ be polynomials. Their \emph{parallel product}, denoted $p\otimes q$, is given by the formula:
\begin{equation}\label{eqn.parallel_def}
p\otimes q\iso\sum_{i\in p(\1)}\sum_{j\in q(\1)}\yon^{p[i]\times q[j]}.
\end{equation}
% On arenas, this is defined by
% \begin{equation}\label{eqn.parallel_product_dependent}
% \sum_{i \in p(\1)}\yon^{p[i]} \otimes \sum_{j \in q(\1)}\yon^{q[j]} := \sum_{(i,j) \in p(\1) \times q(\1)}\yon^{p[i]\times q[j]}.
% \end{equation}
\end{definition}

One should compare this with the formula for the product of polynomials shown in \eqref{eqn.poly_times}. The difference is that the parallel product multiplies exponents where the categorical product adds them.

\begin{example}
We can draw the parallel product of two polynomials in terms of their associated forests. Let $p\coloneqq\yon^\3+\yon$ and $q\coloneqq\yon^\4+\yon^\2+\1$.
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, blue!50!black, "$p$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$} 
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$} 
      child {};
  \end{tikzpicture}
  };
%
	\node (p2) [draw, red!75!black, right=2 of p1, "$q$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$} 
      child {}
      child {}
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$} 
      child {}
      child {};
    \node[right=.5 of 2,"\tiny 3" below] (3) {$\bullet$}
    ;
  \end{tikzpicture}
  };
\end{tikzpicture}
\]
Then $p\otimes q\cong\yon^{\1\2}+\yon^\6+\yon^\4+\yon^\2+\2$.
As arenas, we take all pairs of positions, and for each pair we take the product of the directions.
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, "$p \otimes q$" above] {
	\begin{tikzpicture}[trees, sibling distance=2mm]
    \node["\tiny {(1,1)}" below] (11) {$\bullet$} 
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
    ;
    \node[right=2 of 11, "\tiny {(1,2)}" below] (12) {$\bullet$} 
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
    ;
    \node[right=1.5 of 12, "\tiny {(1,3)}" below] (13) {$\bullet$} 
    ;
   \node[right=1.5 of 13, "\tiny {(2,1)}" below] (21) {$\bullet$} 
      child {}
      child {}
      child {}
      child {}
 		;    
		\node[right=1.5 of 21, "\tiny {(2,2)}" below] (22) {$\bullet$} 
      child {}
      child {}
 		;    
    \node[right=1.5 of 22, "\tiny {(2,3)}" below] (23) {$\bullet$} 
 		;    
	\end{tikzpicture}
	};
\end{tikzpicture}
\]
\end{example}

% \begin{exercise}
% \begin{enumerate}
%     \item Compute the parallel product of monomials $A_1\yon^{B_1}\otimes A_2\yon^{B_2}$.
%     \item 
% \end{enumerate}
% \begin{solution}
% The parallel product of monomials $A_1\yon^{B_1}\otimes A_2\yon^{B_2}$ is $A_1 A_2\yon^{B_1 B_2}$. 
% \end{solution}
% \end{exercise}


\begin{exercise} \label{exc.general_poly_parallel_times}
\begin{enumerate}
    \item \label{exc.general_poly_parallel_times.monomial} Show that for sets $A_1, B_1, A_2, B_2$, we have
    \[
        A_1\yon^{B_1} \otimes A_2\yon^{B_2} \iso A_1 A_2\yon^{B_1 B_2}.
    \]
    \item \label{exc.general_poly_parallel_times.polynomial} Show that for sets $(A_i)_{i \in I}$ and $(B_j)_{j \in J}$, we have
    \[
        \left(\sum_{i \in I} A_i\yon^{B_i}\right) \otimes \left(\sum_{j \in J} A_j\yon^{B_j}\right) \iso \sum_{i \in I} \sum_{j \in J} A_i A_j \yon^{B_i B_j}.
    \]
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We compute the product $A_1\yon^{B_1} \otimes A_2\yon^{B_2}$ using \eqref{eqn.parallel_def}:
    \begin{align*}
        A_1\yon^{B_1} \otimes A_2\yon^{B_2} &\iso \left(\sum_{i \in A_1} \yon^{B_1}\right) \otimes \left(\sum_{j \in A_2} \yon^{B_2}\right) \\
        &\iso \sum_{i \in A_1} \sum_{j \in A_2} \yon^{B_1 \times B_2} \\
        &\iso A_1 A_2\yon^{B_1 B_2}.
    \end{align*}

    \item We expand the product $\left(\sum_{i \in I} A_i\yon^{B_i}\right) \otimes \left(\sum_{j \in J} A_j\yon^{B_j}\right)$ as follows:
    \begin{align*}
        \left(\sum_{i \in I} A_i\yon^{B_i}\right) \otimes \left(\sum_{j \in J} A_j\yon^{B_j}\right) &\iso \left(\sum_{i \in I} \sum_{i' \in A_i} \yon^{B_i}\right) \otimes \left(\sum_{j \in J} \sum_{j' \in A_j} \yon^{B_j}\right) \\
        &\iso \sum_{i \in I} \sum_{i' \in A_i} \sum_{j \in J} \sum_{j' \in A_j} \yon^{B_i \times B_j} \\
        &\iso \sum_{i \in I} \sum_{j \in J} \sum_{i' \in A_i} \sum_{j' \in A_j} \yon^{B_i B_j} \\
        &\iso \sum_{i \in I} \sum_{j \in J} A_i A_j \yon^{B_i B_j}.
    \end{align*}
\end{enumerate}
\end{solution}
\end{exercise}




\begin{exercise}
Let $p\coloneqq\yon^\2+\yon$ and $q\coloneqq\2\yon^\4$.
\begin{enumerate}
	\item Draw $p$ and $q$ as corolla forests.
	\item Draw $pq=p\times q$ as a corolla forest.
	\item Draw $p\otimes q$ as a corolla forest.
\qedhere
\end{enumerate}
\begin{solution}
Here $p\coloneqq\yon^\2+\yon$ and $q\coloneqq\2\yon^\4$.
\begin{enumerate}
\item Here are $p$ and $q$ drawn as corolla forests:
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, blue!50!black, "$p$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$} 
      child {}
      child {};
    \node[right=.5 of 1,"\tiny 2" below] (2) {$\bullet$} 
      child {};
  \end{tikzpicture}
  };
%
	\node (p2) [draw, red!75!black, right=2 of p1, "$q$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$} 
      child {}
      child {}
      child {}
      child {};
    \node[right=1 of 1,"\tiny 2" below] (2) {$\bullet$} 
      child {}
      child {}
      child {}
      child {};
  \end{tikzpicture}
  };
\end{tikzpicture}
\]

\item Here is $pq$ drawn as a corolla forest:
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, "$pq$" above] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny {(1,1)}" below] (11) {$\bullet$} 
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 11, "\tiny {(1,2)}" below] (12) {$\bullet$} 
      child[blue!50!black] {}
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 12, "\tiny {(2,1)}" below] (21) {$\bullet$} 
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
    \node[right=1.5 of 21, "\tiny {(2,2)}" below] (22) {$\bullet$} 
      child[blue!50!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {}
      child[red!75!black] {};
	\end{tikzpicture}
	};
\end{tikzpicture}
\]
\item Here is $p \otimes q$ drawn as a corolla forest:
\[
\begin{tikzpicture}[rounded corners]
	\node (p1) [draw, "$pq$" above] {
	\begin{tikzpicture}[trees, sibling distance=2mm]
    \node["\tiny {(1,1)}" below] (11) {$\bullet$} 
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
    ;
    \node[right=2 of 11, "\tiny {(1,2)}" below] (12) {$\bullet$} 
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
      child {}
    ;
   \node[right=1.5 of 12, "\tiny {(2,1)}" below] (21) {$\bullet$} 
      child {}
      child {}
      child {}
      child {}
 	;    
	\node[right=1.5 of 21, "\tiny {(2,2)}" below] (22) {$\bullet$} 
      child {}
      child {} 
      child {}
      child {}
 	;
	\end{tikzpicture}
	};
\end{tikzpicture}
\]
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Consider the polynomials $p\coloneqq \2\yon^\2+\3\yon$ and $q\coloneqq\yon^\4+\3\yon^\3$.
\begin{enumerate}
	\item What is $p\times q$?
	\item What is $p\otimes q$?
	\item What is the product of the following purely formal expression we'll see \emph{only this once!}:
	\[
	(2\mdot2^\yon+3\mdot 1^\yon) \cdot 
	(1\mdot4^\yon+3\mdot 3^\yon)
	\]
    The factors of the above product are called Dirichlet series.
	\item Describe the connection between the last two parts. (An alternative name we give for the parallel product $\otimes$ is the \emph{Dirichlet product}.) \qedhere
\end{enumerate}
\begin{solution}
Here $p\coloneqq \2\yon^\2+\3\yon$ and $q\coloneqq\yon^\4+\3\yon^\3$.
\begin{enumerate}
    \item We compute $p \times q$ using \cref{exc.general_poly_times} \cref{exc.general_poly_times.polynomial}:
    \begin{align*}
        p \times q &\iso \2\yon^{\2 + \4} + (\2 \times \3)\yon^{\2 + \3} + \3\yon^{\1 + \4} + (\3 \times \3)\yon^{\1 + \3} \\
        &\iso \2\yon^\6 + \6\yon^\5 + \3\yon^\5 + \9\yon^\4 \\
        &\iso \2\yon^\6 + \9\yon^\5 + \9\yon^\4.
    \end{align*}
    
    \item We compute $p \otimes q$ using \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.polynomial}:
    \begin{align*}
        p \otimes q &\iso \2\yon^{\2 \times \4} + (\2 \times \3)\yon^{\2 \times \3} + \3\yon^\4 + (\3 \times \3)\yon^\3 \\
        &\iso \2\yon^\8 + \6\yon^\6 + \3\yon^\4 + \9\yon^\3.
    \end{align*}
    
    \item We evaluate $(2\mdot2^\yon+3\mdot 1^\yon+1) \cdot 
	(1\mdot4^\yon+3\mdot 3^\yon+2)$ using standard high school algebra:
    \begin{align*}
	    (2\mdot2^\yon+3\mdot 1^\yon) \cdot (1\mdot4^\yon+3\mdot 3^\yon) &= 2\mdot1\mdot2^\yon\mdot4^\yon + 2\mdot3\mdot2^\yon\mdot3^\yon + 3\mdot1\mdot1^\yon\mdot4^\yon + 3\mdot3\mdot1^\yon\mdot3^\yon \\
	    &= 2\mdot8^\yon + 6\mdot6^\yon + 3\mdot4^\yon + 9\mdot3^\yon.
	\end{align*}
	
	\item We describe the connection between the last two parts as follows.
	Given a polynomial $p$, we let $d(p)$ denote the Dirichlet series $\sum_{i \in p(\1)} |p[i]|^\yon$.
	Then by \eqref{eqn.parallel_def},
	\begin{align*}
	    d(p \otimes q) &= \sum_{i \in p(\1)} \sum_{j \in q(\1)} |p[i] \times q[j]|^\yon \\
	    &= \sum_{i \in p(\1)} |p[i]|^\yon \sum_{j \in q(\1)} |q[j]|^\yon \\
	    &= d(p) \cdot d(q).
	\end{align*}
	The last two parts are simply an example of this identity for a specific choice of $p$ and $q$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
What is $(\3\yon^\5+\6\yon^\2)\otimes\4$? Hint: $\4=\4\yon^\0$.
\begin{solution}
We compute $(\3\yon^\5+\6\yon^\2)\otimes\4$ using \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.polynomial} and the fact that $\4=\4\yon^\0$:
\begin{align*}
    (\3\yon^\5+\6\yon^\2)\otimes\4\yon^\0 &\iso (\3\times\4)\yon^{\5\times\0} + (\6\times\4)\yon^{\2\times\0} \\
    &\iso \1\2\yon^\0 + \2\4\yon^\0 \\
    &\iso \3\6.
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.prepare_poly_smc}
Let $p,q,r\in\poly$ be any polynomials.
\begin{enumerate}
  \item Show that there is an isomorphism $p\otimes\yon\cong p$.
  \item Show that there is an isomorphism $(p\otimes q)\otimes r\cong p\otimes (q\otimes r)$.
  \item Show that there is an isomorphism $p\otimes q \cong q\otimes p$.
 \qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
  \item We show that $p\otimes\yon\cong p$:
  \begin{align*}
      p \otimes y &\iso \sum_{i \in p(\1)} \sum_{j \in \1} \yon^{p[i] \times \1} \tag*{\eqref{eqn.parallel_def}} \\
      &\iso \sum_{i \in p(\1)} \yon^{p[i]} \iso p.
  \end{align*}
  
  \item We show that $(p\otimes q)\otimes r\cong p\otimes (q\otimes r)$:
  \begin{align*}
      (p \otimes q) \otimes r &\iso \left(\sum_{i \in p(\1)} \sum_{j \in q(\1)} \yon^{p[i] \times q[j]}\right) \otimes r \tag*{\eqref{eqn.parallel_def}} \\
      &\iso \sum_{i \in p(\1)} \sum_{j \in q(\1)} \left(\sum_{k \in r(\1)} \yon^{(p[i] \times q[j]) \times r[k]}\right) \tag*{\eqref{eqn.parallel_def}} \\
      &\iso \sum_{i \in p(\1)} \left(\sum_{j \in q(\1)} \sum_{k \in r(\1)} \yon^{p[i] \times (q[j] \times r[k])}\right) \tag{Associativity of $\sum$ and $\times$} \\
      &\iso p \otimes \left(\sum_{j \in q(\1)} \sum_{k \in r(\1)} \yon^{q[j] \times r[k]}\right) \tag*{\eqref{eqn.parallel_def}} \\
      &\iso p \otimes (q \otimes r). \tag*{\eqref{eqn.parallel_def}} \\
  \end{align*}
  
  \item We show that $(p\otimes q)\cong(q\otimes p)$:
  \begin{align*}
      p \otimes q &\iso \sum_{i \in p(\1)} \sum_{j \in q(\1)} \yon^{p[i] \times q[j]} \tag*{\eqref{eqn.parallel_def}} \\
      &\iso \sum_{j \in q(\1)} \sum_{i \in p(\1)} \yon^{q[j] \times p[i]} \tag{Commutativity of $\sum$ and $\times$} \\
      &\iso q \otimes p. \tag*{\eqref{eqn.parallel_def}} \\
  \end{align*}
\end{enumerate}
\end{solution}
\end{exercise}

In \cref{exc.prepare_poly_smc}, we have gone most of the way to proving that $(\poly,\yon,\otimes)$ is a symmetric monoidal category. 

\begin{proposition}\label{prop.dirichlet_monoidal}
The category $\poly$ has a symmetric monoidal structure $(\yon,\otimes)$ where $\otimes$ is the parallel product from \cref{def.dirichlet}.
\end{proposition}
\begin{proof}[Sketch of proof]
Given lenses $f\colon p\to p'$ and $g\colon q\to q'$, we need to give a map $(f\otimes g)\colon (p\otimes q)\to (p'\otimes q')$. On positions, define
\[
(f\otimes g)_\1(i,j)\coloneqq \big(f_\1(i),g_\1(j)\big)
\]
On directions at $(i,j)\in p(\1)\times q(\1)$, define
\[
  (f\otimes g)^\sharp_{(i,j)}(d,e)\coloneqq
  \big(f^\sharp_i(d),g^\sharp_j(e)\big).
\]
Then \cref{exc.prepare_poly_smc} gives us the unitors, associator, and braiding.
We have not proven the functoriality of $\otimes$, the naturality of the isomorphisms from \cref{exc.prepare_poly_smc}, or all the coherences between these isomorphisms, but we ask the reader to take them on trust or to check them for themselves.
Alternatively, we may invoke the Day convolution to obtain the monoidal structure $(\yon, \otimes)$ directly (see \cref{prop.day}).
\end{proof}

\begin{exercise} \label{exc.some_parallel_prods}
\begin{enumerate}
	\item \label{exc.some_parallel_prods.const} If $p=A$ and $q=B$ are constant polynomials, what is $p\otimes q$?
	\item If $p=A$ is constant and $q$ is arbitrary, what can you say about $p\otimes q$?
	\item \label{exc.some_parallel_prods.lin} If $p=A\yon$ and $q=B\yon$ are linear polynomials, what is $p\otimes q$?
	\item \label{exc.some_parallel_prods.pos_prod} For arbitrary $p,q\in\poly$, what is the relationship between the sets $(p\otimes q)(\1)$ and $p(\1)\times q(\1)$?
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item By \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial}, we have $A \otimes B \iso AB$.
    \item We use \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.polynomial} to compute $A \otimes q$:
    \[
        A \otimes q \iso \sum_{j \in q(\1)} A\yon^{\0 \times q[i]} \iso \sum_{j \in q(\1)} A \iso A \times q(\1).
    \]
    \item By \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial}, we have $A\yon \otimes B\yon \iso AB\yon$.
    \item We show that $(p\otimes q)(\1)$ and $p(\1)\times q(\1)$ are isomorphic. By \eqref{eqn.parallel_def},
    \[
        (p \otimes q)(\1) \iso \sum_{i \in p(\1)} \sum_{j \in q(\1)} \1^{p[i] \times q[j]} \iso p(\1) \times q(\1).
    \]
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.dir_closed_classes}
Which of the following classes of polynomials are closed under $\otimes$? Note also whether they contain $\yon$.
\begin{enumerate}
	\item The set $\{A\yon^\0\mid A\in\smset\}$ of constant polynomials.
	\item The set $\{A\yon\mid A\in\smset\}$ of linear polynomials.
	\item The set $\{A\yon+B\mid A,B\in\smset\}$ of affine polynomials.
	\item The set $\{A\yon^\2+B\yon+C\mid A,B,C\in\smset\}$ of quadradic polynomials.
	\item The set $\{A\yon^B\mid A,B\in\smset\}$ of monomials.
	\item The set $\{S\yon^S\mid S\in\smset\}$ of systematic polynomials.
	\item The set $\{p\in\poly\mid p(\1)\text{ is finite}\}$. \qedhere
\end{enumerate}
\begin{solution}
For each of the following classes of polynomials, we determine whether they are closed under $\otimes$ and whether they contain $\yon$.
\begin{enumerate}
	\item The set $\{A\yon^\0\mid A\in\smset\}$ of constant polynomials is closed under $\otimes$ by the solution to \cref{exc.some_parallel_prods} \cref{exc.some_parallel_prods.const}.
	But the set does not contain $\yon$, as $\yon$ is not a constant polynomial.
	\item The set $\{A\yon\mid A\in\smset\}$ of linear polynomials is closed under $\otimes$ by the solution to \cref{exc.some_parallel_prods} \cref{exc.some_parallel_prods.lin} and does contain $\yon$, as $\yon \iso \1\yon$.
	\item The set $\{A\yon+B\mid A,B\in\smset\}$ of affine polynomials is closed under $\otimes$, for \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.polynomial} yields
	\[
	    (A\yon + B) \otimes (A'\yon + B') \iso AA'\yon + AB' + BA' + BB'.
	\]
	The set contains $\yon$, as $\yon \iso \1\yon + \0 $.
	\item The set $\{A\yon^\2+B\yon+C\mid A,B,C\in\smset\}$ of quadradic polynomials is not closed under $\otimes$, for even though $\yon^\2 \iso \1\yon^\2 + \0\yon + \0$ is a quadratic polynomial, \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial} implies that
	\[
	    \yon^\2 \otimes \yon^\2 \iso \yon^\4,
	\]
	which is not quadratic.
	The set contains $\yon$, as $\yon \iso \0\yon^\2 + \1\yon + \0$.
	\item The set $\{A\yon^B\mid A,B\in\smset\}$ of monomials is closed under $\otimes$ by \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial} and does contain $\yon$, as $\yon \iso \1\yon^\1$.
	\item The set $\{S\yon^S\mid S\in\smset\}$ of systematic polynomials is closed under $\otimes$, for \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial} returns
	\[
	    S\yon^S \otimes T\yon^T \iso ST\yon^{ST}.
	\]
	The set contains $\yon$, as $\yon \iso \1\yon^\1$.
	\item The set $\{p\in\poly\mid p(\1)\text{ is finite}\}$ is closed under $\otimes$ by the solution to \cref{exc.some_parallel_prods} \cref{exc.some_parallel_prods.pos_prod}.
	The set contains $\yon$, as $\yon(\1) \iso \1$ is finite.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
What is the smallest class of polynomials that's closed under $\otimes$ and contains $\yon$?
\begin{solution}
The smallest class of polynomials that's closed under $\otimes$ and contains $\yon$ is just $\{\yon\}$.
This is because by \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial}, we have $\yon \otimes \yon \iso \yon$.
\end{solution}
\end{exercise}

\begin{exercise}
Show that for any $p_1,p_2,q\in\poly$ there is an isomorphism
\[
(p_1+p_2)\otimes q\cong (p_1\otimes q)+(p_2\otimes q).
\qedhere
\]
\begin{solution}
We show that $(p_1 + p_2) \otimes q \iso (p_1 \otimes q) + (p_2 \otimes q)$ using \eqref{eqn.parallel_def}:
\begin{align*}
    (p_1 + p_2) \otimes q &\iso \sum_{k \in \2} \sum_{i \in p_k(\1)} \sum_{j \in q(\1)} \yon^{p_k[i] \times q[j]} \\
    &\iso \sum_{i \in p_1(\1)} \sum_{j \in q(\1)} \yon^{p_1[i] \times q[j]} + \sum_{i \in p_2(\1)} \sum_{j \in q(\1)} \yon^{p_2[i] \times q[j]} \\
    &\iso (p_1 \otimes q) + (p_2 \otimes q).
\end{align*}
\end{solution}
\end{exercise}

\begin{proposition} \label{prop.day}
For any monoidal structure $(I,\star)$ on $\smset$, there is a corresponding monoidal structure $(\yon^I, \odot)$ on $\poly$, where $\odot$ is the Day convolution.
Moreover, $\odot$ distributes over coproducts. 

In the case of $(\0,+)$ and $(\1,\times)$, this procedure returns the $(\1,\times)$ and $(\yon,\otimes)$ monoidal structures respectively.
\end{proposition}
\begin{proof}
Any monoidal structure $(I, \odot)$ on $\smset$ induces a monoidal structure on $[\smset,\smset]$ with the Day convolution $\odot$ as the tensor product and $\yon^I$ as the unit.
To prove that this monoidal structure restricts to $\poly$, it suffices to show that $\poly$ is closed under the Day convolution.

Given polynomials $p\coloneqq\sum_{i \in p(\1)} \yon^{p[i]}$ and $q\coloneqq\sum_{j \in q(\1)} \yon^{q[j]}$, their Day convolution is given by the coend
\begin{equation} \label{eqn.day_conv.coend}
    p \odot q \iso \int^{(A,B)\in\smset^\2} \yon^{A \star B} \times p(A) \times q(B).
\end{equation}
We can rewrite the product $p(A) \times q(B)$ as
\[
    p(A) \times q(B) \iso \left(\sum_{i \in p(\1)} A^{p[i]}\right) \times \left(\sum_{j \in q(\1)} B^{q[i]}\right) \iso \sum_{(i,j) \in p(\1) \times q(\1)} A^{p[i]} \times B^{q[i]} %\\
    % &\iso \sum_{(i,j) \in p(\1) \times q(\1)} \smset(p[i], A) \times \smset(q[j], B) \\
    % &\iso \sum_{(i,j) \in p(\1) \times q(\1)} \smset^\2((p[i], q[j]), (A, B))
\]
So because products distribute over coproducts in $\smset$ and coends commute with coproducts, we can rewrite \eqref{eqn.day_conv.coend} as
\[
    p \odot q \iso \sum_{(i,j) \in p(\1) \times q(\1)} \int^{(A,B)\in\smset^\2} \yon^{A \star B} \times A^{p[i]} \times B^{q[i]},
\]
which, by the co-Yoneda lemma, can be rewritten as
\begin{equation} \label{eqn.day_conv.poly}
    p \odot q \iso \sum_{(i,j) \in p(\1) \times q(\1)} \yon^{p[i] \star q[j]}
\end{equation}
in $\poly$.
That the Day convolution distributes over coproducts also follows from the fact that products distribute over coproducts in $\smset$ and coends commute with coproducts; or, alternatively, directly from \eqref{eqn.day_conv.poly}.

We observe that \eqref{eqn.day_conv.poly} gives $(\yon^I, \odot) = (\1, \times)$ when $(I, \star) = (\0, +)$ and $(\yon^I, \odot) = (\yon, \otimes)$ when $(I, \star) = (\1, \times)$.
\end{proof}

\begin{exercise}
\begin{enumerate}
	\item Show that the operation $(A, B)\mapsto A+AB+B$ on $\smset$ is associative.
	\item Show that $\0$ is unital for the above operation.
	\item Let $(\1,\odot)$ denote the corresponding monoidal structure on $\poly$. Compute the monoidal product $(\yon^\3+\yon)\odot(\2\yon^\2+\2)$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item To show that the operation $(A, B)\mapsto A+AB+B$ on $\smset$ is associative, we observe that
    \begin{align*}
        (A + AB + B) + (A + AB + B)C + C &\iso A + AB + B + AC + ABC + BC + C \\
        &\iso A + AB + ABC + AC + B + BC + C \\
        &\iso A + A(B + BC + C) + (B + BC + C).
    \end{align*}
    \item To show that $\0$ is unital for this operation, we observe that
    \[
        (A, \0) \mapsto A + A\0 + \0 \iso A
    \]
    and
    \[
        (\0, B) \mapsto \0 + \0B + B \iso B.
    \]
    \item We let $(\1,\odot)$ denote the corresponding monoidal product on $\poly$ and evaluate $(\yon^\3+\yon)\odot(\2\yon^\2+\2)$.
    By \eqref{eqn.day_conv.poly}, with $A \star B \iso A + AB + B$, we have
    \begin{align*}
        (\yon^\3+\yon)\odot(\2\yon^\2+\2) &\iso (\yon^\3+\yon^\1)\odot(\yon^\2+\yon^\2+\yon^\0+\yon^\0) \\
        &\iso \yon^{\3\star\2} + \yon^{\3\star\2} + \yon^{\3\star\0} + \yon^{\3\star\0} + \yon^{\1\star\2} + \yon^{\1\star\2} + \yon^{\1\star\0} + \yon^{\1\star\0} \\
        &\iso \2\yon^{\1\1} + \2\yon^\3 + \2\yon^\5 + \2\yon^\1.
    \end{align*}
\end{enumerate}
\end{solution}
\end{exercise}

\begin{remark}
Monoids in $\poly$ with respect to the parallel product $\otimes$ are particularly interesting---they have a kind of collective semantics, letting agents aggregate their contributions and distribute returns on those contributions in a coherent way.
We leave discussion of them to future work, so as not to distract us from our main story.
\end{remark}

% %-------- Section --------%
\section{Summary and further reading}

Thanks to Joachim Kock for telling us about the derivative $\dot{p}$ of a polynomial and the relationship between $\dot{p}(\1)$ and the total number of leaves of $p$.

%-------- Section --------%
\section{Exercise solutions}
\Closesolutionfile{solutions}
{\footnotesize
\input{solution-file2}}

\Opensolutionfile{solutions}[solution-file3]

%------------ Chapter ------------%
\chapter{Dynamical systems as polynomial~morphisms} \label{ch.poly.dyn_sys}

Let's start putting all this $\poly$ stuff to use. 

%-------- Section --------%
\section{Moore machines}

We begin with our simplest example of a dynamical system: a deterministic machine with internal states that can return output and be updated according to input.

\begin{definition}[Moore machine]\label{def.moore_machine}
If $A$, $B$, and $S$ are sets, an $(A,B)$-\emph{Moore machine} with \emph{states} $S$ consists of two functions
\begin{align*}
	\text{return}&\colon S\to B\\
	\text{update}&\colon S\times A\to S 
\end{align*}
\end{definition}

We can visualize a Moore machine as follows.
It has a set $S$ of possible states.
At any point in time, the machine is in one of those states: say $s \in S$.
While there, whenever we ask the machine to produce output, it will give us $\text{return}(s) \in B$.
But if we feed the machine input $a \in A$, the machine will switch to a new state, $\text{update}(s, a)$.
Note that this new state depends not only on the input the machine receives, but also on the state the machine is in when it receives that input.

\begin{example}\label{ex.Moore_three}
Here's a labeled transition diagram of a Moore machine with $S\coloneqq\3$ states:
\begin{equation} \label{eqn.trans_diag}
\begin{tikzpicture}
	\node[draw] {
  \begin{tikzcd}[column sep=small]
  	\LMO{b_1}\ar[rr, dgreen, thick, bend left]\ar[loop left, thick, orange]&&
  	\LMO{b_2}\ar[ll, thick, orange, bend left]\ar[dl, bend left, thick, dgreen]\\&
  	\LMO{b_2} \ar[ul, thick, orange, bend left] \ar[loop left, thick, dgreen]
  \end{tikzcd}
  };
\end{tikzpicture}
\end{equation}
Each state is labeled by the output value it returns: an element of $B\coloneqq\{b_1,b_2\}$. Each state has two outgoing arrows, one orange and one green, representing the two possible inputs, so $A\coloneqq\{{\color{orange}\text{orange}},{\color{dgreen}\text{green}}\}$.
The targets of the arrows indicate the updated state.

For example, let's say the machine starts at the bottom state.
We imagine barking a sequence of inputs---say ``{\color{orange}orange! orange!} {\color{dgreen}green!} {\color{orange}orange!} \ldots''---at this machine to make it run through its states and return the output at each state:
\begin{enumerate}
    \item Starting at the bottom state, the machine returns the output $b_2$.
    \item Receiving the input ``{\color{orange}orange}'' at the bottom state, the machine follows the {\color{orange}orange} arrow from the bottom state to update its state to the left state.
    \item At the left state, the machine returns the output $b_1$.
    \item Receiving the input ``{\color{orange}orange}'' at the left state, the machine follows the {\color{orange}orange} arrow from the left state to update its state to---once again---the left state.
    \item At the left state, the machine returns the output $b_1$.
    \item Receiving the input ``{\color{dgreen}green}'' at the left state, the machine follows the {\color{dgreen}green} arrow from the left state to update its state to the right state.
    \item At the right state, the machine returns the output $b_2$.
    \item Receiving the input ``{\color{orange}orange}'' at the left state, the machine follows the {\color{orange}orange} arrow from the right state to update its state to the left state.
    \item At the left state, the machine returns the output $b_1$.
    
    \ldots
    
    So given a fixed initial state, namely the bottom state, this Moore machine sends the sequence $({\color{orange}\text{orange}}, {\color{orange}\text{orange}}, {\color{dgreen}\text{green}}, {\color{orange}\text{orange}},\ldots)$ of elements in $A$ to the sequence $(b_2,b_1,b_1,b_2,b_1,\ldots)$ of elements in $B$.
\end{enumerate}
\end{example}

In general, given an initial state $s_0\in S$, an $(A,B)$-Moore machine with states $S$ sends every sequence $(a_1,a_2,a_3,\ldots)$ of elements in $A$ to a sequence $(b_0,b_1,b_2,b_3,\ldots)$ of elements in $B$, defined as follows via an intermediary sequence $(s_0,s_1,s_2,s_3,\ldots)$:
\[
    b_k\coloneqq \text{return}(s_k) \qqand s_{k+1}\coloneqq \text{update}(s_k, a_{k+1})
\]
for all $k\in\nn$.
We'll see that $\poly$ gives us a more concise way to express this in \cref{**}.

\subsection{Moore machines as lenses}

Does \cref{def.moore_machine} look familiar?
It's easy to see that an $(A,B)$-Moore machine with states $S$ is just a lens between monomials
\[
(\text{get}, \text{set})\colon S\yon^S\to B\yon^A,
\]
with $\text{get} \coloneqq \text{return}$ and $\text{set} \coloneqq \text{update}$.\footnote{Recall from \eqref{eqn.bimorphic_lens} that ``get'' and ``set'' are our special names for the on-positions and on-directions functions of a lens between monomials.}
Given such a Moore machine, we will call the monomial $B\yon^A$ the \emph{interface}, because it encodes how an outsider can interact with the machine, namely the possible inputs and outputs; and we will call the monomial $S\yon^S$ the \emph{state system}.

\begin{exercise}
Write the Moore machine from \cref{ex.Moore_three} as a lens between monomials.
\begin{enumerate}
    \item What is the state system?
    \item What is the interface?
\end{enumerate}
Call the left state $1$, the right state $2$, and the bottom state $3$.
\begin{enumerate}[resume]
    \item What is the on-positions function ``get''?
    \item What is the on-directions function ``set''? \qedhere
\end{enumerate}
\begin{solution}
We can write the Moore machine from \cref{ex.Moore_three} as a lens $S\yon^S\to B\yon^A$ between monomials as follows.
\begin{enumerate}
    \item As $S=\3$, the state system is $S\yon^S=\3\yon^\3$.
    \item As $B=\{b_1,b_2\}$ and $A=\{{\color{orange}\text{orange}},{\color{dgreen}\text{green}}\}$, the interface is $B\yon^A=\{b_1,b_2\}\yon^{\{{\color{orange}\text{orange}},\,{\color{dgreen}\text{green}}\}}$.
    \item The on-positions function ``get'' sends $1\mapsto b_1, 2\mapsto b_2,$ and $3\mapsto b_2$.
    \item The on-directions function ``set'' sends
    \begin{align*}
        (1, {\color{orange}\text{orange}})\mapsto1&,\quad(1, {\color{dgreen}\text{green}})\mapsto2, \\
        (2, {\color{orange}\text{orange}})\mapsto1&,\quad(2, {\color{dgreen}\text{green}})\mapsto3, \\
        (3, {\color{orange}\text{orange}})\mapsto1&,\quad(3, {\color{dgreen}\text{green}})\mapsto3.
    \end{align*}
\end{enumerate}
\end{solution}
\end{exercise}

\subsection{More Moore machines}

\begin{example}\label{ex.counting_trajectory}
There is a dynamical system that takes unchanging input and produces as output the sequence of natural numbers $0,1,2,3,\ldots.$ It is a Moore machine with states $\nn$ and interface $\nn\yon$. The associated lens $\nn\yon^\nn\to\nn\yon$ is given by the identity $\nn \to \nn$ on positions and the function $\nn \iso \nn \times \1 \to \nn$ sending $n\mapsto n+1$ on directions.
% \[
% \begin{tikzpicture}[polybox, mapstos]
% 	\node[poly, dom] (s) {$n+1$\nodepart{two}$n$};
% 	\node[poly, cod, linear, right=of s] (p) {$\vphantom{1}$\nodepart{two}$n$};
% 	\draw (s_pos) to[first] (p_pos);
% 	\draw (p_dir) to[last] (s_dir);
% \end{tikzpicture}
% \]
\end{example}

\begin{example}\label{ex.R2_moore}
Here's a(n infinite) Moore machine with states $\rr^\2$:
\[
\rr^\2\yon^{\rr^\2}\to\rr^\2\yon^{[0,1]\times[0,2\pi)}%]
\]
Its output type is $\rr^\2$, which we might think of as a location in the 2-dimensional plane, and its input type is $[0,1]\times[0,2\pi)$, which we can think of as a command to move a certain distance in a certain direction. The map itself is given by the lens
\begin{align*}
  \rr^\2&\To{\text{return}}\rr^\2&
  \rr^\2\times[0,1]\times[0,2\pi)&\To{\text{update}}\rr^\2
  	%\nonumber\\\label{eqn.r2moore}
  	\\
  (x,y)&\mapsto(x,y)&
  (x,y,r,\theta)&\mapsto(x+r\cos\theta, y+r\sin\theta)
\end{align*}
\end{example}

\begin{exercise}
Explain in words what the Moore machine in \cref{ex.R2_moore} does.
\begin{solution}
At any point in time, the Moore machine in \cref{ex.R2_moore} is located somewhere on the $2$-dimensional plane, say at the coordinates $(x, y) \in \rr^\2$.
This location is its current state.
Whenever we ask the machine to produce output, it will tell us those coordinates, since $\text{return}(x, y) = (x, y)$.
But if we give the machine input of the form $(r, \theta)$ for some distance $r \in [0,1]$ and direction $\theta \in [0, 2\pi)$, the machine will move by that distance, in that direction, going from $(x, y)$ to
\[
    \text{update}(x,y,r,\theta) = (x,y) + r(\cos\theta, \sin\theta)
\]
(here we treat $\rr^\2$ as a vector space, so that $r(\cos\theta, \sin\theta)$ is a vector of length $r$ in the direction of $\theta$).
\end{solution}
\end{exercise}

\begin{example}[From functions to memoryless Moore machines]\label{ex.funs_to_moore}
For any function $f\colon A\to B$, there is a corresponding $(A,B)$-Moore machine with states $B$ that takes in an element of $A$ and returns the element of $B$ obtained by applying $f$. 

It is given by the map $(\id_B, \pi_A\then f)\colon B\yon^B\to B\yon^A$. That is, it is the identity on positions, returning the state directly as output, and on directions it is the function $B\times A\To{\pi_A}A\To{f} B$, which ignores the current state and applies $f$ to the input to compute the new state.

If the machine begins in state $b_0$ and is given a sequence $(a_1,a_2,\ldots)$ of elements in $A$, the machine's outputs will be $(b_0,f(a_1),f(a_2),\ldots)$. We could say this machine is \emph{memoryless}, because at no point does the state of the machine depend on any previous states.
\end{example}

\begin{exercise}\label{exc.funs_to_moore}
Suppose we have a function $f\colon A\times B\to B$.
\begin{enumerate}
	\item Find a corresponding $(A,B)$-Moore machine $B\yon^B\to B\yon^A$.
	\item Would you say the machine is memoryless?
	\item Now for any function $g\colon A\to B$, give a corresponding $(A,B)$-Moore machine $B\yon^B\to B\yon^A$ by first turning $g\colon A\to B$ into a function $A\times B\to B$ that discards its second input.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We seek an $(A,B)$-Moore machine $B\yon^B\to B\yon^A$ corresponding to the function $f\colon A\times B\to B$.
    We know that an $(A,B)$-Moore machine $B\yon^B \to B\yon^A$ consists of a return function $B \to B$ and an update function $B \times A \to B$.
    So we can simply let the return function be the identity on $B$ and the update function be $B \times A \iso A \times B \To{f} B$, i.e.\ the function $f$ with its inputs swapped.
    
    \item Generally, such a machine is not memoryless.
    Unlike in \cref{ex.funs_to_moore}, the update function $B \times A \iso A \times B \To{f} B$ does appear to depend on its first input, namely the previous state, which $f$ takes as its second input.
    
    However, if $f$ factors through the projection $\pi_A \colon A \times B \to A$, i.e.\ if $f$ can be written as a composite $A \times B \To{\pi_A} A \To{f'} B$ for some $f' \colon A \to B$, then the resulting machine \emph{is} memoryless: it is the memoryless Moore machine from \cref{ex.funs_to_moore} corresponding to $f'$.
    
    \item Given a function $g\colon A\to B$, we can compose it with the projection $\pi_A\colon A\times B\to A$ to obtain a function $A\times B\To{\pi_A}A\To{g}B$, which sends its first input through $g$ and discards its second input.
    Then the corresponding $(A,B)$-Moore machine $B\yon^B \to B\yon^A$ has the identity function on $B$ as its return function and the composite $B\times A\To{\pi_A}A\To{g}B$, which discards its first input and sends its second input through $g$, as its update function.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Find $A,B\in\smset$ such that the following can be identified with a lens $S\yon^S\to B\yon^A$, and explain in words what the corresponding Moore machine does (there may be multiple possible solutions):
\begin{enumerate}
	\item a \emph{discrete dynamical system}, i.e.\ a set of states $S$ and a transition function $S\to S$ that tells us how to move from state to state.
	\item a \emph{magma}, i.e.\ a set $S$ and a function $S\times S\to S$.
	\item a set $S$ and a subset $S'\ss S$.\qedhere
\end{enumerate}
\begin{solution}
For each of the following constructs, we find $A, B \in \smset$ such that the construct can be identified with a lens $S\yon^S \to B\yon^A$, i.e.\ a function $\text{return} \colon S \to B$ and a function $\text{update} \colon S \times A \to S$.
\begin{enumerate}
    \item Given a discrete dynamical system with states $S$ and transition funtion $n \colon S \to S$, we can set $A \coloneqq B \coloneqq \1$.
    Then $\text{return} \colon S \to \1$ is unique, while $\text{update} \colon S \times \1 \to S$ is given by $S \times 1 \iso S \To{n} S$.
    The corresponding Moore machine has unchanging input (you could think of it as a button that says ``advance to the next state'') and unchanging output (which effectively tells us nothing).
    So it is just a set of states, and a deterministic way to move from state to state.
    
    We could have also set $A \coloneqq \0$ and $B \coloneqq S$, so that $\text{return} \coloneqq n$ and $\text{update} \colon S \times \0 \to S$ is unique, but this formulation is somewhat less satisfying: this is a Moore machine that never moves between its states, effectively functioning as a lookup table between whatever state the machine happens to be in and its output, which happens to refer to some state.
    
    \item Given a magma consisting of a set $S$ and a function $m \colon S \times S \to S$, we can set $A \coloneqq S$ and $B \coloneqq 1$.
    Then $\text{return} \colon S \to \1$ is unique, while $\text{update} \colon S \times S \to S$ is equal to $m$.
    The corresponding Moore machine produces unchanging output.
    It uses the binary operation $m$ to combine the current state with the input---which also refers to a state---to obtain the new state.
    
    Alternatively, we could have set the update function to be $m$ with its inputs swapped.
    The difference here is that the new state is given by applying $m$ with the input on the left and the current state on the right, rather than the other way around.
    If $m$ is noncommutative, this would yield a different Moore machine.
    
    We could have also set $A \coloneqq \0$ and $B \coloneqq S^S$, so that $\text{update} \colon S \times \0 \to S$ is unique, while currying $m$ gives $\text{return}$, so that $\text{return}(s)$ is the function $S \to S$ given by $s' \mapsto m(s, s')$.
    Alternatively, $\text{return}(s)$ could be the function $s' \mapsto m(s', s)$.
    Either way, this is a Moore machine that never moves between its states, functioning as a lookup table between the machine's current state and the function $m$ partially applied to that state on one side or the other.
    
    \item Given a set $S$ and a subset $S' \ss S$, we can set $A \coloneqq \0$ and $B \coloneqq \2$.
    Then $\text{update} \colon S \times \0 \to S$ is unique, while we define $\text{return} \colon S \to \2$ by
    \[
        \text{return}(s) =
        \begin{cases}
            1 & \text{if } s \in S' \\
            2 & \text{if } s \notin S',
        \end{cases}
    \]
    so that $S'$ can be recovered from the return function as its fiber over $1$.
    The corresponing Moore machine never moves between its states, but gives one of two outputs indicating whether or not the current state is in the subset $S'$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Consider the Moore machine in \cref{ex.R2_moore}, and think of it as a robot. Using the terminology from that example, modify the robot as follows.

Add to its state a ``health meter,'' which has a real value between 0 and 10. Make the robot lose half its health whenever it moves to a location whose $x$-coordinate is negative. Do not output its health; instead, use its health $h$ as a multiplier, allowing it to move a distance of $hr$ given an input of $r$.
\begin{solution}
We modify the Moore machine from \cref{ex.R2_moore} as follows.
The original Moore machine had states $\rr^\2$, so to add a health meter with values in $[0,10]$, we take the cartesian product to obtain the new set of states $\rr^\2 \times [0,10]$.
The inputs and outputs are unchanged, so the Moore machine is a lens
\[
    \rr^\2 \times [0,10] \yon^{\rr^\2 \times [0,10]} \to \rr^\2 \yon^{[0,1] \times [0,2\pi)}.
\]
Its return function $\rr^\2 \times [0,10] \to \rr^\2$ is the canonical projection, as the machine only outputs its location in $\rr^\2$ and not its health; while its update function
\[
    \rr^\2 \times [0,10] \times [0,1] \times [0,2\pi) \to \rr^\2 \times [0,10]
\]
sends $(x, y, h, r, \theta)$ to
\[
    (x + hr\cos\theta, y + hr\sin\theta, h'),
\]
where $h' = h/2$ if the machine's new $x$-coordinate $x + hr\cos\theta < 0$ and $h' = h$ otherwise.
\end{solution}
\end{exercise}

\begin{exercise}[Tape of a Turing machine]
A Turing machine has a tape. The tape has a cell for each integer, and each cell holds a value $v\in V=\{0,1,-\}$ of 0,1, or blank. At any given time the tape not only holds this function $f\colon\zz\to V$ from cell numbers to values, but also a distinguished choice $c\in\zz$ of the ``current'' cell. Thus the set of states of the tape is $V^\zz\times\zz$.

The Turing machine interacts with the tape by asking for the value at the current cell, an element of $V$, and by telling it to change the value there as well as whether to move left (i.e.\ decrease the current cell number by $1$) or right (i.e.\ increase by $1$). Thus the set of outputs of the tape is $V$ and the set of inputs is $V\times\{L,R\}$.

\begin{enumerate}
	\item Give the form of the tape as a Moore machine, i.e.\ lens $t\colon S\yon^S\to B\yon^A$ for appropriate sets $S, A, B$.
	\item Write down the specific $t$ that makes it act like a tape as specified above.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item The tape of a Turing machine has states $V^\zz \times \zz$, outputs $V$, and inputs $V \times \{L, R\}$, so as a Moore machine, it is a lens
    \[
        t \colon (V^\zz \times \zz)\yon^{V^\zz \times \zz} \to V\yon^{V \times \{L,R\}}.
    \]
    \item The return function of $t$ should output the value at the current cell of the tape. So $\text{return} \colon V^\zz \times \zz \to V$ is the evaluation map: it sends $(f,c)$ with $f \colon \zz \to V$ and $c \in \zz$ to $f(c)$.
    Then the update function of $t$ should write the input value of $V$ in the current cell of the tape, then shift the cell number down or up one according to whether the second input value is $L$ (left) or $R$ (right).
    So
    \[
        \text{update} \colon (V^\zz \times \zz) \times (V \times \{L,R\}) \to V^\zz \times \zz
    \]
    sends old tape $f \colon \zz \to V$, old cell number $c \in \zz$, new value $v \in V$, and direction $D \in \{L,R\}$ to the new tape $f' \colon \zz \to V$ defined by
    \[
        f'(n)\coloneqq
        \begin{cases}
            v & \text{if } n = c \\
            f(n) & \text{if } n \neq c
        \end{cases}
    \]
    and the new cell number $c-1$ if $D=L$ and $c+1$ if $D = R$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.file_reader}
Let's say a file of length $n$ is a function $f\colon\ord{n}\to\Set{ascii}$, where $\Set{ascii}\coloneqq\2\5\6$.
We refer to elements of $\ord{n}=\{1,\ldots,n\}$ as entries in the file and, for each entry $i \in \ord{n}$, the value $f(i)$ as the character at entry $i$.

Given a file $f$, make a file-reading Moore machine whose output type is $\Set{ascii} + \{\text{``done''}\}$
and whose input type is
\[
\{(s,t)\mid 1\leq s\leq t\leq n\}+\{\text{``continue''}\}.
\]
For any input, if it is of the form $(s,t)$, then the file-reader should go to entry $s$ in the file and read the character at that entry.
If the input is ``continue,'' the file-reader should move to the next entry (i.e.\ from $s$ to $s+1$) and read that characterunless the new entry would be greater than $t$, in which case the file-reader should continually output ``done'' until it receives another $(s,t)$ pair.
\begin{solution}
Given a file $f \colon \ord{n} \to \Set{ascii}$, we construct our file-reader as a Moore machine as follows.
There are many options for what states the machine should record, but we will use pairs of values $(i, t) \in \ord{n}^\2$, where $i$ is the entry where the file-reader is currently located, while $t$ is the entry where the file-reader should stop.
But we will also include a ``stopped'' state to record when the file-reader has already stopped.
So the Moore machine is a lens
\[
    (\ord{n}^\2+\{\text{``stopped''}\})\yon^{\ord{n}^\2+\{\text{``stopped''}\}} \to (\Set{ascii} + \{\text{``done''}\})\yon^{\{(s,t)\mid 1\leq s\leq t\leq n\}+\{\text{``continue''}\}}.
\]
If the file-reader's current state is ``stopped,'' then the file-reader should output ``done.'' Otherwise, the file-reader should output the character at the entry where the file-reader is currently located.
So its return function $\ord{n}^\2+\{\text{``done''}\} \to \Set{ascii} + \{\text{``done''}\}$ sends $(i, t)$ to $f(i)$ and ``done'' to ``done.''
Meanwhile, the update function
\[
    (\ord{n}^\2+\{\text{``done''}\}) \times (\{(s,t)\mid 1\leq s\leq t\leq n\}+\{\text{``continue''}\}) \to \ord{n}^\2+\{\text{``done''}\}
\]
sends any state with input $(s,t)$ to the state $(s,t)$.
On the other hand, if the input is ``continue,'' an old state $(i,t)$ is sent to the new state $(i+1,t)$ if $i + 1 \leq t$ and ``done'' otherwise.
Finally, if the old state is ``done'' and the input is ``continue,'' the new state is still ``done.''
\end{solution}
\end{exercise}

While \cref{exc.file_reader} gives us a functioning file-reader, it is a little strange that we are still able to give the input ``continue'' even when the output is ``done,'' or input a new range of entries before the file-reader has finished reading from the previous range.
In \cref{sec.poly.dyn_sys.depend_sys}, we'll introduce a generalization of Moore machines to handle cases like these, where the array of inputs the machine can receive changes depending on the output that it just returned.
In particular, we will be able to let the file-reader ``close its port,'' so that it can't receive signals while it's busy reading, but open its port again once it's ``done''; see \cref{ex.generalized_file_reader}.

\subsubsection{Deterministic state automata}

% Regular languages are very important in computer science. One way to express what they are is to say that they are exactly the languages recognizable by a deterministic state automaton. What is that?

The diagram in \cref{ex.Moore_three} may look familiar to those who have studied automata theory; in fact, a deterministic state automaton can be expressed as a Moore machine (with a preset initial state).

\begin{definition}[Deterministic state automaton, language]\label{def.dfa}
A \emph{deterministic state automaton} consists of
\begin{enumerate}
	\item a set $S$, elements of which are called \emph{states};
	\item a set $A$, elements of which are called \emph{input symbols};
	\item a function $u\colon S\times A\to S$, called the \emph{update function};
	\item an element $s_0\in S$, called the \emph{initial state}; and
	\item a subset $F\ss S$, called the \emph{accept states}.
\end{enumerate}
Given a sequence (or ``word'') $(a_1,\ldots,a_n)$ of elements from $A$ (i.e.\ an element of $\lst(A)$, the free monoid on $A$), we say that the automaton \emph{accepts} this word if starting at the initial state and following the elements in the sequence leads us to an accept state---or, more formally, if the sequence $(s_0,s_1,\ldots,s_n)$ defined
\[
    s_{k+1}\coloneqq u(s_k,a_{k+1})
\]
for all $k\in\nn$ is such that $s_n$ is an accept state: $s_n\in F$.

We call any set of words in $\lst(A)$ a \emph{language}, and we say that the set of all words that the automaton accepts is the language \emph{recognized} by the automaton.
\end{definition}

\begin{remark}
When we study a deterministic state automaton, we are usually interested in which words the automaton accepts and, more generally, what language the automaton recognizes.
While intuitive, the condition we provided for when an automaton accepts a word is rather cumbersome to work with.
In \cref{**}, we'll see a simpler way of saying whether an automaton accepts a word, as well as specifying what language the automaton recognizes.
Better yet, we'll find that this alternative formulation arises naturally from the theory of $\poly$.
\end{remark}

\begin{proposition}
A deterministic state automaton with a set of states $S$ and a set of input symbols $A$ can be identified with a pair of maps
\[
\yon\to S\yon^S\to \2\yon^A.
\]
\end{proposition}
\begin{proof}
A map $\yon\to S\yon^S$ can be identified with an initial state $s_0\in S$.
A map $S\yon^S\to\2\yon^A$ consists of a function $f\colon S\to\2$, which can be identified with a subset of accept states $F \ss S$, together with an update function $u \colon S\times A\to S$.
\end{proof}

But what if we wanted to make a version of this automaton where, whenver the machine hits an accept state, it stops---no longer taking in any inputs? Again, to do this requires a machine whose set of possible inputs is dependent on the output the current state returns.
In this case, instead of an update function $u\colon S\times A\to S$, we want an update function that takes in an input $a\in A$ if the state $s\in S$ is \emph{not} an accept state (say, if $f(s)=1$) but takes in an input in $\0$ (i.e.\ no input) if the state $s$ \emph{is} an accept state (if $f(s)=2$).
So there is one update function $u_s\colon A\to S$ if $f(s)=1$, and a different update function $u_s\colon\0\to S$ if $f(s)=2$.
But these are exactly the on-directions functions of a lens $S\yon^S\to\yon^A+\1$.
Indeed, replacing our interface monomial with a general polynomial is exactly how we will obtain our generalized dependent Moore machines.

\section{Dependent dynamical systems}\label{sec.poly.dyn_sys.depend_sys}

As lenses, each of our Moore machines above has an interface of the form $B\yon^A$, i.e.\ a monomial.
Every representable summand of the interface has the same exponent $A$, corresponding to the fact that the set of available inputs into the machine is always $A$.
But by replacing $B\yon^A$ with an arbitrary polynomial $p$---a sum of monomials, in which different representable summands may have different exponents---we will allow our input set to change.

\begin{definition}[Dependent dynamical system]\label{def.gen_moore}
A \emph{dependent dynamical system} (or a \emph{dependent Moore machine}, or simply a \emph{dynamical system}) is a lens
\[\phi\colon S\yon^S\to p\]
for some $S\in\smset$ and $p\in\poly$. The set $S$ is called the set of \emph{states}---with $S\yon^S$ called the \emph{state system}---and the polynomial $p$ is called the \emph{interface}.
Positions of the interface are called \emph{outputs}, and directions of the interface are called \emph{inputs}.

The lens's on-positions function $\phi_\1\colon S\to p(\1)$ is called the \emph{return function}, and for each $s\in S$, the lens's on-directions function $\phi^\sharp_s\colon p[\phi_\1(s)]\to S$ is called the \emph{update function} at $s$.
\end{definition}

\subsection{Thinking about dependency}

The current output $i$ of a dependent dynamical system with interface $p$ is a $p$-position: an element of the set $p(\1)$.
Then any input provided to the update function at that time must come from the direction-set $p[i]$.
So the set of available inputs varies depending on the current output.
What kind of system has that kind of a relationship between its inputs and outputs?

Think back to our promises for more generalized dynamical systems from \cref{sec.poly.intro.dyn_sys}.
We can begin to think of outputs not only as outward expressions, but as positions that one takes within one's arena---terminology we've been using all along.
If the arena is your body, then your outputs would be the positions that your body could take.
This includes where you go, as well as the direction you're looking with your head and eyes, whether your lips are pursed or not, etc.
In fact, every form of output you provide, from talking to gesturing to moving another object, is performed by changing your position. And your position determines what inputs you'll notice: if your eyes are closed, your input type is different than if your eyes are open.

\slogan{The position you're in is itself a sensory organ: a hand outstretched, an eye open or closed.}

If we squint, we could even see an output more as a sensing apparatus than anything else. This is pretty philosophical, but imagine your outputs---what you say and do---are more there as a question to the world, a way of sensing what the world is like.

\begin{remark}
It may seem limiting that the set of possible inputs of a dependent dynamical system should depend on the current \emph{output}, rather than the current \emph{state}.
Isn't it possible to have a system with different states that give the same output but take in inputs from different sets?

Philosophically, the answer to this question is ``no'' as long as we think about the interface of a dynamical system as capturing everything about how the system interacts with the outside world.
In particular, the output of a system should capture everything an external observer could possibly perceive about the system, while the input set should capture every way an external force can independently decide to interact with the system.

But if the range of ways in which an external agent can choose to interact with the system \emph{changes}, the external agent should be able to detect this fact!
If the internal state changes, but the external output remains the same, the agent wouldn't see any difference---they wouldn't know to interact with the system any differently, so their range of possible inputs would have to stay the same, too.
That's why it makes sense for the set of possible inputs to depend not on the hidden internal state, but on the output currently exposed.
\end{remark}

But however you think of dependent dynamical systems, we need to get a feel for how they work mathematically.
We'll do this by going through several examples.

\subsection{Examples of dependent dynamical systems}
To start, let's finish up the example from the end of the last section.

\begin{example}\label{ex.regular_lang_stop}
Recall deterministic state automata from \cref{def.dfa}.
Say we want such an automaton to halt after reaching an accept state and no longer take input.
For that, rather than use a map $S\yon^S\to \2\yon^A$, we could use a map
\[
\phi\colon S\yon^S\to \yon^A+\1.
\]
To give such a map, we provide a return function $\phi_\1\colon S\to\2$ (here we are thinking of $\2$ as the position-set of $\yon^A+\1$).
A function $\phi_\1$ sends some elements of $S$ to $1$ and others to $2$; those that are sent to $2$ are said to be accept states.

If we reach an accept state, we want the machine to halt.
So at the position $2$, corresponding to the summand $\1$, there are no directions---and thus no inputs available.
In other words, the update function $\phi^\sharp_s$ is trivial when $\phi_\1(s)=2$.

On the other hand, $\phi^\sharp_s$ is not trivial on those elements $s\in S$ for which $\phi_\1(s)=1$.
Instead, these update functions $\phi^\sharp_s\colon A\to S$ specify how the machine updates its state on each input from $A$, if the current state is $s$.
This is in line with the way the automaton's update function behaves.

When equipped with an initial state $s_0\in S$ specified by a map $\yon\to S\yon^S$, we call these dependent dynamical systems \emph{halting deterministic state automata}.
Given a sequence (or ``word'') $(a_1,\ldots,a_n)$ in $\lst(A)$, we say that the automaton \emph{accepts} this word if starting at the initial state and following the elements in the sequence leads us to an accept state, \emph{without hitting an accept state and stopping too early}---or, more formally, if there exists a sequence $(s_0,s_1,\ldots,s_n)$ such that $\phi_\1(s_n)=2$ and, for all $k\in[0,n-1]$, we have $\phi_\1(s_k)=1$ and
\[
    s_{k+1}=\phi^\sharp_{s_k}(a_{k+1}).
\]
We call the set of all words accepted by the automaton the language \emph{recognized} by the automaton.
\end{example}

\begin{remark}
Again, the conditions for when one of these automata accepts a word are rather awkward to formally state.
We will see in \cref{**} a nicer way of saying whether a word is accepted or a language is recognized by a halting deterministic state automaton.
\end{remark}

\begin{exercise}\label{exc.det_fsa_misc_398}
Consider the halting deterministic state automaton shown below:
\begin{equation} \label{eqn.hdsa}
\begin{tikzpicture}
\node[draw] {
\begin{tikzcd}[column sep=small]
	\LMO{}\ar[rr, bend left, orange]\ar[loop left, dgreen]&&
	\LMO{}\ar[dl, bend left, orange]\ar[ll, dgreen, bend left]\\&
	\LMO{}
\end{tikzcd}
};
\end{tikzpicture}
\end{equation}
Call the left state 1, the right state 2, and the bottom state 3.
We designate state $1$ as the initial state.
We can also call the orange arrows ``{\color{orange}orange}'' and the green arrows ``{\color{dgreen}green}.''
Answer the following questions, in keeping with the notation from \cref{ex.regular_lang_stop}.

\begin{enumerate}
	\item What is $S$?
	\item What is $A$?
	\item Based on the labeled transition diagram, which states are accept states, and which are not?
	\item Specify the corresponding lens $S\yon^S\to\yon^A+\1$.
	\item Name a word that is accepted by this automaton.
	\item Name a word that is not accepted by this automaton.
	Why not?
	Can you find another word that is not accepted by this automaton for a different reason?
\qedhere
\end{enumerate}
\begin{solution}
Here we examine the halting deterministic state automaton depicted in \eqref{eqn.hdsa}, with left state (and initial state) 1, right state 2, bottom state 3, and arrows {\color{orange}orange} and {\color{dgreen}green}.

\begin{enumerate}
    \item The set of states is $S\coloneqq\3=\{1,2,3\}$.
    \item The set of input symbols is $A\coloneqq\{{\color{orange}\text{orange}},{\color{dgreen}\text{green}}\}$.
    \item The automaton should halt at the accept states, so the accept states are exactly the states that have no arrows coming out of them---in this case, only state 3.
    States 1 and 2 are not accept states.
    \item Let the corresponding lens be $\phi\colon S\yon^S\to\yon^A+\1$, or $\phi\colon \3\yon^\3\to\yon^{\{{\color{orange}\text{orange}},{\color{dgreen}\text{green}}\}}+\1$.
    According to the previous part, $\phi$ has a return function $\phi_\1\colon S\to\2$ sending states 1 and 2, as non-accept states, to 1, and sending state 3, as an accept state, to 2.
    Then the two nontrivial update functions behave as follows, according to the targets of the arrows in \eqref{eqn.hdsa}:
    \begin{align*}
        \phi^\sharp_1&\colon\quad{\color{orange}\text{orange}}\mapsto2,{\color{dgreen}\text{green}}\mapsto1\\
        \phi^\sharp_2&\colon\quad{\color{orange}\text{orange}}\mapsto3,{\color{dgreen}\text{green}}\mapsto1,
    \end{align*}
    while $\phi^\sharp_3\colon\0\to S$ is trivial.
    \item Some examples of words accepted by this automaton include the word $({\color{orange}\text{orange}},{\color{orange}\text{orange}}),$ the word $({\color{orange}\text{orange}},{\color{dgreen}\text{green}},{\color{orange}\text{orange}},{\color{orange}\text{orange}}),$ and the word $({\color{dgreen}\text{green}},{\color{orange}\text{orange}},{\color{dgreen}\text{green}},{\color{dgreen}\text{green}},{\color{dgreen}\text{green}},{\color{orange}\text{orange}},{\color{orange}\text{orange}})$.
    \item Some words are not accepted by the automaton because they lead you to a non-accept state (1 or 2); others are not accepted by the automaton because they lead you to an accept state (3) too early.
    Some examples of the former possibility include the words $({\color{dgreen}\text{green}},{\color{dgreen}\text{green}})$ and $({\color{orange}\text{orange}},{\color{dgreen}\text{green}},{\color{orange}\text{orange}},{\color{dgreen}\text{green}})$, while some examples of the latter possibility include the words $({\color{dgreen}\text{green}},{\color{orange}\text{orange}},{\color{orange}\text{orange}},{\color{dgreen}\text{green}})$ and $({\color{orange}\text{orange}},{\color{orange}\text{orange}},{\color{orange}\text{orange}},{\color{orange}\text{orange}},{\color{orange}\text{orange}},{\color{orange}\text{orange}},{\color{orange}\text{orange}})$.
\end{enumerate}
\end{solution}
\end{exercise}

Every graph gives rise to a dynamical system---but to ensure that we are talking about the same thing, let us fix the definition of a graph.

\begin{definition}[Graph] \label{def.graph}
A \emph{graph} $G = (E \tto V)$ consists of an edge set $E$, a vertex set $V$, a source function $s\colon E\to V$, and a target function $t\colon E\to V$.
\end{definition}

So when we say ``graph,'' we mean a \emph{directed} graph, and we allow multiple edges between the same pair of vertices as well as self-loops.

\begin{example}[Graphs as dynamical systems] \label{ex.graph_dyn}
Given a graph $G = (E \tto V)$ with source map $s \colon E \to V$ and target map $t \colon E \to V$, there is an associated polynomial
\[
    g := \sum_{v \in V} \yon^{s\inv(v)}.
\]
Its positions are the vertices of the graph, and at each position $v\in V$, its directions are the edges coming out of $v$.
We call this the \emph{emanation polynomial} of $G$.

The graph itself can be seen as a dynamical system $\phi\colon V\yon^V \to g$, where $\phi_\1 = \id_V$ and $\phi^\sharp_v(e) = t(e)$.
So its states are the vertices of the graph, each vertex returns itself as output, and an input at a vertex $v\in V$ is an edge $e\in E$ coming out of that vertex that takes us from the vertex $v=s(e)$ along the edge $e$ to its target vertex $\phi^\sharp_v(e)=t(e)$.
\end{example}

\begin{exercise}
Pick your favorite graph $G$, and consider the associated dynamical system as in \cref{ex.graph_dyn}.
Draw its labeled transition diagram as in \eqref{eqn.trans_diag} or \eqref{eqn.hdsa}.
\begin{solution}
No matter what graph you chose, \cref{ex.graph_dyn} tells us that if you were to draw out the labeled transition diagram of its associated dynamical system, you would just end up with a picture of your graph!
The vertices of your graph are the states, and the edges of your graph are the possible transitions between them.
\end{solution}
\end{exercise}

% \begin{example}[Inputting an initial state]
% Suppose you have a closed system $f^\sharp\colon S\yon^S\to\yon$. The modeler can choose an initial state $\yon\to S\yon^S$, but what if we want some other system to choose the initial state? We haven't gotten to wiring diagrams yet, but the idea is to create a system that starts as not-closed---accepting as input a state $s\in S$---and then dives into its closed loop with that initial state.

% Let $S'\coloneqq S+\1$, so that the initial state $\yon\to S'\yon^{S'}$ now is canonical: it's the new $\1$. We also have a canonical inclusion $S\To{i}S'$. We will give a lens
% \[
% S'\yon^{S'}\to\yon+\yon^S
% \]
% that starts out with its outer box in the mode $\yon^S$ of accepting an $S$-input, and then moves to the mode $\yon$ so that it is a closed system forever after.

% To give a lens $S'\yon^{S'}\to\yon+\yon^S$, it is sufficient to give two morphisms: $S\yon^{S'}\to\yon$ and $\yon^{S'}\to\yon^S$. The first is equivalent to a function $S\to S'$ and we take the map $S\To{f^\sharp}S\To{i}S'$; this means that whenever we want to update the state from a state in $S$ we'll just do whatever our original closed system did. The second is also equivalent to a function $S\to S'$ and we use $i$; this means that whatever state is input at the beginning will be what we take as our first noncanonical state.
% \end{example}

\begin{example}\label{ex.generalized_file_reader}
In \cref{exc.file_reader} one is tasked with building a file-reader as a Moore machine, where a file is a function $f\colon\ord{n}\to\Set{ascii}$. 
Now we turn that file-reader into a dependent dynamical system $\phi\colon S\yon^S\to p$ that cannot take in input while it is reading.

We let $S \coloneqq \{(s,t)\mid 1\leq s\leq t\leq n\}$, so that each state consists of a current entry $s$ and a terminal entry $t$.
Meanwhile, our interface $p$ will have two labeled copies of $\Set{ascii}$ as positions:
\[
    p(\1)\coloneqq\{\text{`ready'}, \text{`busy'}\}\times\Set{ascii}.
\]
So each $p$-position is a pair $(m,c)$, where $c\in\Set{ascii}$ and $m$ is one of two modes: `ready' or `busy.'
These form the possible outputs of the file-reader.
As for the inputs, we define the direction-sets of $p$ as follows, for all $c\in\Set{ascii}$:
\[
    p[(\text{`ready'}, c)]\coloneqq S \qqand p[(\text{`busy'}, c)]\coloneqq\1.
\]
That way, our file-reader can receive any pair of entries in $S$ as input when it is `ready,' but can only be told to advance when it is `busy.'

We want our file-reader to be `ready' if its current entry is the terminal entry; otherwise, it will be `busy.'
In either case, it will return the ascii character at the current position.
So we define the return function $\phi_\1$ such that, for all $(s,t)\in S$,
\begin{align*}
  \phi_\1(s, t) =
  \begin{cases}
    (\text{`ready'}, f(s)) &\mbox{if $s = t$}\\
    (\text{`busy'}, f(s)) &\mbox{otherwise}  
  \end{cases}
\end{align*}

While the file-reader is `ready,' we want it to set its new current and terminal entries to be the input.
So for each $(s,t)\in S$ for which $s=t$, we define the update function $\phi^\sharp_{(s,t)}\colon S\to S$ to be the identity on $S$.

On the other hand, while the file-reader is `busy,' we want it to step forward through the file each time it receives an input.
So for each $(s,t)\in S$ for which $s\neq t$, we let the update function $\phi^\sharp_{(s,t)}\colon \1\to S$ specify the element $(s+1, t)\in S$, thus shifting its current entry up by $1$.
\end{example}

\begin{exercise} \label{exc.file_searcher}
Say instead of a file-reader, we wanted a file-searcher, which acts just like the file-reader from \cref{ex.generalized_file_reader} except that it only emits output $c\in\Set{ascii}$ when $c$ is a specific character---say, $c=100$.
Give the lens for this file-searcher by explicitly defining its return (on-positions) and update (on-directions) functions.
Hint: You should be able to use the same state system.
\begin{solution}
We give a file-searcher $\psi\colon S\yon^S\to q$ that acts just like the file-reader $\phi\colon S\yon^S\to p$ from \cref{ex.generalized_file_reader}, except that it only emits output $o\in\Set{ascii}$ when $c=100$.
In place of any other ascii character, we'll have it output $\_$ instead.
So its possible outputs should form the set
\[
    q(\1)\coloneqq\{\text{`ready'}, \text{`busy'}\}\times\{100,\_\}.
\]
The direction-sets of $q$ can be defined in the same way we defined the direction-sets of $p$: for each $c\in\{100,\_\}$, we have
\[
    q[(\text{`ready'}, c)]\coloneqq S \qqand q[(\text{`busy'}, c)]\coloneqq\1.
\]
Then we define the return function $\psi_\1$ like $\phi_\1$, but first checking to see if the character at the current entry is $100$: so for all $(s,t)\in S$,
\begin{align*}
  \psi_\1(s, t) =
  \begin{cases}
    (\text{`ready'}, 100) &\mbox{if $s=t$ and $f(s)=100$}\\
    (\text{`ready'}, \_) &\mbox{if $s=t$ and $f(s)\neq100$}\\
    (\text{`busy'}, 100) &\mbox{if $s\neq t$ and $f(s)=100$}\\
    (\text{`busy'}, \_) &\mbox{otherwise}\\
  \end{cases}
\end{align*}
Then the update functions of $\psi$ behave just like those of $\phi$.
For each $(s,t)\in S$ for which $s=t$, we define the update function $\psi^\sharp_{(s,t)}\colon S\to S$ to be the identity on $S$.
On the other hand, for each $(s,t)\in S$ for which $s\neq t$, we let the update function $\psi^\sharp_{(s,t)}\colon \1\to S$ specify the element $(s+1, t)\in S$, thus shifting its current entry up by $1$.
\end{solution}
\end{exercise}

In the previous exercise, we manually constructed a file-searcher that acted very much like a file-reader.
In \cref{exc.file_searcher_wrap}, we'll see a simpler way to construct a file-searcher by leveraging the file-reader we have already defined.
Moreover, this construction highlights precisely how our file-searcher is related to our file-reader.
This will be possible using \emph{wrapper interfaces}, which we'll introduce in \cref{subsec.poly.dyn_sys.new.wrap}.

\begin{example}\label{ex.grid_robot}
Choose $n\in\nn$, a \emph{grid size}, and for each $i\in\ord{n}$, let $D_i$ be the set
\[
	D_i\coloneqq
	\begin{cases}
		\{0,+1\}&\tn{ if }i=1\\
		\{-1,0,+1\}&\tn{ if } 1<i<n\\
		\{-1,0\}&\tn{ if }i=n
	\end{cases}
\]
We can think of $D_i$ as the set of all directions a robot could move if at position $i$.
In particular, a robot already at $i=1$ cannot move in the $-1$ direction; likewise, a robot already at $i=n$ cannot move in the $+1$ direction.

Then we can model a robot that can be instructed to move within an $\ord{n}\times\ord{n}$ grid as a dependent dynamical system $\phi\colon S\yon^S\to p$, with $S\coloneqq\ord{n}\times\ord{n}$ and
\[
    p\coloneqq\sum_{(i,j)\in\ord{n}\times\ord{n}}\yon^{D_i\times D_j}.
\]
The robot's state is a position in the grid.
We let $\phi_\1\coloneqq\id_{\ord{n}\times\ord{n}}$ so that each state returns itself as output---in particular, the robot returns a position as output by moving to that position in the grid.

Then for each $(i,j)\in\ord{n}\times\ord{n}$, we let $\phi^\sharp_{(i,j)}$ send each pair of directions $(d,e)\in D_i\times D_j$ to the grid position given by $(i+d,j+e)$.
Concretely, this says that if a robot at position $(i,j)$ in the grid receives the pair of directions $(d,e)$ as its input, its new position in the grid will be $(i+d,j+e)$.
Our definition of $D_i$ for each $i\in\ord{n}$ guarantees that this position is still in the grid.
With this setup, the robot has more movement options when it is in the center of the grid than when it is on the sides or corners:
\[
\begin{tikzpicture}[scale=.5]
  \draw[step=1cm,gray,very thin] (-3,-3) grid (4,4);
	\draw[->, red ] (-2.5,3.5) -- (-1.5, 3.5);
	\draw[->, red ] (-2.5,3.5) -- (-1.5, 2.5);
	\draw[->, red ] (-2.5,3.5) -- (-2.5, 2.5);
	\draw[->, blue] (1.5, 0.5) -- (0.5, 0.5);
	\draw[->, blue] (1.5, 0.5) -- (2.5, 0.5);
	\draw[->, blue] (1.5, 0.5) -- (1.5, 1.5);
	\draw[->, blue] (1.5, 0.5) -- (1.5,-0.5);
	\draw[->, blue] (1.5, 0.5) -- (0.5, 1.5);
	\draw[->, blue] (1.5, 0.5) -- (0.5,-0.5);
	\draw[->, blue] (1.5, 0.5) -- (2.5, 1.5);
	\draw[->, blue] (1.5, 0.5) -- (2.5,-0.5);
\end{tikzpicture}
\]

Note that, in this example, the positions of $p$ are literally the positions in the grid where the robot could be, and the directions of $p$ at each position are literally the directions in which the robot can move!
\end{example}

\begin{exercise} \label{exc.grid_reward}
Modify the dynamical system from \cref{ex.grid_robot} as follows.
\begin{enumerate}
	\item Replace $p$ with another interface $p'$ so that at each grid value, the robot can receive not only the direction it should move in but also a ``reward value'' $r\in\rr$. 
	\item Replace $S$ with another set of states $S'$ so that an element $s\in S'$ may include both the robot's position and a list of all reward values so far.
	\item With your new $p'$ and $S'$, define a new lens $\phi'\colon S'\yon^{S'}\to p'$ that preserves the behavior of $\phi\colon S\yon^S\to p$ from \cref{ex.grid_robot}, but also properly updates the robots list of rewards.
\qedhere
\end{enumerate}
\begin{solution}
We modify the dynamical system from \cref{ex.grid_robot}.
\begin{enumerate}
    \item Previously, the set of inputs at each output $(i,j)\in\ord{n}\times\ord{n}$ of our interface $p$ was $D_i\times D_j$.
    But now we also want to be able to give the robot a ``reward value'' $r\in\rr$.
    So our new input set should be $D_i\times D_j\times\rr$, making
    \[
        p'\coloneqq\sum_{(i,j)\in\ord{n}\times\ord{n}} \yon^{D_i\times D_j\times\rr}.
    \]
    \item Previously, a state was just a position in $\ord{n}\times\ord{n}$.
    But now we want to be able to record a list of reward values as well.
    Since each reward value is a real number, it suffices to define the state set to be $S'\coloneqq\ord{n}\times\ord{n}\times\lst(\rr)$.
    \item The former return function $\phi_\1\colon S\to p(\1)$ was the identity on $\ord{n}\times\ord{n}$.
    The new return function $\phi'_\1$ should still just yield the robot's current grid position, but since it is now a function from $S'=\ord{n}\times\ord{n}\times\lst(\rr)$, it should instead be the canonical projection $\phi'_\1\colon \ord{n}\times\ord{n}\times\lst(\rr)\to\ord{n}\times\ord{n}$.
    
    For each former state $(i,j)\in\ord{n}\times\ord{n}$, the former update function $\phi^\sharp_{(i,j)}\colon D_i\times D_j\to\ord{n}\times\ord{n}$ sent $(d,e)\mapsto(i+d,j+e)$.
    With an extra component $(r_1,\ldots,r_k)\in\lst(\rr)$ of the state, the new update function $(\phi')^\sharp_{(i,j,(r_1,\ldots,r_k))}\colon D_i\times D_j\times\rr\to\ord{n}\times\ord{n}\times\lst(\rr)$ sends $(d,e,r)\mapsto(i+d,j+e,(r_1,\ldots,r_k,r))$, updating the list of rewards.
\end{enumerate}
\end{solution}
\end{exercise}

In the previous exercise, we added a reward system to the robot on the grid by manually redefining the associated lens.
But there is a much simpler way to think about the new system as the juxtaposition of two systems, a robot system and a reward system, in parallel.
We'll see how to express this in terms of lenses in \cref{exc.grid_reward_par}, once we explain how to juxtapose systems like this in general in \cref{subsec.poly.dyn_sys.new.par}.
In fact, we'll see in \cref{exc.grid_robot_par} that the robot-on-a-grid system itself can be viewed as the juxtaposition of two systems, and this perspective will provide a structured way to generalize \cref{ex.grid_robot} to more than two dimensions.


%---- Section ----%
\section{Constructing new dynamical systems from old}\label{sec.poly.dyn_sys.new}

We have now seen how dependent dynamical systems can be modeled as lenses in $\poly$ of the form $S\yon^S\to p$.
But we have yet to take full advantage of the categorical structure that $\poly$ provides.
In particular, purely based on what we know of $\poly$ so far from \cref{ch.poly.func_nat}, we have three rather different ways of obtaining new dynamical systems from old ones:
\begin{enumerate}
    \item Given dynamical systems $S\yon^S\to p$ and $S\yon^S\to q$, we can use the universal property of the \emph{categorical product} to obtain a dynamical system $S\yon^S\to p\times q$; see \cref{subsec.poly.dyn_sys.new.prod}.
    \item Given dynamical systems $\phi\colon S\yon^S\to p$ and $\psi\colon T\yon^T\to q$, we can take their parallel product to obtain a dynamical system $\phi\otimes\psi\colon ST\yon^{ST}\to p\otimes q$; see \cref{subsec.poly.dyn_sys.new.par}.
    \item Given a dynamical system $\phi\colon S\yon^S\to p$ and a lens $f\colon p\to q$, we can compose them to obtain a dynamical system $\phi\then f\colon S\yon^S\to q$; see \cref{subsec.poly.dyn_sys.new.wrap}.
\end{enumerate}
Each of these operations has a concrete interpretation in terms of dynamical systems.
In this section, we'll review each of them in turn.

%---- Subsection ----%
\subsection{Categorical products: multiple interfaces operating on the same states}\label{subsec.poly.dyn_sys.new.prod}

For $n\in\nn$, say that we have $n$ dynamical systems, $\phi_i\colon S\yon^S\to p_i$ for each $i\in\ord{n}$, that all share the same state system.
Then by the universal property of products in $\poly$, there is an induced lens \[\phi\colon S\yon^S\to\prod_{i\in\ord{n}}p_i,\] which is itself a dynamical system with state system $S\yon^S$.

We can characterize the dynamics of $\phi$ in terms of each original dynamical system $\phi_i$ as follows.
By \cref{exc.poly_prod}, the return function \[\phi_\1\colon S\to\prod_{i\in\ord{n}}p_i(\1)\] sends each state $s\in S$ to the corresponding $n$-tuple of outputs $((\phi_i)_\1(s))_{i\in\ord{n}}$ returned by each of the original dynamical systems at that state.
Then at each state $s\in S$, the update function \[\phi^\sharp_s\colon\sum_{i\in\ord{n}}p_i[(\phi_i)_\1(s)]\to S\] sends each pair $(i,d)$ in its domain, with $i\in\ord{n}$ and $d\in p_i[(\phi_i)_\1(s)]$, to where the update function of $\phi_i$ at $s$ sends $d$: namely $(\phi_i)^\sharp_s(d)$.

In other words, if there are multiple interfaces that can drive the same set of states, we may view them as a single product interface that can drives those states.
This single dynamical system returns output in all of the original systems at once; then it can receive input from any one of the original systems' input sets and update its state accordingly.
It's as though each of the dynamical systems can see where the combined system is at any time, but only one of them can actually operate it at any given time.
So products give us a universal way to combine multiple polynomial interfaces into one.

\begin{exercise}
Given $n\in\nn$, suppose we have an $(A_i,B_i)$-Moore machine with state set $S$ for each $i \in \ord{n}$.
Show that there is an induced $\left(\sum_{i \in \ord{n}} A_i, \prod_{i \in \ord{n}} B_i\right)$-Moore machine, again with state set $S$.
\begin{solution}
We are given $n\in\nn$ and an $(A_i,B_i)$-Moore machine with state set $S$, i.e.\ a lens $S\yon^S\to B_i\yon^{A_i}$, for each $i\in\ord{n}$.
The universal property of products in $\poly$ gives us a lens
\[
    S\yon^S\to\prod_{i\in\ord{n}}B_i\yon^{A_i}\iso\left(\prod_{i\in\ord{n}}B_i\right)\yon^{\sum_{i\in\ord{n}}A_i},
\]
which is a $\left(\sum_{i \in \ord{n}} A_i, \prod_{i \in \ord{n}} B_i\right)$-Moore machine with state set $S$ (after all, the product of monomials is still a monomial).
\end{solution}
\end{exercise}

\begin{example} \label{ex.prod_diagrams}
Consider two four-state dependent dynamical systems $\phi\colon\4\yon^\4\to\rr\yon^{\{r,b\}}$ and $\psi\colon\4\yon^\4\to \zz_{\geq0}\yon^{\{g,p\}}+\zz_{<0}\yon^{\{g\}}$, drawn below as labeled transition diagrams (we think of $r,b,g,$ and $p$ as red, blue, green, and purple, respectively):
\[
\begin{tikzpicture}
	\node[draw] (1) {
  \begin{tikzcd}[row sep=15pt]
  	\LMO{\pi}\ar[r, bend left=15pt, red]\ar[loop left=15pt, blue]&
  	\LMO{0}\ar[l, bend left=15pt, red]\ar[d, bend left=15pt, blue]\\
  	\LMO[under]{-1.41}\ar[u,bend left=15pt, red]\ar[r, bend right=15pt, blue]&
  	\LMO[under]{2.72}\ar[l, bend right=15pt, red]\ar[loop right=15pt, blue]
  \end{tikzcd}
	};
	\node[draw, right=of 1] {
  \begin{tikzcd}[row sep=15pt]
  	\LMO{-2}\ar[d, green!50!black]&
  	\LMO{4}\ar[l, green!50!black]\ar[d, blue!50!purple]\\
  	\LMO[under]{-8}\ar[loop left, green!50!black]&
  	\LMO[under]{16}\ar[ul, green!50!black]\ar[l, blue!50!purple]
  \end{tikzcd}
  };
 \end{tikzpicture}
\]

The universal property of products provides a unique way to put these systems together to obtain a dynamical system $\4\yon^\4\to\rr\zz_{\geq0}\yon^{\{r,b,g,p\}}+\rr\zz_{<0}\yon^{\{r,b,g\}}$ that looks like this:
\[
\begin{tikzpicture}
	\node[draw] (1) {
  \begin{tikzcd}
  	\LMO{(\pi,-2)}\ar[r, bend left=15pt, red]\ar[loop left, blue]\ar[d, bend left=15pt, green!50!black]&
  	\LMO{(0,4)}\ar[l, bend left=15pt, red]\ar[d, bend left=15pt, blue]\ar[d, bend right=15pt, blue!50!purple]\ar[l, green!50!black]\\
  	\LMO[under]{(-1.41,-8)}\ar[u,bend left=15pt, red]\ar[r, bend right=15pt, blue]\ar[loop left, green!50!black]&
  	\LMO[under]{(2.72,16)}\ar[l, bend right=15pt, red]\ar[l, blue!50!purple]\ar[loop right=15pt, blue]\ar[ul, green!50!black]
  \end{tikzcd}
  };
\end{tikzpicture}
\]
Each state now returns two outputs: one according to the return function of $\phi$, and another according to the return function of $\psi$.
As for the possible inputs, we now have the option of giving either an input from a direction-set of $\phi$ (either $r$ or $b$), in which case the dynamical system will update its state according to the corresponding update function of $\phi$, or an input from a direction-set of $\psi$ (either $g$ or sometimes $p$), in which case the dynamical system will update its state according to the corresponding update function of $\psi$.
\end{example}

\begin{exercise}[Toward event-based systems]
Let $\phi\colon S\yon^S\to p$ be a dynamical system. It is constantly needing input at each time step. An event-based system is one that doesn't always get input, and only reacts when it does.

So suppose we want to allow our dynamical system not to do anything. That is, rather than needing to press a button corresponding to a direction of $p$ at each time step, we want to be able to \emph{not} press any button, in which case the system just stays where it is. We want a new system $\phi'\colon S\yon^S\to p'$ that has this behavior; what are $p'$ and $\phi'$?
\begin{solution}
Given a dynamical system $\phi\colon S\yon^S\to p$, we seek a new dynamical system $\phi'\colon S\yon^S\to p'$ that has the added option to provide no input at a step so that the state does not change.
We can think of this as having two different interfaces acting on the same system: the original interface $p$ of $\phi$, and a new interface with only one possible input---namely the option to provide no input at all---that does not change the state.
This latter interface also does not need to distinguish between its outputs; it should have just one possible output that says nothing.
So the second interface we want acting on $S\yon^S$ is $\yon$.

If $\yon$ were the only interface acting on the system, we would have a Moore machine $\epsilon\colon S\yon^S\to\yon$ whose return function is the unique function $S \to \1$ and whose update function is the identity function on $S$, since the input never changes the system.
Then $p'$ is the product of the two interfaces $p$ and $\yon$, while $\phi'\colon S\yon^S \to p'$ is the unique map induced by $\phi\colon S\yon^S\to p$ and $\epsilon\colon S\yon^S\to\yon$.
In particular, $p'\iso p\yon\iso\sum_{i\in p(\1)}\yon^{p[i]+\1}$, while $\phi'$ consists of a return function $\phi'_\1\colon S \to p(\1)$ that is the same as the return function of $\phi$ and, for each state $s\in S$, an update function $\phi'^\sharp_s\colon p[i]+\1\to S$ that behaves like the update function of $\phi$ at $s$ when the input is from $p[i]$ (sending $d\in p[i]$ to $\phi^\sharp_s(d)$) but does not change the state when the input is from $\1$ (sending the unique element of $\1$ to $s$).

This construction is actually universal in a way we'll find important later. Indeed, it's called \emph{copointing}; see \cref{prop.copointing}.
\end{solution}
\end{exercise}

%---- Subsection ----%
\subsection{Parallel products: juxtaposing dynamical systems}\label{subsec.poly.dyn_sys.new.par}

Another way to combine two polynomials---and indeed two lenses---is by taking their parallel product, as in \cref{subsec.poly.func_nat.monoidal.par}.
In particular, the parallel product of two state systems is still a state system.
So parallel products give us another way to create new dynamical systems from old ones.
In fact, it's about as easy as you could hope: you just multiply the set of states, multiply the set of outputs, and multiply the set of inputs at each of those outputs.

For $n\in\nn$, say that we have $n$ dynamical systems, $\phi_i\colon S_i\yon^{S_i}\to p_i$ for every $i\in\ord{n}$.
Then we can take the parallel product of all of them to get a lens \[\phi\colon \left(\prod_{i\in\ord{n}}S_i\right)\yon^{\prod_{i\in\ord{n}}S_i}\iso\bigotimes_{i\in\ord{n}}S_i\yon^{S_i}\to\bigotimes_{i\in\ord{n}}p_i,\] which is itself a dynamical system.

We can characterize the dynamics of $\phi$ in terms of each constituent dynamical system $\phi_i$ as follows.
By the proof of \cref{prop.dirichlet_monoidal}, the return function \[\phi_\1\colon \prod_{i\in\ord{n}}S_i\to\prod_{i\in\ord{n}}p_i(\1)\] sends each $n$-tuple of states $(s_i)_{i\in\ord{n}}$ in its domain, with each $s_i\in S_i$, to the $n$-tuple of outputs $((\phi_i)_\1(s_i))_{i\in\ord{n}}$ returned by each of the constituent dynamical systems at each state.
Then at the $n$-tuple of states $(s_i)_{i\in\ord{n}}\in\prod_{i\in\ord{n}}S_i$, the update function \[\phi^\sharp_{(s_i)_{i\in\ord{n}}}\colon\prod_{i\in\ord{n}}p_i[(\phi_i)_\1(s_i)]\to\prod_{i\in\ord{n}}S_i\] sends each $n$-tuple of directions $(d_i)_{i\in\ord{n}}$ in its domain, with each $d_i\in p_i[(\phi_i)_\1(s_i)]$, to the $n$-tuple $((\phi_i)^\sharp_{s_i}(d_i))_{i\in\ord{n}}$ consisting of states where the update function of each $\phi_i$ at $s_i$ sends $d_i$.

In other words, multiple dynamical systems running in parallel can be thought of as a single dynamical system.
This system stores the states of all the constituent systems at once and returns output from all of them together; then it can receive input from all of the constituent systems' input sets at once and update each constituent state accordingly.
So parallel products give us a way to juxtapose multiple dynamical systems in parallel to form a single system.

\begin{exercise}
Given $n\in\nn$, suppose we have an $(A_i,B_i)$-Moore machine with state set $S_i$ for every $i \in \ord{n}$.
Show that there is an induced $\left(\prod_{i \in \ord{n}} A_i, \prod_{i \in \ord{n}} B_i\right)$-Moore machine with state set $\prod_{i \in \ord{n}} S_i$.
\begin{solution}
We are given $n\in\nn$ and an $(A_i,B_i)$-Moore machine with state set $S_i$, i.e.\ a lens $S_i\yon^{S_i}\to B_i\yon^{A_i}$, for each $i\in\ord{n}$.
Taking their parallel product in $\poly$ gives us a lens
\[
    \left(\prod_{i\in\ord{n}}S_i\right)\yon^{\prod_{i\in\ord{n}}S_i}\iso\bigotimes_{i\in\ord{n}}S_i\yon^{S_i}\to\bigotimes_{i\in\ord{n}}B_i\yon^{A_i}\iso\left(\prod_{i\in\ord{n}}B_i\right)\yon^{\prod_{i\in\ord{n}}A_i},
\]
which is a $\left(\sum_{i \in \ord{n}} A_i, \prod_{i \in \ord{n}} B_i\right)$-Moore machine with state set $S$ (after all, the product of monomials is still a monomial).
\end{solution}
\end{exercise}

\begin{example} \label{ex.par_diagrams}
Consider two dependent dynamical systems $\phi\colon\2\yon^\2\to \rr_{<0}\yon^{\{b,r\}}+\rr_{\geq0}\yon^{\{b\}}$ and $\psi\colon\3\yon^\3\to\zz_{<0}\yon^{\{r\}}+\{0\}\yon^{\{r,y\}}+\zz_{>0}\yon^{\{y\}}$, drawn below as labeled transition diagrams (we think of $b,r,$ and $y$ as blue, red, and yellow, respectively):
\[
\begin{tikzpicture}
	\node[draw] (1) {
  \begin{tikzcd}[row sep=15pt]
  	\LMO{\sqrt{7}}\ar[d, bend left=15pt, blue]\\
  	\LMO[under]{-e}\ar[u, bend left=15pt, red]\ar[loop right=15pt, blue]
  \end{tikzcd}
	};
	\node[draw, right=of 1] {
  \begin{tikzcd}[row sep=15pt]
  	\LMO{-5}\ar[r, bend left=15pt, red]&
  	\LMO{0}\ar[l, bend left=15pt, dyellow]\ar[r, red]&
  	\LMO{8}\ar[loop right=15pt, dyellow]
  \end{tikzcd}
  };
 \end{tikzpicture}
\]
Taking their parallel product, we obtain a dynamical system with state system $\6\yon^\6$ and interface
\begin{align*}
    &\rr_{<0}\zz_{<0}\yon^{\{(b,r),(r,r)\}}+\rr_{<0}\{0\}\yon^{\{(b,r),(b,y),(r,r),(r,y)\}}+\rr_{<0}\zz_{>0}\yon^{\{(b,y),(r,y)\}}\\
    +\:&\rr_{\geq0}\zz_{<0}\yon^{\{(b,r)\}}+\rr_{\geq0}\{0\}\yon^{\{(b,r),(b,y)\}}+\rr_{\geq0}\zz_{>0}\yon^{\{(b,y)\}}
\end{align*}
that looks like this (we use purple to indicate $(b,r)$, red to indicate $(r,r)$, green to indicate $(b,y)$, and orange to indicate $(r,y)$):
\[
\begin{tikzpicture}
	\node[draw] (1) {
  \begin{tikzcd}
    \LMO{(\sqrt{7},-5)}\ar[dr, bend left=15pt, blue!50!purple] &
    \LMO{(\sqrt{7},0)}\ar[dl, green!50!black]\ar[dr, blue!50!purple] &
    \LMO{(\sqrt{7},8)}\ar[d, bend left=15pt, green!50!black] \\
    \LMO[under]{(-e,-5)}\ar[ur, bend left=15pt, red]\ar[r, blue!50!purple] &
    \LMO[under]{(-e,0)}\ar[ul, orange!75!black]\ar[ur, red]\ar[l, bend left=15pt, green!50!black]\ar[r, blue!50!purple] &
    \LMO[under]{(-e,8)}\ar[u, bend left=15pt, orange!75!black]\ar[loop right=15pt, green!50!black]
  \end{tikzcd}
  };
\end{tikzpicture}
\]
Each state---really a pair of states from the constituent state sets---returns two outputs, one according to the return function of $\phi$ and another according to the return function of $\psi$.
Then every input must be a pair of inputs from the constituent state sets, with the update function updating each state in the pair given each input in the pair according to the constituent update functions $\phi$ and $\psi$.
\end{example}

\begin{exercise} \label{exc.grid_reward_par}
Explain how the dynamical system $\phi'\colon S'\yon^{S'}\to p'$ you built in \cref{exc.grid_reward} can be expressed as the parallel product of the robot-on-a-grid dynamical system $\phi\colon S\yon^S\to p$ from \cref{ex.grid_robot} with another dynamical system, $\psi\colon T\yon^T\to q$.
Be sure to specify $T, q,$ and $\psi$.
\begin{solution}
We will show that taking the parallel product of the robot-on-a-grid dynamical system $\phi\colon S\yon^S\to p$ from \cref{ex.grid_robot} and a reward-tracking dynamical system $\psi\colon T\yon^T\to q$ that we will define yields the dynamical system $\phi'\colon S'\yon^{S'}\to p'$ from \cref{exc.grid_reward}.

The reward-tracking dynamical system should have states in $\lst(\rr)$ to record a list of reward values, unchanging output, and inputs in $\rr$ to give new reward values.
So it is the lens $\lst(\rr)\yon^{\lst(\rr)}\to\yon^\rr$ that has a uniquely defined return function, while its update function sends each state $(r_1,\ldots,r_k)\in\lst(\rr)$ and each input $r\in\rr$ to the new state $(r_1,\ldots,r_k,r)$.

Then the dynamical system from \cref{exc.grid_reward} is the parallel product of the robot-on-a-grid dynamical system from \cref{ex.grid_robot} with the reward-tracking dynamical system $\lst(\rr)\yon^{\lst(\rr)}\to\yon^\rr$, as can be seen in the solution to \cref{exc.grid_reward}.
\end{solution}
\end{exercise}

\begin{exercise} \label{exc.grid_robot_par}
\begin{enumerate}
    \item Explain how the robot-on-a-grid dynamical system $\phi\colon S\yon^S\to p$ from \cref{ex.grid_robot} can be written as the parallel product of some dynamical system with itself.
    \item Use $k$-fold parallel products to generalize \cref{ex.grid_robot} to robots on $k$-dimensional grids.\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item The robot-on-a-grid dynamical system from \cref{ex.grid_robot} can be written as the parallel product of two robot-on-a-line dynamical systems of the form $\lambda\colon\ord{n}\yon^{\ord{n}}\to\sum_{i\in\ord{n}}\yon^{D_i}$, where $\lambda_\1\coloneqq\id_{\ord{n}}$ and $\lambda^\sharp_i$ for each $i\in\ord{n}$ sends each direction $d\in D_i$ to the position on the line given by $i+d$.
    This yields a robot that can move along a single axis, and the parallel product of this robot with itself yields a robot that can move along two different axes at once, which is precisely our robot-on-a-grid dynamical system.
    \item To create a dynamical system consisting of a robot moving in a $k$-dimensional grid of size $n$ along every dimension, we just take the $k$-fold parallel product of the dynamical system $\lambda\colon\ord{n}\yon^{\ord{n}}\to\sum_{i\in\ord{n}}\yon^{D_i}$ we just defined to obtain a dynamical system \[\lambda^{\otimes k}\colon\ord{n}^\ord{k}\yon^{\ord{n}^\ord{k}}\to\sum_{(i_1,\ldots,i_k)\in\ord{n}^\ord{k}}\yon^{\prod_{j\in\ord{k}}D_{i_j}}.\]
    In fact, we could have used a different $n_j$ for each $j\in\ord{k}$ instead of $n$ to obtain a robot moving in an arbitrary $k$-dimensional grid of size $n_1\times\cdots\times n_k$ as a $k$-fold parallel product.
\end{enumerate}
\end{solution}
\end{exercise}

The parallel product takes two dynamical systems and essentially puts them in the same room together so that they can be run at the same time.
But it doesn't allow for any interaction \emph{between} the two systems.
For that, we will need to use what we call a wrapper interface.
We'll introduce wrapper interfaces in the next section, before describing how they can be used in conjunction with parallel products to model general interaction in \cref{sec.poly.dyn_sys.interact}.

%---- Subsection ----%
\subsection{Composing lenses: wrapper interfaces}\label{subsec.poly.dyn_sys.new.wrap}

The idea behind a wrapper interface is simple.
Given a dynamical system $\phi\colon S\yon^S\to p$, say that we wanted to interact with it using a new interface $q$ rather than $p$.
We can do this as long as we have a lens $f\colon p\to q$, which we can then compose with our original dynamical system to obtain a new system $S\yon^S\To{\phi}p\To{f}q$.
We call the lens $f$ the \emph{wrapper} and its codomain $q$ the \emph{wrapper interface}, which we \emph{wrap} around $\phi$ (or sometimes just $p$, if a dynamical system $\phi$ has yet to be specified) using $f$.

To see how this new composite system $\phi\then f$ relates to the original dynamical system $\phi$, it is best to view $f$ as a delegation of decisions like we did in \cref{subsec.poly.func_nat.morph.concrete}.
The lens $f$ converts an output $i$ from $p$ to an output $f_\1(i)$ from $q$ on positions, but at the same time is allowing the decision of which input in $p[i]$ to choose next to depend on a choice of input from $q[f_\1(i)]$ instead, via its on-directions function at $i$.
So the wrapper $f$ converts output from the original interface $p$ to output from the wrapper interface $q$, and it converts input into the wrapper interface $q$ to input into the original interface $p$.
Precomposed with a dynamical system, we obtain a new dynamical system that allows you to interact with the original system using only this new interface wrapped around it.

\begin{example} \label{ex.wrap_diagrams}
Consider a dependent dynamical system $\phi\colon\6\yon^\6\to p$ with 
\[
    p\coloneqq\{1\}\yon^{\{b,y,r\}}+\{2\}\yon^{\{b,r\}}+\{3\}\yon^{\{b\}}+\{4\}\yon^{\{r\}},
\]
drawn below as a labeled transition diagram (we think of $b,y,$ and $r$ as blue, yellow, and red, respectively):
\[
\begin{tikzpicture}
	\node[draw] (1) {
  \begin{tikzcd}
    \LMO{1}\ar[r, blue]\ar[dr, dyellow]\ar[d, red] &
    \LMO{2}\ar[loop above=5pt, blue]\ar[d, bend right=15pt, red] &
    \LMO{3}\ar[l, blue] \\
    \LMO[under]{4}\ar[loop left=15pt, red] &
    \LMO[under]{1}\ar[l, blue]\ar[u, bend right=15pt, dyellow]\ar[r, red] &
    \LMO[under]{4}\ar[u, red]
  \end{tikzcd}
  };
\end{tikzpicture}
\]
We will wrap the interface \[q\coloneqq\{a\}\yon^{\{g,p,o\}}+\{b\}\yon^{\{g,p\}}+\{c\}\] around $\phi$ using the following lens $f\colon p\to q$ (we think of $g,p,$ and $o$ as green, purple, and orange, respectively):
\[
\begin{tikzpicture}
	\node (p1) {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 1" below] (1) {$\bullet$} 
      child[blue] {coordinate (11)}
      child[dyellow] {coordinate (12)}
      child[red] {coordinate (13)};
    \node[right=1.5 of 1, "\tiny $b$" below] (2) {$\bullet$} 
      child[green!50!black] {coordinate (21)}
      child[blue!50!purple] {coordinate (22)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (13);
      \draw[postaction={decorate}] (22) to (12);
    \end{scope}
  \end{tikzpicture}	
	};	
%
	\node (p2) [below right=-1.05cm and 1 of p1] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 2" below] (1) {$\bullet$} 
      child[blue] {coordinate (11)}
      child[red] {coordinate (12)};
    \node[right=of 1, "\tiny $c$" below] (2) {$\bullet$};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
  \end{tikzpicture}	
	};	
%
	\node (p3) [right=3.5 of p1] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 3" below] (1) {$\bullet$} 
      child[blue] {coordinate (11)};
    \node[right=1.5 of 1, "\tiny $b$" below] (2) {$\bullet$} 
      child[green!50!black] {coordinate (21)}
      child[blue!50!purple] {coordinate (22)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (11);
      \draw[postaction={decorate}] (22) to (11);
    \end{scope}
  \end{tikzpicture}	
	};	
%
	\node (p4) [right=1 of p3] {
	\begin{tikzpicture}[trees, sibling distance=2.5mm]
    \node["\tiny 4" below] (1) {$\bullet$} 
      child[red] {coordinate (11)};
    \node[right=1.5 of 1, "\tiny $a$" below] (2) {$\bullet$} 
      child[green!50!black] {coordinate (21)}
      child[blue!50!purple] {coordinate (22)}
      child[orange!75!black] {coordinate (23)};
    \draw[|->, shorten <= 3pt, shorten >= 3pt] (1) -- (2);
    \begin{scope}[densely dotted, bend right, decoration={markings, mark=at position 0.75 with \arrow{stealth}}]
      \draw[postaction={decorate}] (21) to (11);
      \draw[postaction={decorate}] (22) to (11);
      \draw[postaction={decorate}] (23) to (11);
    \end{scope}
  \end{tikzpicture}	
	};	
\end{tikzpicture}
\]
Composing $\phi$ with $f$, we obtain a dynamical system $\6\yon^\6\To{\phi}p\To{f}q$ that looks like this:
\[
\begin{tikzpicture}
	\node[draw] (1) {
  \begin{tikzcd}
    \LMO{b}\ar[d, green!50!black]\ar[dr, blue!50!purple] &
    \LMO{c} &
    \LMO{b}\ar[l, bend right=15pt, green!50!black]\ar[l, bend left=15pt, blue!50!purple] \\
    \LMO[under]{a}\ar[loop left=15pt, green!50!black]\ar[loop below=5pt, blue!50!purple]\ar[loop right=15pt, orange!75!black] &
    \LMO[under]{b}\ar[r, green!50!black]\ar[u, blue!50!purple] &
    \LMO[under]{a}\ar[u, bend left=20pt, green!50!black]\ar[u, blue!50!purple]\ar[u, bend right=20pt, orange!75!black]
  \end{tikzcd}
  };
\end{tikzpicture}
\]
Each state returns an output in $q$ according to where the on-positions function of $f$ sends the output the state returns in $p$.
Then each input in $q$ is passed to an input in $p$ via the corresponding on-directions function of $f$, whereupon the update function of $\phi$ computes the new state.
So $f$ allows us to operate $\phi$ with the wrapper interface $q$ instead of the original interface $p$.
\end{example}

\begin{exercise} \label{exc.file_searcher_wrap}
In \cref{exc.file_searcher}, you constructed a file-searcher $\psi\colon S\yon^S\to q$ by taking the file-reader $\phi\colon S\yon^S\to p$ from \cref{ex.generalized_file_reader} and replacing its interface $p$ with a new interface $q$ while keeping its state system $S\yon^S$ the same.
Express this construction as wrapping $q$ around $\phi$ by giving a lens $f\colon p\to q$ for which composing $\phi$ with $f$ yields $\psi$.
\begin{solution}
We give a lens $f\colon p\to q$ for which composing the file-reader $\phi\colon S\yon^S\to p$ from \cref{ex.generalized_file_reader} with $f$ yields the file-searcher $\psi\colon S\yon^S\to q$ from \cref{exc.file_searcher}.
The file-searcher returns the same output as the file-reader when the second coordinate is $100$, but replaces the second coordinate with a blank $\_$ otherwise.
So the on-positions function of $f$ should send each $(m,c)\in p(\1)$ to
\[
    f_\1(m,c) = 
        \begin{cases}
            (m,c) & \text{if } c = 100 \\
            (m,\_) & \text{otherwise}.
        \end{cases}
\]
Then the file-searcher acts just like the file-reader does on inputs, so the on-directions functions of $f$ should be $\id_S$ at each position whose first coordinate is `ready' and $\id_\1$ at each position whose first coordinate is `busy.'
\end{solution}
\end{exercise}

In the next section, we describe a special kind of wrapper.

%---- Subsection ----%
\subsection{Situations as enclosures}\label{subsec.poly.dyn_sys.new.sit_encl}
Say we wanted to model a dynamical system $\phi\colon S\yon^S\to p$ within a closed system, for which an external agent can perceive no change in output and effect no change in input.
We can think of this as wrapping $\yon$, the interface with one output and one input, around $\phi$.
To do so, we must specify a wrapper $\gamma\colon p\to\yon$.
We call such a wrapper an \emph{enclosure} for an interface $p$, since it is a way of closing off $p$ to the outside world.

Let us zoom out from the dynamical systems interpretation of polynomials for the time being to examine the categorical properties of enclosures.
Given a polynomial $p$, we denote the set of lenses $p\to\yon$ by
\begin{equation} \label{eqn.gamma_def}
\Gamma(p)\coloneqq\poly(p,\yon)
\end{equation}
as we did in \cref{prop.adjoint_quadruple}.
By \eqref{eqn.main_formula}, we have that
\begin{equation} \label{eqn.gamma_prod}
    \Gamma(p) \iso \prod_{i \in p(\1)} p[i],
\end{equation}
so in terms of arenas, a map $\gamma\in\Gamma(p)$ can be thought of as a dependent function that assigns each $p$-position $i$ to a direction $\gamma(i)$ of $p$ at $i$.
So we call an element of $\Gamma(p)$ a \emph{situation} for $p$.
The idea is that the situation you're in gives you a direction to go along at any position you may take.

Returning to the language of dynamical systems, a situation for $p$ corresponds to an enclosure for the interface $p$, in that it chooses a fixed input at every output of $p$.
An enclosure for your interface dictates what you'll see (the input you receive) given anything you might do (the output you provide); there is no need for any further outside interference.

\begin{remark}
Although they both refer to lenses into $\yon$, we will favor the term \emph{enclosure} in the context of dynamical systems and wrappers and the term \emph{situation} more generally.
\end{remark}

\begin{exercise} \label{exc.enclosures_as_functions}
Let $\phi\colon S\yon^S\to B\yon^A$ be an $(A,B)$-Moore machine.
\begin{enumerate}
	\item Is it true that an enclosure $\gamma\colon B\yon^A\to\yon$ can be identified with a function $A\to B$?
	\item Describe how to interpret an enclosure $\gamma\colon B\yon^A\to\yon$ as a wrapper around an interface $B\yon^A$.
	\item Given an enclosure $\gamma$, describe the dynamics of the composite Moore machine $S\yon^S\To{\phi}B\yon^A\To{\gamma}\yon$ obtained by wrapping $\yon$ around $\phi$ using $\gamma$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
	\item No, it represents a function $B\to A$!
	An enclosure assigns each output $b\in B$ to an input $a\in A$.
	\item As a wrapper around an interface $B\yon^A$, an enclosure $\gamma\colon B\yon^A\to\yon$ corresponds to a function $g\colon B\to A$ that feeds the input $g(b)\in A$ into the system whenever it returns the output $b\in B$.
	\item Composing our original Moore machine $S\yon^S\to B\yon^A$ with an enclosure $\gamma$ yields a Moore machine $S\yon^S\To{\phi}B\yon^A\To{\gamma}\yon$ that returns unchanging output and receives unchanging input.
	If we identify the Moore machine with its return function $S\to B$ and its update function $S\times A\to S$, and if we identify the enclosure $\gamma$ with a function $g\colon B\to A$, then their composite Moore machine $S\yon^S\to\yon$ can be identified with a function $S\to S$, equal to the composite
	\[
	    S\To{\Delta}S\times S\To{\id_S\times\text{return}}S\times B\To{\id_S\times g}S\times A\To{\text{update}}S,
	\]
	where $\Delta$ is the diagonal map $s\mapsto(s,s)$.
	This composite map $S\to S$ sends every state to the next according to the output the original state returns, the input that the enclosure gives in response to that output, and the update function that sends the original state and the input to the new state.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{example}[The do-nothing enclosure] \label{ex.do_nothing}
There is something rather off-putting about the way we model dynamical systems as lenses $\phi\colon S\yon^S\to p$.
We know that $\phi$ tells us how state-positions return output-positions and, given a current state-position, how input-directions update state-directions.
But we rely only on the labels of elements in $S$ to tell us which positions and directions refer to the same states!

Nothing inherent in the language of $\poly$ makes these associations between state-positions and state-directions for us; we have to bank on the position-set and direction-sets of the state system being the same set for the machine to work properly.
Put another way, the monomials $\{4,6\}\yon^{\{4,6\}},\{4,6\}\yon^{\{4,8\}},$ and $\{3,5\}\yon^{\{6,7\}}$ are all isomorphic in $\poly$, but the first can be a state system while the other two cannot!

To address this issue, we need a way to connect the positions of a polynomial to its own directions in the language of $\poly$.
Here's where situations save the day: a lens $S\yon^S\to\yon$ is just a way of assigning each position in $S$ to a direction in $S$.
So we can define $\epsilon\colon S\yon^S\to\yon$ to be the situation that assigns each position $s\in S$ to the direction $s$ at $s$ corresponding to the same state.
Note that $\epsilon$ can be identified with the identity function on $S$ (see \cref{exc.enclosures_as_functions}).
Now $\poly$ knows which direction is associated with the same state as the position it is at.

In this way, we can generalize our notion of state systems to monomials $S\yon^{S'}$ equipped with a bijection $S\to S'$, which we can then translate to a situation $\epsilon\colon S\yon^{S'}\to\yon$.
But for convenience of notation, we will continue to identify the position-set of a state system with each of its direction-sets.

More concretely, the enclosure $\epsilon\colon S\yon^S\to\yon$ acts as a very special (if rather unexciting) dynamical system: it is the \emph{do-nothing enclosure}, with only one possible output and one possible input that always keeps the current state the same.
While the system does, well, nothing, we do know one key fact about it: every state system ought to come with a dynamical system like this.

Yet this isn't the whole story.
The do-nothing enclosure knows, at each position, the direction that keeps the system at the same state; but it doesn't know which of the other directions send the system to which of the other positions' states.
We're still relying on the labels of direction-sets being the same for that: for instance, the polynomials $\{1',2',3'\}\yon^{\{1,2,3\}}$ and $\{1'\}\yon^{\{0,1,4\}}+\{2'\}\yon^{\{2,5,6\}}+\{3'\}\yon^{\{-8,-1,3\}}$ are isomorphic, but even with a do-nothing enclosure matching $1'\mapsto1,2'\mapsto2,3'\mapsto3$ to make the first one into a state system, we don't have a way to tell $\poly$ how to make the second one a state system yet.

From another perspective, $\epsilon\colon S\yon^S\to\yon$ does nothing, while $\phi\colon S\yon^S\to p$ does ``one thing'': it steps through the system once, generating the current state's output with the return function and taking in input with the update function.
It's all set to take another step, but how does $\poly$ know which state to visit next?
Is there a lens that does ``two things,'' ``$n$ things,'' or ``arbitrarily many things''?
Can we actually $\emph{run}$ a dynamical system in $\poly$?
We'll develop the machinery to answer these questions over the course of the three chapters in \cref{part.comon}, starting in \cref{subsec.comon.comp.def.dyn_sys}.
\end{example}

\begin{exercise}
The notation $\Gamma(p)$ for the set of situations for a polynomial $p$ comes from the common mathematical concept of a ``global section.''
Show that situations $\gamma\colon p\to\yon$ are precisely \emph{sections} (as defined in \cref{exc.dependent_product_as_sections}) of the canonical function $\pi_p\colon\dot{p}(\1)\to p(\1)$ from \cref{exc.deriv_directions}.
\begin{solution}
Given a polynomial $p$, we wish to show that situations for $p$ are precisely sections of the function $\pi_p\colon\dot{p}(\1)\to p(\1)$ from \cref{exc.deriv_directions}.
That function sends every direction of $p$ to the position at which it is located.
So a section $\gamma\colon p(\1)\to\dot{p}(\1)$ of $\pi_p$ would send each $p$-position to a direction of $p$ at that position, which is also exactly what a situation for $p$ does.
Alternatively, we can directly apply \cref{exc.dependent_product_as_sections} to deduce that the dependent product $\Gamma(p)\iso\prod_{i \in p(\1)}p[i]$ is isomorphic to the set of sections of the projection $\sum_{i\in p(\1)}p[i]\to p(\1)$ defined by $(i,x)\mapsto i$, which is exactly $\pi_p$.
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.gamma_pres_coproduct}
The situations functor $\Gamma\colon\poly\to\smset\op$ sends $(0,+)$ to $(1,\times)$:
\[
	\Gamma(\0)\iso\1
	\qqand
	\Gamma(p+q)\iso\Gamma(p)\times\Gamma(q).
\]
\end{proposition}
Technically, one could say that $\Gamma$ preserves coproducts, since coproducts in $\smset\op$ are products in $\smset$.

\begin{exercise}
Prove \cref{prop.gamma_pres_coproduct}.
\begin{solution}
\cref{prop.gamma_pres_coproduct} follows directly from \cref{prop.poly_coprods}: we have that $\Gamma(\0) = \poly(\0,\yon) \iso \1$ since $\0$ is initial in $\poly$, and $\Gamma(p + q) = \poly(p+q,\yon) \iso \poly(p+q,\yon) = \Gamma(p) \times \Gamma(q)$ since $+$ gives coproducts in $\poly$. 
\end{solution}
\end{exercise}

\begin{remark}
The situations functor $\Gamma\colon\poly\to\smset\op$ is also normal lax monoidal in the sense that there are canonical functions
\[
	\1\cong\Gamma(\yon)
	\qqand
	\Gamma(p)\times\Gamma(q)\to\Gamma(p\otimes q)
\]
satisfying certain well-known laws. But we won't need this, so we omit its proof.
\end{remark}

% \subsubsection{The polynomial $S\yon^S$ as a comonad on $\smset$}\label{page.poly_comonad}

% A \emph{comonad} on $\smset$ is a functor $F\colon\smset\to\smset$, equipped with two natural transformations $\epsilon\colon F\to\id$ and $\delta\colon F\to F\circ F$, satisfying three equations. We don't need this now, so we won't get into it here. But we will note that every comonad comes from an adjunction, and the adjunction corresponding to $S\yon^S$ is
% \[
% \adj{\smset}{-\times S}{-^S}{\smset}
% \]
% the ``curry/uncurry'' adjunction. In functional programming, the comonad $S\yon^S$ is called the \emph{state comonad},%
% \footnote{The comonad $S\yon^S$ is sometimes called the \emph{store} comonad.} 
% and the elements of $S$ are called states. \niu{Are they??} It is no coincidence that we also refer to elements of $S$ as states. \niu{How does the interpretation of the store comonad as a data structure indexed by $S$ along with a ``current location'' element in $S$ actually relate to this?}

% Again, we will be \emph{very} interested in polynomial comonads later---as mentioned in \cref{prop.ahman_uustalu1}, they are exactly categories!!---but for now we move on to things we can use right away in our story about dynamical systems.%
% \footnote{If you're curious what category the comonad $S\yon^S$ corresponds to, it's the one with object set $S$ and a unique morphism $s_1\to s_2$ for every pair of objects $s_1,s_2\in S$.}

%---- Section ----%
\section{General interaction}\label{sec.poly.dyn_sys.interact}

We now have all the pieces we need to fulfill the promises of \cref{sec.poly.intro.dyn_sys} by modeling interactions between dependent dynamical systems, which can change their interfaces and interaction patterns, using $\poly$.

\subsection{Wrapping juxtaposed dynamical systems together}

When wrapper interaces are used in conjunction with parallel products, they may encode multiple interacting dynamical systems as a single system.
Explicitly, given $n\in\nn$ and $n$ dynamical systems, $\phi_i\colon S_i\yon^{S_i}\to p_i$ for every $i\in\ord{n}$, we can first juxtapose them into a single dynamical system
\[\phi\colon \left(\prod_{i\in\ord{n}}S_i\right)\yon^{\prod_{i\in\ord{n}}S_i}\to\bigotimes_{i\in\ord{n}}p_i\]
by taking their parallel product.
Then we can wrap an interface $q$ around $\phi$ using a wrapper $f$, yielding a new dynamical system
\[\left(\prod_{i\in\ord{n}}S_i\right)\yon^{\prod_{i\in\ord{n}}S_i}\To{\phi}\bigotimes_{i\in\ord{n}}p_i\To{f}q.\]
On positions, $f$ gives a way of combining all the outputs of the constituent interfaces into a single output of the wrapper interace.
On directions, $f$ takes into account the current outputs of each of the constituent interfaces, as well as any new output given to the wrapper interface, then uses all of this to give input to each of the constituent interfaces.
In particular, a judiciously chosen on-directions function could feed output from some interfaces as inputs to others.
When $f$ is a wrapper around a parallel product of interfaces, we call $f$ the \emph{interaction pattern} between those interfaces.

\begin{example}[Repeater]
Suppose we have a dependent dynamical system $\phi\colon S\yon^S\to A\yon+\yon$, which takes unchanging input and sometimes returns elements of $A$ as output while other times returning only silence.
What if we wanted to construct a system $\psi$ that operates just like $\phi$, but \emph{always} returns elements of $A$ as output?
Where $\phi$ would have returned silence, we want $\psi$ to instead \emph{repeat} the last element of $A$ that it returned. (We allow $\psi$ to repeat an arbitrary element of $A$ if $\phi$ returns silence before it has returned any elements of $A$ yet.)

Let's think like a programmer.
What we need is a way to store an element of $A$ and then retrieve it.
So whenever $\phi$ returns an element of $A$ as output, we store it; then when $\phi$ returns silence, we retrieve the last element of $A$ we stored and return that instead.

What should this storage-retrieval dynamical system look like?
It needs to take elements of $A$ as input, return elements of $A$ as output, and store elements of $A$ as states.
In fact, the identity lens $\iota\colon A\yon^A\to A\yon^A$ works perfectly: it returns the element of $A$ currently stored as output and updates its state to the input it receives.

Now we can juxtapose our original system $\phi$ with the storage-retrieval system $\iota$ by taking their parallel product, yielding a dynamical system
\[
    \phi\otimes\iota\colon SA\yon^{SA}\to (A\yon+\yon)\otimes A\yon^A
\]
that runs both systems simultaneously---and independently.
But what we want is for $\phi$ and $\iota$ to interact with each other, and for the resulting system to only output elements of $A$.
To do so, we need to wrap an interface $A\yon$ around $\phi\otimes\iota$ by composing it with some lens
\[
    f\colon(A\yon+\yon)\otimes A\yon^A\to A\yon,
\]
the interaction pattern between the interfaces $A\yon+\yon$ and $A\yon^A$, which we must define.

Since $\otimes$ distributes over $+$, it suffices to give maps
\[
    g\colon A\yon\otimes A\yon^A\to A\yon \qqand h\colon\yon\otimes A\yon^A\to A\yon.
\]
The former corresponds to the case where $\phi$ outputs an element of $A$, while the latter corresponds to the case where $\phi$ is silent.

When $\phi$ outputs an element of $A$, we want to return that output, but we also want to give that output as input to $\iota$ so that it can be stored.
We don't need to do anything with the output of $\iota$; we can simply discard it.
So $g$ should send $(a,a')\mapsto a$ on positions, returning the output of $\phi$ and discarding the output of $\iota$; and the on-directions function $g^\sharp_{(a,a')}\colon\1\to A$ should specify the direction $a\in A$, feeding the output of $\phi$ as input to $\iota$.

Meanwhile, when $\phi$ outputs silence, we want to return the output of $\iota$ instead.
We also need to feed the output of $\iota$ back into $\iota$ as input so that it can continue to be stored.
So $h$ should be the identity on positions as well as the identity on directions.
\end{example}

\begin{example}[Paddling]\label{ex.paddler}
Say we wanted to build a Moore machine with interface $\nn\yon$; we may interpret its natural number output as the machine's current location.
What if we don't want this machine to jump around wildly?
Instead, suppose we want to be very strict about what how far the machine can move and what makes it move.

To accomplish this, we introduce two intermediary systems, which we call the \emph{paddler} and the \emph{tracker}:%
\footnote{Perhaps one could refer to the tracker as the \emph{demiurge}; it is responsible for maintaining the material universe.}
\[
  \text{paddler}\colon S\yon^S\to\2\yon
  \qqand
  \text{tracker}\colon T\yon^T\to\nn\yon^\2
\]
The paddler has interface $\2\yon$ because it is blind (i.e.\ takes no inputs) and can only move (i.e.\ output) its paddle to the left side or the right side: $\2\cong\{\text{left, right}\}$. The tracker has interface $\nn\yon^\2$ because it will announce the location of the machine (as an element $n\in\nn$) and watch what side the paddler is on (as an element of $\2$). 
We can wrap an interface $\nn\yon$ around them both using an interaction pattern
\[
    \2\yon\otimes\nn\yon^\2\to\nn\yon
\]
whose on-positions function is the projection $\2\nn\to\nn$, returning the location returned by the tracker, and whose on-directions function is the projection $\2\nn\to\2$, passing the output of the paddler as input to the tracker.

Let's leave the paddler's dynamics alone---how you make that paddler behave is totally up to you---and instead focus on the dynamics of the tracker.
We want it to watch for when the paddle switches from left to right or from right to left; at that moment it should push the paddler forward one unit. Thus the states of the tracker are given by $T\coloneqq\2\nn$, storing what side the paddler is on and the current location.
The on-positions function of the tracker is the projection $\2\nn\to\nn$ that returns the current location; then at each $(d,i)\in\2\nn$, the on-directions function of the tracker $\2\to\2\nn$ sends
\[
  d'\mapsto
	\begin{cases}
		(d',i)&\tn{if }d=d'\\
		(d',i+1)&\tn{if }d\neq d',
	\end{cases}
\]
storing the new direction of the paddler as well as moving the machine forward one unit if the paddle switches while keeping the machine still if the paddle stays still.
\end{example}

\begin{exercise}
Change the dynamics and state system of the tracker in \cref{ex.paddler} so that it exhibits the following behavior.

When the paddle switches once and stops, the tracker increases its location by one unit and stops, as before in \cref{ex.paddler}. But when the paddle switches twice in a row, the tracker increases its location by two units on the second switch! So if it is quiet for a while and then switches three times in a row, the tracker will increase its location by one then two then two.
\begin{solution}
We define a new tracker $T'\yon^{T'}\to\nn\yon^\2$ based on the one from \cref{ex.paddler} to watch for when the paddle switches sides once, at which point the tracker should increase its location by one, and watch for when the paddle switches sides twice in a row, at which point the tracker should increase its location by two.
To do this, we need the tracker to remember not just the current side the paddle is on, but the previous side the paddle was on as well.
The tracker should still remember the current location.
Thus the states of the tracker are given by $T\coloneqq\2\times\2\nn$, storing the previous side the paddler was on, the current side the paddler is on, and the current location.
The on-positions function of the tracker is the projection $\2\times\2\nn\to\nn$ that returns the current location; then at each $(d,d',i)\in\2\nn$, the on-directions function of the tracker $\2\to\2\times\2\nn$ sends
\[
  d''\mapsto
	\begin{cases}
		(d',d'',i)&\tn{if }d'=d''\\
		(d',d'',i+1)&\tn{if }d'\neq d''\tn{ and }d=d'\\
		(d',d'',i+2)&\tn{if }d'\neq d''\tn{ and }d\neq d'
	\end{cases}
\]
storing both the last side the paddle was on and the new side the paddle is on as well as moving the machine forward one unit if the paddle switches after not switching and two units if the paddle switches after just switching.
\end{solution}
\end{exercise}

\begin{example}
Suppose you have two systems with the same interface $p\coloneqq q\coloneqq\rr^\2\yon^{\rr^\2-\{(0,0)\}}$. 
\[
\begin{tikzpicture}
	\node (m1) {\faMotorcycle};
	\node[above=-.15 of m1] (e1) {\faEye};
	\node[draw, thick, blue!10, fit = (m1) (e1)] {};
	\node[below right=0 and 1 of m1] (m2) {\scalebox{-1}[1]{\faMotorcycle}};
	\node[above=-.15 of m2] (e2) {\faEye};
	\node[draw, thick, blue!10, fit = (m2) (e2)] {};
\end{tikzpicture}
\]
The output of each interface indicates the location of the system, while the range of possible inputs indicate the locations that the system could observe, relative to the location of the system itself.
Taking all pairs of reals except $(0,0)$ corresponds to the fact that the eye cannot see that which is at the same position as the eye.

Let's have the two systems constantly approaching each other with a force equal to the reciprocal of the squared distance between them.
If they finally collide, let's have the whole thing come to a halt.
To do this, we want the wrapper interface to be $\{\text{`go'}\}\yon+\{\text{`stop'}\}$, so that if the system returns `go' it can still advance to the next state, but if it returns `stop' it halts.
The wrapper $\rr^\2\yon^{\rr^\2-\{(0,0)\}}\otimes\rr^\2\yon^{\rr^\2-\{(0,0)\}}\to\{\text{`go'}\}\yon+\{\text{`stop'}\}$ is given on positions by
\[
  \big((x_\1,y_1),(x_2,y_2)\big)\mapsto
	\begin{cases}
		\text{`stop'}&\mbox{ if $x_1=x_2$ and $y_1=y_2$}\\
		\text{`go'}&\mbox{ otherwise}.
	\end{cases}
\]
On directions, we use the function
\[
  \big((x_1,y_1),(x_2,y_2)\big)\mapsto \big((x_2-x_1,y_2-y_1),(x_1-x_2,y_1-y_2)\big),
\]
so that each system is able to see the location of the other system relative to its own, i.e.\ the vector pointing from itself to the other system (unless that vector is zero, in which case the whole thing should have already halted). 

We can use these vectors to define the internal dynamics of each system so that they move the way we want them to.
Each system will hold as its internal state its current location and velocity, i.e.\ $S=\rr^\2\times\rr^\2$.
To define a lens $S\yon^S\to\rr^\2\yon^{\rr^\2-\{(0,0)\}}$ we simply return the current location, update the current location by adding the current velocity, and update the current velocity by adding a vector with appropriate magnitude pointing to the other system:
\begin{align*}
	\rr^\2\times\rr^\2&\To{\text{return}}\rr^\2\\
	\big((x,y),(v_x, v_y)\big)&\Mapsto{\text{return}}(x,y)
\end{align*}
\begin{align*}
	\rr^\2\times\rr^\2\times(\rr^\2-\{(0,0)\})&\To{\text{update}}\rr^\2\times\rr^\2\\
	\big((x,y),(v_x,v_y),(a,b)\big)&\Mapsto{\text{update}}\left(x+v_x,y+v_y,v_x+\frac{a}{(a^2+b^2)^{3/2}},v_y+\frac{b}{(a^2+b^2)^{3/2}}\right)
\end{align*}
\end{example}

\begin{exercise}
Suppose $(X,d)$ is a metric space, i.e.\ $X$ is a set of points and $d\colon X\times X\to\rr_{\geq0}$ is a distance function satisfying the usual laws.
Let's have robots interact in this space.

Let $A,A'$ be sets, each thought of as a set of signals, and let $a_0\in A$ and $a_0'\in A'$ be elements, each thought of as a default value. Let $p\coloneqq AX\yon^{A'X}$ and $p'\coloneqq A'X\yon^{AX}$, and imagine there are two robots, one with interface $p$, returning a signal as an element of $A$ and its location as a point in $X$, and one with interface $p'$, returning a signal as an element of $A'$ and also its location as a point in $X$.
\begin{enumerate}
	\item Write down an interaction pattern $p\otimes p'\to\yon$ such that each robot receives the other's location, but that it only receives the other's signal when their locations $x,x'$ are sufficiently close, namely when $d(x,x')<1$.
	Otherwise, it receives the default signal.
	\item Write down an interaction pattern $p\otimes p'\to\yon^{[0,5]}$ where the value $s\in [0,5]$ is a scalar, allowing the signal to travel $s$ times further.
	\item Suppose that each robot has a set $S,S'$ of possible private states in addition to their locations.
	What functions are involved in providing a dynamical system $\phi\colon SX\yon^{SX}\to AX\yon^{A'X}$, if the location state $x\in X$ is directly returned without modification?
	\item Change the setup in any way so that each robot only extends a port to hear the other's signal when the distance between them is less than $s$. Otherwise, they can only detect the position (element of $X$) that the other currently inhabits.
	(Don't worry too much about timing---one missed signal when the robots first get close or one extra signal when the robots first get far is okay.)
\qedhere
\end{enumerate}
\begin{solution}
Here $(X,d)$ is a metric space, $A,A'$ are sets of signals with default signals $a_0\in A$ and $a'_0\in A'$, and there are two robots, one with interface $p\coloneqq AX\yon^{A'X}$ returning a signal in $A$ and a location in $X$ and another with interface $p'\coloneqq A'X\yon^{AX}$ returning a signal in $A'$ and a location in $X$.
\begin{enumerate}
    \item An interaction pattern $p\otimes p'\to\yon$ consists of a trivial on-positions function $AX\times A'X\to\1$ (indicating that no outputs leave the system) and an on-directions function $AX\times A'X\to A'X\times AX$ indicating what inputs the robots should receive according to the outputs they return.
    To model the fact that the robots receive each others' locations, but only receive each others' signals rather than the default signals when the distance between their locations is less than $1$ according to the distance function $d$, this on-directions function should send
    \[
        ((a,x),(a',x'))\mapsto
          \begin{cases}
          	((a',x'),(a,x))&\tn{ if }d(x,x')<1\\
          	((a'_0,x'),(a_0,x))&\tn{ otherwise}.
          \end{cases}
    \]
    \item An interaction pattern $p\otimes p'\to\yon^{[0,5]}$ that allows the signal to travel $s\in[0,5]$ times further consists of a still trivial on-positions function and an on-directions function $AX\times A'X\times[0,5]\to A'X\times AX$ indicating what inputs the robots should receive according to the external input $s\in[0,5]$ as well as the outputs they return.
    To model the fact that the robots receive each others' locations, but only receive each others' signals rather than the default signals when the distance between their locations is less than $s$ according to the distance function $d$, this on-directions function should send
    \[
        ((a,x),(a',x'),s)\mapsto
          \begin{cases}
          	((a',x'),(a,x))&\tn{ if }d(x,x')<s\\
          	((a'_0,x'),(a_0,x))&\tn{ otherwise}.
          \end{cases}
    \]
    \item To provide a dynamical system $\phi\colon SX\yon^{SX}\to AX\yon^{A'X}$ under the condition that the on-positions function preserves the second coordinate $x\in X$, we must provide the first projection $SX\to A$ of an on-positions function that turns the robot's private state and current location into the signal it returns, as well as an on-directions function $SX\times A'X\to SX$ that provides a new private state and location for the robot given its old private state, old location, and the signal and location it receives from the other robot.
    
    \item To have the robots listen for each others' signals only when they are sufficiently close, we must move away from monomial interfaces and Moore machines to leverage dependency.
    There are several ways of doing this; we give just one method below.
    With $D\coloneqq\{\text{`close'},\text{`far'}\}$, let the robots' new interfaces be
    \[
        p\coloneqq \{\text{`close'}\}AX\yon^{DA'X}+\{\text{`far'}\}AX\yon^{DX} \qqand p'\coloneqq \{\text{`close'}\}A'X\yon^{DAX}+\{\text{`far'}\}A'X\yon^{DX},
    \]
    so that they may receive input telling them whether they are close or far, but cannot receive signals in $A$ or $A'$ when they are `far.'
    
    Then by the distributivity of $\otimes$ over $+$, their new interaction pattern $p\otimes p'\to\yon^{[0,5]}$ can be specified by four lenses, all trivial on positions: the lens
    \[
        \{\text{`close'}\}AX\yon^{DA'X}\otimes\{\text{`close'}\}A'X\yon^{DAX}\to\yon^{[0,5]},
    \]
    given by the on-directions function
    \[
        ((\text{`close'},a,x),(\text{`close'},a',x'),s)\mapsto
          \begin{cases}
          	((\text{`close'},a',x'),(\text{`close'},a,x))&\tn{ if }d(x,x')<s\\
          	((\text{`far'},a'_0,x'),(\text{`far'},a_0,x))&\tn{ otherwise};
          \end{cases}
    \]
    the lens
    \[
        \{\text{`far'}\}AX\yon^X\otimes\{\text{`far'}\}A'X\yon^X\to\yon^{[0,5]},
    \]
    given by the on-directions function
    \[
        ((\text{`far'},a,x),(\text{`far'},a',x'),s)\mapsto
          \begin{cases}
          	((\text{`close'},x'),(\text{`close'},x))&\tn{ if }d(x,x')<s\\
          	((\text{`far'},x'),(\text{`far'},x))&\tn{ otherwise};
          \end{cases}
    \]
    and two other lenses that can be defined arbitrarily, as they should never come up in practice.
    
    Finally, in order for each robot to properly remember whether the other is close or far, we record an element of $D$ in its state that is returned and updated: one robot is a lens
    \[
        \phi\colon DSX\yon^{DSX}\to \{\text{`close'}\}AX\yon^{DA'X}+\{\text{`far'}\}AX\yon^{DX}
    \]
    whose on-positions function preserves not just the third coordinate $x\in X$ but also the first coordinate $d\in D$, while the on-directions function also preserves the first coordinate $d\in D$; and the other robot is constructed similarly.
\end{enumerate}
\end{solution}
\end{exercise}

\subsection{Enclosing juxtaposed dynamical systems together}

We saw in \cref{subsec.poly.dyn_sys.new.sit_encl} that a situation (i.e.\ lens into $\yon$) for the interface of a dynamical system encloses that dynamical system in a closed system.
So it should not come as a surprise that a situation for a parallel product of interfaces yields an interaction pattern between the interfaces that only allows the interfaces to interact with each other, cutting off any other interaction with the outside world.

\begin{example}[Picking up the chalk]\label{ex.pickup_chalk}
Imagine that you see some chalk and you pinch it between your thumb and forefinger. 
An amazing thing about reality is that you will then have the chalk, in the sense that you can move it around.
How might we model this in $\poly$?
We will construct a closed dynamical system---one with interface $\yon$---consisting of only you and the chalk.
To do so, we will provide an interface for you, and interface for the chalk, and an enclosure for your juxtaposition.

Let's say that your hand can be at one of two heights, down or up, and that you can either press (apply pressure between your thumb and forefinger) or not press. Let's also say that you take in information about the chalk's height. Here are the two sets we'll be using:
\[
	H\coloneqq\{\text{`down', `up'}\}
	\qqand
	P\coloneqq\{\text{`press', `no press'}\}.
\]
Your interface is $HP\yon^H$: returning your own height and pressure, and receiving the chalk's height.

As for the chalk, it is either `in' your possession or `out' of it.
Either way, it also returns its height, which is either `down' or `up' in the air.
The chalk always takes in information about whether pressure is being applied or not.
When it's `out' of your possession, that's the whole story, but when it is `in' your possession, it also receives your hand's height.
All together, here are the two interfaces:
\[
	\const{You}\coloneqq HP\yon^H
	\qqand
	\const{Chalk}\coloneqq \{\text{`out'}\}H\yon^P + \{\text{`in'}\}H\yon^{HP}.
\]

Now we want to give the interaction pattern between you and the chalk.
As we said before, you see the chalk's height.
If your hand is not at the height of the chalk, the chalk receives no pressure.
Otherwise, your hand is at the height of the chalk, so the chalk receives your pressure (or lack thereof).
Furthermore, if the chalk is in your possession, it also receives your hand's height. 

To provide a map $\gamma\colon\const{You}\otimes\const{Chalk}\to\yon$, we use the fact that $\const{Chalk}$ is a sum and that $\otimes$ distributes over $+$.
Thus we need to give two maps
\[
	\alpha\colon HP\yon^H\otimes H\yon^P\to\yon
	\qqand
	\beta\colon HP\yon^H\otimes H\yon^{HP}\to\yon
\]
The map $\beta$, corresponding to when the chalk is in your possession, is quite easy to describe; it can be unfolded to a function
$HPH\to HHP$, and we take it to be the obvious map sending your height and pressure to the chalk and the chalk's height to you; see \cref{exc.pickup_chalk}. But $\alpha$ is more semantically interesting: it is given by the map
\[
  (h_\const{You},p_\const{You},h_\const{Chalk})\mapsto
  \begin{cases}
  	(h_\const{Chalk},\text{`no press'}) & \tn{ if } h_\const{You} \neq h_\const{Chalk} \\
  	(h_\const{Chalk},p_\const{You}) & \tn{ if } h_\const{You} = h_\const{Chalk}.
  \end{cases}
\]

So now we've got you and the chalk in enclosed together by $\gamma$, so we are ready to add some dynamics.
Your dynamics can be whatever you want, so let's just add some dynamics to the chalk (you'll get to give yourself some dynamics in \cref{exc.pickup_chalk}).
The chalk has only four states $C\coloneqq \{\text{`out'}, \text{`in'}\} \times H \cong\4$: the $H$ coordinate is its current height, and the other coordinate is whether or not it is in your possession.
We will give a dynamical system $C\yon^C\to\const{Chalk}$ with states $C$ and interface $\const{Chalk}$, i.e.\ a lens
\begin{equation}\label{eqn.chalk_dynamics}
	\{\text{`out'}, \text{`in'}\} \times H\yon^{\{\text{`out'}, \text{`in'}\} \times H}\to \{\text{`out'}\}H\yon^P + \{\text{`in'}\}H\yon^{HP}.
\end{equation}
On positions, as you might guess, the chalk returns its height and whether it is in your possession directly.
On directions, if it's not in your possession, it falls down unless you catch it (i.e.\ apply pressure to it so that it enters your possession); if it is in your possession, it takes whatever height you give it.
So we can express the on-directions function of \eqref{eqn.chalk_dynamics} at $(\text{`out'}, h_\const{Chalk})$ as
\begin{align*}
	\text{`no press'} &\mapsto (\text{`out', `down'}) \\
	\text{`press'} &\mapsto (\text{`in'}, h_\const{Chalk})
\end{align*}
and the on-directions function of \eqref{eqn.chalk_dynamics} at $(\text{`in'}, h_\const{Chalk})$ as
\begin{align*}
	(h_\const{You}, \text{`no press'}) &\mapsto (\text{`out'}, h_\const{You}) \\
	(h_\const{You}, \text{`press'}) &\mapsto (\text{`in'}, h_\const{You}).
\end{align*}
Obviously, this is all quite complicated, intricate, and contrived.
Our goal here is to show that you can define interactions in which one system can engage with or disengage from another, where one system controls the behavior of the other when the two are engaged.
\end{example}

\begin{exercise}\label{exc.pickup_chalk}
\begin{enumerate}
	\item In \cref{ex.pickup_chalk}, we said that $\beta\colon HP\yon^H\otimes H\yon^{HP}\to\yon$ was easy to describe and given by a function $HPH\to HHP$. Explain what's being said, and provide the function.
	\item Provide dynamics to the $\const{You}$ interface (i.e.\ specify a dynamical system with interface $\const{You}$) so that you repeatedly reach down and grab the chalk, lift it with your hand, and drop it. 
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item A lens $HP\yon^H \otimes H\yon^{HP} \to \yon$ consists of an on-positions function $HPH \to \1$ and an on-directions function $HPH \times \1 \to HHP$.
    This amounts to a function $HPH \to HHP$.
    We can easily define this function to be the isomorphism that sends $(h,p,h') \in HPH$ to $(h,h',p)$.
    \item To model the way in which you cycle through three possible actions---reaching down and grabbing the chalk, lifting it with your hand, and dropping it---it is simplest to work with a set of $\3$ possible states.
    So we will give your dynamics as a lens $\3\yon^\3 \to HP\yon^H$, where the return function $\3 \to HP$ indicates what happens at each state, sending $1 \mapsto (\text{down, press}), 2 \mapsto (\text{up, press})$, and $3 \mapsto (\text{up, no press})$.
    Then the update function $\3H \to \3$ always goes to the next state, regardless of input: it ignores the $H$ coordinate and sends $1$ to $2$, $2$ to $3$, and $3$ to $1$.
\end{enumerate}
\end{solution}
\end{exercise}

Given $n\in\nn$ and polynomials $p_1,\ldots,p_n$ as interfaces, a situation $p_1\otimes\cdots\otimes p_n\to\yon$ puts these $n$ interfaces in an enclosure together.
The following proposition provides an alternative perspective on such situations.

\begin{proposition}\label{prop.situations2}
Given polynomials $p,q\in\poly$, there is a bijection
\begin{equation} \label{eqn.situations2}
\Gamma(p\otimes q)\cong\smset\big(q(\1),\Gamma(p)\big)\;\times\;\smset\big(p(\1),\Gamma(q)\big).
\end{equation}
\end{proposition}
The idea is that specifying an enclosure for interfaces $p$ and $q$ together is equivalent to specifying an enclosure for $p$ for every output $q$ might return and specifying an enclosure for $q$ for every output $p$ might return.
\begin{proof}[Proof of \cref{prop.situations2}]
This is a direct calculation:
\begin{align*}
	\Gamma(p\otimes q) &\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}(p[i]\times q[j]) \\
	&\iso
	\left(\prod_{j\in q(\1)}\prod_{i\in p(\1)}p[i]\right)\times
		 \left(\prod_{i\in p(\1)}\prod_{j\in q(\1)}q[j]\right) \\
	&\iso
	\smset(q(\1),\Gamma(p))\times\smset(p(\1),\Gamma(q)).
\end{align*}
\end{proof}

\begin{example}
An enclosure $f\colon B\yon^A\otimes B'\yon^{A'}\to \yon$, corresponds to a map $BB'\to AA'$. In other words, for every pair of outputs $(b,b')\in BB'$, the enclosure $f$ specifies a pair of inputs $(a,a')\in AA'$. 

Let's think of elements of $B$ and $B'$ not as outputs, but as locations that two machines may occupy.
\[
\begin{tikzpicture}[oriented WD, bb port length=0]
	\node[bb={1}{0}, fill=blue!10, dotted] (p) {$b$};
	\node[bb={1}{0}, fill=blue!10, dotted, below right=-0.5 and 0.5 of p] (q) {$b'$};
	\node[bb={0}{0}, inner sep=10pt, fit=(p) (q)] {};
	\node at (p_in1) {\faEye};
	\node at (q_in1) {\faEye};
\end{tikzpicture}
\hspace{.5in}
\begin{tikzpicture}[oriented WD, bb port length=0]
	\node[bb={1}{0}, fill=blue!10, dotted] (p) {$b$};
	\node[bb={1}{0}, fill=blue!10, dotted, below left=-0.5 and 0.5 of p] (q) {$b'$};
	\node[bb={0}{0}, inner sep=10pt, fit=(p) (q)] {};
	\node at (p_in1) {\faEye};
	\node at (q_in1) {\faEye};
\end{tikzpicture}
\]
Then given a pair of locations $(b,b')$, the interaction pattern $f$ tells us what the two eyes see, i.e.\ what values of $(a,a')$ they get.
Equivalently, \eqref{eqn.situations2} says that the interaction pattern tells us what values the first eye sees at any location when the second eye's location is fixed at $b'$, as well as what values the second eye sees at any location when the first eye's location is fixed at $b$.

Here we see that \eqref{eqn.situations2} provides two ways to interpret the interaction pattern between two interfaces in a closed system: either as an enclosure around each interface that the other is part of, or as a single enclosure around them both.
\end{example}

\begin{exercise}
Let $p\coloneqq q\coloneqq\nn\yon^\nn$.
We wish to specify an enclosure around their juxtaposition.
\begin{enumerate}
    \item Say we wanted to feed the output of $q$ as input to $p$.
    What function $f\colon q(\1)\to\Gamma(p)$ captures this behavior?
    \item Say we wanted to feed the sum of the outputs of $p$ and $q$ as input to $q$.
    What function $g\colon p(\1)\to\Gamma(q)$ captures this behavior?
    \item What enclosure $\gamma\colon p\otimes q\to\yon$ does the pair of functions $(f,g)$ correspond to via \eqref{eqn.situations2}?
	\item Let dynamical systems $\phi\colon\nn\yon^\nn\to p$ and $\psi\colon\nn\yon^\nn\to q$ both be the identity on $\nn\yon^\nn$.
	Suppose $\phi$ starts in the state $0\in\nn$ and $\psi$ starts in the state $1\in\nn$.
	Describe the behavior of the system obtained by enclosing $\phi$ and $\psi$ together with $\gamma$, i.e.\ the system $(\phi\otimes\psi)\then\gamma$.
\qedhere
\end{enumerate}
\begin{solution}
We have $p\coloneqq q\coloneqq\nn\yon^\nn$.
\begin{enumerate}
    \item If we want to feed the output of $q$ as input to $p$, the corresponding function $f\colon q(\1)\to\Gamma(p)$ should send any output $b\in q(\1)$ to the enclosure $\nn\to\nn$ of $p$ that sends any natural number output of $p$ to $b$ itself.
    That is, $f$ is the function $b\mapsto(\_\mapsto b)$.
    \item If we want to feed the sum of the outputs of $p$ and $q$ as input to $q$, the corresponding function $g\colon p(\1)\to\Gamma(q)$ should send any output $a\in q(\1)$ to the enclosure $\nn\to\nn$ of $q$ that sends every natural number output $b$ of $q$ to the sum $a+b$.
    That is, $g$ is the function $a\mapsto(b\mapsto a+b)$.
    \item Together, $f$ and $g$ form a function $\nn\times\nn\to\nn\times\nn$ mapping $(a,b)\mapsto((f(b))(a),(g(a))(b))=(b,a+b)$, which is the enclosure $\gamma\colon p\otimes q\to\yon$ that $(f,g)$ corresponds to via \eqref{eqn.situations2}.
    \item As $\phi$ and $\psi$ are both the identity, the system $(\phi\otimes\psi)\then\gamma$ is really just the system $\gamma\colon p\otimes q\to\yon$.
    When it is at state $(a,b)$, its next state will be $(b,a+b)$.
    So if its initial state is $(0,1)$, its following states will be $(1,1),(1,2),(2,3),(3,5),(5,8),(8,13),\ldots$, forming the familiar Fibonacci sequence. 
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
We will use \eqref{eqn.situations2} to consider the interaction pattern $\gamma$ between \const{You} and \const{Chalk} from \cref{ex.pickup_chalk} as a pair of functions $\const{You}(\1)\to\Gamma(\const{Chalk})$ and $\const{Chalk}(\1)\to\Gamma(\const{You})$.
\begin{enumerate}
	\item How does the chalk's output specify an enclosure for you? That is, write the map $\const{Chalk}(\1)\to\Gamma(\const{You})$.
	\item How does your output specify an enclosure for the chalk? That is, write the map $\const{You}(\1)\to\Gamma(\const{Chalk})$.
\qedhere
\end{enumerate}
\begin{solution}
We wish to write the enclosure $\gamma\colon\const{You}\otimes\const{Chalk}\to\yon$ from \cref{ex.pickup_chalk} as a pair of functions $\const{You}(\1)\to\Gamma(\const{Chalk})$ and $\const{Chalk}(\1)\to\Gamma(\const{You})$ via \eqref{eqn.situations2}.
\begin{enumerate}
    \item Fix an output $(s_\const{Chalk}, h_\text{chalk})\in\const{Chalk}(\1)=\{\text{`out'},\text{`in'}\}H$ of the chalk.
    If $s_\const{Chalk}=\text{`out'}$, then the corresponding enclosure $\const{You}\iso HP\yon^H\to\yon$ given by $f$ via \eqref{eqn.situations2} can be thought of as the function $HP\to H$ sending
    \[
        (h_\const{You},p_\const{You})\mapsto h_\const{Chalk},
    \]
    according to the behavior of $\alpha\colon HP\yon^H\otimes H\yon^P\to\yon$ when we fix $h_\text{chalk}$ to be position in the rightmost $H$ and focus on the result in the exponent $H$ on the left.
    Meanwhile, if $s_\const{Chalk}=\text{`in'}$, then the corresponding enclosure $\const{You}\iso HP\yon^H\to\yon$ can also be thought of as the function $HP\to H$ sending
    \[
        (h_\const{You},p_\const{You})\mapsto h_\const{Chalk},
    \]
    according to the behavior of $\beta\colon HP\yon^H\otimes H\yon^{HP}\to\yon$ when we fix $h_\text{chalk}$ to be position in the rightmost $H$ and focus on the result in the exponent $H$ on the left.
    So overall, the map $\const{Chalk}(\1)\to\Gamma(\const{You})$ sends
    \[
        (\_,h_\text{chalk})\mapsto((\_,\_)\mapsto h_\const{Chalk}).
    \]
    
    \item Fix an output $(h_\const{You},p_\const{You})\in\const{You}(\1)=HP$ of the chalk.
    Then the corresponding enclosure $\const{Chalk}\iso\{\text{`out'}\}H\yon^P + \{\text{`in'}\}H\yon^{HP}\to\yon$ can be thought of as a pair of functions: one $\{\text{`out'}\}H\to P$ sending
    \[
        (\text{`out'},h_\const{Chalk})\mapsto
            \begin{cases}
  	            \text{`no press'} & \tn{ if } h_\const{You} \neq h_\const{Chalk} \\
  	            p_\const{You} & \tn{ if } h_\const{You} = h_\const{Chalk}.
            \end{cases}
    \]
    according to the behavior of $\alpha\colon HP\yon^H\otimes H\yon^P\to\yon$ when we fix $(h_\const{You},p_\const{You})$ to be position in $HP$ on the left and focus on the result in the exponent $P$ on the right; and another $\{\text{`in'}\}H\to HP$ sending
    \[
        (\text{`in'},h_\const{Chalk})\mapsto(h_\const{You},p_\const{You})
    \]
    according to the behavior of $\beta\colon HP\yon^H\otimes H\yon^{HP}\to\yon$ when we fix $(h_\const{You},p_\const{You})$ to be position in $HP$ on the left and focus on the result in the exponent $HP$ on the right.
    So overall, the map $\const{You}(\1)\to\Gamma(\const{Chalk})$ sends
    \[
        (h_\const{You},p_\const{You})\mapsto\left(
            \begin{aligned}
                (\text{`out'},h_\const{Chalk})&\mapsto
                    \begin{cases}
          	            \text{`no press'} & \tn{ if } h_\const{You} \neq h_\const{Chalk} \\
          	            p_\const{You} & \tn{ if } h_\const{You} = h_\const{Chalk}
                    \end{cases} \\
                (\text{`in'},h_\const{Chalk})&\mapsto(h_\const{You},p_\const{You})
            \end{aligned}
        \right).
    \]
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
\begin{enumerate}
	\item State and prove a generalization of \eqref{eqn.situations2} from \cref{prop.situations2} for $n$-many polynomials $p_1,\ldots,p_n\in\poly$.
	\item Generalize the ``idea'' statement between \cref{prop.situations2} and its proof.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We generalize \eqref{eqn.situations2} for $n$ polynomials as follows.
    Given polynomials $p_1,\ldots,p_n\in\poly$, we claim there is a bijection
    \[
        \Gamma\left(\bigotimes_{i=1}^n p_i \right) \iso \prod_{i=1}^n \smset\left(\prod_{\substack{1 \leq j \leq n, \\ j \neq i}} p_j(\1), \Gamma(p_i)\right).
    \]
    The $n=1$ case is clear, and the $n=2$ case is given by \eqref{eqn.situations2}.
    Then by induction on $n$, we have
    \begin{align*}
        \Gamma\left(\bigotimes_{i=1}^n p_i \right) &\iso \smset\left(p_n(\1), \Gamma\left(\bigotimes_{i=1}^{n-1} p_i \right)\right) \times \smset\left(\prod_{i=1}^{n-1} p_i(\1), \Gamma(p_n)\right) \tag*{\eqref{eqn.situations2}} \\
        &\iso \smset\left(p_n(\1), \prod_{i=1}^{n-1} \smset\left(\prod_{\substack{1 \leq j \leq n-1, \\ j \neq i}} p_j(\1), \Gamma(p_i)\right)\right) \times \smset\left(\prod_{i=1}^{n-1} p_i(\1), \Gamma(p_n)\right) \tag{Inductive hypothesis} \\
        &\iso \prod_{i=1}^{n-1} \smset\left(\prod_{\substack{1 \leq j \leq n, \\ j \neq i}} p_j(\1), \Gamma(p_i)\right) \times \smset\left(\prod_{i=1}^{n-1} p_i(\1), \Gamma(p_n)\right) \tag{Universal properties of products and internal homs},
    \end{align*}
    and the result follows.
    \item The general idea is that specifying an enclosure for interfaces $p_1,\ldots,p_n$ together is equivalent to specifying an enclosure for $p_i$ for every output all the other interfaces might return, for each $i\in\ord{n}$.
\end{enumerate}
\end{solution}
\end{exercise}

\subsection{Wiring diagrams as interaction patterns}

We first saw interactions between systems drawn as wiring diagrams in \cref{sec.poly.intro.dyn_sys}.
They depict systems as boxes, showing how they can send inputs and outputs to each other through the wires between them, as well as how multiple systems can combine to form a larger system whenever smaller boxes are nested within a larger box.

Formally, and more precisely, we can think of each box in a wiring diagram as an interface given by some monomial.
Then the entire wiring diagram---specifying how these boxes nest within a larger box---is just an interaction pattern between the interfaces, with the larger box playing the role of the wrapper interface.

\begin{example}
Here is a simple wiring diagram.
\begin{equation}\label{eqn.control_diag}
\begin{tikzpicture}[oriented WD, baseline=(B)]
	\node[bb={2}{1}, fill=blue!10] (plant) {\texttt{Plant}};
	\node[bb={1}{1}, below left=-1 and 1 of plant, fill=blue!10]  (cont) {\texttt{Controller}};
	\node[circle, inner sep=1.5pt, fill=black, right=.1] at (plant_out1) (pdot) {};
	\node[bb={0}{0}, inner ysep=25pt, inner xsep=1cm, fit=(plant) (pdot) (cont)] (outer) {};
	\coordinate (outer_out1) at (outer.east|-plant_out1);
	\coordinate (outer_in1) at (outer.west|-plant_in1);
	\begin{scope}[above, font=\footnotesize]
  	\draw (outer_in1) -- node {$A$} (plant_in1);
  	\draw (cont_out1) to node (B) {$B$} (plant_in2);
  	\draw (plant_out1) to node {$C$} (outer_out1);
  	\draw
  		let 
  			\p1 = (cont.south west-| pdot),
  			\p2 = (cont.south west),
  			\n1 = \bby,
  			\n2 = \bbportlen
  		in
  			(pdot) to[out=0, in=0]
  			(\x1+\n2, \y1-\n1) --
  			(\x2-\n2, \y2-\n1) to[out=180, in=180]
  			(cont_in1);
		\end{scope}
	\node[below=0of outer.north] {\texttt{System}};
\end{tikzpicture}
\end{equation}
The plant is receiving information from the world outside the system, as well as from the controller. It's also producing information for the outside world which is being monitored by the controller.

There are three boxes shown in \eqref{eqn.control_diag}: the controller, the plant, and the system. Each has a fixed set of inputs and outputs, and so we can consider the box as a monomial interface.
\begin{equation}\label{eqn.basic_diagram}
	\const{Plant}\coloneqq C\yon^{AB}
	\qquad\quad
	\const{Controller}\coloneqq B\yon^C
	\qquad\quad
	\const{System}\coloneqq C\yon^A.
\end{equation}
The wiring diagram itself is a wrapper
\[
	w\colon\const{Plant}\otimes\const{Controller}\to\const{System},
\]
specifying an interaction pattern between $\const{Plant}$ and $\const{Controller}$ with wrapper interface $\const{System}$.
Concretely, $w$ is a lens $CB\yon^{ABC}\to C\yon^A$ that dictates how wires are feeding from outputs to inputs.
Like all lenses between monomials, $w$ consists of an on-positions function $CB\to C$ and an on-directions function $CBA\to ABC$.

The wiring diagram is a picture that tells us which maps to use.
The on-positions function says ``inside the system you have boxes outputting values of type $C$ and $B$.
The system needs to produce an output of type $C$; how shall I obtain it?''
The answer, according to the wiring diagram, is to send $(c,b)\mapsto c$.

Meanwhile, the on-directions function says ``inside the system you have boxes outputting values of type $C$ and $B$, and the system itself is receiving an input value of type $A$.
The boxes inside need input values of type $A$, $B$, and $C$; how shall I obtain them?''
Again, we can read the answer off the wiring diagram: send $(c,b,a)\mapsto (a,b,c)$.

Note that neither the wiring diagram nor any of the boxes within it are dynamical systems on their own.
Rather, each box is a monomial that could be the interface of a dynamical system.
When we assign to a box a dynamical system having that box as its interface, we say that \emph{give dynamics} to the box.
So the entire wiring diagram is a wrapper that tells us how, given the dynamics for each inner box,
\[
\phi\colon S\yon^S\to\const{Plant}
\qqand
\psi\colon T\yon^T\to\const{Controller},
\]
we can obtain the dynamics for the outer box:
\[
ST\yon^{ST}\To{\phi\otimes\psi}\const{Plant}\otimes\const{Controller}\To{w}\const{System}.
\]
\end{example}

\begin{exercise}
\begin{enumerate}
	\item Make a new wiring diagram like \eqref{eqn.control_diag} except where the controller also receives information of type $A'$ from the outside world.
	\item What are the monomials represented by the boxes in your diagram (replacing \eqref{eqn.basic_diagram})?
	\item What is the interaction pattern represented by this wiring diagram?
	Give the corresponding lens, including its on-positions and on-directions functions.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item Here is the wiring diagram \eqref{eqn.control_diag} modified so that the controller also receives information of type $A'$ from the outside world.
\[
\begin{tikzpicture}[oriented WD, baseline=(B)]
	\node[bb={2}{1}, fill=blue!10] (plant) {\texttt{Plant}};
	\node[bb={2}{1}, below left=-1 and 1 of plant, fill=blue!10]  (cont) {\texttt{Controller}};
	\node[circle, inner sep=1.5pt, fill=black, right=.1] at (plant_out1) (pdot) {};
	\node[bb={0}{0}, inner ysep=25pt, inner xsep=1cm, fit=(plant) (pdot) (cont)] (outer) {};
	\coordinate (outer_out1) at (outer.east|-plant_out1);
	\coordinate (outer_in1) at (outer.west|-plant_in1);
	\coordinate (outer_in2) at (outer.west|-cont_in1);
	\begin{scope}[above, font=\footnotesize]
  	\draw (outer_in1) -- node {$A$} (plant_in1);
  	\draw (outer_in2) -- node {$A'$} (cont_in1);
  	\draw (cont_out1) to node (B) {$B$} (plant_in2);
  	\draw (plant_out1) to node {$C$} (outer_out1);
  	\draw
  		let 
  			\p1 = (cont.south west-| pdot),
  			\p2 = (cont.south west),
  			\n1 = \bby,
  			\n2 = \bbportlen
  		in
  			(pdot) to[out=0, in=0]
  			(\x1+\n2, \y1-\n1) --
  			(\x2-\n2, \y2-\n1) to[out=180, in=180]
  			(cont_in2);
		\end{scope}
	\node[below=0of outer.north] {\texttt{System}};
\end{tikzpicture}
\]
    \item The monomials represented by the boxes in this diagram are the same, except that $\const{Controller}$ and $\const{System}$ each have an extra $A'$ exponent:
    \[
	\const{Plant}\coloneqq C\yon^{AB}
	\qquad\quad
	\const{Controller}\coloneqq B\yon^{A'C}
	\qquad\quad
	\const{System}\coloneqq C\yon^{AA'}.
    \]
    
    \item The interaction pattern represented by this wiring diagram is the lens
    \[
    	w'\colon \const{Plant}\otimes\const{Controller}\to\const{System}
    \]
    consisting of an on-positions function $CB\to C$ given by $(c,b)\mapsto c$ and an on-directions function $CBAA'\to ABA'C$ given by $(c,b,a,a')\mapsto(a,b,a',c)$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Consider the following wiring diagram.
\[
\begin{tikzpicture}[oriented WD, font=\footnotesize, bb port sep=1, bb port length=2.5pt, bb min width=.4cm, bby=.2cm, inner xsep=.2cm, x=.5cm, y=.3cm, text height=1.5ex, text depth=.5ex]
  	\node[bb={2}{1}, fill=blue!10] (Trf) {$\const{Alice}$};
  	\node[bb={1}{2}, fill=blue!10, below=1 of Trf] (Trg) {$\const{Bob}$};
		\node[bb={2}{2}, fill=blue!10] at ($(Trf)!.5!(Trg)+(1.5,0)$) (Trh) {$\const{Carl}$}; 
  	\node[bb={0}{0}, fit={($(Trf.north west)+(-.25,4)$) (Trg) ($(Trh.north east)+(.25,0)$)}] (Tr) {};
		\node[below] at (Tr.north) {$\const{Team}$};
  	\node[coordinate] at (Tr.west|-Trf_in2) (Tr_in1) {};
  	\node[coordinate] at (Tr.west|-Trg_in1) (Tr_in2) {};
  	\node[coordinate] at (Tr.east|-Trh_out2) (Tr_out1) {};
  	\node at ($(Trg_out2)+(5pt,0)$) (dot) {$\bullet$};
\begin{scope}[font=\tiny]
  	\draw[shorten <=-2pt] (Tr_in1) -- node[below=-3pt] {$A$} (Trf_in2);
  	\draw[shorten <=-2pt] (Tr_in2) -- node[below=-3pt] {$B$} (Trg_in1);
		\draw (Trf_out1) to node[above=-3pt] {$D$} (Trh_in1);
		\draw (Trg_out1) to node[above=-3pt] {$E$} (Trh_in2);
  	\draw (Trg_out2) -- node[below=-3pt] {$F$} (dot.center);
  	\draw[shorten >=-2pt] (Trh_out2) -- node[below=-3pt] {$G$} (Tr_out1);
  	\draw let \p1=(Trh.east), \p2=(Trf.north west), \n1=\bbportlen, \n2=\bby in
  		(Trh_out1) to[in=0] (\x1+\n1,\y2+\n2) -- node[pos=.3, below=-3pt] {$H$} (\x2-\n1,\y2+\n2) to[out=180] (Trf_in1);
	\end{scope}
\end{tikzpicture}
\]
\begin{enumerate}
	\item Write out the polynomials for each of $\const{Alice}$, $\const{Bob}$, and $\const{Carl}$.
	\item Write out the polynomial for the outer box, $\const{Team}$.
	\item The wiring diagram constitutes a lens $f$ in $\poly$; what is its domain and codomain?
	\item What lens is it?
	\item Suppose we have dynamical systems $\alpha\colon A\yon^A\to\const{Alice}$, $\beta\colon B\yon^B\to\const{Bob}$, and $\gamma\colon C\yon^C\to\const{Carl}$. What is the induced dynamical system with interface $\const{Team}$?
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item According to the wiring diagram, we have that $\const{Alice}\coloneqq D\yon^{HA},$ that $\const{Bob}\coloneqq EF\yon^B,$ and that $\const{Carl}\coloneqq HG\yon^{DE}.$
    \item According to the wiring diagram, we have that $\const{Team}\coloneqq G\yon^{AB}.$
    \item The wiring diagram constitutes a wrapper
    \[
        f\colon\const{Alice}\otimes\const{Bob}\otimes\const{Carl}\to\const{Team}.
    \]
    Its domain is $\const{Alice}\otimes\const{Bob}\otimes\const{Carl}\iso DEFHG\yon^{HABDE}$, while its codomain is $\const{Team}=G\yon^{AB}$.
    \item On positions, the lens $f$ is a function $DEFHG\to G$ that sends $(d,e,f,h,g)\mapsto g$.
    On directions, $f$ is a function $DEFHGAB\to HABDE$ that sends $(d,e,f,h,g,a,b)\mapsto(h,a,b,d,e)$.
    \item Given dynamical systems $\alpha\colon A\yon^A\to\const{Alice}$, $\beta\colon B\yon^B\to\const{Bob}$, and $\gamma\colon C\yon^C\to\const{Carl}$, the dynamical system induced by the wiring diagram is given by the composite lens
    \[
        ABC\yon^{ABC}\To{\alpha\otimes\beta\otimes\gamma}\const{Alice}\otimes\const{Bob}\otimes\const{Carl}\To{f}\const{Team}.
    \]
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}[Long division]
\begin{enumerate}
	\item Let $\fun{divmod}\colon\nn\times\nn_{\geq1}\to\nn\times\nn$ send $(a,b)\mapsto(a\bdiv b, a\bmod b)$; for example, it sends $(10,7)\mapsto(1,3)$ and $(30,7)\mapsto(4,2)$.
	Use \cref{exc.funs_to_moore} to turn it into a dynamical system.
	\item Interpret the following wiring diagram, where we have already given dynamics to each box as indicated by their labels:
\[
\begin{tikzpicture}[oriented WD, bb small]
	\node[bb port sep=3, fill=blue!10, bb={2}{2}] (divmod) {divmod};
	\node[bb={0}{1}, fill=blue!10, left=of divmod_in2] (7) {$7$};
	\node[bb port sep=2, bb={2}{1}, fill=blue!10, below right=-1 and 3 of divmod_out2] (times) {$*$};
	\node[bb={0}{1}, fill=blue!10, below left=-1 and 1 of times_in2] (10) {$10$};
	\node[bb={0}{0}, inner xsep=\bbx, fit=(divmod) (times)(7) (10)] (outer) {};
	\coordinate (outer_in1) at (outer.west|-divmod_in1);
	\coordinate (outer_out1) at (outer.east|-divmod_out1);
	\coordinate (outer_out2) at (outer.east|-times_out1);
	\draw (outer_in1) -- (divmod_in1);
	\draw (7_out1) -- (divmod_in2);
	\draw (10_out1) -- (times_in2);
	\draw (divmod_out1) -- (outer_out1);
	\draw (divmod_out2) to (times_in1);
	\draw (times_out1) -- (outer_out2);
\end{tikzpicture}
\]
	\item Use the above and a diagram of the following form to create a dynamical system that alternates between spitting out $0$'s and the base-$10$ digits of $1/7$ after the decimal point, like so: 
\[
\begin{tikzpicture}[oriented WD]
	\node[bb={1}{2}, fill=blue!10] (inner) {};
	\node[bb={0}{0}, inner xsep=1cm, inner ysep=1cm] (outer) {};
	\coordinate (outer_out1) at (outer.east|-inner_out1);
	\draw[shorten >=-3pt] (inner_out1) -- (outer_out1);
	\draw 
		let \p1=(inner.south east), \p2=(inner.south west), \n1=\bbportlen, \n2=\bby in
		(inner_out2) to[in=0] (\x1+\n1,\y1-\n2) -- (\x2-\n1,\y1-\n2) to[out=180] (inner_in1);
		\node[right, font=\footnotesize] at (outer_out1) {$010402080507010402080507010402080507\cdots$};
\end{tikzpicture}
\]
We will see in \cref{**} how to use a system to build another dynamical system that only returns every other output of the original system, then apply this to the above system in \cref{**}.
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item Using \cref{exc.funs_to_moore}, we can turn $\fun{divmod}$ into the dynamical system $\fun{divmod}\colon\nn\times\nn\yon^{\nn\times\nn}\to\nn\times\nn\yon^{\nn\times\nn_{\geq1}}$ whose return function is the identity on $\nn\times\nn$ and whose update function $\nn\times\nn\times\nn\times\nn_{\geq1}\to\nn\times\nn$ sends $(\_,\_,a,b)\mapsto(a\bdiv b,a\bmod b)$.
    \item From left to right, the inner boxes represent monomial interfaces $\nn_{\geq1}\yon, \nn\times\nn\yon^{\nn\times\nn_{\geq1}}, \nn\yon,$ and $\nn\yon^{\nn\times\nn}$.
    The box labeled $7$ is given dynamics $7\colon\yon\to\nn_{\geq1}\yon$ so that it always returns the output $7$; similarly, the box labeled $10$ is given dynamics $10\colon\yon\to\nn\yon$ so that it always returns the output $10$.
    Meanwhile, the box labeled is given dynamics $\fun{divmod}\colon\nn\times\nn\yon^{\nn\times\nn}\to\nn\times\nn\yon^{\nn\times\nn_{\geq1}}$ from the previous part; while we can again apply \cref{exc.funs_to_moore} to the standard multiplication function $*\colon\nn\times\nn\to\nn$ to give the box labeled $*$ dynamics as well, yielding a dynamical system $*\colon\nn\yon^\nn\to\nn\yon^{\nn\times\nn}$ whose return function is the identity on $\nn$ and whose update function $\nn\times\nn\times\nn\to\nn$ sends $(\_,m,n)\mapsto m*n$.
    
    Then the outer box is the monomial interface $\nn\times\nn\yon^\nn$, and the wiring diagram is the interaction pattern
    \[
        w\colon\nn_{\geq1}\yon\otimes\left(\nn\times\nn\yon^{\nn\times\nn_{\geq1}}\right)\otimes\nn\yon\otimes\nn\yon^{\nn\times\nn}\to\nn\times\nn\yon^\nn
    \]
    with on-positions function $(s,q,r,t,p)\mapsto(q,p)$ and on-directions function $(s,q,r,t,p,a)\mapsto(a,s,r,t)$.
    So the dynamical system induced by the wiring diagram is the composite lens $\phi$ given by
    \[
        \yon\otimes\left(\nn\times\nn\yon^{\nn\times\nn}\right)\otimes\yon\otimes\nn\yon^\nn\To{7\otimes\fun{divmod}\otimes10\otimes*}\nn_{\geq1}\yon\otimes\left(\nn\times\nn\yon^{\nn\times\nn_{\geq1}}\right)\otimes\nn\yon\otimes\nn\yon^{\nn\times\nn}\To{w}\nn\times\nn\yon^\nn,
    \]
    whose return function is given by the composite map $(q,r,p)\mapsto(7,q,r,10,p)\mapsto(q,p)$ and whose update function at state $(q,r,p)$ is given by the composite map $a\mapsto(a,7,r,10)\mapsto(a\bdiv7,a\bmod7,r*10)$.
    
    In other words, the dynamical system $\phi$ behaves as follows: its state consists of a quotient $q$, a remainder $r$, and a product $p$, of which it returns the quotient and the product.
    Then it is fed a dividend $a$ and evaluates $a\bdiv7$ to obtain the new quotient and $a\bmod7$ to obtain the new remainder.
    Meanwhile, the new product is given by the previous remainder multiplied by $10$.
    
    \item This second wiring diagram specifies an interaction pattern
    \[
        w'\colon\nn\times\nn\yon^\nn\to\nn\yon
    \]
    with on-positions function $(q,p)\mapsto q$ and on-directions function $(q,p)\mapsto p$.
    So the dynamical system induced by nesting the first wiring diagram within the inner box in this second wiring diagram is a composite lens
    \[
        \left(\nn\times\nn\yon^{\nn\times\nn}\right)\otimes\nn\yon^\nn\To{\phi}\nn\times\nn\yon^\nn\To{w'}\nn\yon
    \]
    whose return function is given by the composite map $(q,r,p)\mapsto(q,p)\mapsto q$ and whose update function at state $(q,r,p)$ is specifies the new state $(p\bdiv7,p\bmod7,r*10)$.
    
    In other words, the dynamical system $\phi$ behaves as follows: its state consists of a quotient $q$, a remainder $r$, and a product $p$, of which it returns just the quotient.
    Then it advances to a new state by evaluating $p\bdiv7$ to obtain the new quotient and $p\bmod7$ to obtain the new remainder.
    Meanwhile, the new product is given by the previous remainder multiplied by $10$.
    
    If the initial state is $(q,r,p)=(0,0,10)$, then all the states will be as follows, with the values of $q$ in the left column giving us the outputs:
    \begin{table}[hbt!]
        \centering
        \footnotesize
        \begin{tabular}{c|c|c}
            $q$ $(p\bdiv7)$ & $r$ $(p\bmod7)$ & $p$ $(10r)$ \\
            \hline
            0 & 0 & 10 \\
            1 & 3 & 0 \\
            0 & 0 & 30 \\
            4 & 2 & 0 \\
            0 & 0 & 20 \\
            2 & 6 & 0 \\
            0 & 0 & 60 \\
            8 & 4 & 0 \\
            0 & 0 & 40 \\
            5 & 5 & 0 \\
            0 & 0 & 50 \\
            7 & 1 & 0 \\
            0 & 0 & 10 \\
            $\cdots$ & $\cdots$ & $\cdots$
        \end{tabular}
    \end{table}
\end{enumerate}
\end{solution}
\end{exercise}

\begin{example}[Graphs as wiring diagrams and cellular automata]\label{ex.graph_interaction}
Suppose we have a graph $G=(E\tto V)$ as in \cref{def.graph} and a set $\tau(v)$ associated with each vertex $v\in V$:
\[
\begin{tikzcd}
	E\ar[r, shift left=3pt, "s"]\ar[r, shift right=3pt, "t"']&
	V\ar[r, "\tau"]&
	\smset
\end{tikzcd}
\]
We can think of $G$ as an alternative representation of a specific kind of wiring diagram, one where each inner box has exactly one output wire and the outer box is closed.
The vertices $v\in V$ are the inner boxes, the set $\tau(v)$ is the set associated with $v$'s output wire, and each edge $e$ is a wire connecting the output wire of its target $t(e)$ to an input wire of its source $t(e)$.
An edge from a vertex $v_0$ to a vertex $v_1$ indicates that the inputs to $v_0$ depend on the outputs of $v_1$.\footnote{We could have defined the edges in the opposite directions, so that they would point in the direction of data flow rather than in the direction of data dependencies; this was an arbitrary choice.}

In other words, we can associate each vertex $v\in V$ with the monomial
\[
	p_v\coloneqq\tau(v)\yon^{\prod_{e\in E_v}\tau(t(e))}
\]
specifying its inputs and outputs, where $E_v\coloneqq s\inv(v)\ss E$ denotes the set of edges emanating from $v$.
The graph then determines an enclosure
\[
\gamma\colon\bigotimes_{v\in V}p_v\to\yon
\]
given by a function
\[
    \prod_{v\in V}\tau(v)\too\prod_{e\in E}\tau(t(e))
\]
that sends each dependent function $o\colon(v\in V)\to\tau(v)$ to the dependent function $(e\in E)\to\tau(t(e))$ sending $e\mapsto o(t(e))$.
In other words, given the output $o(v)\in\tau(v)$ for every vertex $v\in V$, we know for each edge $e\in E$ that the input $s(e)$ receives is the output of $t(e)$.

Hence, once we give dynamics to each $p_v$, namely by specifying a dynamical system $S_v\yon^{S_v}\to p_v$ with outputs in $\tau(v)$ and inputs in $\prod_{e\in E_v}\tau(t(e))$, we will obtain a closed dynamical system that transitions from each vertex's state to the next according to the information that they pass each other along their edges.

Effectively, by interpreting a graph as a wiring diagram and giving each vertex dynamics, we have created what is known as a \emph{cellular automaton}---a network of vertices (or \emph{cells}) with states, such that each vertex $v\in V$ ``listens'' to the signals its \emph{neighbors} in $E_v$ send based on their states, then responds accordingly by updating its own state.

For example, a common graph found in cellular automata is a 2-dimensional integer lattice, with vertices $V\coloneqq\zz\times\zz$. The edges indicate which vertices are neighbors and thus ``hear'' which other vertices. One might use
\[E\coloneqq(\{-1,0,1\}\times\{-1,0,1\}-\{(0,0)\})\times V\]
with $s(i,j,m,n)=(m,n)$ and $t(i,j,m,n)=(m+i, n+j)$, so that the neighbors of each vertex are the eight vertices that surround it.
\end{example}

\begin{exercise}[Conway's Game of Life]\label{exc.conway}
Conway's Game of Life is played on a 2-dimensional integer lattice as follows.
Each lattice point is either \emph{live} or \emph{dead}, and each point observes its eight \emph{neighbors} to which it is horizontally, vertically, or diagonally adjacent.
The following occurs at every time step:
\begin{itemize}
    \item Any live point with 2 or 3 live neighbors remains live.
    \item Any dead point with 3 live neighbors becomes live.
    \item All other points either become or remain dead.
\end{itemize}
We can use \cref{ex.graph_interaction} to model Conway's Game of Life as a closed dynamical system.
\begin{enumerate}
	\item What is the appropriate graph $E\tto V$?
	\item What is the appropriate assignment of sets $\tau\colon V\to\smset$?
	\item What are the polynomials $p_v$ from \cref{ex.graph_interaction}?
	\item What is the appropriate state set $S_v$ for each interface $p_v$?
	\item What is the appropriate dynamical system map $S_v\yon^{S_v}\to p_v$?
\qedhere
\end{enumerate}
\begin{solution}
We seek to model Conway's Game of Life as a closed dynamical system using \cref{ex.graph_interaction}.
\begin{enumerate}
    \item Following the suggestion from the end of \cref{ex.graph_interaction}, we can use a graph with $V\coloneq\zz\times\zz$ and $E\coloneqq(\{-1,0,1\}\times\{-1,0,1\}-\{(0,0)\})\times V$ with $s(i,j,m,n)=(m,n)$ and $t(i,j,m,n)=(m+i,n+j)$ to model cellular automata like Conway's Game of Life on a 2-dimensional integer lattice in which each point listens only to its eight immediate neighbors.
    \item Each vertex needs only return whether it is live or dead, so we assign $\tau(v)\coloneqq\{\text{live},\text{dead}\}$ for every $v\in V$.
    \item For each $v\in V$, the monomial represented by $v$ from \cref{ex.graph_interaction} can be written as
    \[
        p_v\iso\{\text{live},\text{dead}\}\yon^{\smset(\{-1,0,1\}\times\{-1,0,1\}-\{(0,0)\},\, \{\text{live},\text{dead}\})}.
    \]
    Every vertex returns as output whether it is live or dead and receives as input whether each of its eight neighbors is alive or dead. 
    \item Each vertex $v\in V$ only needs to record whether it is live or dead, so $S_v\coloneqq\{\text{live},\text{dead}\}$.
    \item The appropriate dynamical system map $S_v\yon^S_v\to p_v$ for each vertex $v\in V$ should have the identity function on $\{\text{live},\text{dead}\}$ as its return function, while its update function should be a map
    \[
        \{\text{live},\text{dead}\}\smset(\{-1,0,1\}\times\{-1,0,1\}-\{(0,0)\},\, \{\text{live},\text{dead}\})\to\{\text{live},\text{dead}\}
    \]
    that executes the rules from Conway's Game of Life, using the data of whether the vertex itself is live or dead as well as whether each of its eight neighbors is live or dead to determine whether it should be live or dead in the next time step.
\end{enumerate}
\end{solution}
\end{exercise}

\subsection{More examples of general interaction}

While wiring diagrams are a handy visualization tool for certain simple interaction patterns, there are more general interaction patterns that cannot be captured by such a diagram.
For example, here we generalize our previous cellular automata example.

\begin{example}[Generalized cellular automata: voting on who your neighbors are]\label{ex.cell_auto_vote_interaction}
Recall from \cref{ex.graph_interaction} how we constructed a cellular automaton on a graph $G=(E\tto V)$.
For each $v\in V$, the graph specifies the set $N(v)\coloneqq t(E_v)$ of vertices at the ends of edges coming out of $v$.
These vertices are the \emph{neighbors} of $v$, or the vertices that $v$ can ``listen'' to.
We call the map $N\colon V\to\2^V$ from each vertex to the set of its neighbors the \emph{neighbor function}.
For simplicitly, we let each vertex store and return one of two states, so $S_v\coloneqq\tau(v)\coloneqq\2$.

Now just take the vertices and forget the edges of our graph.
Suppose instead that we are given a function $n\colon V\to\nn$ that we think of as specifying the number $n(v)$ of neighbors each $v\in V$ could potentially have.
Let $\ord{n}(v)\coloneqq\{1,2,\ldots,n(v)\}$.
Then we can think of the monomial that each vertex represents as
\[
    p_v\iso\2\yon^{\2^{\ord{n}(v)}},
\]
returning its own state as output and receiving its potential neighbors' states as input.

Say that a neighbor function $N\colon V\to\2^V$ \emph{respects} $n$ if we have an isomorphism $N(v)\iso\ord{n}(v)$ for each $v\in V$.
Now suppose we have a function $N'_-\colon \2^V\to (\2^V)^V$ that sends each set of vertices $S\in\2^V$ to a neighbor function $N'_S\colon V\to \2^V$ that respects $n$.
In other words, each possible state configuration $S$ of all the vertices in $V$ determines a neighbor function $N'_S$.
In the case of \cref{ex.graph_interaction}, when we had a graph, it told us what the neighbor function should always be.
Now we can think of it like all the vertices are voting, via $N'$, on what neighbor function to use to determine which vertices are listening to which others.

We can put this all together by providing an enclosure for all the vertices,
\begin{equation}\label{eqn.polymap_misc9237}
    \bigotimes_{v\in V}p_v\cong\2^V\yon^{\2^{\sum_{v\in V}\ord{n}(v)}}\too\yon.
\end{equation}
Specifying such an enclosure amounts to specifying a function $g\colon \2^V\to\2^{\sum_{v\in V}\ord{n}(v)}$ that assigns each possible state configuration $S\in\2^V$ of all the vertices in $V$ to a function $g(S)\colon\sum_{v\in V}\ord{n}(v)\to\2$ specifying the states every vertex hears.
But we already have a neighbor function assigned to $S$ that respects $\ord{n}$, namely $N'_S$, for which $N'_S(v)\iso\ord{n}(v)$ for all $v\in V$.
So we can think of $g(S)$ equivalently as a function $g(S)\colon\sum_{v\in V}N'_S(v)\to\2$ that says for each $v\in V$ what signal in $\2$ it should receive from its neighbor $w\in N'_S(v)$.
But we can just have it receive the current state of its neighbor, as given by $S$:
\[
    g(S)(v,w)\coloneqq S(w).
\]

We have accomplished our goal: the vertices ``vote'' on how they ought to be connected, in that their states together determine the neighbor function.
Of course, we don't mean to imply that this vote needs to be democratic or fair in any way: it is an arbitrary function $N'_-\colon \2^V\to(\2^V)^V$.
It could be dictated by a given vertex $v_0\in V$ in the sense that its state completely determines the neighbor function $V\to\2^V$; this would be expressed by saying that $N'_-$ factors as $\2^V\to\2^{\{v_0\}}\cong\2\To{I_0}(\2^V)^V$ for some $I_0$.
\end{example}

\begin{exercise}
We can change \cref{ex.cell_auto_vote_interaction} slightly by replacing the wrapper interface $\yon$ with some other interface.
\begin{enumerate}
	\item First change it to $A\yon$ for some set $A$ of your choice, and update \eqref{eqn.polymap_misc9237} so that the system outputs some aspect of the current state configuration of all the vertices $S\in\2^V$.
	\item What would it mean to change \eqref{eqn.polymap_misc9237} to a map $\bigotimes_{v\in V}p_v\to\yon^A$ for some $A$?
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item 
    \item 
\end{enumerate}
\end{solution}
\end{exercise}

Finally, we are ready to formalize the examples we previewed in \cref{sec.poly.intro.dyn_sys}.

\begin{example}\label{ex.bonds_break}
Recall the first picture from \cref{ex.changing_wiring_bonds_supplier_assemble}. We said that when too much force is applied to a material, bonds can break. Let's simplify the picture a bit.
\[
\begin{tikzpicture}[oriented WD, bb small, bb port length=0]
	\node[bb={1}{1}, fill=blue!10] (x1) {$\phi_1$};
	\node[bb={1}{1}, fill=blue!10, right=of x1] (x2) {$\phi_2$};
	\node[bb={1}{1}, fit= (x1) (x2)] (outer) {};
	\draw[->, shorten >= -4mm] (x1_in1) -- (outer_in1) node[left=4.5mm, font=\tiny] {Force};
	\draw (x1_out1) -- (x2_in1);
	\draw[->, shorten >= -4mm] (x2_out1) -- (outer_out1) node[right=4.5mm, font=\tiny] (L) {Force};
%
	\node[bb={1}{1}, fill=blue!10, right=2in of L] (y1) {$\phi_1$};
	\node[bb={1}{1}, fill=blue!10, right=of y1] (y2) {$\phi_2$};
	\node[bb={1}{1}, fit= (y1) (y2)] (outer) {};
	\draw[->, shorten >= -4mm] (y1_in1) -- (outer_in1) node[left=4.5mm, font=\tiny] (R){Force};
	\draw[->, shorten >= -4mm] (y2_out1) -- (outer_out1) node[right=4.5mm, font=\tiny] {Force};
	\node[starburst, draw, minimum width=2cm, minimum height=1.5cm,red,fill=orange,line width=1.5pt] at ($(L)!.5!(R)$)
{Snap!};
\end{tikzpicture}
\]
We will imagine the dependent dynamical systems $\phi_1\colon S\yon^S\to p_1$ and $\phi_2\colon S\yon^S\to p_2$ as initially connected in space.
They experience forces from the outside world, and---for as long as they are connected---they experience forces from each other.
More precisely, each interface is defined by
\[
	p_1\coloneqq p_2\coloneqq F\yon^{FF}+\{\text{`snapped'}\}\yon^F.
\]
Elements of $F$ will be called \emph{forces}.
We need to be able to add and compare forces, i.e.\ we need $F$ to be an ordered monoid; let's say $F=\nn$ for simplicity.
The idea is that the interface has two kinds of output it can return: either a force $f_i\in F$ on the other system, at which point it can receive an input in $FF$ indicating a force acting on the system from its left and another force acting on it from its right; or `snapped,' indicating that the system is no longer connected to the other system, at which point it only receives a single force in $F$ from the outside.

The wrapper interface is defined to be
\[
    p\coloneqq\yon^{FF};
\]
it takes as input two forces $(f_L, f_R)$ and returns unchanging output.

Though the systems $\phi_1$ and $\phi_2$ may be initially connected, if the forces on either one surpass a threshold, that system stops sending and receiving forces from the other. The connection is broken and neither system ever receives forces from the other again. This is what we will implement explicitly below.

To do so, we need to define an interaction pattern $p_1\otimes p_2\to p$ that wraps $p$ around $\phi_1$ and $\phi_2$.
That is, we need to give a lens
\[
    \kappa\colon (F\yon^{FF}+\{\text{`snapped'}\}\yon^F)\otimes (F\yon^{FF}+\{\text{`snapped'}\}\yon^F)\to\yon^{FF}.
\]
By the distributivity of $\otimes$ over $+$, it suffices to give four maps:
\begin{equation}\label{eqn.snapped_maps}
\arraycolsep=1.4pt
\begin{array}{lll}
	\kappa_{11}\colon&~ FF\yon^{(FF)(FF)}&\to\yon^{FF}\\
	\kappa_{12}\colon&~ F\{\text{`snapped'}\}\yon^{(FF)F}&\to\yon^{FF}\\
	\kappa_{21}\colon&~ \{\text{`snapped'}\}F\yon^{F(FF)}&\to\yon^{FF}\\
	\kappa_{22}\colon&~ \{\text{`snapped'}\}\{\text{`snapped'}\}\yon^{FF}&\to\yon^{FF}
\end{array}
\end{equation}
The middle two maps $\kappa_{12}$ and $\kappa_{21}$ won't actually occur in our dynamics, so we take them to be arbitrary.
We take the last map $\kappa_{22}$ to be the obvious isomorphism, passing the forces from outside to the two internal interfaces.
The first map $\kappa_{11}$ is equivalent to a function $(FF)(FF)\to (FF)(FF)$ which we take to be $((f_1,f_2),(f_L,f_R))\mapsto((f_L,f_2),(f_1,f_R))$.
While the multiple $F$'s may be a little hard to keep track of, what this map says is that if $\phi_1$ returns the force $f_1$ on $\phi_2$ as output and $\phi_2$ returns the force $f_2$ on $\phi_1$ as output, then $\phi_1$ receives the force $f_2$ from the right as input and $\phi_2$ receives the force $f_1$ from the left as input; and in the meantime the left external force $f_L$ is given to $\phi_1$ on the left, while the right external force is given to $\phi_2$ on the right.

Now that we have the interfaces wrapped together, it remains to specify each dynamical system.
The states in the two cases will be identical, namely $S\coloneqq F+\{`snapped'\}$, meaning that at any point the system will either be in the state of applying a force to the other system or not.
The dynamical systems themselves will be identical as well, up to a symmetry swapping left and right; let's just define the left system for now.
It is given by a lens
\[\phi_1\colon (F+\{\text{`snapped'}\})\yon^{F+\{\text{`snapped'}\}}\to F\yon^{FF}+\{\text{`snapped'}\}\yon^F\]
which we write as the sum of two maps
\[F\yon^{F+\{\text{`snapped'}\}}\to F\yon^{FF} \qqand \{\text{`snapped'}\}\yon^{F+\{\text{`snapped'}\}}\to\{\text{`snapped'}\}\yon^F.\]
Both maps are the identity on positions, directly returning their current state.
The second map corresponds to when the connection is broken, after which the connection should remain broken, so its on-directions function is constant, sending any input to `snapped.'
Meanwhile, the first map corresponds to the case where the systems are still connected; this system receives two input forces and must update its state---the force it applies---accordingly.
We let the on-directions function $F(FF)\to F+\{\text{`snapped'}\}$ send
\[
(f_1,(f_L,f_2))\mapsto
\begin{cases}
	f_L&\tn{ if }f_1+f_2<100\\
	\text{`snapped'}&\tn{ otherwise}
\end{cases}
\]
Thus, when the sum of forces is high enough, the internal state is updated to the `snapped' state; otherwise, it is sent to the force it receives from outside, which it is now ready to transfer to the other system.
\end{example}

\begin{example}\label{ex.supplier_change}
Recall the second picture from \cref{ex.changing_wiring_bonds_supplier_assemble}.
We want to consider the case of a company that may change its supplier based on its internal state. The company returns two possible outputs, corresponding to who it wants to receive widgets $W$ from:
\[
\begin{tikzpicture}[oriented WD, every node/.style={fill=blue!10}]
	\node[bb={0}{1}] (s1) {Supplier 1};
	\node[bb={0}{1}, below=of s1] (s2) {Supplier 2};
	\node[bb={1}{0}, right=0.5 of s1] (c) {Company};
	\draw (s1_out1) to node[above, fill=none, font=\tiny] {$W$} (c_in1);
	\draw (s2_out1) to +(5pt,0) node[fill=none] {$\bullet$};
\begin{scope}[xshift=3.5in]
	\node[bb={0}{1}] (s1') {Supplier 1};
	\node[bb={0}{1}, below=of s1'] (s2') {Supplier 2};
	\node[bb={1}{0}, right=0.5 of s2'] (c') {Company};
	\draw (s2'_out1) to node[above, fill=none, font=\tiny] {$W$} (c'_in1);
	\draw (s1'_out1) to +(5pt,0) node[fill=none] {$\bullet$};
\end{scope}
	\node[starburst, draw, minimum width=2cm, minimum height=2cm,align=center,fill=white, font=\small,line width=1.5pt] at ($(c.east)!.5!(s2'.west)$)
{Change\\supplier!};
\end{tikzpicture}
\]
So the company has interface $\2\yon^W$, and each supplier has interface $W\yon$.
Then an enclosure for the company and the suppliers is just a lens $\2\yon^W\otimes W\yon\otimes W\yon\to\yon$, corresponding to a function $\2W^\2\to W$ given by evaluation.
In other words, the company's output determines its supplier.
\end{example}

\begin{example}\label{ex.assemble_machine}
Recall the third picture from \cref{ex.changing_wiring_bonds_supplier_assemble}.
When someone assembles a machine, their own outputs dictate the interaction pattern of the machine's components.
\begin{equation*}%\label{eqn.someone2}
\begin{tikzpicture}[oriented WD, font=\ttfamily, bb port length=0, every node/.style={fill=blue!10}, baseline=(someone.north)]
	\node[bb port sep=.5, bb={0}{1}] (A) {unit A};
	\node[bb port sep=.5, bb={1}{0}, right=of A] (B) {unit B};
	\coordinate (helper) at ($(A)!.5!(B)$);
	\node[bb={1}{1}, below=2 of helper] (someone) {\tikzsymStrichmaxerl[3]};
	\draw[->, dashed, blue] (someone_in1) to[out=180, in=270] (A.270);
	\draw[->, dashed, blue] (someone_out1) to[out=0, in=270] (B.270);
	\draw[->] (A_out1) -- +(10pt,0);
	\draw (B_in1) -- +(-10pt,0);
%
\begin{scope}[xshift=3.5in]
	\node[bb port sep=.5, bb={0}{1}] (A') {unit A};
	\node[bb port sep=.5, bb={1}{0}, right=.5of A'] (B') {unit B};
	\coordinate (helper') at ($(A')!.5!(B')$);
	\node[bb={1}{1}, below=2 of helper'] (someone') {\tikzsymStrichmaxerl[3]};
	\draw[->, dashed, blue] (someone'_in1) to[out=180, in=270] (A'.270);
	\draw[->, dashed, blue] (someone'_out1) to[out=0, in=270] (B'.270);
	\draw[->] (A'_out1) -- (B'_in1);
\end{scope}
%
	\node[starburst, draw, minimum width=2cm, minimum height=2cm,fill=blue!50,line width=1.5pt, align=center, font=\upshape] at ($(B)!.5!(A')-(0,.6cm)$)
{Attach!};
\end{tikzpicture}
\end{equation*}
In order for the above picture to make sense, the output set of \texttt{unit A} should be the same as the input set of \texttt{unit B}.
Call this set $X$, so that \texttt{unit A} has interface $X\yon$ and \texttt{unit B} has interface $\yon^X$.
We fix a default value $x_0\in X$ for the input to \texttt{unit B} when it is not connected to \texttt{unit A}.
Meanwhile, the person takes no input and dictates whether the units are attached or not, so we give it interface $\2\yon$.

Then an enclosure for the person and the units is a lens $\2\yon\otimes X\yon\otimes \yon^X\to\yon$. The lens $\2X\yon^X\to\yon$, corresponding to a function $\2X\to X$ that maps $(1,x)\mapsto x_0$ and $(2,x)\mapsto x$.
\end{example}

We can easily generalize \cref{ex.assemble_machine}.
Indeed, we will see in the next section that there is an interface $\ihom{q_1\otimes\cdots\otimes q_k\,,\,r}$ that represents all the interaction patterns between $q_1,\ldots,q_k$ with wrapper interface $r$, and that wrapping it around $p$ to it is just a larger interaction pattern:
\[
\poly(p,[q_1\otimes\cdots\otimes q_k\,,\,r])\cong\poly(p\otimes q_1\otimes\cdots\otimes q_k\,,\,r).
\]
In other words, if $p$ is deciding the interaction pattern between $q_1,\ldots,q_k$ with wrapper interface $r$, and gets feedback from that interaction pattern itself, then this is equivalent to an interaction pattern with wrapper interface $r$ that $p$ is part of alongside $q_1,\ldots,q_k$ inside of $r$.

What it also means is that if you want, you can put a little dynamical system inside of $[q_1\otimes\cdots\otimes q_k,r]$ and have it be constantly choosing interaction patterns. Let's see how it works.


%---- Section ----%
\section{Closure of $\otimes$}%[-,-]

The parallel monoidal product is closed---we have a monoidal closed structure on $\poly$---meaning that there is a closure operation, which we denote $\ihom{-,-}\colon\poly\op\times\poly\to\poly$, such that there is an isomorphism
\begin{equation}\label{eqn.monoidal_closure}
  \poly(p\otimes q,r) \iso \poly(p,\ihom{q,r})
\end{equation}
natural in $p,q,r$.
The closure operation is defined on $q,r$ as follows:
\begin{equation}\label{eqn.dir_hom}
	\ihom{q,r} \coloneqq \prod_{j\in q(\1)}r\circ(q[j]\yon)
\end{equation}
Here $\circ$ denotes standard functor composition; informally, $r \circ (q[j]\yon)$ is the polynomial you get when you replace each appearance of $\yon$ in $r$ by $q[j]\yon$.
Composition, together with the unit $\yon$, is in fact yet another monoidal structure, as we will see in more depth in \cref{part.comon}.

Before we prove that the isomorphism \eqref{eqn.monoidal_closure} holds naturally, let us investigate the properties of the closure operation, starting with some simple examples.

\begin{exercise}
Calculate $\ihom{q,r}$ for $q,r\in\poly$ given as follows.
\begin{enumerate}
	\item $q\coloneqq \0$ and $r$ arbitrary.
	\item $q\coloneqq \1$ and $r$ arbitrary.
	\item $q\coloneqq\yon$ and $r$ arbitrary.
	\item $q\coloneqq A$ for $A\in\smset$ (constant) and $r$ arbitrary.
	\item $q\coloneqq A\yon$ for $A\in\smset$ (linear) and $r$ arbitrary.
	\item $q\coloneqq\yon^\2+\2\yon$ and $r\coloneqq\2\yon^\3+\3$.
\qedhere
\end{enumerate}
\begin{solution}
We compute $\ihom{q,r}$ for various values of $q, r \in \poly$ using \eqref{eqn.dir_hom}.
\begin{enumerate}
    \item If $q \coloneqq \0$, then $q(\1) \iso \0$, so $\ihom{q,r}$ is an empty product.
    Hence $\ihom{q,r} \iso \1$.
    \item If $q \coloneqq \1$, then $q(\1) \iso \1$ and $q[1] \iso \0$, so $\ihom{q,r} \iso r \circ (\0\yon) \iso r(\0)$.
    \item If $q \coloneqq \yon$, then $q(\1) \iso \1$ and $q[1] \iso \1$, so $\ihom{q,r} \iso r \circ (\1\yon) \iso r$.
	\item If $q \coloneqq A$ for $A \in \smset$, then $q(\1) \iso A$ and $q[j] \iso \0$ for every $j \in A$, so $\ihom{q,r} \iso \prod_{j \in A} (r \circ (\0\yon)) \iso r(\0)^A$.
	\item If $q \coloneqq A\yon$ for $A\in\smset$, then $q(\1) \iso A$ and $q[j] \iso \1$ for every $j \in A$, so $\ihom{q,r} \iso \prod_{j \in A} (r \circ (\1\yon)) \iso r^A$.
	\item If $q\coloneqq\yon^\2+\2\yon$ and $r\coloneqq\2\yon^\3+\3$, then
	\begin{align*}
	    \ihom{q,r} &\iso (r \circ (\2\yon))(r \circ (\1\yon))^\2 \\
	    &\iso \left(\2(\2\yon)^\3 + \3\right)\left(\2\yon^\3 + \3\right)^\2 \\
	    &\iso \6\4\yon^\9 + \2\0\4\yon^\6 + \1\8\0\yon^\3 + \2\7 \\
	\end{align*}
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.sum_times_closure}
Show that for any polynomials $p_1,p_2,q$, we have an isomorphism
\[
\ihom{p_1 + p_2, q} \iso \ihom{p_1, q} \times \ihom{p_2, q}.
\]
\begin{solution}
We wish to show that for all $p_1, p_2, q \in \poly$, we have $\ihom{p_1 + p_2, q} \iso \ihom{p_1, q} \times \ihom{p_2, q}$.
By \eqref{eqn.dir_hom},
\[
    \ihom{p_1 + p_2, q} \iso \left(\prod_{i \in p_1(\1)} q \circ (p_1[i]\yon)\right) \left(\prod_{i \in p_2(\1)} q \circ (p_2[i]\yon)\right) \iso \ihom{p_1, q} \times \ihom{p_2, q}.
\]
\end{solution}
\end{exercise}

\begin{exercise} \label{exc.dir_hom_sum}
Show that there is an isomorphism
\begin{equation} \label{eqn.dir_hom_sum}
\scalebox{1.3}{$\displaystyle
\ihom{q,r} \iso \sum_{f\colon q\to r}\yon^{\sum_{j\in q(\1)}r[f_\1(j)]}
$}
\end{equation}
where the sum is indexed by $f\in\poly(q,r)$.
\begin{solution}
We may compute
\begin{align*}
    \ihom{q, r} &\iso \prod_{j \in q(\1)} r \circ (q[j]\yon) \tag*{\eqref{eqn.dir_hom}} \\
    &\iso \prod_{j \in q(\1)} \, \sum_{k \in r(\1)} (q[j]\yon)^{r[k]} \tag{Replacing each $\yon$ in $r$ by $q[j]\yon$} \\
    &\iso \sum_{f_\1 \colon q(\1) \to r(\1)} \, \prod_{j \in q(\1)} (q[j]\yon)^{r[f_\1(j)]} \tag*{\eqref{eqn.push_prod_sum_set_indep}} \\
    &\iso \sum_{f_\1 \colon q(\1) \to r(\1)} \, \left(\prod_{j \in q(\1)} q[j]^{r[f_\1(j)]} \right)\left(\prod_{j \in q(\1)} \yon^{r[f_\1(j)]} \right) \\
    &\iso \sum_{f_\1 \colon q(\1) \to r(\1)} \; \sum_{f^\sharp \in \prod_{j \in q(\1)} q[j]^{r[f_\1(j)]}} \yon^{\sum_{j \in q(\1)} r[f_\1(j)]} \\
    &\iso \sum_{f \colon q \to r} \yon^{\sum_{j \in q(\1)} r[f_\1(j)]}. \tag*{\eqref{eqn.main_formula}}
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise} \label{exc.dir_hom_p_yon_dir_p}
Verify that \eqref{eqn.dir_hom_p_yon_dir_p} holds.t
\begin{solution}
We verify \eqref{eqn.dir_hom_p_yon_dir_p} as follows:
\begin{align*}
    \ihom{p, \yon} \otimes p
    &\iso
    \left(\sum_{f \colon p \to \yon} \yon^{\sum_{i \in p(\1)} \yon[f_1(i)]}\right) \otimes p
    \tag*{\eqref{eqn.dir_hom_sum}} \\
    &\iso
    \sum_{f \in \Gamma(p)} \yon^{p(\1)} \otimes \sum_{i \in p(\1)} \yon^{p[i]} \\
    &\iso
    \sum_{f \in \Gamma(p)} \; \sum_{i \in p(\1)} \yon^{p(\1) \times p[i]}
    \tag*{\eqref{eqn.parallel_def}} \\
    &\iso
    \sum_{f \in \prod_{i \in p(\1)} p[i]} \; \sum_{i \in p(\1)} \yon^{p(\1) \times p[i]}.
    \tag*{\eqref{eqn.gamma_prod}}
\end{align*}
\end{solution}
\end{exercise}

\begin{example}\label{ex.dirichlet_dual}
For any $A\in\smset$ we have
\[
  \ihom{\yon^A,\yon} \iso A\yon
  \qqand
  \ihom{A\yon,\yon} \iso \yon^A.
\]
More generally, for any polynomial $p\in\poly$ we have
\begin{equation}\label{eqn.dir_dual}
  \ihom{p,\yon} \iso \Gamma(p)\yon^{p(\1)}.
\end{equation}
All these facts follow directly from \eqref{eqn.dir_hom}.
\end{example}

\begin{exercise}
Verify the three facts above.
\begin{solution}
We have that
\[
    \ihom{\yon^A, \yon} \iso \prod_{j \in \yon^A(\1)} \yon \circ (\yon^A[j]\yon) \iso \prod_{j \in \1} A\yon \iso A\yon,
\]
that
\[
    \ihom{A\yon, \yon} \iso \prod_{j \in A\yon(\1)} \yon \circ ((A\yon)[j]\yon) \iso \prod_{j \in A} \yon \iso \yon^A,
\]
and that
\begin{align*}
    \ihom{p, \yon} &\iso \sum_{f \colon p \to \yon} \yon^{\sum_{i \in p(\1)} \yon[f_1(i)]} \tag{\cref{exc.dir_hom_sum}} \\
    &\iso \sum_{f \in \Gamma(p)} \yon^{\sum_{i \in p(\1)} \1} \\
    &\iso \Gamma(p)\yon^{p(\1)}.
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}
Show that for any $p\in\poly$, if there is an isomorphism $\ihom{\ihom{p,\yon},\yon} \iso p$, then $p$ is either linear $A\yon$ or representable $\yon^A$ for some $A$. Hint: first show that $p$ must be a monomial.
\begin{solution}
Given $p \in \poly$ and an isomorphism $\ihom{\ihom{p,\yon},\yon} \iso p$, we wish to show that $p$ is either linear or representable.
Applying \eqref{eqn.dir_dual} twice, we have that
\[
    p \iso \ihom{\ihom{p,\yon},\yon} \iso \Gamma\left(\Gamma(p)\yon^{p(\1)}\right)\yon^{\Gamma(p)}.
\]
We can compute the coefficient of $p$ via \eqref{eqn.gamma_prod} to obtain
\[
    \Gamma\left(\Gamma(p)\yon^{p(\1)}\right) \iso \prod_{\gamma \in \Gamma(p)} p(\1) \iso p(\1)^{\Gamma(p)}.
\]
Hence
\begin{equation} \label{eqn.p_as_gamma_monomial}
    p \iso p(\1)^{\Gamma(p)}\yon^{\Gamma(p)}.
\end{equation}
In particular, $p$ is a monomial, so we can write $p \coloneqq B\yon^A$ for some $A,B \in \smset$.
Then $p(\1) \iso B$ and \eqref{eqn.gamma_prod} tells us that $\Gamma(p) \iso A^B$.
It follows from \eqref{eqn.p_as_gamma_monomial} that $A \iso A^B$ and that $B \iso B^A$.

We conclude with some elementary set theory.
If either one of $A$ or $B$ were $\1$, then $p$ would be either linear or representable, and we would be done.
Meanwhile, if either one of $A$ or $B$ were $\0$, then the other would be $\1$, and we would again be done.
Otherwise, $|A|,|B| \geq 2$.
But by Cantor's theorem,
\[
    |B| < \big|\2^B\big| \leq \big|A^B\big| = |A| \qqand |A| < \big|\2^A\big| \leq \big|B^A\big| = |B|,
\]
a contradiction.
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.dirichlet_closure}
With $\ihom{-,-}$ as defined in \eqref{eqn.dir_hom}, there is a natural isomorphism
\begin{equation}\label{eqn.poly_closure_brackets}
	\poly(p\otimes q,r)\cong\poly(p,\ihom{q,r}).
\end{equation}
\end{proposition}
\begin{proof}
We have the following chain of natural isomorphisms:
\begin{align*}
	\poly(p\otimes q,r)
	&\iso
	\poly\Big(\sum_{i\in p(\1)}\sum_{j\in q(\1)}\yon^{p[i]q[j]},r\Big) \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}\poly(\yon^{p[i]q[j]},r)
	\tag{Universal property of coproducts} \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}r(p[i]q[j])
	\tag{Yoneda lemma} \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}\poly(\yon^{p[i]},r\circ(q[j]\yon))
	\tag{Yoneda lemma} \\
	&\iso
	\poly\Big(\sum_{i\in p(\1)}\yon^{p[i]},\prod_{j\in q(\1)}r\circ(q[j]\yon)\Big)
	\tag{Universal property of (co)products} \\
	&\iso
	\poly(p,\ihom{q,r}).
\end{align*}
\end{proof}

\begin{exercise}\label{exc.poly_plug_1}
Show that for any $p,q$ we have an isomorphism of sets
\[
\poly(p,q) \iso \ihom{p,q}(\1).
\]
Hint: you can either use the formula \eqref{eqn.dir_hom}, or just use 
\eqref{eqn.poly_closure_brackets} with the Yoneda lemma and the fact that $\yon\otimes p \iso p$.
\begin{solution}
The isomorphism $\poly(p,q) \iso \ihom{p,q}(\1)$ follows directly from \cref{exc.dir_hom_sum} when both sides are applied to $\1$.
Alternatively, we can apply \eqref{eqn.poly_closure_brackets}.
Since $p \iso \yon \otimes p$, we have that
\begin{align*}
    \poly(p, q) &\iso \poly(\yon \otimes p, q) \\
    &\iso \poly(\yon, \ihom{p,q}) \tag*{\eqref{eqn.poly_closure_brackets}} \\
    &\iso \ihom{p,q}(\1). \tag{Yoneda lemma}
\end{align*}
\end{solution}
\end{exercise}

The closure of $\otimes$ implies that for any $p,q\in\poly$, there is a canonical \emph{evaluation} map
\begin{equation}\label{eqn.eval_dirichlet}
  \fun{eval}\colon \ihom{p,q}\otimes p\too q.
\end{equation}
As in any closed monoidal category, such an evaluation map has the universal property that for any $r\in\poly$ and map $f\colon p\otimes q\to r$, there is a unique lens $f'\colon p\to\ihom{q,r}$ such that the following diagram commutes:
\[
    \begin{tikzcd}
    	p\otimes q\ar[r, "f'\otimes q"]\ar[rr, bend right, "f"']&
    	{\ihom{q,r}}\otimes q\ar[r, "\fun{eval}"]&
    	r
    \end{tikzcd}
\]

\begin{exercise} \label{exc.eval_dirichlet}
Obtain the evaluation map $\fun{eval}\colon \ihom{p,q}\otimes p\too q$ from \eqref{eqn.eval_dirichlet}.
\begin{solution}
To obtain the evaluation map $\fun{eval}\colon \ihom{p,q}\otimes p\too q$, we consider the following special case of the isomorphism \eqref{eqn.poly_closure_brackets}:
\[
    \poly(\ihom{p,q} \otimes p, q) \iso \poly(\ihom{p,q}, \ihom{p,q}).
\]
Then the evaluation map is the map corresponding to the identity lens on $\ihom{p,q}$ under the above isomorphism.
To recover this map, we can start from the identity lens on $\ihom{p,q}$ and work our way along a chain of natural isomorphisms from $\poly(\ihom{p,q}, \ihom{p,q})$ until we get to $\poly(\ihom{p,q} \otimes p, q)$.
To start, \cref{exc.dir_hom_sum} implies that
\begin{align*}
    \poly(\ihom{p,q}, \ihom{p,q})
    &\iso
    \poly\left(\sum_{f \colon p \to q} \, \prod_{i' \in p(\1)} \yon^{q[f_\1(i')]}, \prod_{i \in p(\1)} \, \sum_{j \in q(\1)} (p[i]\yon)^{q[j]}\right) \\
    &\iso
    \prod_{f \colon p \to q} \, \prod_{i \in p(\1)} \poly\left(\prod_{i' \in p(\1)} \yon^{q[f_\1(i')]}, \sum_{j \in q(\1)} (p[i]\yon)^{q[j]}\right),
\end{align*}
where the second isomorphism follows from the universal properties of products and coproducts.
In particular, under this isomorphism, the identity lens on $\ihom{p,q}$ corresponds to a collection of lenses, namely for each $f \colon p \to q$ and each $i \in p(\1)$ the composite
\[
    \prod_{i' \in p(\1)} \yon^{q[f_\1(i')]} \to \yon^{q[f_\1(i)]} \to \sum_{g \colon q[f_\1(i)] \to p[i]} \yon^{q[f_\1(i)]} \iso (p[i]\yon)^{q[f_\1(i)]} \to \sum_{j \in q(\1)} (p[i]\yon)^{q[j]}
\]
of the canonical projection with index $i' = i$, the canonical inclusion with index $g = f^\sharp_i$, and the canonical inclusion with index $j = f_\1(i)$.
On positions, this map picks out the position of $\sum_{j \in q(\1)} (p[i]\yon)^{q[j]}$ corresponding to $j = f_\1(i) \in q(\1)$ and $f^\sharp_i \colon q[f_\1(i)] \to p[i]$; on directions, the map is the canonical inclusion $q[f_\1(i)] \to \sum_{i' \in p(\1)} q[f_\1(i')]$ with index $i' = i$.

We can reinterpret each of these maps as a map
\[
    \yon^{p[i] \times \sum_{i' \in p(\1)} q[f_\1(i')]} \to \sum_{j \in q(\1)} \yon^{q[j]} \iso q
\]
that, on positions, picks out the position $f_\1(i) \in q(\1)$ of $q$ and, on directions, is the map $q[f_\1(i)] \to p[i] \times \sum_{i' \in p(\1)} q[f_\1(i')]$ induced by the universal property of products applied to the map $f^\sharp_i \colon q[f_\1(i)] \to p[i]$ and the inclusion $q[f_\1(i)] \to \sum_{i' \in p(\1)} q[f_\1(i')]$.
Then by the universal property of coproducts, this collection of maps induces a single map $\fun{eval} \colon \ihom{p, q} \otimes p \to q$ that sends each position $f \colon p \to q$ of $\ihom{p,q}$ and position $i \in p(\1)$ of $p$ to the position $f_\1(i)$ of $q$, with the same behavior on directions as the corresponding map described previously.
\end{solution}
\end{exercise}

\begin{exercise}
\begin{enumerate}
	\item For any set $S$, obtain the map $S\yon^S\to\yon$ whose on-directions map is the identity on $S$ using eval and \cref{ex.dirichlet_dual}.
	\item Show that four maps in \eqref{eqn.snapped_maps} from \cref{ex.bonds_break}, written equivalently as
	\begin{equation} \label{eqn.snapped_maps2}
	\arraycolsep=1.4pt
    \begin{array}{lll}
    	\kappa_{11}\colon&~ F\yon^{FF}\otimes F\yon^{FF}&\to\yon^F\otimes\yon^F\\
    	\kappa_{12}\colon&~ F\yon^{FF}\otimes \yon^F&\to\yon^F\otimes\yon^F\\
    	\kappa_{21}\colon&~ \yon^F\otimes F\yon^{FF}&\to\yon^F\otimes\yon^F\\
    	\kappa_{22}\colon&~ \yon^F\otimes\yon^F&\to\yon^F\otimes\yon^F,
    \end{array}
	\end{equation}
	can be obtained by taking the parallel product of identity maps and evaluation maps.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item Given a set $S$, we wish to obtain the map $S\yon^S \to \yon$ whose on-directions map is the identity by using eval and \cref{ex.dirichlet_dual}.
    The example shows that
    \[
        \ihom{S\yon, \yon} \otimes (S\yon) \iso \yon^S \otimes (S\yon) \iso S\yon^S,
    \]
    so by setting $p \coloneqq S\yon$ and $q \coloneqq S$ in \eqref{eqn.eval_dirichlet}, we obtain an evaluation map $\fun{eval} \colon S\yon^S \to \yon$.
    By the solution to \cref{exc.eval_dirichlet}, given a position $s \in S$ of $S\yon^S$, the evaluation map on directions is the map $\1 \to S$ that picks out $s$.
    In other words, it is indeed the identity on directions.
    \item We wish to write the four maps in \eqref{eqn.snapped_maps2} from \cref{ex.bonds_break} as the parallel product of identity maps and eval maps.
    By the solution to \cref{exc.eval_dirichlet}, the eval map $\ihom{F,\yon^F}\otimes F\to\yon^F$
    is a map from
    \[
        \ihom{F,\yon^F}\otimes F\iso F\left(\sum_{f\colon F\to\yon^F}\prod_{i\in F}\yon^F\right)\iso F\yon^{FF}
    \]
    to $\yon^F$ that is unique on positions and has the on-directions function $FF\to FF$ given by the identity.
    Then we can verify that $\kappa_{11}$ is equivalent to the parallel product of this eval map with itself.
    We can define $\kappa_{12}$ and $\kappa_{21}$ to be the parallel product of this eval map with the identity on $\yon^F$, while $\kappa_{22}$ is the parallel product of the identity on $\yon^F$ with itself.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{example}[Modeling your environment without knowing what it is]
Let's imagine a robot whose interface is an arbitrary polynomial $p$. Let's imagine it is part of an interaction pattern
\[
	f\colon (q_1\otimes\cdots\otimes q_n)\otimes p\to r
\]
with some other robots whose interfaces are $q_1,\ldots,q_n$; let $q\coloneqq(q_1\otimes\cdots\otimes q_n)$. The interaction pattern induces a lens $f'\colon q\to \ihom{p,r}$ such that the original system $f$ factors through the evaluation $\ihom{p,r}\otimes p\to r$.

In other words, $\ihom{p,r}$ holds within it all of the possible ways $p$ can interact with other systems when they are all wrapped in $r$.
For example, in the case of $r\coloneqq\yon$, note that $\ihom{p,\yon}\cong\prod_{i\in p(\1)}p[i]\yon$.
That is, for each $p$-position it produces a direction there, which is just what $p$ needs as input in a closed system.

Now suppose we were to populate the interface $p$ with dynamics, a map $S\yon^S\to p$. One could aim to choose a set $S$ along with an interesting map $g\colon S\to\poly(p,r)$. Then each state $s$ would include a guess $g(s)$ about the state of its environment. This is not the real environment $q$, but just the environment as it affects $p$, namely $\ihom{p,r}$. The robot's states model environmental conditions.
\end{example}

\begin{example}[Chu $\&$]
Suppose we have polynomials $p_1,p_2,q_1,q_2,r\in\poly$ and lenses
\[
	\varphi_1\colon p_1\otimes q_1\to r
	\qqand
	\varphi_2\colon p_2\otimes q_2\to r.
\]
One might call these ``$r$-Chu spaces.'' One operation you can do with these as Chu spaces is to return something denoted $\varphi_1\&\varphi_2$, or ``$\varphi_1$ \emph{with} $\varphi_2$'' of the following type:
\[
\varphi_1\&\varphi_2\colon (p_1\times p_2)\otimes (q_1+q_2)\to r
\]
Suppose we are given a position in $p_1$ and a position in $p_2$. Then given a position in either $q_1$ or $q_2$, one evaluates either $\varphi_1$ or $\varphi_2$ respectively to get a position in $r$; given a direction there, one returns the corresponding direction in $q_1$ or $q_2$ respectively, as well as a direction in $p_1\times p_2$ which is either a direction in $p_1$ or in $p_2$.

This sounds complicated, but it can be done formally, once we have monoidal closure. We first rearrange both $\varphi_1,\varphi_2$ to be $p$-centric, using monoidal currying:
\[
\psi_1\colon p_1\to \ihom{q_1,r}
\qqand
\psi_2\colon p_2\to \ihom{q_2,r}
\]
Now we multiply to get $\psi_1\times\psi_2\colon p_1\times p_2\to\ihom{q_1,r}\times\ihom{q_2,r}$. Then we apply \cref{exc.sum_times_closure} to see that $\ihom{q_1,r}\times\ihom{q_2,r}\cong\ihom{q_1+q_2,r}$, and finally monoidal-uncurry to obtain $(p_1\times p_2)\otimes(q_1+q_2)\to r$ as desired.
\end{example}

% %-------- Section --------%
\section{Exercise solutions}
\Closesolutionfile{solutions}
{\footnotesize
\input{solution-file3}}

\Opensolutionfile{solutions}[solution-file4]

%------------ Chapter ------------%
\chapter{More categorical properties of polynomials} \label{sec.bonus_poly}

The category $\poly$ has very useful formal properties, including completion under colimits and limits, various adjunctions with $\smset$, factorization systems, and so on. Most of the following material is not necessary for the development of our main story, but we collect it here for reference. The reader can skip directly to \cref{part.comon} if so inclined, and we will direct them back here when needed. Better yet might be to just gently leaf through \cref{sec.bonus_poly}, to see how well-behaved and versatile the category $\poly$ really is.

%-------- Section --------%
\section{Special polynomials and adjunctions}

There are a few special classes of polynomials that are worth discussing: 
\begin{enumerate}[label=\alph*)]
	\item constant polynomials $\0,\1,\2,A$; 
	\item linear polynomials $\0,\yon, \2\yon, A\yon$;
	\item pure-power (or representable) polynomials $\1, \yon, \yon^\2, \yon^A$; and 
	\item monomials $\0, A, \yon, \2\yon^\3, A\yon^B$.
\end{enumerate}
The first two classes, constant and linear polynomials, are interesting because they both put a copy of $\smset$ inside $\poly$, as we'll see in \cref{prop.ff_const_set_to_poly,prop.ff_lin_set_to_poly}. 
The third puts a copy of $\smset\op$ inside $\poly$: it is the Yoneda embedding that we saw way back in \cref{exc.finish_proof_yoneda}.
Finally, the fourth puts a copy of bimorphic lenses inside $\poly$, as we saw in \cref{subsec.poly.func_nat.morph.bimorphic_lens}.

\begin{exercise}
Which of the four classes above are closed under
\begin{enumerate}
	\item the cocartesian monoidal structure $(\0,+)$ (i.e.\ addition)?
	\item the cartesian monoidal structure $(\1,\times)$ (i.e.\ multiplication)?
	\item the parallel monoidal structure $(\yon,\otimes)$ (i.e.\ taking the parallel product)?
	\item composition of polynomials $p\circ q$?
\qedhere
\end{enumerate}
\begin{solution}
Here $A, B, A', B' \in \smset$.
\begin{enumerate}
    \item We determine whether various classes of polynomials are closed under addition.
    \begin{enumerate}
        \item Constant polynomials are closed under addition: given constants $A, B$, their sum $A + B$ is also a constant polynomial.
        \item Linear polynomials are closed under addition: given linear polynomials $A\yon, B\yon$, their sum $A\yon + B\yon \iso (A + B)\yon$ is also a linear polynomial.
        \item Representable polynomials are \emph{not} closed under addition: for example, $\yon$ is a representable polynomial, but the sum of $\yon$ with itself, $\2\yon$, is not.
        \item Monomials are \emph{not} closed under addition: for example, $\yon$ and $\2\yon^\3$ are monomials, but their sum $\yon + \2\yon^\3$ is not.
    \end{enumerate}
    \item We determine whether various classes of polynomials are closed under multiplication.
    The results below follow from \cref{exc.general_poly_times} \cref{exc.general_poly_times.monomial}.
    \begin{enumerate}
        \item Constant polynomials are closed under multiplication: given constants $A, B$, their product $AB$ is also a constant polynomial.
        \item Linear polynomials are \emph{not} closed under multiplication: for example, $\yon$ and $\2\yon$ are linear polynomials, but their product $\2\yon^\2$ is not.
        \item Representable polynomials are closed under multiplication: given representables $\yon^A, \yon^B$, their product $\yon^{A+B}$ is also a representable polynomial.
        \item Monomials are closed under multiplication: given monomials $B\yon^A, B'\yon^{A'}$, their product $BB'\yon^{A+A'}$ is also a monomial.
    \end{enumerate}
    \item We determine whether various classes of polynomials are closed under taking parallel products.
    The results below follow from \cref{exc.general_poly_parallel_times} \cref{exc.general_poly_parallel_times.monomial}.
    \begin{enumerate}
        \item Constant polynomials are closed under taking parallel products: given constants $A, B$, their parallel product $AB$ is also a constant polynomial.
        \item Linear polynomials are closed under taking parallel products: given linear polynomials $A\yon, B\yon$, their parallel product $AB\yon$ is also a linear polynomial.
        \item Representable polynomials are closed under taking parallel products: given representables $\yon^A, \yon^B$, their parallel product $\yon^{AB}$ is also a representable polynomial.
        \item Monomials are closed under taking parallel products: given monomials $B\yon^A, B'\yon^{A'}$, their parallel product $BB'\yon^{AA'}$ is also a monomial.
    \end{enumerate}
    \item We determine whether various classes of polynomials are closed under composition. (Recall that we can think of computing the composite $p \circ q$ of $p, q \in \poly$ as replacing each appearance of $\yon$ in $p$ with $q$.)
    \begin{enumerate}
        \item Constant polynomials are closed under composition: given constants $A, B$, their composite $A \circ B \iso A$ is also a constant polynomial.
        \item Linear polynomials are closed under composition: given linear polynomials $A\yon, B\yon$, their composite $A\yon \circ B\yon \iso A(B\yon) \iso AB\yon$ is also a linear polynomial.
        \item Representable polynomials are closed under composition: given representables $\yon^A, \yon^B$, their composite $\yon^A \circ \yon^B \iso (\yon^B)^A \iso \yon^{BA}$ is also a representable polynomial.
        \item Monomials are closed under taking parallel products: given monomials $B\yon^A, B'\yon^{A'}$, their composite $B\yon^A \circ B'\yon^{A'} \iso B(B'\yon^{A'})^A \iso BB'^A\yon^{A'A}$ is also a monomial.
    \end{enumerate}
\end{enumerate}
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.ff_const_set_to_poly}
There is a fully faithful functor $\smset\to\poly$ sending $A\mapsto A\yon^\0=A$.
\end{proposition}
\begin{proof}
By \eqref{eqn.colax_poly_map}, a map $f\colon A\yon^\0\to B\yon^\0$ consists of a function $f\colon A\to B$ and, for each $a\in A$, a function $\0\to\0$. There is only one function $\0\to\0$, so $f$ can be identified with just a map of sets $A\to B$.
\end{proof}

\begin{proposition}\label{prop.ff_lin_set_to_poly}
There is a fully faithful functor $\smset\to\poly$ sending $A\mapsto A\yon$.
\end{proposition}
\begin{proof}
By \eqref{eqn.colax_poly_map}, a map $f\colon A\yon^\1\to B\yon^\1$ consists of a function $f\colon A\to B$ and for each $a\in A$ a function $\1\to\1$. There is only one function $\1\to\1$, so $f$ can be identified with just a map of sets $A\to B$.
\end{proof}

\begin{theorem}\label{thm.adjoint_quadruple}
$\poly$ has an adjoint quadruple with $\smset$:
\begin{equation}\label{eqn.adjoints_galore}
\begin{tikzcd}[column sep=60pt, background color=theoremcolor]
  \smset
  	\ar[r, shift left=7pt, "A" description]
		\ar[r, shift left=-21pt, "A\yon"']&
  \poly
  	\ar[l, shift right=21pt, "p(\0)"']
  	\ar[l, shift right=-7pt, "p(\1)" description]
	\ar[l, phantom, "\scriptstyle\Leftarrow"]
	\ar[l, phantom, shift left=14pt, "\scriptstyle\Rightarrow"]
	\ar[l, phantom, shift right=14pt, "\scriptstyle\Rightarrow"]
\end{tikzcd}
\end{equation}
where the functors have been labeled by where they send $A\in\smset$ and $p\in \poly$. 

Both rightward functors are fully faithful.
\end{theorem}
\begin{proof}
For any set $A$, there is a functor $\poly\to\smset$ given by sending $p$ to $p(A)$; by the Yoneda lemma, it is the functor $\poly(\yon^A,-)$. This, together with \cref{prop.ff_const_set_to_poly,prop.ff_lin_set_to_poly}, gives us the four functors and the fact that the two rightward functors are fully faithful. It remains to provide the following three natural isomorphisms:
\[
\poly(A,p)\iso\smset(A,p(\0))\qquad
\poly(p,A)\iso\smset(p(\1),A)\qquad
\poly(A\yon,p)\iso\smset(A,p(\1)).
\]
All three come from our formula \eqref{eqn.main_formula} for computing general hom-sets in $\poly$; we leave the details to the reader in \cref{exc.adjoint_quadruple}.
\end{proof}

\begin{exercise}\label{exc.adjoint_quadruple}
Here we prove the remainder of \cref{thm.adjoint_quadruple} using \eqref{eqn.main_formula}:
\begin{enumerate}
	\item Provide a natural isomorphism $\poly(A,p)\iso\smset(A,p(\0))$.
	\item \label{exc.adjoint_quadruple.pos_const} Provide a natural isomorphism $\poly(p,A)\iso\smset(p(\1),A)$.
	\item \label{exc.adjoint_quadruple.linear_pos} Provide a natural isomorphism $\poly(A\yon,p)\iso\smset(A,p(\1))$.
\qedhere
\end{enumerate}
\begin{solution}
We complete the proof of \cref{thm.adjoint_quadruple} by exhibiting three natural isomorphisms, all special cases of \eqref{eqn.main_formula}, as follows.
\begin{enumerate}
    \item By \eqref{eqn.main_formula}, we have the natural isomorphism
    \[
        \poly(A, p) % \iso \prod_{a \in A} \sum_{i \in p(\1)} A[a]^{p[i]}
        \iso \prod_{a \in A} \sum_{i \in p(\1)} \0^{p[i]}.
    \]
    As $\0^{p[i]}$ is $\1$ if $p[i] \iso \0$ and $\0$ otherwise, it follows that
    \[
        \poly(A, p) \iso \prod_{a \in A} \{ i \in p(\1) \mid p[i] \iso \0 \} \iso \prod_{a \in A} p(\0) \iso \smset(A, p(0)).
    \]
    \item By \eqref{eqn.main_formula}, we have the natural isomorphism
    \begin{align*}
        \poly(p, A) %&\iso \prod_{i \in p(\1)} \sum_{a \in A} p[i]^{A[a]} \\
        &\iso \prod_{i \in p(\1)} \sum_{a \in A} p[i]^\0 \\
        &\iso \prod_{i \in p(\1)} \sum_{a \in A} \1 \\
        &\iso \prod_{i \in p(\1)} A \\
        &\iso \smset(p(\1), A).
    \end{align*}
    \item By \eqref{eqn.main_formula}, we have the natural isomorphism
    \begin{align*}
        \poly(A\yon, p) %&\iso \prod_{a \in A} \sum_{i \in p(\1)} (A\yon)[a]^{p[i]} \\
        &\iso \prod_{a \in A} \sum_{i \in p(\1)} \1^{p[i]} \\
        &\iso \prod_{a \in A} \sum_{i \in p(\1)} \1 \\
        &\iso \prod_{a \in A} p(\1) \\
        &\iso \smset(A, p(\1)).
    \end{align*}
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}\label{exc.positions_maps_yon}
Show that for any polynomial $p$, its set $p(\1)$ of positions is in bijection with the set of functions $\yon\to p$.
\begin{solution}
Given $p \in \poly$, we wish to show that $p(\1)$ is in bijection with the set of functions $\yon \to p$.
In fact, this follows directly from the Yoneda lemma, but we can also invoke the isomorphism from \cref{exc.adjoint_quadruple} \cref{exc.adjoint_quadruple.linear_pos} with $A \coloneqq \1$ to observe that
\[
    p(1) \iso \smset(\1, p(\1)) \iso \poly(\yon, p).
\]
\end{solution}
\end{exercise}

In \cref{thm.adjoint_quadruple} we see that $p\mapsto p(\0)$ and $p\mapsto p(\1)$ have left adjoints. This is true more generally for any set $A$ in place of $\0$ and $\1$, as we show in \cref{cor.substituting_adj}. However, the fact that $p\mapsto p(\1)$ is itself the left adjoint of the left adjoint of $p\mapsto p(\0)$---and hence that we have the \emph{quadruple} of adjunctions in \eqref{eqn.adjoints_galore}---is special to $A=\0,\1$.

We also have a copower-hom-power two-variable adjunction between $\poly,\smset,$ and $\poly$.

\begin{proposition}\label{prop.two_var_adj}
There is a two-variable adjunction between $\poly$, $\smset$, and $\poly$:
\begin{equation}\label{eqn.two_var_adj}
\poly(Ap,q) \iso \smset(A,\poly(p,q)) \iso \poly(p,q^A).
\end{equation}
\end{proposition}
\begin{proof}
Since $Ap$ is the $A$-fold coproduct of $p$ and $q^A$ is the $A$-fold product of $q$, the universal properties of coproducts and products give natural isomorphisms
\[\poly(Ap,q)\cong\prod_{a\in A}\poly(p,q)\cong\poly(p,q^A).\]
The middle set is naturally isomorphic to $\smset(A,\poly(p,q))$, completing the proof.
\end{proof}

Replacing $p$ with $\yon^B$ in \eqref{eqn.two_var_adj}, we obtain the following using the Yoneda lemma.

\begin{corollary}\label{cor.substituting_adj}
For any set $B$ there is an adjunction
\[
\adj{\smset}{A\yon^B}{q(B)}{\poly}
\]
where the functors are labeled by where they send $q\in\poly$ and $A\in\smset$.
\end{corollary}

\begin{exercise}
Prove \cref{cor.substituting_adj} from \cref{prop.two_var_adj}.
\begin{solution}
To prove \cref{cor.substituting_adj}, it suffices to exhibit a natural isomorphism
\[
    \poly(A\yon^B, q) \iso \smset(A, q(B)).
\]
Replacing $p$ with $\yon^B$ in \eqref{eqn.two_var_adj} from \cref{prop.two_var_adj}, we obtain the natural isomorphism
\[
    \poly(A\yon^B, q) \iso \smset(A, \poly(\yon^B, q)).
\]
By the Yoneda lemma, $\poly(\yon^B, q)$ is naturally isomorphic to $q(B)$, yielding the desired result.
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.yoneda_left_adjoint}
The Yoneda embedding $A\mapsto \yon^A$ has a left adjoint
\[
\adjr{\smset\op}{\yon^-}{\Gamma}{\poly}
\]
where $\Gamma(p) \coloneqq \poly(p, \yon) \iso \prod_{i\in p(\1)}p[i]$, as in \eqref{eqn.gamma_def} and \eqref{eqn.gamma_prod}.
That is, there is a natural isomorphism
\begin{equation} \label{eqn.yoneda_left_adjoint}
    \poly(p, \yon^A) \iso \smset(A, \Gamma(p)).
\end{equation}
\end{proposition}
\begin{proof}
By \eqref{eqn.main_formula}, we have the natural isomorphism
\[
    \poly(p, \yon^A) \iso \prod_{i \in p(\1)} p[i]^A,
\]
which in turn is naturally isomorphic to $\smset(A, \Gamma(p))$ by \eqref{eqn.gamma_prod}.
\end{proof}

% \begin{exercise} % Already basically done
% Show that $\Gamma(p)\cong\ihom{p,\yon}(\1)$ where $\ihom{-,-}$ is as in \cref{prop.dirichlet_closure}.
% \end{exercise}

\begin{corollary}[Principle monomial]\label{cor.principle_monomial}
There is an adjunction
\[
    \adj{\poly}{{(p(\1),\Gamma(p))}}{A\yon^B}{\smset\times\smset\op}
\]
where the functors are labeled by where they send $p\in\poly$ and $(A,B)\in\smset\times\smset\op$.
That is, there is a natural isomorphism
\begin{equation} \label{eqn.principle_monomial}
    \poly(p, A\yon^B) \iso \smset(p(1), A) \times \smset(B, \Gamma(p)).
\end{equation}
\end{corollary}
\begin{proof}
By the universal property of the product of $A$ and $\yon^B$, we have a natural isomorphism
\[
    \poly(p, A\yon^B) \iso \poly(p, A) \times \poly(p, \yon^B).
\]
Then the desired natural isomorphism follows from \cref{exc.adjoint_quadruple} \cref{exc.adjoint_quadruple.pos_const} and \eqref{eqn.yoneda_left_adjoint}.
\end{proof}

\begin{exercise}
Use \eqref{eqn.principle_monomial} together with \eqref{eqn.dir_dual} and \eqref{eqn.poly_closure_brackets} to find an alternative proof for \cref{prop.situations2}, i.e.\ that there is an isomorphism
\[
    \Gamma(p\otimes q) \iso \smset\big(q(\1),\Gamma(p)\big) \times \smset\big(p(\1),\Gamma(q)\big).
\]
for any $p,q\in\poly$.
\begin{solution}
% The universal properties of the adjunctions
% \[
%       \adj[40pt]{\poly}{{(-(\1),\Gamma(-))}}{-\yon^-}{\smset\times\smset\op}
%       \qqand
%       \adj{\poly}{-\otimes q}{{\ihom{q,-}}}{\poly}
% \]
% from \cref{cor.principle_monomial,prop.dirichlet_closure} together with the isomorphism $\ihom{p,\yon} \iso \Gamma(p)\yon^{p(\1)}$ from \eqref{eqn.dir_dual} give
We have the following chain of natural isomorphisms:
\begin{align*}
	\Gamma(p \otimes q) &=
	\poly(p \otimes q,\yon) 
	\tag*{\eqref{eqn.gamma_def}} \\
	&\iso
	\poly(p, \ihom{q,\yon}) 
	\tag*{\eqref{eqn.poly_closure_brackets}} \\
	&\iso
	\poly(p, \Gamma(q)\yon^{q(\1)})
	\tag*{\eqref{eqn.dir_dual}} \\
% 	&\iso
% 	(\smset\times\smset\op)\Big(\big((p(\1),\Gamma(p)\big),\big(\Gamma(q),q(\1)\big)\Big) \\
	&\iso
	\smset\big(p(\1),\Gamma(q)\big) \times \smset\big(q(\1),\Gamma(p)\big).
	\tag*{\eqref{eqn.principle_monomial}}
\qedhere
\end{align*}
\end{solution}
\end{exercise}

%-------- Section --------%
\section{Epi-mono factorization}

\begin{proposition}\label{prop.monics_in_poly}
Let $f \colon p \to q$ be a lens in $\poly$. It is a monomorphism if and only if the on-positions function $f_\1 \colon p(\1) \to q(\1)$ is a monomorphism in $\smset$ and, for each $i \in p(\1)$, the on-directions function $f^\sharp_i \colon q[f_\1(i)]\to p[i]$ is an epimorphism in $\smset$.
\end{proposition}
\begin{proof}
To prove the forward direction, suppose that $f$ is a monomorphism.
Since $p\mapsto p(\1)$ is a right adjoint (\cref{thm.adjoint_quadruple}), it preserves monomorphisms, so the on-positions function $f_\1$ is also a monomorphism.

We now need to show that for any $i\in p(\1)$, the on-directions function $f^\sharp_i \colon q[f_\1(i)] \to p[i]$ is an epimorphism.
Suppose we are given a set $A$ and a pair of functions $g^\sharp,h^\sharp\colon p[i]\tto A$ with $f^\sharp_i \then g^\sharp = f^\sharp_i \then h^\sharp$.
Then there exist lenses $g,h \colon \yon^A \tto p$ whose on-positions functions both pick out $i$ and whose on-directions functions are $g^\sharp$ and $h^\sharp$, so that $g \then f = h \then f$.
As $f$ is a monomorphism, $g = h$; in particular, their on-directions functions $g^\sharp$ and $h^\sharp$ are equal, as desired.

Conversely, suppose that $f_\1$ is a monomorphism and that, for each $i\in p(\1)$, the function $f^\sharp_i$ is an epimorphism.
Let $r$ be a polynomial and $g,h\colon r\tto p$ be two lenses such that $g \then f = h \then f$.
Then $g_\1 \then f_\1 = h_\1 \then f_\1$, which implies $g_\1 = h_\1$; we'll consider $g_\1$ the default representation.
We also have that $f^\sharp_{g_\1(k)} \then g^\sharp_k = f^\sharp_{g_\1(k)} \then h^\sharp_k$ for any $k \in r(\1)$. But $f^\sharp_{g_\1(k)}$ is an epimorphism, so in fact $g^\sharp_k = h^\sharp_k$, as desired.
\end{proof}

\begin{example}\label{ex.clock_in_N}
Choose a finite nonempty set $\ord{k}$ for $1\leq k\in\nn$, e.g.\ $\ord{k}=\1\2$. There is a monomorphism
\[
f \colon\ord{k}\yon^{\ord{k}}\to\nn\yon^\nn
\]
such that the trajectory ``going around and around the $k$-clock'' comes from the usual counting trajectory \cref{ex.counting_trajectory} $\nn\yon^\nn\to\yon$.

On positions, we have $f_\1(i)=i$ for all $i \in \ord{k}$. On directions, for any $i \in \ord{k}$, we have $f^\sharp_i(n) = n \mod k$ for all $n \in \nn$.
\end{example}

\begin{exercise}
In \cref{ex.clock_in_N}, we gave a map $\1\2\yon^{\1\2}\to\nn\yon^\nn$. This allows us to turn any dynamical system with $\nn$-many states into a dynamical system with 12 states, while keeping the same interface---say, $p$. 

Explain how the behavior of the new system $\1\2\yon^{\1\2}\to p$ would be seen to relate to the behavior of the old system $\nn\yon^\nn\to p$.
\begin{solution}
We are given a monomorphism $f \colon \1\2\yon^{\1\2} \to \nn\yon^\nn$ from \cref{ex.clock_in_N}.
Let $g \colon \nn\yon^\nn \to p$ be a dynamical system with return function $g_\1 \colon \nn \to p(\1)$ and update functions $g^\sharp_n \colon p[g_\1(n)] \to \nn$ for each state $n \in \nn$.
Then the new composite dynamical system $h \coloneqq f \then g$ has a return function $h_\1 \colon \1\2 \to p(\1)$ which sends each state $i \in \1\2$ to the output $h_\1(i) = g_\1(f_\1(i)) = g_\1(i)$, the same output that the original system returned in the state $i \in \nn$.
Meanwhile, the update function for each state $i \in \1\2$ is a function $h^\sharp_i \colon p[g_\1(i)] \to \1\2$ which, given an input $d \in p[g_\1(i)]$, updates the state from $i$ to $h^\sharp_i(d) = f^\sharp_{g_\1(i)}(g^\sharp_i(d)) = g^\sharp_i(d) \mod 12$, which is where the original system would have taken the same state to, but reduced modulo 12.
In other words, the new system behaves like the old system but with only the states in $\1\2 \ss \nn$ retained, and on any input that would have caused the old system to move to a state outside of $\1\2$, the new system moves to the equivalent state (modulo 12) within $\1\2$ instead. 
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.epis_in_poly}
Let $f \colon p \to q$ be a lens in $\poly$. It is an epimorphism if and only if the function $f_\1 \colon p(\1) \to q(\1)$ is an epimorphism in $\smset$ and, for each $j\in q(\1)$, the induced function
\[
    f^\flat_j \colon q[j] \to \prod_{\substack{i\in p(\1), \\ f_\1(i)=j}} p[i]
\]
from \eqref{eqn.useful_misc472} is a monomorphism.
\end{proposition}
\begin{proof}
To prove the forward direction, suppose that $f$ is an epimorphism. Since $p \mapsto p(\1)$ is a left adjoint (\cref{thm.adjoint_quadruple}), it preserves epimorphisms, so the on-positions function $f_\1$ is also a epimorphism.

We now need to show that for any $j\in q(\1)$, the induced function $f^\flat_j$ is a monomorphism.
Suppose we are given a set $A$ and a pair of functions $g',h'\colon A\tto q[j]$ with $g' \then f^\flat_j = h' \then f^\flat_j$.
They can be identified with lenses $g,h\colon q\tto \yon^A+\1$, which send the $j$-component to the first component, $\yon^A$, and send all other component to the second component, $\1$. It is easy to check that $fg=fh$, hence $g=h$, and hence $g^\sharp=h^\sharp$ as desired.

Then we can construct lenses $g,h \colon q \tto \yon^A+\1$ whose on-positions functions both send $j$ to the first position, corresponding to $\yon^A$, and all other positions to the second position, corresponding to $\1$.
In addition, we let the on-directions functions be $g^\sharp_j \coloneqq g'$ and $h^\sharp_j \coloneqq h'$.
Then $f \then g = f \then h$.
As $f$ is an epimorphism, $g = h$; in particular, their on-directions functions are equal, so $g' = h'$, as desired.

Conversely, suppose that $f_\1$ is an epimorphism and that, for each $j\in q(\1)$, the function $f^\flat_j$ is a monomorphism. 
Let $r$ be a polynomial and $g,h\colon q\tto r$ be two lenses such that $f \then g = f \then h$.
Then $f_\1 \then g_\1 = f_\1 \then h_\1$, which implies $g_\1=h_\1$; we'll consider $g_\1$ the default representation.
We also have that $g^\sharp_{f_\1(i)} \then f^\sharp_i = h^\sharp_{f_\1(i)} \then f^\sharp_i$ for any $i\in p(\1)$.
It follows that, for any $j \in q(\1)$, the two composites
\[
\begin{tikzcd}
	r[g_\1(j)] \ar[r, shift left, "g^\sharp_j"] \ar[r, shift right, "h^\sharp_j"'] & q[j] \ar[r, "f^\flat_j"] & \displaystyle\prod_{\substack{i\in p(\1), \\ f_\1(i)=j}} p[i]
\end{tikzcd}
\]
are equal, which implies that $g^\sharp_j=h^\sharp_j$ as desired.
\end{proof}

% Insert exercise exploring the difference between the epi proposition and the one about monos

\begin{exercise}
Show that the only way for a map $p\to\yon$ to \emph{not} be an epimorphism is when $p=0$.
\begin{solution}
Given $p \in \poly$ and a map $f \colon p \to \yon$, we will use \cref{prop.epis_in_poly} to show that either $f$ is an epimorphism or $p = \0$.
First, note that $f_\1 \colon p(\1) \to \1$ must be an epimorphism unless $p(\1) \iso \0$, in which case $p = \0$.
Next, note that the induced function
\[
    f^\flat \colon \1 \to \prod_{i\in p(\1)} p[i]
\]
from \eqref{eqn.useful_misc472} must be a monomorphism.
So it follows from \cref{prop.epis_in_poly} that either $f$ is an epimorphism or $p = \0$.
\end{solution}
\end{exercise}

\begin{exercise}
Let $A$ and $B$ be sets and $AB$ their product. Find an epimorphism $\yon^A+\yon^B\surj\yon^{AB}$.
\begin{solution}
Given sets $A$ and $B$, by \cref{prop.epis_in_poly}, a lens $f \colon \yon^A + \yon^B \to \yon^{AB}$ is an epimorphism if its on-positions function $f_\1 \colon \2 \to \1$ is an epimorphism (which must be true) and if the induced function
\[
    f^\flat \colon AB \to \prod_{i \in \2} (\yon^A + \yon^B)[i] \iso AB
\]
is a monomorphism.
If we take the on-directions functions $AB \to A$ and $AB \to B$ of $f$ to be the canonical projections, then the induced function $f^\flat \colon AB \to AB$ would be the identity, which is indeed a monomorphism.
So $f$ would be an epimorphism.
\end{solution}
\end{exercise}

\begin{exercise}
Suppose a lens is both a monomorphism and an epimorphism; it is then an isomorphism? (That is, is $\poly$ \emph{balanced}?)

Hint: You may use the following facts.
\begin{enumerate}
    \item A function that is both a monomorphism and an epimorphism in $\smset$ is an isomorphism.
    \item A lens is an isomorphism if and only if the on-positions function is an isomorphism and every on-directions function is an isomorphism.
\end{enumerate}
\begin{solution}
Let $f \colon p \to q$ be a lens in $\poly$ that is both a monomorphism and an epimorphism.
We claim that $f$ is an isomorphism.
By \cref{prop.monics_in_poly} and \cref{prop.epis_in_poly}, the on-positions function $f_\1 \colon p(\1) \to q(\1)$ is both a monomorphism and an epimorphism, so it is an isomorphism.
Meanwhile, \cref{prop.epis_in_poly} says that, for each $j \in q(\1)$, the induced function
\[
    f^\flat_j \colon q[j] \to \prod_{\substack{i \in p(\1), \\ f_\1(i) = j}} p[i]
\]
is a monomorphism.
As $f_\1$ is an isomorphism, it follows that for each $i \in p(\1)$, the function
\[
    f^\flat_{f_\1(i)} \colon q[f_\1(i)] \to p[i]
\]
is a monomorphism.
But this is just the on-directions function $f^\sharp_i$ of $f$.
From \cref{prop.monics_in_poly}, we also know that $f^\sharp_i$ is an epimorphism.
It follows that every on-directions function of $f$ is an isomorphism.
Hence $f$ itself is an isomorphism.
\end{solution}
\end{exercise}

We are often interested in whether epimorphisms and monomorphisms form what is called a \emph{factorization system} in a given category, which we define below.

\begin{definition}[Factorization system] \label{def.factor}
Given a category $\cat{C}$ and two classes of morphisms $E$ and $M$ in $\cat{C}$, we say that $(E, M)$ is a \emph{factorization system} of $\cat{C}$ if:
\begin{enumerate}
    \item every morphism $f$ in $\cat{C}$ factors uniquely (up to unique isomorphism) as a morphism $e \in E$ composed with a morphism $m\in M$, so that $f = e\then m$;
    \item $E$ and $M$ each contain every isomorphism; and
    \item $E$ and $M$ are each closed under composition.
\end{enumerate}
If $E$ is the class of epimorphisms and $M$ is the class of monomorphisms (in which case conditions 2 and 3 are automatically satisfied), we say that $\cat{C}$ has \emph{epi-mono factorization}.
\end{definition}

\begin{example}[Epi-mono factorization in $\smset$] \label{ex.epi_mono_set}
The category $\smset$ has epi-mono factorization: a function $f\colon X\to Y$ can be uniquely factored into an epimorphism (surjection) $e$ followed by a monomorphism (injection) $i$, as follows.
The epimorphism $e\colon X\to f(X)$ is given by restricting the codomain of $f$ to its image (also known as \emph{corestricting} $f$), so $e$ sends $x\mapsto f(x)$ for all $x\in X$.
The monomorphism $i\colon f(X)\to Y$ is then given by including the image into the codomain, so $i$ sends $y\mapsto y$ for all $y\in f(X)\ss Y$.
\end{example}

\begin{proposition}
$\poly$ has epi-mono factorization.
\end{proposition}
\begin{proof}
Take an arbitrary lens $\phi\colon p\to q$.
It suffices to show that there exists a unique polynomial $r$ equipped with an epimorphism $\epsilon\colon p\to r$ and a monomorphism $\mu\colon r\to q$ such that $\phi=\epsilon\then\mu$.

On positions, we must have $\phi_\1 = \epsilon_\1\then\mu_\1$, with $\mu_\1$ a monomorphism and $\epsilon_\1$ an epimorphism per \cref{prop.monics_in_poly,prop.epis_in_poly}.
By \cref{ex.epi_mono_set}, since $\smset$ has epi-mono factorization, such $r(\1), \epsilon_\1,$ and $\mu_\1$ uniquely exist.
In particular, we must have that $r(\1)\iso \phi_\1(p(\1))$, that $\epsilon_\1\colon p(\1)\to\phi_\1(p(\1))$ is the corestriction of $\phi_\1$ sending $i\mapsto\phi_\1(i)$ for each $p$-position $i$, and that $\mu_\1\colon\phi_\1(p(\1))\to q(\1)$ is the inclusion sending $j\mapsto j$ for each $r$-position $j$.

Then on directions, for any $i\in p(\1)$, we must have that
\[
\begin{tikzcd}
    q[\phi_\1(i)] \ar[r, "\mu^\sharp_{\phi_\1(i)}"] \ar[dr, "\phi^\sharp_i"'] & r[\phi_\1(i)] \ar[d, "\epsilon^\sharp_i"] \\
    & p[i]
\end{tikzcd}
\]
commutes---or, equivalently, for every $j\in r(\1)\iso\phi_\1(p(\1))$,
\[
\begin{tikzcd}
    q[j] \ar[r, "\mu^\sharp_j"] \ar[dr, "\phi^\flat_j"'] & r[j] \ar[d, "\epsilon^\flat_j"] \\
    & \prod\limits_{\substack{i\in p(\1), \\ \phi_\1(i)=j}} p[i]
\end{tikzcd}
\]
commutes (here $\phi^\flat_j$ and $\epsilon^\flat_j$ are the induced functions from \eqref{eqn.useful_misc472}), with $\mu_j^\sharp$ an epimorphism and $\epsilon^\flat_j$ a monomorphism per
\cref{prop.monics_in_poly,prop.epis_in_poly}.
So again since $\smset$ has epi-mono factorization, such $r[j], \mu^\sharp_j,$ and $\epsilon^\flat_j$ uniquely exist.
Hence such $p \To{\epsilon} r \To{\mu} q$ uniquely exists overall.
\end{proof}

%---- Section ----%
\section{Cartesian closure}

We have already seen the closure operation $\ihom{-,-}$ for one monoidal structure on $\poly$, namely $(\yon,\otimes)$.
But this is not the only closed monoidal structure on $\poly$: in fact, we will show that $\poly$ is cartesian closed as well.

For any two polynomials $q,r$, define $r^q\in\poly$ by the formula
\begin{equation}\label{eqn.exponential}
  r^q\coloneqq\prod_{j\in q(\1)}r\circ(\yon+q[j])
\end{equation}
where $\circ$ denotes composition.

Before proving that this really is an exponential in $\poly$, which we do in \cref{thm.poly_cart_closed}, we first get some practice with it.

\begin{example}
Let $A$ be a set. We've been writing the polynomial $A\yon^\0$ simply as $A$, so it better be true that the there is an isomorphism 
\[
    \yon^A \iso \yon^{A\yon^\0}
\]
in order for the notation to be consistent. 
Luckily, this is true.
By \eqref{eqn.exponential}, we have
\[
    \yon^{A\yon^\0} = \prod_{a\in A} \yon \circ (\yon+\0) \iso \yon^A
\]
\end{example}

\begin{exercise}
Compute the following exponentials in $\poly$ using \eqref{eqn.exponential}:
\begin{enumerate}
	\item $p^\0$ for an arbitrary $p\in\poly$.
	\item $p^\1$ for an arbitrary $p\in\poly$.
	\item $\1^p$ for an arbitrary $p\in\poly$.
	\item $A^p$ for an arbitrary $p\in\poly$ and $A\in\smset$.
	\item $\yon^\yon$.
	\item $\yon^{\4\yon}$.
	\item $(\yon^A)^{\yon^B}$ for arbitrary sets $A,B\in\smset$.
\qedhere
\end{enumerate}
\begin{solution}
We use \eqref{eqn.exponential} to compute various exponentials.
Here $p \in \poly$ and $A, B \in \smset$.
\begin{enumerate}
    \item We have that $p^\0$ is an empty product, so $p^\0 \iso \1$ as expected.
	\item We have that $p^\1 \iso p \circ (\yon + \0) \iso p$, as expected.
	\item We have that $\1^p \iso \prod_{i \in p(\1)} \1 \circ (\yon + p[i]) \iso \1$, as expected.
	\item We have that $A^p \iso \prod_{i \in p(\1)} A \circ (\yon + p[i]) \iso A^{p(\1)}$.
	\item We have that $\yon^\yon \iso \yon \circ (\yon + \1) \iso \yon + \1$.
	\item We have that $\yon^{\4\yon} \iso \prod_{j \in \4} \yon \circ (\yon + \1) \iso (\yon + \1)^\4 \iso \yon^\4 + \4\yon^\3 + \6\yon^\2 + \4\yon + \1$.
	\item We have that $(\yon^A)^{\yon^B} \iso (\yon^A) \circ (\yon + B) \iso (\yon + B)^A \iso \sum_{f \colon A \to \2} B^{f\inv(1)} \yon^{f\inv(2)}$.
\end{enumerate}
\end{solution}
\end{exercise}

% \begin{exercise} % Immediate from previous exercise
% Using \eqref{eqn.exponential}, show that the functor $\smset\to\poly$ that sends each set $A$ to the constant polynomial $A$ preserves exponentials.
% That is, given sets $A, B \in \smset$, the set $B^A$ as a constant polynomial coincides with the exponential in $\poly$ that is the constant polynomial $B$ raised to the constant polynomial $A$.
% \begin{solution}
% By \eqref{eqn.exponential}, the exponential that is the constant polynomial $B$ raised to the constant polynomial $A$ can be written as
% \[
%     \prod_{a \in A} B \circ (\yon + A[a]) \iso \prod_{a \in A} B \iso B^A.
% \]
% \end{solution}
% \end{exercise}

\begin{theorem}\label{thm.poly_cart_closed}
The category $\poly$ is cartesian closed. That is, we have a natural isomorphism
\[
    \poly(p,r^q) \iso \poly(p\times q,r),
\]
where $r^q$ is the polynomial defined in \eqref{eqn.exponential}.
\end{theorem}
\begin{proof}
We have the following chain of natural isomorphisms:
\begin{align*}
	\poly(p, r^q) &\iso
	\poly\Big(p, \prod_{j \in q(\1)} r \circ (\yon+q[j])\Big)
	\tag*{\eqref{eqn.exponential}} \\
% 	&\iso
% 	\prod_{j\in q(\1)}\poly(p,r\circ(\yon+q[j]))
% 	\tag{Universal property of products} \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}\poly\big(\yon^{p[i]},r\circ(\yon+q[j])\big)
	\tag{Universal property of (co)products} \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}r\circ(p[i]+q[j])
	\tag{Yoneda lemma} \\
	&\iso
	\prod_{i\in p(\1)}\prod_{j\in q(\1)}\sum_{k\in r(\1)}(p[i]+q[j])^{r[k]}
	\\
	&\iso
	\prod_{(i,j) \in (p \times q)(\1)} \; \sum_{k\in r(\1)}(p \times q)[(i, j)]^{r[k]}
	\tag*{\eqref{eqn.poly_times}} \\
	&\iso
	\poly(p \times q,r).
	\tag*{\eqref{eqn.main_formula}}
\end{align*}
\end{proof}

\begin{exercise}
Use \cref{thm.poly_cart_closed} to show that for any polynomials $p,q$, there is a canonical evaluation map
\begin{equation*}%\label{eqn.eval_times}
	\text{eval}\colon p^q \times q \to p.
\end{equation*}
\begin{solution}
By \cref{thm.poly_cart_closed}, there is a natural isomorphism
\[
    \poly(p^q, p^q) \iso \poly(p^q \times q, p).
\]
Under this isomorphism, there exists a map $\text{eval} \colon p^q \times q \to p$ corresponding to the identity map on $p^q$.
The map $\text{eval}$ is the canonical evaluation map.
\end{solution}
\end{exercise}


%-------- Section --------%
\section{Limits and colimits}

We have already seen that $\poly$ has all coproducts (\cref{prop.poly_coprods}) and products (\cref{prop.poly_prods}).
We will now see that $\poly$ has all small limits and colimits.

\begin{theorem}\label{thm.poly_limits}
The category $\poly$ has all small limits.
\end{theorem}
\begin{proof}
A category has all small limits if and only if it has products and equalizers, so by \cref{prop.poly_prods}, it suffices to show that $\poly$ has equalizers. 

We claim that equalizers in $\poly$ are simply equalizers on positions and coequalizers on directions.
More precisely, let $f,g \colon p \tto q$ be two lenses.
We construct the equalizer $p'$ of $f$ and $g$ as follows.\footnote{If we're being precise, a ``(co)equalizer'' is an object equipped with a map, but we will use the term to refer to either just the object or just the map when the context is clear.}
We define its position-set $p'(\1)$ to be the equalizer of $f_\1,g_\1 \colon p(\1) \tto q(\1)$ in $\smset$; that is,
\[
    p'(\1) \coloneqq \{i \in p(\1) \mid f_\1(i) = g_\1(i)\}.
\]
Then for each $i \in p'(\1)$, we can define the direction-set $p'[i]$ to be the coequalizer of $f^\sharp_i, g^\sharp_i \colon q[f_\1(i)] \tto p[i]$.
In this way, we obtain a polynomial $p'$ that comes equipped with a lens $e \colon p' \to p$.
One can check that $p'$ together with $e$ satisfies the universal property of the equalizer of $f$ and $g$; see \cref{exc.poly_limits}.
\end{proof}

\begin{exercise}\label{exc.poly_limits}
Complete the proof of \cref{thm.poly_limits} as follows:
\begin{enumerate}
	\item We said that $p'$ comes equipped with a lens $e \colon p' \to p$; what is it?
	\item Show that $e \then f = e \then g$.
	\item Show that $e$ is the equalizer of the pair $f,g$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item The lens $e \colon p' \to p$ can be characterized as follows.
    The on-positions function $e_\1 \colon p'(\1) \to p(\1)$ is the equalizer of $f_\1, g_\1 \colon p(\1) \tto q(\1)$ in $\smset$.
    In particular, $e_\1$ is the canonical inclusion that sends each element of $p'(\1)$ to the same element in $p(\1)$.
    Then for each $i \in p'(\1)$, the on-directions function $e^\sharp_i \colon p[i] \to p'[i]$ is the coequalizer of $f^\sharp_i, g^\sharp_i \colon q[f_\1(i)] \tto p[i]$ in $\smset$.
    
    \item To show that $e \then f = e \then g$, it suffices to show that both sides are equal on positions and on directions.
    On positions, $e_\1$ is defined to be the equalizer of $f_\1$ and $g_\1$, so $e_\1 \then f_\1 = e_\1 \then g_\1$.
    Then for each $i \in p'(\1)$, the on-directions function $e^\sharp_i$ is defined to be the coequalizer of $f^\sharp_i$ and $g^\sharp_i$, so $f^\sharp_i \then e^\sharp_i = g^\sharp_i \then e^\sharp_i$.
    
    \item To show that $e$ is the equalizer of $f$ and $g$, it suffices to show that for any $r \in \poly$ and map $a \colon r \to p$ satisfying $a \then f = a \then g$, there exists a unique map $h \colon r \to p'$ for which $a = h \then e$, so that the following diagram commutes.
    \begin{equation*} %\label{eqn.eq_univ_prop}
    \begin{tikzcd}
        p' \ar[r, "e"] & p \ar[r, "f", shift left] \ar[r, "g"', shift right] & q \\
        r \ar[u, "h", dashed] \ar[ur, "a"']
    \end{tikzcd}
    \end{equation*}
    In order for $a = h \then e$ to hold, we must have $a_\1 = h_\1 \then e_\1$ on positions.
    But we have that $a_\1 \then f_\1 = a_\1 \then g_\1$, so by the universal property of $p'(\1)$ and the map $e_\1$ as the equalizer of $f_\1$ and $g_\1$ in $\smset$, there exists a unique $h_\1$ for which $a_\1 = h_\1 \then e_\1$.
    Hence $h$ is uniquely characterized on positions.
    In particular, it must send each $k \in r(\1)$ to $a_\1(k) \in p'(\1)$.
    
    Then for $a = h \then e$ to hold on directions, we must have that $a^\sharp_k = e^\sharp_{a_\1(k)} \then h^\sharp_k$ for each $k \in r(\1)$.
    But we have that $f^\sharp_{a_\1(k)} \then a^\sharp_{a_\1(k)} = g^\sharp_{a_\1(k)} \then a^\sharp_{a_\1(k)}$, so by the universal property of $p'[a_\1(k)]$ and the map $e^\sharp_{a_\1(k)}$ as the coequalizer of $f^\sharp_{a_\1(k)}$ and $g^\sharp_{a_\1(k)}$ in $\smset$, there exists a unique $h^\sharp_k$ for which $a^\sharp_k = e^\sharp_{a_\1(k)} \then h^\sharp_k$, so that the diagram below commutes.
    \begin{equation*} %\label{eqn.eq_univ_prop_dir}
    \begin{tikzcd}[sep=large]
        p'[a_\1(k)] \ar[d, "h^\sharp_k"', dashed] & p[a_\1(k)] \ar[l, "e^\sharp_{a_\1(k)}"'] \ar[dl, "a^\sharp_k"] & q[f_\1(a_\1(k))] \ar[l, "f^\sharp_{a_\1(k)}"', shift right] \ar[l, "g^\sharp_{a_\1(k)}", shift left] \\
        r[k]
    \end{tikzcd}
    \end{equation*}
    Hence $h$ is also uniquely characterized on directions, so it is unique overall.
    Moreover, we have shown that we can define $h$ on positions so that $a_\1 = h_\1 \then e_\1$, and that we can define $h$ on directions such that $a^\sharp_k = e^\sharp_{a_\1(k)} \then h^\sharp_k$ for all $k \in r(\1)$.
    It follows that there exists $h$ for which $a = h \then e$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{example}[Computing general limits in $\poly$]
The proof of \cref{thm.poly_limits} justifies the following mnemonic for limits in $\poly$:
\slogan{The positions of a limit are the limit of the positions. \\ The directions of a limit are the colimit of the directions.}
We can make this precise as follows: the limit of a functor $p_-\colon\cat{J}\to\poly$ is the polynomial whose position-set is
\begin{equation} \label{eqn.lim_pos}
    \left(\lim_{j\in\cat{J}} p_j\right)(\1) \iso \lim_{j\in\cat{J}} p_j(\1),
\end{equation}
equipped with a canonical projection $\pi_j$ to each $p_j(\1)$, and whose direction-set for each position $i$ is
\begin{equation} \label{eqn.lim_dir}
    \left(\lim_{j\in\cat{J}} p_j\right)[i] \iso \colim_{j\in\cat{J}\op} p_j[\pi_j(i)].
\end{equation}
This notation obscures what is occuring on lenses, but in particular, each lens $\phi\colon p_j\to p_{j'}$ in the diagram $p_-$ induces an on-positions function $\phi_\1\colon p_j(\1)\to p_{j'}(\1)$ in the diagram whose limit we take in \eqref{eqn.lim_pos} and, for every position $i$ of the limit, an on-directions function $\phi^\sharp_{\pi_j(i)}\colon p_{j'}[\pi_{j'}(i)]\to p_j[\pi_j(i)]$ in the diagram whose colimit we take in \eqref{eqn.lim_dir}.
(Note that, by the definition of a limit, $\phi_\1(\pi_j(i)) = \pi_{j'}(i)$.)

We have seen \eqref{eqn.lim_pos} and \eqref{eqn.lim_dir} to be true for products: the position-set of the product is just the product of the original position-sets, while the direction-set at a tuple of the original positions is just the coproduct of the direction-sets at every position in the tuple.
We have also just shown \eqref{eqn.lim_pos} and \eqref{eqn.lim_dir} to be true for equalizers in the proof of \cref{thm.poly_limits}.
It follows from the construction of any limit as an equalizer of products that it is true for arbitrary limits.
\end{example}

\begin{example}[Pullbacks in $\poly$]\label{ex.pullbacks_in_poly}
Given $q,q',r \in \poly$ and lenses $q\To{f} r\From{f'} q'$, the pullback 
\[
\begin{tikzcd}
	p\ar[r, "g'"]\ar[d, "g"']&
	q'\ar[d, "f'"]\\
	q\ar[r, "f"']&
	r\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
is given as follows.
The position-set of $p$ is the pullback of the position-sets of $q$ and $q'$ over that of $r$ in $\smset$.
Then at each position $(i, i') \in p(\1) \ss q(\1) \times q'(\1)$ with $f_\1(i)=f'_\1(i')$, we take the direction-set $p[(i, i')]$ to be the pushout of the direction-sets $q[i]$ and $q'[i']$ over $r[f_\1(i)]=r[f_\1'(i')]$ in $\smset$.
These pullback and pushout squares also give the lenses $g$ and $g'$ on positions and on directions:
\begin{equation}\label{eqn.pullback_poly}
\begin{tikzcd}
	p(\1)\ar[r, "g'_\1"]\ar[d, "g_\1"']&
	q'(\1)\ar[d, "f_\1'"]\\
	q(\1)\ar[r, "f_\1"']&
	r(\1)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\qqand
\begin{tikzcd}
	p[(i,i')]\ar[from=r, "(g')^\sharp_{(i,i')}"']\ar[from=d, "g^\sharp_{(i,i')}"]&
	q'[i']\ar[from=d, "(f')^\sharp_{i'}"']\\
	q[i]\ar[from=r, "f^\sharp_i"]&
	r[f_1(i)]\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\end{equation}
\end{example}

\begin{exercise}
Let $p$ be any polynomial.
\begin{enumerate}
	\item There is a canonical choice of lens $\eta\colon p\to p(\1)$; what is it?
	\item Given an element $i\in p(\1)$, i.e.\ a function (or lens between constant polynomials) $i\colon\1\to p(\1)$, let $p_i$ be the pullback
	\[
	\begin{tikzcd}
	p_i\ar[r, "g"]\ar[d, "f"']&
	p\ar[d, "\eta"]\\
	\1\ar[r, "i"']&
	p(\1)\ar[ul, phantom, very near end, "\lrcorner"]
	\end{tikzcd}
	\]
	What is $p_i$? What are the maps $f \colon p_i \to \1$ and $g \colon p_i \to p$? \qedhere
\end{enumerate}
\begin{solution}
Here $p \in \poly$.
\begin{enumerate}
    \item The canonical lens $\eta \colon p \to p(\1)$ is the identity $\eta_\1 \colon p(\1) \to p(\1)$ on positions and the empty function on directions.
    
    \item On positions, we have that $p_i(\1)$ along with $f_\1$ and $g_\1$ form the following pullback square in $\smset$:
    \[
	\begin{tikzcd}
    	p_i(\1) \ar[r, "g_\1"] \ar[d, "f_\1"'] &
    	p(\1) \ar[d, equals] \\
    	\1 \ar[r, "i"'] &
    	p(\1) \ar[ul, phantom, very near end, "\lrcorner"]
	\end{tikzcd}
	\]
	So $p_i(\1) \coloneqq \{(a, i') \in \1 \times p(\1) \mid i = i' \} = \{(1, i)\}$, with $f_\1$ uniquely determined and $g_1$ picking out $i \in p(\1)$.
	Then on directions, we have that $p_i[(1,i)]$ along with $f^\sharp_{(1,i)}$ and $g^\sharp_{(1,i)}$ form the following pushout square in $\smset$:
	\[
	\begin{tikzcd}
    	p_i[(1,i)] \ar[from=r, "g^\sharp_{(1,i)}"'] \ar[from=d, "f^\sharp_{(1,i)}"] &
    	p[i] \ar[from=d, "!"'] \\
    	\0 \ar[from=r, "!"] &
    	\0 \ar[ul, phantom, very near end, "\lrcorner"]
    \end{tikzcd}
    \]
    So $p_i[(1,i)] \coloneqq p[i]$, with $f^\sharp_{(1,i)}$ uniquely determined and $g^\sharp_{(1,i)}$ as the identity.
    It follows that $p_i \coloneqq \{(1,i)\}\yon^{p[i]} \iso \yon^{p[i]}$, where $f \colon p_i \to \1$ is uniquely determined and $g \colon p_i \to p$ picks out $i \in p(\1)$ on positions and is the identity on $p[i]$ on directions.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Let $q\coloneqq \yon^\2+\yon$, $q'\coloneqq\2\yon^\3+\yon^\2$, and $r\coloneq\yon+\1$.
\begin{enumerate}
	\item Choose lenses $f\colon q\to r$ and $f'\colon q'\to r$ and write them down.
	\item Find the pullback of $q\To{f} r\From{f'} q'$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item There are many possible answers, but one lens $f \colon q \to r$, on positions, sends $1 \in q(\1)$ (corresponding to $\yon^\2$) to $2 \in r(\1)$ (corresponding to $\1$) and $2 \in q(\1)$ (corresponding to $\yon$) to $1 \in r(\1)$ (corresponding to $\yon$).
    Then the on-directions functions $f^\sharp_\1 \colon \0 \to \2$ and $f^\sharp_2 \colon \1 \to \1$ are uniquely determined.
    Another morphism $f' \colon q' \to r$, on positions, sends $1 \in q'(\1)$ (corresponding to one of the $\yon^\3$ terms) to $2 \in r(\1)$ and both $2 \in q'(\1)$ (corresponding to the other $\yon^\3$ term) and $3 \in q'(\1)$ (corresponding to the $\yon^\2$ term) to $1 \in r(\1)$.
    Then the on-directions function $(f')^\sharp_1 \colon \0 \to \3$ is uniquely determined, while we can let $(f')^\sharp_2 \colon \1 \to \3$ pick out $3$ and $(f')^\sharp_3 \colon \1 \to \2$ pick out $1$.
    
    \item We compute the pullback $p$ along with the lenses $g \colon p \to q$ and $g' \colon p \to q'$ of $q\To{f} r\From{f'} q'$ by following \cref{ex.pullbacks_in_poly}.
    We can compute $p(\1)$ by taking the pullback in $\smset$:
    \[
        p(\1) \coloneqq \{(i, i') \in \2 \times \3 \mid f_\1(i) = f'_\1(i)\} = \{(1,1), (2,2), (2,3)\}.
    \]
    Moreover, the on-positions functions $g_\1$ and $g'_\1$ send each pair in $p(\1)$ to its left component and its right component, respectively.
    
    To compute the direction-set at each $p$-position, we must compute a pushout.
    At $(1,1)$, we have $r[f_\1(1)] = r[f'_\1(1)] = r[2] = \0$, so the pushout $p[(1,1)]$ is just the sum $q[1] + q'[1] = \2 + \3 \iso \5$.
    Moreover, the on-directions functions $g^\sharp_{(1,1)}$ and $(g')^\sharp_{(1,1)}$ are the canonical inclusions $\2 \to \2 + \3$ and $\3 \to \2 + \3$.
    
    At $(2,2)$, we have $r[f_\1(2)] = r[f'_\1(2)] = r[1] = \1$, with $f^\sharp_2$ picking out $1 \in \1 = q[2]$ and $(f')^\sharp_2$ picking out $3 \in \3 = q'[2]$.
    So the pushout $p[(2,2)]$ is the set $\1 + \3 = \{(1,1), (2,1), (2,2), (2,3)\}$ but with $(1,1)$ identified with $(2,3)$; we can think of it as the set of equivalence classes $p[(2,2)] \iso \{\{(1,1), (2,3)\}, \{(2,1)\}, \{(2,2)\}\} \iso \3$.
    Moreover, the on-directions function $g^\sharp_{(2,2)}$ maps $1 \mapsto \{(1,1), (2,3)\}$, while the on-directions function $(g')^\sharp_{(2,2)}$ maps $1 \mapsto \{(2,1)\}, 2 \mapsto \{(2,2)\},$ and $3 \mapsto \{(1,1), (2,3)\}$.
    
    Finally, at $(2,3)$, we have $r[f_\1(2)] = r[f'_\1(3)] = r[1] = \1$, with $f^\sharp_2$ still picking out $1 \in \1 = q[2]$ and $(f')^\sharp_3$ picking out $1 \in \2 = q'[3]$.
    So the pushout $p[(2,3)]$ is the set $\1 + \2 = \{(1,1), (2,1), (2,2)\}$ but with $(1,1)$ identified with $(2,1)$; we can think of it as the set of equivalence classes $p[(2,3)] \iso \{\{(1,1), (2,1)\}, \{(2,2)\}\} \iso \2$.
    Moreover, the on-directions function $g^\sharp_{(2,3)}$ maps $1 \mapsto \{(1,1), (2,1)\}$, while the on-directions function $(g')^\sharp_{(2,3)}$ maps $1 \mapsto \{(1,1), (2,1)\}$ and $2 \mapsto \{(2,2)\}$.

    It follows that $p \iso \yon^\5 + \yon^\3 + \yon^\2$, with $g$ and $g'$ as described.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise} \label{exc.refl_limits}
An alternative way to prove \cref{thm.poly_limits} would have been to show that the equalizer of two natural transformations between polynomial functors in $[\smset,\smset]$ is still a polynomial functor---since the full subcategory inclusion $\poly \to [\smset,\smset]$ reflects these equalizers, it would follow that $\poly$ has equalizers.
But we already know what polynomial the equalizer should be from the proof of \cref{thm.poly_limits}.
So in this exercise, we will show that the equalizer of polynomials we found in $\poly$ is also the equalizer of those same functors in $[\smset,\smset]$.

Let $f,g\colon p\tto q$ be a pair of natural transformations $f,g\colon p\tto q$ between polynomial functors $p$ and $q$, and let $e\colon p'\to p$ be their equalizer in $\poly$ that we computed in the proof of \cref{thm.poly_limits}.
\begin{enumerate}
    \item Given a set $X$, show that $e_X\colon p'(X)\to p(X)$ is the equalizer of the $X$-components $f_X,g_X\colon p(X)\tto q(X)$ in $\smset$.
    \item Deduce that equalizers in $\poly$ coincide with equalizers in $[\smset,\smset]$.
    \item Conclude that limits in $\poly$ coincide with limits in $[\smset,\smset]$. \qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item By \cref{prop.morph_arena_to_func}, $f_X$ (resp.\ $g_X$) sends each $(i,h)\in p(X)$ with $i\in p(\1)$ and $h\colon p[i]\to X$ to $(f_\1(i), f^\sharp_i\then h)$ (resp.\ $(g_\1(i), g^\sharp_i\then h)$) in $q(X)$.
    So the equalizer of $f_X$ and $g_X$ is the set of all $(i,h)\in p(X)$ for which both $f_\1(i) = g_\1(i)$ and $f^\sharp_i\then h = g^\sharp_i\then h$.
    
    Indeed, by our construction of $p'$, the set $p'(X)$ consists of all pairs $(i,h')$ with $i\in p(\1)$ such that $f_\1(i) = g_\1(i)$ and $h'\colon p'[i]\to X$, where $p'[i]$ is the coequalizer of $f^\sharp_i,g^\sharp_i\colon q[f_\1(i)]\tto p[i]$.
    By the universal property of the coequalizer, functions $h'\colon p'[i]\to X$ precisely correspond to functions $h\colon p[i]\to X$ for which $f^\sharp_i\then h = g^\sharp_i\then h$.
    So $p'(X)$ is indeed the equalizer of $f_X$ and $g_X$.
    
    The equalizer natural transformation $e'\colon p'\to p$ has the inclusion $e'_X\colon p'(X)\to p(X)$ as its $X$-component, so by \cref{cor.morph_func_to_arena}, it is the lens whose on-positions function is the canonical equalizer inclusion $e'_\1\colon p'(\1)\to p(\1)$, while its on-directions function at $i\in p'(\1)$ is the map $p[i]\to p'[i]$ corresponding to the identity on $p'[i]$ given by the universal property of the coequalizer---which is just the canonical coequalizer map $p[i]\to p'[i]$.
    But this is exactly the map $e\colon p'\to p$ constructed in the proof of \cref{prop.poly_prods}, as desired.
    \item By \cref{prop.presheaf_lim_ptwise}, limits---including equalizers---in $[\smset,\smset]$ are computed pointwise.
    So if $e_X\colon p'(X)\to p(X)$ is the equalizer of $f_X,g_X\colon p(X)\tto q(X)$ for every $X\in\smset$, then $e\colon p'\to p$ is the equalizer of $f,g\colon p(X)\tto q(X)$.
    \item We have just shown that equalizers in $\poly$ coincide with equalizers in $[\smset,\smset]$.
    We saw in the proof of \cref{prop.poly_prods} that products in $\poly$ also coincide with products in $[\smset,\smset]$.
    Since every limit can be computed as an equalizer of products, we can conclude that limits in $\poly$ coincide with limits in $[\smset,\smset]$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{theorem}\label{thm.poly_colimits}
The category $\poly$ has all small colimits.
\end{theorem}
\begin{proof}
A category has all small colimits if and only if it has coproducts and coequalizers, so by \cref{prop.poly_coprods}, it suffices to show that $\poly$ has coequalizers.

Let $s,t \colon p \tto q$ be two lenses.
We construct the coequalizer $q'$ of $s$ and $t$ as follows.
The pair of functions $s_\1, t_\1 \colon p(\1) \tto q(\1)$ define a graph $G \colon \fbox{$\bullet\tto\bullet$} \to \smset$ with vertices in $q(\1)$, edges in $p(\1)$, sources indicated by $s_\1$, and targets indicated by $t_\1$.
Then the set $C$ of connected components of $G$ is given by the coequalizer $g_\1 \colon q(\1) \to C$ of $s_\1$ and $t_\1$.
We define the position-set of $q'$ to be $C$.
Each direction-set of $q'$ will be a limit of a diagram of direction-sets of $p$ and $q$, but expressing this limit, as we proceed to do, is a bit involved.

For each connected component $c \in C$, we have a connected subgraph $G_c \ss G$ with vertices $V_c \coloneqq g_\1\inv(c)$ and edges $E_c \coloneqq s_\1\inv(g_\1\inv(c)) = t_\1\inv(g_\1\inv(c))$.
Note that $E_c\ss p(\1)$ and $V_c\ss q(\1)$, so to each $e\in E_c$ (resp.\ to each $v\in V_c$) we have an associated direction-set $p[e]$ (resp.\ $q[v]$).

The category of elements $\int G_c$ has objects $E_c+V_c$ and two kinds of (non-identity) morphisms, $e \to s_\1(e)$ and $e \to t_\1(e)$, associated to each $e \in E_c$, all pointing from an object in $E_c$ to an object in $V_c$.
There is a functor $F \colon (\int G_c)\op \to \smset$ sending every $v \mapsto q[v]$, every $e \mapsto p[e]$, and every morphism to a function between them, namely either $s^\sharp_e \colon q[s_\1(e)] \to p[e]$ or $t^\sharp_e \colon q[t_\1(e)] \to p[e]$.
So we can define $q'[c]$ to be the limit of $F$ in $\smset$.

We claim that $q'\coloneqq\sum_{c\in C}\yon^{q'[c]}$ is the coequalizer of $s$ and $t$. We leave the complete proof to the interested reader in \cref{exc.poly_colimits}.
\end{proof}

\begin{exercise}\label{exc.poly_colimits}
Complete the proof of \cref{thm.poly_colimits} as follows:
\begin{enumerate}
	\item Provide a map $g \colon q \to q'$.
	\item Show that $s \then g = t \then g$.
	\item Show that $g$ is a coequalizer of the pair $s, t$.
\qedhere
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We define a map $g \colon q \to q'$ as follows.
    The on-positions function $g_\1 \colon q(\1) \to q'(\1)$ is the coequalizer of $s_\1, t_\1 \colon p(\1) \tto q(\1)$.
    In particular, $g_\1$ sends each vertex in $q(\1)$ to its corresponding connected component in $q'(\1) = C$.
    Then for each $v \in q(\1)$, if we let its corresponding connected component be $c \coloneqq g_\1(v)$, we can define the on-directions function $g^\sharp_v \colon q'[c] \to q[v]$ to be the projection from the limit $q'[c]$ to its component $q[v]$.
    
    \item To show that $s \then g = t \then g$, we must show that both sides are equal on positions and on directions.
    The on-positions function $g_\1$ is defined to be the coequalizer of $s_\1$ and $t_\1$, so $s_\1 \then g_\1 = t_\1 \then g_\1$.
    So it suffices to show that for all $e \in p(\1)$, if we let its corresponding connected component be $c \coloneqq g_\1(s_\1(e)) = g_\1(t_\1(e))$, then the following diagram of on-directions functions commutes:
    \[
    \begin{tikzcd}[sep=small]
        & q[s_\1(e)] \ar[dl, "s^\sharp_e"'] \\
        p[e] & & q'[c] \ar[ul, "g^\sharp_{s_\1(e)}"'] \ar[dl, "g^\sharp_{t_\1(e)}"] \\
        & q[t_\1(e)] \ar[ul, "t^\sharp_e"]
    \end{tikzcd}
    \]
    But this is automatically true by the definition of $q'[c]$ as a limit---specifically the limit of a functor with $s^\sharp_e$ and $t^\sharp_e$ in its image---and the definitions of $g^\sharp_{s_\1(e)}$ and $g^\sharp_{t_\1(e)}$ as projections from this limit.
    
    \item To show that $g$ is the coequalizer of $s$ and $t$, it suffices to show that for any $r \in \poly$ and map $f \colon q \to r$ satisfying $s \then f = t \then f$, there exists a unique map $h \colon q' \to r$ for which $f = g \then h$, so that the following diagram commutes.
    \begin{equation*} %\label{eqn.eq_univ_prop}
    \begin{tikzcd}
        p \ar[r, "s", shift left] \ar[r, "t"', shift right] & q \ar[r, "g"] \ar[dr, "f"'] & q' \ar[d, "h", dashed] \\
        & & r
    \end{tikzcd}
    \end{equation*}
    In order for $f = g \then h$ to hold, we must have $f_\1 = g_\1 \then h_\1$ on positions.
    But we have that $s_\1 \then f_\1 = t_\1 \then f_\1$, so by the universal property of $q'(\1)$ and the map $g_\1$ as the coequalizer of $s_\1$ and $t_\1$ in $\smset$, there exists a unique $h_\1$ for which $f_\1 = g_\1 \then h_\1$.
    Hence $h$ is uniquely characterized on positions.
    In particular, it must send each connected component $c \in q'(\1)$ to the element in $r(\1)$ to which $f_\1$ sends every vertex $v \in V_c = g_\1\inv(c)$ that lies in the connected component $c$.
    
    Then for $f = g \then h$ to hold on directions, we must have that $f^\sharp_v = h^\sharp_{g_\1(v)} \then g^\sharp_v$ for each $v \in q(\1)$.
    Put another way, given $c \in q'(\1)$, we must have that $f^\sharp_v = h^\sharp_c \then g^\sharp_v$ for every $v \in V_c$.
    But $s \then f = t \then f$ implies that for each $e \in E_c = s_\1\inv(g_\1\inv(c)) = t_\1\inv(g_\1\inv(c)) \ss p(\1)$, the following diagram of on-directions functions commutes:
    \[
    \begin{tikzcd}[sep=small]
        & q[s_\1(e)] \ar[dl, "s^\sharp_e"'] \\
        p[e] & & r[f_\1(v)] \ar[ul, "f^\sharp_{s_\1(e)}"'] \ar[dl, "f^\sharp_{t_\1(e)}"] \\
        & q[t_\1(e)] \ar[ul, "t^\sharp_e"]
    \end{tikzcd}
    \]
    It follows that $r[f_\1(v)]$ together with the maps $(f^\sharp_v)_{v \in V_c}$ form a cone over the functor $F$.
    So by the universal property of the limit $q'[c]$ of $F$ with projection maps $(g^\sharp_v)_{v \in V_c}$, there exists a unique $h^\sharp_c \colon r[f_\1(v)] \to q'[c]$ for which $f^\sharp_v = h^\sharp_c \then g^\sharp_v$ for every $v \in V_c$.
    Hence $h$ is also uniquely characterized on directions, so it is unique overall.
    Moreover, we have shown that we can define $h$ on positions so that $f_\1 = g_\1 \then h_\1$, and that we can define $h$ on directions such that $f^\sharp_v = h^\sharp_c \then g^\sharp_v$ for all $c \in q'(\1)$ and $v \in V_c$.
    It follows that there exists $h$ for which $f = g \then h$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{example}
Given a diagram in $\poly$, one could either take its (co)limit as a diagram of \emph{polynomial} functors (i.e.\ its (co)limit in $\poly)$ or its (co)limit simply as a diagram of functors (i.e.\ its (co)limit in $[\smset,\smset]$).
We saw in \cref{exc.refl_limits} that in the case of limits, these yield the same result.
So, too, in the case of coproducts, per \cref{prop.poly_coprods}.

But in the case of general colimits, there are diagrams that yield different results: by the co-Yoneda lemma, \emph{every} functor $\smset \to \smset$---even those that are not polynomials---can be written as the colimit of representable functors in $[\smset,\smset]$, yet the colimit of the same representables in $\poly$ can only be another polynomial.

For a concrete example, consider the two distinct projections $\yon^\2\to\yon$, which form the diagram
\begin{equation} \label{eqn.2_projs}
    \yon^\2\tto\yon.
\end{equation}
According to \cref{thm.poly_colimits}, the colimit of \eqref{eqn.2_projs} in $\poly$ has the coequalizer of $\1 \tto \1$, namely $\1$, as its position-set, and the limit of the diagram $\1 \tto \2$ consisting of the two inclusions as its sole direction-set.
But this latter limit is just $\0$, so in fact the colimit of \eqref{eqn.2_projs} in $\poly$ is the constant functor $\1\yon^\0\iso\1$.

But as functors, by \cref{prop.presheaf_lim_ptwise}, the colimit of \eqref{eqn.2_projs} can be computed pointwise: it is the (nonconstant!) functor
\[
  X\mapsto
  \begin{cases}
  	\0&\tn{ if }X=\0\\
  	\1&\tn{ if }X\neq\0
  \end{cases}
\]
\end{example}

\begin{exercise}
By \cref{prop.adjoint_quadruple}, for any polynomial $p$, there are canonical maps
\[
	\epsilon \colon p(\1)\yon\to p
	\qqand
	\eta \colon p\to \yon^{\Gamma(p)}.
\]
\begin{enumerate}
	\item Characterize the behavior of the canonical map $\epsilon \colon p(\1)\yon\to p$.
	\item Characterize the behavior of the canonical map $\eta \colon p\to \yon^{\Gamma(p)}$.
	\item Show that the following is a pushout in $\poly$:
    \begin{equation} \label{eqn.pushout_adjoint}
    \begin{tikzcd}
    	p(\1)\yon\ar[r, "!"]\ar[d, "\epsilon"']&
    	\yon\ar[d, "!"]\\
    	p\ar[r, "\eta"']&
    	\yon^{\Gamma(p)}\ar[ul, phantom, very near start, "\ulcorner"]
    \end{tikzcd}
    \end{equation}
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item We characterize the map $\epsilon \colon p(\1)\yon \to p$ as follows.
    On positions, it is the identity on $p(\1)$.
    Then for each $i \in p(\1)$, on directions, it is the unique map $p[i] \to \1$.
    
    \item We characterize the map $\eta \colon p \to \yon^{\Gamma(p)}$ as follows.
    On positions, it is the unique map $p(\1) \to \1$.
    Then for each $i \in p(\1)$, on directions, it is the canonical projection $\Gamma(p) \iso \prod_{i' \in p(\1)} p[i'] \to p[i]$.
    
    \item Showing that \eqref{eqn.pushout_adjoint} is a pushout square is equivalent to showing that, in the diagram
    \begin{equation} \label{eqn.coeq_adjoint}
    \begin{tikzcd}[sep=large]
        & \yon \ar[d, "\iota"] \ar[dr, "!"] \\
        p(\1)\yon \ar[ur, "!"] \ar[dr, "\epsilon"'] \ar[r, "s", shift left] \ar[r, "t"', shift right] & \yon + p \ar[r, "g"] & \yon^{\Gamma(p)} \\
        & p \ar[u, "\iota'"'] \ar[ur, "\eta"']
    \end{tikzcd}
    \end{equation}
    in which $\iota, \iota'$ are the canonical inclusions and the four triangles commute, $\yon^{\Gamma(p)}$ equipped with the lens $g$ is the coequalizer of $s$ and $t$.
    To do so, we apply \cref{thm.poly_colimits} to compute the coequalizer $q'$ of $s$ and $t$.
    The position-set of $q'$ is the coequalizer of $s_\1 = (! \then \iota)_\1$, which sends every $i \in p(\1)$ to the position of $\yon + p$ corresponding to the summand $\yon$, and $t_\1 = (\epsilon \then \iota')_\1$, which sends each $i \in p(\1)$ to the corresponding position in the summand $p$ of $\yon + p$.
    It follows that the coequalizer of $s_\1$ and $t_\1$ is $\1$, so $q'(\1) \iso \1$.
    
    Then the direction-set of $q'$ at its sole position is the limit of the functor $F$ whose image consists of lenses of the form $\1 \to \1$ or $p[i] \to \1$ for every $i \in p(\1)$.
    It follows that the limit of $F$ is just a product, namely $\prod_{i \in p(\1)} p[i] \iso \Gamma(p)$.
    Hence $q' \iso \yon^{\Gamma(p)}$, as desired.
    
    It remains to check that the upper right and lower right triangles in \eqref{eqn.coeq_adjoint} commute.
    The upper right triangle must commute by the uniqueness of morphisms $\yon \to \yon^{\Gamma(p)}$; and the lower right triangle must commute on positions.
    Moreover, the on-directions function of the coequalizer morphism $g$ at each position $i \in p(\1) \ss (\yon+p)(\1)$ must be the canonical projection $\Gamma(p) \to p[i]$, which matches the behavior of the corresponding on-directions function of $\eta$; hence the lower right triangle also commutes on directions.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.tensor_as_pushout}
For polynomials $p,q$, the following is a pushout:
\[
\begin{tikzcd}
	p(\1)\yon\otimes q(\1)\yon\ar[r]\ar[d]&
	p(\1)\yon\otimes q\ar[d]\\
	p\otimes q(\1)\yon\ar[r]&
	p\otimes q\ar[ul, phantom,very near start, "\ulcorner"]
\end{tikzcd}
\]
\end{proposition}
\begin{proof}
All the maps shown are identities on positions, so the displayed diagram is the coproduct over all $(i,j)\in p(\1)\times q(\1)$ of the diagram shown left
\[
\begin{tikzcd}
	\yon\ar[r]\ar[d]&
	\yon^{q[j]}\ar[d]\\
	\yon^{p[i]}\ar[r]&
	\yon^{p[i]\times q[j]}\ar[ul, phantom, very near start, "\ulcorner"]
\end{tikzcd}
\begin{tikzcd}
	\1\ar[dr, phantom, very near end, "\ulcorner"]&
	q[j]\ar[l]\\
	p[i]\ar[u]&
	p[i]\times q[j]\ar[l]\ar[u]
\end{tikzcd}
\]
where we used $(p\otimes q)[(i,j)]\cong p[i]\times q[j]$. This is the image under the Yoneda embedding of the diagram of sets shown right, which is clearly a pullback. The result follows by \cref{prop.yoneda_left_adjoint}.
\end{proof}

This means that to give a map $\varphi\colon p\otimes q\to r$, it suffices to give two maps $\varphi_p\colon p\otimes q(\1)\yon\to r$ and $\varphi_q\colon p(\1)\yon\otimes q\to r$ that agree on positions. The map $\varphi_p$ says how information about $q$'s position is transferred to $p$, and the map $\varphi_q$ says how information about $p$'s position is transferred to $q$.

\begin{corollary}\label{cor.tensor_as_pushout}
Suppose we have polynomials $p_1,\ldots,p_n\in\poly$. Then $p_1\otimes\cdots\otimes p_n$ is isomorphic to the wide pushout
\[
  \colim
  \left(
  \begin{tikzcd}[column sep=-10pt]
  	&
		p_1(\1)\yon\otimes\cdots\otimes  p_n(\1)\yon\ar[dl]\ar[dr]\\
		p_1\otimes p_2(\1)\yon\otimes\cdots\otimes p_n(\1)\yon&
		\cdots&
		p_1(\1)\yon\otimes\cdots\otimes p_{n-1}(\1)\yon \otimes p_n
  \end{tikzcd}
  \right)
\]
\end{corollary}
\begin{proof}
We proceed by induction on $n\in\nn$.
When $n=0$, the wide pushout has no legs and the empty parallel product is $\yon$, so the result holds.
If the result holds for $n$, then it holds for $n+1$ by \cref{prop.tensor_as_pushout}.
\end{proof}

% We can use \cref{cor.tensor_as_pushout} to characterize interaction patterns. Given polynomials $p_1,\ldots, p_n,$ and $q$, and a map
% \[\varphi\colon p_1\otimes\cdots\otimes p_n\to q\]
% we may want to know which $p_i$'s can ``hear'' which $p_j$'s. To make this precise, we will associate to $\varphi$ a simple directed graph that describes the information flow. Given a set $V$, define
% \[
% \Cat{SimpleGraph}(V)\coloneqq\{A\ss V\times V\mid \forall(v\in V). (v,v)\not\in A\}.
% \]
% In fact, we will start with a more general result that explains how maps such as $\varphi$ can be \emph{mode-dependent}. But before we do, let's explain the idea in an example.

% \begin{example}[Introducing random process]\label{ex.random_process}
% What is it about flipping a coin or rolling a die that is random? This is a different question than whether the coin or die is fair or even consistent. What's important is that we don't have any influence over the result of the outcome.

% Suppose that we have a system $\varphi\colon p\otimes c\to r$, where we think of $p$ as a player and $c$ as a coin. By \cref{prop.tensor_as_pushout} we know that $\varphi$ can be identified with a commuting diagram
% \[
% \begin{tikzcd}
% 	p(\1)\yon\otimes c(\1)\yon\ar[r]\ar[d]&
% 	p(\1)\yon\otimes c\ar[d]\\
% 	p\otimes c(\1)\yon\ar[r]&
% 	r
% \end{tikzcd}
% \]
% Assuming for simplicity that the player and coin form a closed system, i.e.\ that$r=\yon$, the map $\varphi$ can be identified with two maps: $\varphi_c\colon p(\1)\yon\otimes c\to\yon$ and $\varphi_p\colon p\otimes c(\1)\yon\to\yon$. To say that $p$'s position does not influence $c$ is to say that the former map factors through the projection $\pi\colon p(\1)\yon\otimes c\to c$
% \[
% 	p(\1)\yon\otimes c\To{\pi} c\to \yon.
% \]
% In general, we will be interested in the interaction pattern between a number of systems, in terms of who influences who.
% \end{example}

% \begin{example}
% Carrying on from \cref{ex.random_process}, suppose we have polynomials $p_1,\ldots,p_n,r$ and a map
% \[\varphi\colon p_1\otimes\cdots\otimes p_n\to r.\]
% By \cref{cor.tensor_as_pushout}, $\varphi$ can be identified with a tuple of maps 
% \[\varphi_i\colon p_i\otimes\bigotimes_{j\neq i}p_j(\1)\yon\too r\]
% that agree on positions $\varphi(\1)\colon p(\1)\otimes\cdots\otimes p_n(\1)\to r(\1)$.

% Now for each $1\leq i\leq n$, we can ask for the smallest subset $A_i\ss\{j\mid 1\leq j\leq n, j\neq i\}$ such that $\varphi_i$ factors through the projection
% \[
% 	p_i\otimes\bigotimes_{j\neq i}p_j(\1)\yon\To{\pi_A}
% 	p_i\otimes\bigotimes_{j\in A_i}p_j(\1)\yon\to
% 	r.
% \]
% The result is a simple directed graph: its vertices are the numbers $V\coloneqq\{i\mid 1\leq i\leq n\}$ and for each vertex we have a subset $A_i\ss V\setminus \{i\}$, which we consider as the arrows with target $i$. The set of all arrows in this graph is $A\coloneqq\sum_{i}A_i$.

% The simple directed graph associated to the player-and-coin model in \cref{ex.random_process} would be
% \[
% \LMO{c}\to\LMO{p}
% \]
% indicating that the coin's position can be noticed by the player but the player's position cannot be noticed by the coin. In general, we may have something like the following:
% \[
% \begin{tikzcd}
% 	&
% 	\LMO{p_1}\ar[dl]\ar[dr]\ar[r, shift left]&
% 	\LMO{p_2}\ar[l, shift left]\ar[dr]\\
% 	\LMO{p_3}&&
% 	\LMO{p_4}&
% 	\LMO{p_5}\ar[l]
% \end{tikzcd}
% \]
% which says that $p_1$ is heard by $p_2,p_3,p_4$ but not by $p_5$, etc.
% \end{example}

% For any $p\in\poly$, let $p/\poly$ denote the coslice category, i.e.\ the category whose objects are maps $p\to q$ emanating from $p$, and whose lenses are commutative triangles.

% \begin{proposition}[Interaction graphs]
% For any polynomials $p_1,\ldots,p_n\in\poly$ there is an adjunction
% \[
% 	\adj
% 		{\Cat{SimpleGraph}(\ord{n})\op}
% 		{}{}
% 		{p_1\otimes\cdots\otimes p_n/\poly}
% \]
% Moreover, the left adjoint is fully faithful if and only if $n\leq 1$ or $|p_i(\1)|\geq 2$ for each $1\leq i\leq n$, i.e.\ if and only if each polynomial has at least two positions.
% \end{proposition}
% \begin{proof}
% **
% \end{proof}

%-------- Section --------%
\section{Vertical-cartesian factorization}

Aside from epi-mono factorization, there is another factorization system on $\poly$ that will show up frequently.

\begin{definition}[Vertical and cartesian lenses] \label{def.vert_cart}
Let $f\colon p\to q$ be a lens.
It is called \emph{vertical} if $f_\1\colon p(\1)\to q(\1)$ is an isomorphism.
It is called \emph{cartesian} if, for each $i\in p(\1)$, the function $f^\sharp_i\colon q[f(i)]\to p[i]$ is an isomorphism.
\end{definition}

\begin{proposition}\label{prop.vert_cart_factorization}
Vertical and cartesian lenses form a factorization system of $\poly$.
\end{proposition}
\begin{proof}
It is easy to check that isomorphisms are both vertical and cartesian, and that vertical and cartesian lenses are each closed under composition.
It remains to show that every lens in $\poly$ can be uniquely (up to unique isomorphism) factored as a vertical lens composed with a cartesian lens.

Recall from \eqref{eqn.colax_poly_map} that a lens in $\poly$ can be written as to the left; we can thus rewrite it as to the right:
\[
\begin{tikzcd}[column sep=small]
	p(\1)\ar[dr, bend right, "{p[-]}"']\ar[rr, "f_\1"]&~&
	q(\1)\ar[dl, bend left, "{q[-]}"]\\&
	\smset\ar[u, phantom, near end, "\overset{f^\sharp}{\Leftarrow}"]
\end{tikzcd}
\hspace{1in}
\begin{tikzcd}
	p(\1)\ar[dr, bend right, "{p[-]}"']\ar[r, equal, ""' name=equal]&
	p(\1)\ar[d, "{q[f_\1(-)]}"]\ar[r, "f_\1"]&
	q(\1)\ar[dl, bend left, "{q[-]}"]\\&
	|[alias=set]|\smset\ar[from=equal, to=set, pos=.3, phantom, "\overset{f^\sharp}{\Leftarrow}"]
\end{tikzcd}
\]
We can see that the intermediary object $\sum_{i\in p(\1)} \yon^{q[f_\1(i)]}$ is unique up to unique isomorphism.
\end{proof}

\begin{proposition}
Vertical lenses satisfy 2-out-of-3: given $p\To{f}q\To{g}r$ with $h = f \then g$, if any two of $f,g,h$ are vertical, then so is the third.

If $g$ is cartesian, then $h$ is cartesian if and only if $f$ is cartesian.
\end{proposition}
\begin{proof}
Given $h = f \then g$, we have that $h_\1 = f_\1 \then g_\1$.
Since isomorphisms satisfy 2-out-of-3, it follows that vertical lenses satisfy 2-out-of-3 as well.

Now assume $g$ is cartesian.
On directions, $h = f \then g$ implies that for every $i \in p(\1)$, we have $h^\sharp_i = g^\sharp_{f_\1(i)} \then f^\sharp_i$.
Since $g^\sharp_{f_\1(i)}$ is an isomorphism, it follows that every $h^\sharp_i$ is an isomorphism if and only if every $f^\sharp_i$ is an isomorphism, so $h$ is cartesian if and only if $f$ is cartesian.
\end{proof}

\begin{exercise}
Give an example of polynomials $p,q,r$ and maps $p\To{f}q\To{g}r$ such that $f$ and $f \then g$ are cartesian but $g$ is not.
\begin{solution}
Consider the maps $\yon \To{f} \yon^\2 + \yon \To{g} \yon$ where $f$ is the canonical inclusion and $g$ is uniquely determined on positions and picks out $1 \in \2$ and $1 \in \1$ on directions.
Then the only on-directions function of $f$ is a function $\1 \to \1$, an isomorphism, so $f$ is cartesian.
Meanwhile, one of the on-directions functions of $g$ is a function $\1 \to \2$, which is not an isomorphism, so $g$ is not cartesian.
Finally, $f \then g$ can only be the unique lens $\yon \to \yon$, namely the identity, which is cartesian.
\end{solution}
\end{exercise}

Here is an alternative characterization of a cartesian lens in $\poly$.
Recall from \cref{exc.deriv_directions} that for any polynomial $p$, there is a corresponding function $\pi_p\colon\dot{p}(\1)\to p(\1)$, i.e.\ the set of all directions mapping to the set of positions.
A lens $(f_\1,f^\sharp)\colon p\to q$ can then be described as a function $f_\1\colon p(\1)\to q(\1)$ along with a function $f^\sharp$ that makes the following diagram in $\smset$ commute:
\begin{equation}\label{eqn.poly_map_usu}
\begin{tikzcd}
	\dot{p}(\1)\ar[d, "\pi_p"']&
	\bullet\ar[l, "f^\sharp"']\ar[r]\ar[d]&
	\dot{q}(\1)\ar[d, "\pi_q"]\\
	p(\1)\ar[r, equal]&
	p(\1)\ar[r, "f_\1"']&
	q(\1)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\end{equation}
Here, the pullback denoted by the dot $\bullet$ is the set of pairs comprised of a $p$-position $i$ and a $q[f_\1(i)]$-direction $e$.
The function $f^\sharp$ sends each such pair to a direction $f^\sharp_i(e)$ of $p$, and the commutativity of the left square implies that $f^\sharp_i(e)$ is specifically a $p[i]$-direction.
So $f^\sharp_i$ is indeed our familiar on-directions function $q[f_\1(i)]\to p[i]$, and $f^\sharp$ is just the sum of all these on-directions functions over $i\in p(\1)$.

\begin{exercise} \label{exc.cart_pullbacks}
Show that a lens $f\colon p\to q$ in $\poly$ is cartesian if and only if the square on the left hand side of \eqref{eqn.poly_map_usu} is also a pullback:
\[
\begin{tikzcd}
	\dot{p}(\1)\ar[d, "\pi_p"']&
	\bullet\ar[l, "f^\sharp"']\ar[r]\ar[d]&
	\dot{q}(\1)\ar[d, "\pi_q"]\\
	p(\1)\ar[r, equal]\ar[ur, phantom, very near end, "\llcorner"]&
	p(\1)\ar[r, "f_\1"']&
	q(\1)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
\begin{solution}
We wish to show that a lens $f\colon p\to q$ in $\poly$ is cartesian if and only if the square on the left hand side of \eqref{eqn.poly_map_usu} is a pullback.
We already know that that square commutes, so it is a pullback if and only if $f^\sharp$ is an isomorphism.
The right pullback square tells us that the $\bullet$ is $\sum_{i\in p(\1)}q[f_\1(i)]$.
So $f^\sharp_i\colon q[f_\1(i)]\to p[i]$ is an isomorphism for every $i\in p(\1)$ if and only if their sum $f^\sharp\colon\sum_{i\in p(\1)}q[f_\1(i)]\to\sum_{i\in p(\1)}p[i]\iso\dot{p}(\1)$ is an isomorphism as well.
Hence $f$ is cartesian if and only if $f^\sharp$ is an isomorphism, as desired.
\end{solution}
\end{exercise}

\begin{exercise}
Is the pushout of a cartesian map always cartesian?
\begin{solution}
The pushout of a cartesian map is \emph{not} necessarily cartesian.
Take the pushout square \eqref{eqn.pushout_adjoint}.
The map $!\colon p(\1)\yon\to\yon$ has $\1\to\1$ as every on-directions function, so it is cartesian, but its pushout $\eta\colon p\to\yon^{\Gamma(p)}$ is not going to be cartesian as long as there is some $i\in p(\1)$ for which $\Gamma(p)\not\iso p[i]$.
For instance, when $p\coloneqq\yon+\1$, we have that $\Gamma(p)\iso\0\not\iso\1\iso p[1]$, so $\eta$ is not cartesian.
\end{solution}
\end{exercise}

Why do we use the word \emph{cartesian} to describe cartesian morphisms? It turns out that, as natural transformations, cartesian morphisms are precisely what are known as cartesian natural transformations.

\begin{definition}[Cartesian natural transformation] \label{def.cart_nat_trans}
A \emph{cartesian natural transformation} is a natural transformation whose naturality squares are all pullbacks.
That is, given categories $\cat{C},\cat{D}$, functors $F,G$, and natural transformation $\alpha$, we say that $\alpha$ is \emph{cartesian} if for all morphisms $h\colon c\to c'$ in $\cat{C}$,
\[
\begin{tikzcd}
    Fc \ar[d, "Fh"'] \ar[r, "\alpha_c"] & Gc \ar[d, "Gh"] \\
    Fd \ar[r, "\alpha_d"'] & Gd \ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
is a pullback.
\end{definition}

\begin{proposition}\label{prop.cart_as_nt}
Let $f\colon p\to q$ be a morphism in $\poly$. The following are equivalent:
	\begin{enumerate}
		\item viewed as a lens, $f$ is cartesian in the sense of \cref{def.vert_cart}: for each $i\in p(\1)$, the on-directions function $f^\sharp_i$ is a bijection;
		\item the square on the left hand side of \eqref{eqn.poly_map_usu} is also a pullback:
\[
\begin{tikzcd}
	\dot{p}(\1)\ar[d, "\pi_p"']&
	\bullet\ar[l, "f^\sharp"']\ar[r]\ar[d]&
	\dot{q}(\1)\ar[d, "\pi_q"]\\
	p(\1)\ar[r, equal]\ar[ur, phantom, very near end, "\llcorner"]&
	p(\1)\ar[r, "f_\1"']&
	q(\1)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]		
		\item viewed as a natural transformation, $f$ is cartesian in the sense of \cref{def.cart_nat_trans}: for any sets $A,B$ and function $h\colon A\to B$, the naturality square
\begin{equation} \label{eqn.cart_nt_pullback}
\begin{tikzcd}
	p(A)\ar[r, "f_A"]\ar[d, "p(h)"']&
	q(A)\ar[d, "q(h)"]\\
	p(B)\ar[r, "f_B"']&
	q(B)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\end{equation}
is a pullback.
  \end{enumerate}
\end{proposition}
\begin{proof}
We already showed that the first two are equivalent in \cref{exc.cart_pullbacks}, and we will complete this proof in \cref{exc.cart_as_nt}.
\end{proof}

\begin{exercise} \label{exc.cart_as_nt}
In this exercise, you will complete the proof of \cref{prop.cart_as_nt}.

First, we will show that $1\Rightarrow3$.
In the following, let $f\colon p\to q$ be a cartesian lens in $\poly$ and $h\colon A\to B$ be a function.
\begin{enumerate}
    \item Using \cref{prop.morph_arena_to_func} to translate $f$ from a lens in $\poly$ to a natural transformation and \cref{prop.poly_on_functions} to interpret $q(h)$, characterize the pullback of $p(B)\To{f_B}q(B)\From{q(h)}q(A)$ in $\smset$.
    \item Show that this pullback coincides with the naturality square \eqref{eqn.cart_nt_pullback}, hence proving $1\Rightarrow3$.
\end{enumerate}
Next, we show that $3\Rightarrow1$.
In the following, let $f\colon p\to q$ be a lens in $\poly$ that is cartesian when viewed as a natural transformation, so that \eqref{eqn.cart_nt_pullback} is a pullback for any function $h\colon A\to B$.
Also fix $i\in p(\1)$.
\begin{enumerate}[resume]
    \item Show that the diagram 
    \begin{equation} \label{eqn.cart_nt_pullback_cone}
        \begin{tikzcd}[column sep=50pt]
        	\1 \ar[r, "{(f_\1(i),\,\id_{q[f_\1(i)]})}"]\ar[d, "{(i,\,\id_{p[i]})}"']&
        	q(q[f_\1(i)])\ar[d, "q(f^\sharp_i)"]\\
        	p(p[i])\ar[r, "f_{p[i]}"']&
        	q(p[i])\ar[ul, phantom, very near end, "\lrcorner"]
        \end{tikzcd}
    \end{equation}
    commutes.
    Hint: Use \cref{prop.poly_on_functions}, \cref{prop.morph_arena_to_func}, and/or \cref{cor.morph_func_to_arena}.
    \item Apply the universal property of the pullback \eqref{eqn.cart_nt_pullback} to the diagram \eqref{eqn.cart_nt_pullback_cone} above to exhibit an element of $p(q[f_\1(i)])$.
    Conclude from the existence of this element that $f^\sharp_i$ is an isomorphism, hence proving $3\Rightarrow1$.\qedhere
\end{enumerate}
\begin{solution}
First, we will show that $1\Rightarrow3$ in \cref{prop.cart_as_nt}.
Here $f\colon p\to q$ is a cartesian lens in $\poly$ and $h\colon A\to B$ is a function.
\begin{enumerate}
    \item An element of $p(B)$ is a pair comprised of a $p$-position $i$ and a function $k\colon p[i]\to B$, and \cref{prop.morph_arena_to_func} tells us that $f_B\colon p(B)\to q(B)$ sends $(i,k)\mapsto(f_\1(i),f^\sharp_i\then k)$.
    Meanwhile, an element of $q(A)$ is a pair comprised of a $q$-position $j$ and a function $\ell\colon q[j]\to A$, and \cref{prop.poly_on_functions} tells us that $q(h)$ sends $(j,\ell)\mapsto(j,\ell\then h)$.
    So $((i,k),(j,\ell))$ is in the pullback of $p(B)\To{f_B}q(B)\From{q(h)}q(A)$ if and only if $f_\1(i)=j$ and $f^\sharp_i\then k=\ell\then h$.
    
    As $f$ is cartesian, $f^\sharp_i$ is an isomorphism, so we can rewrite the latter equation as $k=g_i\then\ell\then h$, where $g_i$ is the inverse of $f^\sharp_i$.
    In fact, if we let $\ell'\coloneqq g_i\then\ell$, we observe that the values of $j,k,$ and $\ell$ are all already determined by the values of $i$ and $\ell'$: we have that $j=f_\1(i)$, that $k=\ell'\then h$, and that $\ell=f^\sharp_i\then\ell'$
    It follows that the pullback is equivalently the set of pairs $(i,\ell')$ comprised of a $p$-position $i$ and a function $\ell'\colon p[i]\to A$ (with no other restrictions on $i$ and $\ell'$).
    The projection from the pullback to $p(B)$ sends $(i,\ell')\mapsto(i,\ell'\then h)$, and the projection from the pullback to $q(A)$ sends $(i,\ell')\mapsto(f_\1(i),f^\sharp_i\then\ell')$.
    \item The pullback described above---the set of pairs $(i,\ell')$ comprised of a $p$-position $i$ and a function $\ell'\colon p[i]\to A$---is exactly the set $p(A)$.
    Moreover, the projection to $p(B)$ sending $(i,\ell')\mapsto(i,\ell'\then h)$ is $p(h)$, and the projection to $q(A)$ sending $(i,\ell')\mapsto(f_\1(i),f^\sharp_i\then\ell')$ is $f_A$ by \cref{prop.morph_arena_to_func}.
    So \eqref{eqn.cart_nt_pullback} is a pullback, as desired.
\end{enumerate}
Next, we will show that $3\Rightarrow1$ in \cref{prop.cart_as_nt}, with $f\colon p\to q$ as a lens in $\poly$ that is a cartesian natural transformation and $i\in p(\1)$.
\begin{enumerate}[resume]
    \item By \cref{cor.morph_func_to_arena}, $f_{p[i]}$ sends $(i,\id_{p[i]})\mapsto(f_\1(i),f^\sharp_i)$, and by \cref{prop.poly_on_functions}, $q(f^\sharp_i)$ sends $(f_\1(i),\id_{q[f_\1(i)]})\mapsto(f_\1(i),f^\sharp_i)$ as well, so \eqref{eqn.cart_nt_pullback_cone} commutes.
    
    \item Taking $A\coloneqq q[f_\1(i)], B\coloneqq p[i],$ and $h\coloneqq f^\sharp_i$ in \eqref{eqn.cart_nt_pullback} and applying its universal property to \eqref{eqn.cart_nt_pullback_cone} induces an element $(i',g)$ of $p(q[f_\1(i)])$, with $i'\in p(\1)$ and $g\colon p[i']\to q[f_\1(i)]$, such that $p(f^\sharp_i)$ sends $(i',g)\mapsto(i,\id_{p[i]})$ and $f_{q[f_\1(i)]}$ sends $(i',g)\mapsto(f_\1(i),\id_{q[f_\1(i)]})$.
    It follows from the behavior of $p(f^\sharp_i)$ (by \cref{prop.poly_on_functions}) that $i'=i$ and $g\then f^\sharp_i=\id_{p[i]}$, and it follows from the behavior of $f_{q[f_\1(i)]}$ (by \cref{prop.morph_arena_to_func}) that $f^\sharp_i\then g=\id_{q[f_\1(i)}$.
    So $g$ is the inverse of $f^\sharp_i$, proving that $f^\sharp_i$ is an isomorphism, as desired.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{proposition}\label{prop.monoidal_pres_vert_cart}
The monoidal structures $+$, $\times$, and $\otimes$ preserve both vertical and cartesian morphisms.
\end{proposition}
\begin{proof}
Suppose that $f\colon p\to p'$ and $g\colon q\to q'$ are vertical, so that the on-positions functions $f_\1$ and $g_\1$ are isomorphisms.

We can obtain the on-positions function of a lens by passing it through the functor $\poly\To{p(\1)}\smset$ from \cref{thm.adjoint_quadruple}.
As this functor is both a left adjoint and a right adjoint, it preserves both sums and products, so $(f+g)_\1 = f_\1+g_\1$ and $(f\times g)_1 = f_1\times g_1$.
Hence $f+g$ and $f\times g$ are both vertical.
On-positions, the behavior of $\otimes$ is identical to the behavior of $\times$, so $f\otimes g$ must be vertical as well.

Now suppose that $f\colon p\to p'$ and $g\colon q\to q'$ are cartesian. 

A position of $p+q$ is a position $i\in p(\1)$ or a position $j\in q(\1)$, and the map $(f+g)^\sharp$ at that position is either $f^\sharp_i$ or $g^\sharp_j$; either way it is an isomorphism, so $f+g$ is cartesian.

A position of $p\times q$ (resp.\ of $p\otimes q$) is a pair $(i,j)\in p(\1)\times q(\1)$. The lens $(f\times g)^\sharp_{(i,j)}$ (resp.\ $(f\otimes g)^\sharp_{(i,j)}$) is $f^\sharp_i+g^\sharp_j$ (resp.\ $f^\sharp_i\times g^\sharp_j$) which is again an isomorphism if $f^\sharp_i$ and $g^\sharp_j$ are. Hence $f\times g$ (resp.\ $f\otimes g$) is cartesian, completing the proof.
\end{proof}

\begin{proposition}\label{prop.pullback_vert_cart}
Pullbacks preserve vertical (resp.\ cartesian) lenses.
In other words, if $f\colon p\to q$ is a lens and $g\colon q'\to q$ a vertical (resp.\ cartesian) lens, then the pullback $g'$ of $g$ along $p$
\[
\begin{tikzcd}
	p\times_qq'\ar[r]\ar[d, "g'"']&
	q'\ar[d, "g"]\\
	p\ar[r, "f"']&
	q\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
is vertical (resp.\ cartesian).
\end{proposition}
\begin{proof}
This follows from \cref{ex.pullbacks_in_poly}, since the pullback (resp.\ pushout) of an isomorphism is an isomorphism.
\end{proof}

%-------- Section --------%
\section{Monoidal $*$-bifibration over $\smset$}

We will see that the functor $p\mapsto p(\1)$ has special properties making it what
\cite{shulman2008framed} refers to as a \emph{monoidal $*$-bifibration}. This means that $\smset$ acts as a sort of remote controller on the category $\poly$, grabbing every polynomial by its positions and pushing or pulling it this way and that. 

For example, suppose one has a set $A$ and a function $f\colon A\to p(\1)$, which we can also think of as a cartesian lens between constant polynomials.
Then we get a new polynomial $f^*p$ with positions $A$, as follows.
It is given by a pullback
\begin{equation}\label{eqn.f^*_defined}
\begin{tikzcd}
	f^*p\ar[r, "\fun{cart}"]\ar[d]&
	p\ar[d, "\eta_p"]\\
	A\ar[r, "f"']&
	p(\1)\ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\end{equation}
Here $\eta_p$ is the unit of the adjunction $\adjr{\smset}{A}{p(\1)}{\poly}$; it is a vertical lens.
We could evaluate this pullback using \cref{ex.pullbacks_in_poly}.
Alternatively, we can use \cref{prop.pullback_vert_cart} to deduce that the top map $f^*p\to p$ (which we presciently labeled $\fun{cart}$) is cartesian like $f$ and that the left map $f^*p\to A$ is vertical like $\eta_p$. Furthermore, $\fun{cart}_1 = f$.
Hence
\[
    f^*p \iso \sum_{a \in A} \yon^{p[f(a)]}.
\]
We'll see this as part of a bigger picture in \cref{prop.basechange,thm.triple_adjoint_basechange}, but first we need the following definitions and a result about cartesian lenses.

\begin{definition}[Slice category] \label{def.slice}
Given an object $c$ in a category $\cat{C}$, the \emph{slice category} of $\cat{C}$ over $c$, denoted $\cat{C}/c$, is the category whose objects are morphisms in $\cat{C}$ with codomain $c$ and whose morphisms are commutative triangles in $\cat{C}$.
\end{definition}

\begin{definition}[Exponentiable morphism]
Given a category $\cat{C}$ with objects $c, d$ and morphism $f \colon c \to d$ such that all pullbacks along $f$ exist in $\cat{C}$, we say that $f$ is \emph{exponentiable} if the functor $f^* \colon \cat{C}/d \to \cat{C}/c$ given by pulling back along $f$ is a left adjoint.
\end{definition}

\begin{theorem}\label{thm.cart_exponentiable}
Cartesian lenses in $\poly$ are exponentiable.
That is, if $f\colon p\to q$ is cartesian, then the functor $f^*\colon\poly/q\to\poly/p$ given by pulling back along $f$ is a left adjoint:
\[
\begin{tikzcd}[column sep=50pt, background color=theoremcolor]
	\poly/p\ar[r, shift right=7pt, "f_*"']&
	\poly/q\ar[l, shift right=7pt, "f^*"']\ar[l, phantom, "\Leftarrow"]
\end{tikzcd}
\]
\end{theorem}
\begin{proof}
Fix $e\colon p'\to p$ and $g\colon q'\to q$.
\[
\begin{tikzcd}
	p'\ar[d, "e"']&q'\ar[d, "g"]\\
	p\ar[r, "f"']&q
\end{tikzcd}
\]
We need to define a functor $f_*\colon\poly/p\to\poly/q$ and prove the analogous isomorphism establishing it as right adjoint to $f^*$. We first establish some notation. Given a set $Q$ and sets $(P'_i)_{i\in I}$, each equipped with a map $Q\to P'_i$, let $Q/\sum_{i\in I}P'_i$ denote the coproduct in $Q/\smset$, or equivalently the wide pushout of sets $P'_i$ with apex $Q$. Then we give the following formula for $f_*p'$, which we write in larger font for clarity:
\begin{equation}\label{eqn.cart_exp}
f_*p'\coloneqq
\scalebox{1.3}{$\displaystyle
\sum_{j\in q(\1)}\;\sum_{i'\in\prod\limits_{i\in f_\1\inv(j)}e_\1\inv(i)}\;\yon^{q[j]/\sum_{i\in f_\1\inv(j)}p'[i'(i)]}
$}
\end{equation}
Again, $q[j]/\sum_{i\in f_\1\inv(j)}p'[i'(i)]$ is the coproduct of the $p'[i'(i)]$, taken in $q[j]/\smset$. Since $p[i]\cong q[f(i)]$ for any $i\in p(\1)$ by the cartesian assumption on $f$, we have the following chain of natural isomorphisms
\begin{align*}
	(\poly/p)(f^*q', p')&\cong
	\prod_{i\in p(\1)}\;\prod_{\{j'\in q'(\1)\,\mid\,g_\1(j')=f_\1(i)\}}\;\sum_{\{i'\in p'(\1)\,\mid\,e_\1(i')=i\}}\;(p[i]/\smset)(p'[i'],p[i]+_{q[f(i)]}q'[j'])\\&\cong
	\prod_{i\in p(\1)}\;\prod_{\{j'\in q'(\1)\,\mid\,g_\1(j')=f_\1(i)\}}\;\sum_{\{i'\in p'(\1)\,\mid\,e_\1(i')=i\}}\;(q[f(i)]/\smset)(p'[i'],q'[j'])\\&\cong
	\prod_{j\in q(\1)}\;\prod_{\{j'\in q'(\1)\,\mid\, g_\1(j')=j\}}\;\prod_{\{i\in p(\1)\,\mid\,f_\1(i)=j\}}\;\sum_{\{i'\in p'(\1)\,\mid\,e_\1(i')=i\}}\;(q[j]/\smset)(p'[i'],q'[j'])\\&\cong
	\prod_{j\in q(\1)}\;\prod_{\{j'\in q'(\1)\,\mid\, g_\1(j')=j\}}\;\sum_{i'\in\prod_{i\in f_\1\inv(j)}e_\1\inv(i)}\;\prod_{i\in f_\1\inv(j)}\;(q[j]/\smset)(p'[i'(i)],q'[j'])\\&\cong
	\prod_{j\in q(\1)}\;\prod_{\{j'\in q'(\1)\,\mid\, g_\1(j')=j\}}\;\sum_{i'\in\prod_{i\in f_\1\inv(j)}e_\1\inv(i)}\;(q[j]/\smset)\Big(\sum_{i\in f_\1\inv(j)}p'[i'(i)],q'[j']\Big)\\&\cong
	(\poly/q)(q',f_*p')
\end{align*}
\end{proof}

\begin{example}
Let $p\coloneqq\2\yon^\2$, $q\coloneqq\yon^\2+\yon^\0$, and $f\colon p\to q$ the unique cartesian lens between them.
Then for any $e\colon p'\to p$ over $p$, \eqref{eqn.cart_exp} provides the following description for the pushforward $f_*p'$.
%We use the isomorphisms $p(\1)\cong\2$ and $q(\1)\cong\2$ to talk about the positions of $p$ and $q$.

Over the $j=2$ position, $f_\1\inv(2)=\0$ and $q[2]=\0$, so $\prod_{i \in f_\1\inv(2)} e_\1\inv(i)$ is an empty product and $q[2]/\sum_{i\in f_\1\inv(2)} p'[i'(i)]$ is an empty pushout.
Hence the corresponding summand of \eqref{eqn.cart_exp} is simply $\yon^\0\cong\1$.

Over the $j=1$ position, $f_\1\inv(1)=\2$ and $q[1]=p[1]=p[2]=\2$, so $\prod_{i'\in f_\1\inv(1)} e_\1\inv(i) \iso e_\1\inv(1)\times e_\1\inv(2)$.
For $i' \in e_\1\inv(1) \times e_\1\inv(2)$, we have that $q[1]/\sum_{i\in f_\1\inv(2)} p'[i'(i)] \iso X_{i'}$ in the following pushout square:
\[
\begin{tikzcd}
	X_{i'} \ar[from=r] \ar[from=d] &
	p'[i'(2)] \ar[from=d, "e^\sharp_{i'(2)}"'] \\
	p'[i'(1)] \ar[from=r, "e^\sharp_{i'(1)}"] &
	\2 \ar[ul, phantom, very near end, "\lrcorner"]
\end{tikzcd}
\]
Then in sum we have
\[
    f_*p' \iso \left(\sum_{i' \in e_\1\inv(1) \times e_2\inv(2)} \yon^{X_{i'}}\right) + \1.
\]
\end{example}

\begin{exercise}
Prove that the unique map $f\colon\yon\to\1$ is exponentiable.
\begin{solution}
Choose $p\in\poly$ and $q'\in\poly/\yon$. Then there is $q\in\poly$ such that $q'\cong q\yon$, equipped with the projection $q\yon\to\yon$. The pushforward is given by the exponential
\[f_*(q\yon)\coloneqq q^\yon\]
from the cartesian closure; see \eqref{eqn.exponential}. Indeed, we have
\begin{align*}
	\poly/\yon(f^*p,q\yon)&\cong
	\poly/\yon(p\yon,q\yon)\\&\cong
	\poly(p\yon,q)\\&\cong
	\poly(p,q^\yon).
\end{align*}
\end{solution}
\end{exercise}

For any set $A$, let $\poly[A.]$ denote the category whose objects are polynomials $p$ equipped with an isomorphism $A\cong p(\1)$, and whose morphisms are lenses respecting the isomorphisms with $A$.

\begin{proposition}[Base change]\label{prop.basechange}
For any function $f\colon A\to B$, pullback $f^*$ along $f$ induces a functor $\poly[B.]\to\poly[A.]$, which we also denote $f^*$.
\end{proposition}
\begin{proof}
This follows from \eqref{eqn.pullback_poly} with $q\coloneqq A$ and $r\coloneqq B$, since pullback of an iso is an iso.
\end{proof}

\begin{theorem}\label{thm.triple_adjoint_basechange}
For any function $f\colon A\to B$, the pullback functor $f^*$ has both a left and a right adjoint
\begin{equation}\label{eqn.adjoint_triple_monoidal_fib}
\begin{tikzcd}[column sep=large, background color=theoremcolor]
	\poly[A.]\ar[r, shift left=16pt, "f_!"]\ar[r, shift right=16pt, "f_*"']
	\ar[r, phantom, shift left=9pt, "\Rightarrow"]\ar[r, phantom, shift right=9pt, "\Leftarrow"]
&
	\poly[B.]\ar[l, "f^*" description]
\end{tikzcd}
\end{equation}
Moreover $\otimes$ preserves the op-cartesian arrows, making this a monoidal $*$-bifibration in the sense of \cite[Definition 12.1]{shulman2008framed}.
\end{theorem}
\begin{proof}
Let $p$ be a polynomial with $p(\1)\cong A$. Then the formula for $f_!p$ and $f_*p$ are given as follows:
\begin{equation}\label{eqn.f_!andf_*}
f_!p\cong\scalebox{1.3}{$\displaystyle\sum_{b\in B}\yon^{\big(\prod\limits_{a\mapsto b}p[a]\big)}$}
\qqand
f_*p\cong\scalebox{1.3}{$\displaystyle\sum_{b\in B}\yon^{\big(\sum\limits_{a\mapsto b}p[a]\big)}$}
\end{equation}
It may at first be counterintuitive that the left adjoint $f_!$ involves a product and the right adjoint $f_*$ involves a sum. The reason for this comes from \cref{prop.dlens_self_indexing}, namely that $\poly$ is equivalent to the Grothendieck construction applied to the functor $\smset\op\to\smcat$ sending each set $A$ to the category $(\smset/A)\op$. The fact that functions $f\colon A\to B$ induces an adjoint triple between $\smset/A$ and $\smset/B$, and hence between $(\smset/A)\op$ and $(\smset/B)\op$ explains the variance in \eqref{eqn.f_!andf_*} and simultaneously establishes the adjoint triple \eqref{eqn.adjoint_triple_monoidal_fib}.

The functor $p\mapsto p(\1)$ is strong monoidal with respect to $\otimes$ and strict monoidal if we choose the lens construction as our model of $\poly$. By \cref{prop.monoidal_pres_vert_cart}, the monoidal product $\otimes$ preserves cartesian lenses; thus we will have established the desired monoidal $*$-bifibration in the sense of \cite[Definition 12.1]{shulman2008framed} as soon as we know that $\otimes$ preserves op-cartesian lenses.

Given $f$ and $p$ as above, the op-cartesian lens is the lens $p\to f_!p$ obtained as the composite $p\to f^*f_!p\to f_!p$ where the first map is the unit of the $(f_!,f^*)$ adjunction and the second is the cartesian lens for $f_!p$. On positions $p\to f_!p$ acts as $f$, and on directions it is given by projection. 

If $f\colon p(\1)\to B$ and $f'\colon p'(\1)\to B'$ are functions then we have
\begin{align*}
	f_!(p)\otimes f'_!(p')&\cong
	\sum_{b\in B}\sum_{b'\in B'}\yon^{\big(\prod_{a\mapsto b}p[a]\big)\times\big(\prod_{a'\mapsto b'}p'[a']\big)}\\&\cong
	\sum_{(b,b')\in B\times B'}\yon^{\big(\prod_{(a,a')\mapsto(b,b')}p[a]\times p[b]\big)}\\&
	\cong (f_!\otimes f'_!)(p\otimes p')
\end{align*}
and the op-cartesian lenses are clearly preserved since projections in the second line match with projections in the first.
\end{proof}

%-------- Section --------%
\section{Summary and further reading}

...

See work by Gambino and Kock, Joyal, Paul Taylor, Michael Abbott and Neil Ghani (containers), ...

%-------- Section --------%
\section{Exercise solutions}
\Closesolutionfile{solutions}
{\footnotesize
\input{solution-file4}}

\end{document}
